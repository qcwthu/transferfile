04/06/2022 14:10:35 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/quail/', task_name='quail', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
04/06/2022 14:10:35 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail
04/25/2022 23:06:44 - INFO - __main__ - Namespace(task_dir='data/quail/', task_name='quail', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/25/2022 23:06:44 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail
04/25/2022 23:06:44 - INFO - __main__ - Namespace(task_dir='data/quail/', task_name='quail', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/25/2022 23:06:44 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail
04/25/2022 23:06:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
04/25/2022 23:06:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
04/25/2022 23:06:46 - INFO - __main__ - args.device: cuda:0
04/25/2022 23:06:46 - INFO - __main__ - args.device: cuda:1
04/25/2022 23:06:46 - INFO - __main__ - Using 2 gpus
04/25/2022 23:06:46 - INFO - __main__ - Using 2 gpus
04/25/2022 23:06:46 - INFO - __main__ - Fine-tuning the following samples: ['quail_32_100', 'quail_32_13', 'quail_32_21', 'quail_32_42', 'quail_32_87']
04/25/2022 23:06:46 - INFO - __main__ - Fine-tuning the following samples: ['quail_32_100', 'quail_32_13', 'quail_32_21', 'quail_32_42', 'quail_32_87']
04/25/2022 23:06:53 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.5, bsz=8 ...
04/25/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:06:54 - INFO - __main__ - Printing 3 examples
04/25/2022 23:06:54 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:06:54 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:06:54 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:06:54 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:06:54 - INFO - __main__ - Printing 3 examples
04/25/2022 23:06:54 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:06:54 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:06:54 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:06:54 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:06:54 - INFO - __main__ - Printing 3 examples
04/25/2022 23:06:54 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:06:54 - INFO - __main__ - ['5 minutes']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:06:54 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:06:54 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:06:54 - INFO - __main__ - Printing 3 examples
04/25/2022 23:06:54 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:06:54 - INFO - __main__ - ['5 minutes']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:06:54 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:06:54 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:06:54 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:07:12 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:07:12 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:07:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:07:13 - INFO - __main__ - Starting training!
04/25/2022 23:07:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:07:20 - INFO - __main__ - Starting training!
04/25/2022 23:07:26 - INFO - __main__ - Step 10 Global step 10 Train loss 1.96 on epoch=4
04/25/2022 23:07:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.49 on epoch=9
04/25/2022 23:07:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=14
04/25/2022 23:07:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=19
04/25/2022 23:07:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.77 on epoch=24
04/25/2022 23:07:47 - INFO - __main__ - Global step 50 Train loss 1.29 ACC 0.15625 on epoch=24
04/25/2022 23:07:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 23:07:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
04/25/2022 23:07:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
04/25/2022 23:08:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
04/25/2022 23:08:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
04/25/2022 23:08:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
04/25/2022 23:08:12 - INFO - __main__ - Global step 100 Train loss 0.54 ACC 0.1875 on epoch=49
04/25/2022 23:08:12 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=49, global_step=100
04/25/2022 23:08:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
04/25/2022 23:08:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=59
04/25/2022 23:08:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.35 on epoch=64
04/25/2022 23:08:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
04/25/2022 23:08:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
04/25/2022 23:08:38 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.3125 on epoch=74
04/25/2022 23:08:38 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.3125 on epoch=74, global_step=150
04/25/2022 23:08:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
04/25/2022 23:08:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
04/25/2022 23:08:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
04/25/2022 23:08:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
04/25/2022 23:09:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
04/25/2022 23:09:03 - INFO - __main__ - Global step 200 Train loss 0.32 ACC 0.21875 on epoch=99
04/25/2022 23:09:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
04/25/2022 23:09:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
04/25/2022 23:09:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
04/25/2022 23:09:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
04/25/2022 23:09:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
04/25/2022 23:09:29 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.21875 on epoch=124
04/25/2022 23:09:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
04/25/2022 23:09:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
04/25/2022 23:09:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
04/25/2022 23:09:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
04/25/2022 23:09:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
04/25/2022 23:09:54 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.1875 on epoch=149
04/25/2022 23:09:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
04/25/2022 23:10:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=159
04/25/2022 23:10:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
04/25/2022 23:10:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
04/25/2022 23:10:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
04/25/2022 23:10:19 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.15625 on epoch=174
04/25/2022 23:10:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
04/25/2022 23:10:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/25/2022 23:10:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/25/2022 23:10:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
04/25/2022 23:10:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
04/25/2022 23:10:44 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.15625 on epoch=199
04/25/2022 23:10:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
04/25/2022 23:10:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
04/25/2022 23:10:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 23:11:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
04/25/2022 23:11:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
04/25/2022 23:11:09 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.1875 on epoch=224
04/25/2022 23:11:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/25/2022 23:11:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 23:11:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/25/2022 23:11:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
04/25/2022 23:11:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 23:11:35 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.21875 on epoch=249
04/25/2022 23:11:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 23:11:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
04/25/2022 23:11:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
04/25/2022 23:11:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
04/25/2022 23:11:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
04/25/2022 23:12:00 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.15625 on epoch=274
04/25/2022 23:12:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
04/25/2022 23:12:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
04/25/2022 23:12:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/25/2022 23:12:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
04/25/2022 23:12:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
04/25/2022 23:12:25 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.21875 on epoch=299
04/25/2022 23:12:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/25/2022 23:12:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/25/2022 23:12:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
04/25/2022 23:12:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 23:12:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 23:12:50 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.15625 on epoch=324
04/25/2022 23:12:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/25/2022 23:12:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 23:13:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
04/25/2022 23:13:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/25/2022 23:13:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
04/25/2022 23:13:15 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.15625 on epoch=349
04/25/2022 23:13:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
04/25/2022 23:13:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
04/25/2022 23:13:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
04/25/2022 23:13:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/25/2022 23:13:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 23:13:40 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.25 on epoch=374
04/25/2022 23:13:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
04/25/2022 23:13:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 23:13:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/25/2022 23:13:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 23:14:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 23:14:06 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.28125 on epoch=399
04/25/2022 23:14:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/25/2022 23:14:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/25/2022 23:14:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 23:14:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 23:14:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/25/2022 23:14:31 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.1875 on epoch=424
04/25/2022 23:14:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 23:14:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 23:14:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 23:14:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/25/2022 23:14:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 23:14:56 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.25 on epoch=449
04/25/2022 23:15:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 23:15:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
04/25/2022 23:15:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 23:15:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/25/2022 23:15:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/25/2022 23:15:21 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.21875 on epoch=474
04/25/2022 23:15:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
04/25/2022 23:15:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 23:15:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 23:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 23:15:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
04/25/2022 23:15:46 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.1875 on epoch=499
04/25/2022 23:15:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/25/2022 23:15:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 23:16:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 23:16:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 23:16:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 23:16:12 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.21875 on epoch=524
04/25/2022 23:16:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
04/25/2022 23:16:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 23:16:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/25/2022 23:16:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 23:16:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/25/2022 23:16:37 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.1875 on epoch=549
04/25/2022 23:16:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 23:16:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 23:16:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/25/2022 23:16:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 23:16:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 23:17:02 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
04/25/2022 23:17:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 23:17:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 23:17:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 23:17:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 23:17:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 23:17:28 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.15625 on epoch=599
04/25/2022 23:17:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/25/2022 23:17:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 23:17:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/25/2022 23:17:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 23:17:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 23:17:53 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
04/25/2022 23:17:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 23:18:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 23:18:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/25/2022 23:18:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 23:18:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 23:18:18 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.1875 on epoch=649
04/25/2022 23:18:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 23:18:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 23:18:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 23:18:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 23:18:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 23:18:43 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.25 on epoch=674
04/25/2022 23:18:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 23:18:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 23:18:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 23:19:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 23:19:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 23:19:08 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.1875 on epoch=699
04/25/2022 23:19:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
04/25/2022 23:19:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 23:19:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 23:19:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 23:19:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/25/2022 23:19:33 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.1875 on epoch=724
04/25/2022 23:19:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/25/2022 23:19:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 23:19:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 23:19:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 23:19:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 23:19:58 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
04/25/2022 23:20:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 23:20:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 23:20:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 23:20:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 23:20:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
04/25/2022 23:20:23 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.1875 on epoch=774
04/25/2022 23:20:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 23:20:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
04/25/2022 23:20:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
04/25/2022 23:20:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
04/25/2022 23:20:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/25/2022 23:20:49 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.21875 on epoch=799
04/25/2022 23:20:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 23:20:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
04/25/2022 23:21:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 23:21:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 23:21:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 23:21:14 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/25/2022 23:21:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 23:21:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 23:21:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/25/2022 23:21:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 23:21:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 23:21:39 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.1875 on epoch=849
04/25/2022 23:21:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 23:21:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 23:21:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 23:21:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 23:22:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 23:22:04 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.21875 on epoch=874
04/25/2022 23:22:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 23:22:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 23:22:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 23:22:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 23:22:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 23:22:29 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
04/25/2022 23:22:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/25/2022 23:22:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 23:22:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 23:22:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 23:22:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 23:22:54 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.15625 on epoch=924
04/25/2022 23:22:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 23:23:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/25/2022 23:23:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 23:23:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
04/25/2022 23:23:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 23:23:20 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.3125 on epoch=949
04/25/2022 23:23:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 23:23:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/25/2022 23:23:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 23:23:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 23:23:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
04/25/2022 23:23:45 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.1875 on epoch=974
04/25/2022 23:23:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 23:23:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 23:23:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 23:24:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 23:24:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 23:24:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:24:09 - INFO - __main__ - Printing 3 examples
04/25/2022 23:24:09 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:24:09 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:24:09 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:24:09 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:24:09 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:24:09 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:24:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:24:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:24:09 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:24:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:24:09 - INFO - __main__ - Printing 3 examples
04/25/2022 23:24:09 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:24:09 - INFO - __main__ - ['5 minutes']
04/25/2022 23:24:09 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:24:09 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:24:09 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:24:09 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:24:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:24:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:24:09 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:24:11 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.28125 on epoch=999
04/25/2022 23:24:11 - INFO - __main__ - save last model!
04/25/2022 23:24:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 23:24:11 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 23:24:11 - INFO - __main__ - Printing 3 examples
04/25/2022 23:24:11 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:24:11 - INFO - __main__ - ['about 10 minutes']
04/25/2022 23:24:11 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:24:11 - INFO - __main__ - ['Larry likes sex.']
04/25/2022 23:24:11 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:24:11 - INFO - __main__ - ['That Candy had all her teeth']
04/25/2022 23:24:11 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:24:12 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:24:13 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 23:24:27 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:24:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:24:28 - INFO - __main__ - Starting training!
04/25/2022 23:25:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.5_8_predictions.txt
04/25/2022 23:25:47 - INFO - __main__ - ACC on test data: 0.2600
04/25/2022 23:25:48 - INFO - __main__ - prefix=quail_32_100, lr=0.5, bsz=8, dev_performance=0.3125, test_performance=0.26
04/25/2022 23:25:48 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.4, bsz=8 ...
04/25/2022 23:25:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:25:49 - INFO - __main__ - Printing 3 examples
04/25/2022 23:25:49 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:25:49 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:25:49 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:25:49 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:25:49 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:25:49 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:25:49 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:25:49 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:25:49 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:25:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:25:49 - INFO - __main__ - Printing 3 examples
04/25/2022 23:25:49 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:25:49 - INFO - __main__ - ['5 minutes']
04/25/2022 23:25:49 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:25:49 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:25:49 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:25:49 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:25:49 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:25:49 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:25:49 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:26:08 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:26:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:26:08 - INFO - __main__ - Starting training!
04/25/2022 23:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 2.08 on epoch=4
04/25/2022 23:26:18 - INFO - __main__ - Step 20 Global step 20 Train loss 1.62 on epoch=9
04/25/2022 23:26:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.11 on epoch=14
04/25/2022 23:26:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.80 on epoch=19
04/25/2022 23:26:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.30 on epoch=24
04/25/2022 23:26:34 - INFO - __main__ - Global step 50 Train loss 1.78 ACC 0.15625 on epoch=24
04/25/2022 23:26:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 23:26:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.24 on epoch=29
04/25/2022 23:26:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.20 on epoch=34
04/25/2022 23:26:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=39
04/25/2022 23:26:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=44
04/25/2022 23:26:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=49
04/25/2022 23:26:59 - INFO - __main__ - Global step 100 Train loss 1.08 ACC 0.21875 on epoch=49
04/25/2022 23:26:59 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=49, global_step=100
04/25/2022 23:27:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=54
04/25/2022 23:27:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=59
04/25/2022 23:27:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=64
04/25/2022 23:27:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.74 on epoch=69
04/25/2022 23:27:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=74
04/25/2022 23:27:25 - INFO - __main__ - Global step 150 Train loss 0.79 ACC 0.125 on epoch=74
04/25/2022 23:27:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=79
04/25/2022 23:27:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=84
04/25/2022 23:27:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=89
04/25/2022 23:27:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=94
04/25/2022 23:27:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=99
04/25/2022 23:27:50 - INFO - __main__ - Global step 200 Train loss 0.65 ACC 0.1875 on epoch=99
04/25/2022 23:27:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=104
04/25/2022 23:27:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=109
04/25/2022 23:28:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=114
04/25/2022 23:28:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=119
04/25/2022 23:28:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
04/25/2022 23:28:15 - INFO - __main__ - Global step 250 Train loss 0.50 ACC 0.15625 on epoch=124
04/25/2022 23:28:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=129
04/25/2022 23:28:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=134
04/25/2022 23:28:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
04/25/2022 23:28:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
04/25/2022 23:28:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
04/25/2022 23:28:41 - INFO - __main__ - Global step 300 Train loss 0.43 ACC 0.21875 on epoch=149
04/25/2022 23:28:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
04/25/2022 23:28:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
04/25/2022 23:28:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
04/25/2022 23:28:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=169
04/25/2022 23:29:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
04/25/2022 23:29:06 - INFO - __main__ - Global step 350 Train loss 0.36 ACC 0.25 on epoch=174
04/25/2022 23:29:06 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=174, global_step=350
04/25/2022 23:29:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
04/25/2022 23:29:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
04/25/2022 23:29:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
04/25/2022 23:29:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
04/25/2022 23:29:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
04/25/2022 23:29:32 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.21875 on epoch=199
04/25/2022 23:29:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
04/25/2022 23:29:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
04/25/2022 23:29:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
04/25/2022 23:29:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
04/25/2022 23:29:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=224
04/25/2022 23:29:57 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.1875 on epoch=224
04/25/2022 23:30:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
04/25/2022 23:30:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
04/25/2022 23:30:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
04/25/2022 23:30:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
04/25/2022 23:30:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
04/25/2022 23:30:23 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.21875 on epoch=249
04/25/2022 23:30:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
04/25/2022 23:30:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=259
04/25/2022 23:30:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=264
04/25/2022 23:30:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
04/25/2022 23:30:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=274
04/25/2022 23:30:48 - INFO - __main__ - Global step 550 Train loss 0.19 ACC 0.15625 on epoch=274
04/25/2022 23:30:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
04/25/2022 23:30:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
04/25/2022 23:31:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
04/25/2022 23:31:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
04/25/2022 23:31:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
04/25/2022 23:31:14 - INFO - __main__ - Global step 600 Train loss 0.15 ACC 0.21875 on epoch=299
04/25/2022 23:31:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
04/25/2022 23:31:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
04/25/2022 23:31:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=314
04/25/2022 23:31:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
04/25/2022 23:31:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
04/25/2022 23:31:39 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.15625 on epoch=324
04/25/2022 23:31:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
04/25/2022 23:31:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
04/25/2022 23:31:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
04/25/2022 23:31:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
04/25/2022 23:32:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
04/25/2022 23:32:05 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.1875 on epoch=349
04/25/2022 23:32:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
04/25/2022 23:32:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=359
04/25/2022 23:32:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
04/25/2022 23:32:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
04/25/2022 23:32:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
04/25/2022 23:32:30 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.21875 on epoch=374
04/25/2022 23:32:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
04/25/2022 23:32:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
04/25/2022 23:32:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
04/25/2022 23:32:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=394
04/25/2022 23:32:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
04/25/2022 23:32:55 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.1875 on epoch=399
04/25/2022 23:33:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
04/25/2022 23:33:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
04/25/2022 23:33:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
04/25/2022 23:33:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
04/25/2022 23:33:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
04/25/2022 23:33:21 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.21875 on epoch=424
04/25/2022 23:33:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
04/25/2022 23:33:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
04/25/2022 23:33:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=439
04/25/2022 23:33:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
04/25/2022 23:33:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
04/25/2022 23:33:46 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.1875 on epoch=449
04/25/2022 23:33:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
04/25/2022 23:33:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
04/25/2022 23:33:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 23:34:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
04/25/2022 23:34:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 23:34:11 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.21875 on epoch=474
04/25/2022 23:34:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
04/25/2022 23:34:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/25/2022 23:34:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
04/25/2022 23:34:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
04/25/2022 23:34:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=499
04/25/2022 23:34:36 - INFO - __main__ - Global step 1000 Train loss 0.21 ACC 0.25 on epoch=499
04/25/2022 23:34:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
04/25/2022 23:34:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
04/25/2022 23:34:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
04/25/2022 23:34:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
04/25/2022 23:34:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 23:35:02 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.1875 on epoch=524
04/25/2022 23:35:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/25/2022 23:35:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/25/2022 23:35:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
04/25/2022 23:35:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 23:35:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
04/25/2022 23:35:28 - INFO - __main__ - Global step 1100 Train loss 0.06 ACC 0.25 on epoch=549
04/25/2022 23:35:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 23:35:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
04/25/2022 23:35:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/25/2022 23:35:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
04/25/2022 23:35:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 23:35:53 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.25 on epoch=574
04/25/2022 23:35:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
04/25/2022 23:36:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
04/25/2022 23:36:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
04/25/2022 23:36:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 23:36:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
04/25/2022 23:36:19 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.1875 on epoch=599
04/25/2022 23:36:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
04/25/2022 23:36:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
04/25/2022 23:36:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
04/25/2022 23:36:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
04/25/2022 23:36:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
04/25/2022 23:36:44 - INFO - __main__ - Global step 1250 Train loss 0.06 ACC 0.15625 on epoch=624
04/25/2022 23:36:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 23:36:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 23:36:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 23:37:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
04/25/2022 23:37:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/25/2022 23:37:10 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.21875 on epoch=649
04/25/2022 23:37:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/25/2022 23:37:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
04/25/2022 23:37:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
04/25/2022 23:37:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
04/25/2022 23:37:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
04/25/2022 23:37:35 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.1875 on epoch=674
04/25/2022 23:37:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
04/25/2022 23:37:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
04/25/2022 23:37:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
04/25/2022 23:37:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
04/25/2022 23:37:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
04/25/2022 23:38:01 - INFO - __main__ - Global step 1400 Train loss 0.05 ACC 0.28125 on epoch=699
04/25/2022 23:38:01 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=699, global_step=1400
04/25/2022 23:38:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
04/25/2022 23:38:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/25/2022 23:38:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/25/2022 23:38:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
04/25/2022 23:38:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/25/2022 23:38:26 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.25 on epoch=724
04/25/2022 23:38:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 23:38:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
04/25/2022 23:38:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 23:38:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
04/25/2022 23:38:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 23:38:52 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.21875 on epoch=749
04/25/2022 23:38:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 23:39:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 23:39:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
04/25/2022 23:39:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 23:39:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=774
04/25/2022 23:39:18 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.21875 on epoch=774
04/25/2022 23:39:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 23:39:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 23:39:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
04/25/2022 23:39:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 23:39:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/25/2022 23:39:43 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.28125 on epoch=799
04/25/2022 23:39:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
04/25/2022 23:39:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 23:39:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
04/25/2022 23:40:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/25/2022 23:40:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
04/25/2022 23:40:09 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.1875 on epoch=824
04/25/2022 23:40:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 23:40:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 23:40:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/25/2022 23:40:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
04/25/2022 23:40:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
04/25/2022 23:40:34 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.21875 on epoch=849
04/25/2022 23:40:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 23:40:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 23:40:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
04/25/2022 23:40:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/25/2022 23:40:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 23:40:59 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.25 on epoch=874
04/25/2022 23:41:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
04/25/2022 23:41:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 23:41:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
04/25/2022 23:41:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/25/2022 23:41:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
04/25/2022 23:41:25 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.1875 on epoch=899
04/25/2022 23:41:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/25/2022 23:41:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
04/25/2022 23:41:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/25/2022 23:41:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
04/25/2022 23:41:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
04/25/2022 23:41:50 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.21875 on epoch=924
04/25/2022 23:41:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/25/2022 23:41:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 23:42:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
04/25/2022 23:42:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/25/2022 23:42:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
04/25/2022 23:42:15 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.25 on epoch=949
04/25/2022 23:42:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 23:42:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 23:42:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
04/25/2022 23:42:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 23:42:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 23:42:41 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.1875 on epoch=974
04/25/2022 23:42:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 23:42:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 23:42:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 23:42:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 23:43:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 23:43:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:43:05 - INFO - __main__ - Printing 3 examples
04/25/2022 23:43:05 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:43:05 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:43:05 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:43:05 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:43:05 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:43:05 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:43:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:43:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:43:05 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:43:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:43:05 - INFO - __main__ - Printing 3 examples
04/25/2022 23:43:05 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:43:05 - INFO - __main__ - ['5 minutes']
04/25/2022 23:43:05 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:43:05 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:43:05 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:43:05 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:43:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:43:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:43:05 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:43:07 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.21875 on epoch=999
04/25/2022 23:43:07 - INFO - __main__ - save last model!
04/25/2022 23:43:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 23:43:07 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 23:43:07 - INFO - __main__ - Printing 3 examples
04/25/2022 23:43:07 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:43:07 - INFO - __main__ - ['about 10 minutes']
04/25/2022 23:43:07 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:43:07 - INFO - __main__ - ['Larry likes sex.']
04/25/2022 23:43:07 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/25/2022 23:43:07 - INFO - __main__ - ['That Candy had all her teeth']
04/25/2022 23:43:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:43:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:43:10 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 23:43:24 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:43:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:43:24 - INFO - __main__ - Starting training!
04/25/2022 23:44:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.4_8_predictions.txt
04/25/2022 23:44:41 - INFO - __main__ - ACC on test data: 0.2720
04/25/2022 23:44:41 - INFO - __main__ - prefix=quail_32_100, lr=0.4, bsz=8, dev_performance=0.28125, test_performance=0.272
04/25/2022 23:44:41 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.3, bsz=8 ...
04/25/2022 23:44:42 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:44:42 - INFO - __main__ - Printing 3 examples
04/25/2022 23:44:42 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/25/2022 23:44:42 - INFO - __main__ - ['Looking stupid']
04/25/2022 23:44:42 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/25/2022 23:44:42 - INFO - __main__ - ['Under 10 minutes.']
04/25/2022 23:44:42 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/25/2022 23:44:42 - INFO - __main__ - ['For a couple weeks']
04/25/2022 23:44:42 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:44:42 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:44:42 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 23:44:42 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 23:44:42 - INFO - __main__ - Printing 3 examples
04/25/2022 23:44:42 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/25/2022 23:44:42 - INFO - __main__ - ['5 minutes']
04/25/2022 23:44:42 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/25/2022 23:44:42 - INFO - __main__ - ['after going to Mayo Clinic']
04/25/2022 23:44:42 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/25/2022 23:44:42 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/25/2022 23:44:42 - INFO - __main__ - Tokenizing Input ...
04/25/2022 23:44:42 - INFO - __main__ - Tokenizing Output ...
04/25/2022 23:44:42 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 23:45:01 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 23:45:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 23:45:02 - INFO - __main__ - Starting training!
04/25/2022 23:45:07 - INFO - __main__ - Step 10 Global step 10 Train loss 1.96 on epoch=4
04/25/2022 23:45:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.53 on epoch=9
04/25/2022 23:45:16 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=14
04/25/2022 23:45:20 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=19
04/25/2022 23:45:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=24
04/25/2022 23:45:27 - INFO - __main__ - Global step 50 Train loss 1.36 ACC 0.09375 on epoch=24
04/25/2022 23:45:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.09375 on epoch=24, global_step=50
04/25/2022 23:45:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=29
04/25/2022 23:45:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
04/25/2022 23:45:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
04/25/2022 23:45:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=44
04/25/2022 23:45:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=49
04/25/2022 23:45:52 - INFO - __main__ - Global step 100 Train loss 0.65 ACC 0.1875 on epoch=49
04/25/2022 23:45:52 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.1875 on epoch=49, global_step=100
04/25/2022 23:45:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
04/25/2022 23:46:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
04/25/2022 23:46:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
04/25/2022 23:46:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
04/25/2022 23:46:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
04/25/2022 23:46:17 - INFO - __main__ - Global step 150 Train loss 0.45 ACC 0.1875 on epoch=74
04/25/2022 23:46:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
04/25/2022 23:46:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
04/25/2022 23:46:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
04/25/2022 23:46:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
04/25/2022 23:46:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
04/25/2022 23:46:42 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.125 on epoch=99
04/25/2022 23:46:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
04/25/2022 23:46:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
04/25/2022 23:46:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
04/25/2022 23:47:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
04/25/2022 23:47:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
04/25/2022 23:47:07 - INFO - __main__ - Global step 250 Train loss 0.33 ACC 0.125 on epoch=124
04/25/2022 23:47:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
04/25/2022 23:47:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
04/25/2022 23:47:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
04/25/2022 23:47:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=144
04/25/2022 23:47:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
04/25/2022 23:47:32 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.1875 on epoch=149
04/25/2022 23:47:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
04/25/2022 23:47:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
04/25/2022 23:47:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
04/25/2022 23:47:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
04/25/2022 23:47:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=174
04/25/2022 23:47:58 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.125 on epoch=174
04/25/2022 23:48:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=179
04/25/2022 23:48:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
04/25/2022 23:48:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
04/25/2022 23:48:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
04/25/2022 23:48:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
04/25/2022 23:48:23 - INFO - __main__ - Global step 400 Train loss 0.18 ACC 0.1875 on epoch=199
04/25/2022 23:48:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=204
04/25/2022 23:48:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
04/25/2022 23:48:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
04/25/2022 23:48:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
04/25/2022 23:48:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
04/25/2022 23:48:49 - INFO - __main__ - Global step 450 Train loss 0.15 ACC 0.1875 on epoch=224
04/25/2022 23:48:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/25/2022 23:48:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
04/25/2022 23:49:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
04/25/2022 23:49:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
04/25/2022 23:49:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
04/25/2022 23:49:14 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.15625 on epoch=249
04/25/2022 23:49:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
04/25/2022 23:49:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
04/25/2022 23:49:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
04/25/2022 23:49:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
04/25/2022 23:49:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
04/25/2022 23:49:39 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.125 on epoch=274
04/25/2022 23:49:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
04/25/2022 23:49:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
04/25/2022 23:49:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/25/2022 23:49:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
04/25/2022 23:50:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
04/25/2022 23:50:04 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.1875 on epoch=299
04/25/2022 23:50:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
04/25/2022 23:50:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
04/25/2022 23:50:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
04/25/2022 23:50:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/25/2022 23:50:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
04/25/2022 23:50:30 - INFO - __main__ - Global step 650 Train loss 0.08 ACC 0.15625 on epoch=324
04/25/2022 23:50:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
04/25/2022 23:50:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
04/25/2022 23:50:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/25/2022 23:50:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 23:50:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
04/25/2022 23:50:55 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.15625 on epoch=349
04/25/2022 23:51:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
04/25/2022 23:51:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/25/2022 23:51:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
04/25/2022 23:51:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/25/2022 23:51:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
04/25/2022 23:51:20 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.1875 on epoch=374
04/25/2022 23:51:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
04/25/2022 23:51:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 23:51:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
04/25/2022 23:51:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/25/2022 23:51:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
04/25/2022 23:51:46 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.21875 on epoch=399
04/25/2022 23:51:46 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=399, global_step=800
04/25/2022 23:51:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/25/2022 23:51:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
04/25/2022 23:51:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/25/2022 23:52:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
04/25/2022 23:52:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
04/25/2022 23:52:11 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.28125 on epoch=424
04/25/2022 23:52:11 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=424, global_step=850
04/25/2022 23:52:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/25/2022 23:52:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
04/25/2022 23:52:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 23:52:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
04/25/2022 23:52:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 23:52:37 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.1875 on epoch=449
04/25/2022 23:52:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 23:52:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
04/25/2022 23:52:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
04/25/2022 23:52:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
04/25/2022 23:52:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/25/2022 23:53:02 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.25 on epoch=474
04/25/2022 23:53:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/25/2022 23:53:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 23:53:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 23:53:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/25/2022 23:53:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 23:53:27 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.25 on epoch=499
04/25/2022 23:53:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
04/25/2022 23:53:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 23:53:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
04/25/2022 23:53:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 23:53:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/25/2022 23:53:52 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.21875 on epoch=524
04/25/2022 23:53:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/25/2022 23:54:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/25/2022 23:54:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
04/25/2022 23:54:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/25/2022 23:54:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 23:54:18 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.25 on epoch=549
04/25/2022 23:54:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 23:54:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 23:54:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/25/2022 23:54:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/25/2022 23:54:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 23:54:43 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.1875 on epoch=574
04/25/2022 23:54:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 23:54:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 23:54:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
04/25/2022 23:55:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 23:55:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/25/2022 23:55:09 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.25 on epoch=599
04/25/2022 23:55:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 23:55:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/25/2022 23:55:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 23:55:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 23:55:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 23:55:34 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
04/25/2022 23:55:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
04/25/2022 23:55:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 23:55:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 23:55:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
04/25/2022 23:55:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 23:55:59 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.15625 on epoch=649
04/25/2022 23:56:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/25/2022 23:56:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 23:56:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 23:56:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
04/25/2022 23:56:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
04/25/2022 23:56:24 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.21875 on epoch=674
04/25/2022 23:56:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 23:56:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 23:56:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
04/25/2022 23:56:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
04/25/2022 23:56:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 23:56:50 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.25 on epoch=699
04/25/2022 23:56:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 23:56:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 23:57:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 23:57:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 23:57:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/25/2022 23:57:15 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
04/25/2022 23:57:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/25/2022 23:57:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
04/25/2022 23:57:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 23:57:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 23:57:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 23:57:40 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.21875 on epoch=749
04/25/2022 23:57:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 23:57:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 23:57:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 23:57:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
04/25/2022 23:58:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
04/25/2022 23:58:06 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.25 on epoch=774
04/25/2022 23:58:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 23:58:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 23:58:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 23:58:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 23:58:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 23:58:31 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
04/25/2022 23:58:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 23:58:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 23:58:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 23:58:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 23:58:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
04/25/2022 23:58:56 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.21875 on epoch=824
04/25/2022 23:59:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
04/25/2022 23:59:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 23:59:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 23:59:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 23:59:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 23:59:22 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.25 on epoch=849
04/25/2022 23:59:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 23:59:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 23:59:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 23:59:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/25/2022 23:59:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
04/25/2022 23:59:47 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.15625 on epoch=874
04/25/2022 23:59:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
04/25/2022 23:59:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/26/2022 00:00:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/26/2022 00:00:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
04/26/2022 00:00:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/26/2022 00:00:12 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.21875 on epoch=899
04/26/2022 00:00:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/26/2022 00:00:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/26/2022 00:00:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/26/2022 00:00:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
04/26/2022 00:00:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/26/2022 00:00:38 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
04/26/2022 00:00:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/26/2022 00:00:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/26/2022 00:00:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
04/26/2022 00:00:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
04/26/2022 00:01:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/26/2022 00:01:03 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.1875 on epoch=949
04/26/2022 00:01:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/26/2022 00:01:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/26/2022 00:01:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
04/26/2022 00:01:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
04/26/2022 00:01:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/26/2022 00:01:28 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.21875 on epoch=974
04/26/2022 00:01:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/26/2022 00:01:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/26/2022 00:01:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/26/2022 00:01:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/26/2022 00:01:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
04/26/2022 00:01:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:01:52 - INFO - __main__ - Printing 3 examples
04/26/2022 00:01:52 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/26/2022 00:01:52 - INFO - __main__ - ['Looking stupid']
04/26/2022 00:01:52 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/26/2022 00:01:52 - INFO - __main__ - ['Under 10 minutes.']
04/26/2022 00:01:52 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/26/2022 00:01:52 - INFO - __main__ - ['For a couple weeks']
04/26/2022 00:01:52 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:01:52 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:01:52 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:01:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:01:52 - INFO - __main__ - Printing 3 examples
04/26/2022 00:01:52 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/26/2022 00:01:52 - INFO - __main__ - ['5 minutes']
04/26/2022 00:01:52 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/26/2022 00:01:52 - INFO - __main__ - ['after going to Mayo Clinic']
04/26/2022 00:01:52 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/26/2022 00:01:52 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/26/2022 00:01:52 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:01:52 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:01:52 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:01:53 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.21875 on epoch=999
04/26/2022 00:01:53 - INFO - __main__ - save last model!
04/26/2022 00:01:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/26/2022 00:01:53 - INFO - __main__ - Start tokenizing ... 1000 instances
04/26/2022 00:01:53 - INFO - __main__ - Printing 3 examples
04/26/2022 00:01:53 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:01:53 - INFO - __main__ - ['about 10 minutes']
04/26/2022 00:01:53 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:01:53 - INFO - __main__ - ['Larry likes sex.']
04/26/2022 00:01:53 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:01:53 - INFO - __main__ - ['That Candy had all her teeth']
04/26/2022 00:01:53 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:01:55 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:01:56 - INFO - __main__ - Loaded 1000 examples from test data
04/26/2022 00:02:10 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:02:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:02:11 - INFO - __main__ - Starting training!
04/26/2022 00:03:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.3_8_predictions.txt
04/26/2022 00:03:32 - INFO - __main__ - ACC on test data: 0.2810
04/26/2022 00:03:32 - INFO - __main__ - prefix=quail_32_100, lr=0.3, bsz=8, dev_performance=0.28125, test_performance=0.281
04/26/2022 00:03:32 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.2, bsz=8 ...
04/26/2022 00:03:33 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:03:33 - INFO - __main__ - Printing 3 examples
04/26/2022 00:03:33 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
04/26/2022 00:03:33 - INFO - __main__ - ['Looking stupid']
04/26/2022 00:03:33 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
04/26/2022 00:03:33 - INFO - __main__ - ['Under 10 minutes.']
04/26/2022 00:03:33 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
04/26/2022 00:03:33 - INFO - __main__ - ['For a couple weeks']
04/26/2022 00:03:33 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:03:33 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:03:33 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:03:33 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:03:33 - INFO - __main__ - Printing 3 examples
04/26/2022 00:03:33 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
04/26/2022 00:03:33 - INFO - __main__ - ['5 minutes']
04/26/2022 00:03:33 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
04/26/2022 00:03:33 - INFO - __main__ - ['after going to Mayo Clinic']
04/26/2022 00:03:33 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
04/26/2022 00:03:33 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
04/26/2022 00:03:33 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:03:33 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:03:33 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:03:52 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:03:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:03:53 - INFO - __main__ - Starting training!
04/26/2022 00:03:58 - INFO - __main__ - Step 10 Global step 10 Train loss 2.14 on epoch=4
04/26/2022 00:04:03 - INFO - __main__ - Step 20 Global step 20 Train loss 1.78 on epoch=9
04/26/2022 00:04:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=14
04/26/2022 00:04:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.33 on epoch=19
04/26/2022 00:04:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=24
04/26/2022 00:04:20 - INFO - __main__ - Global step 50 Train loss 1.58 ACC 0.15625 on epoch=24
04/26/2022 00:04:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/26/2022 00:04:25 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=29
04/26/2022 00:04:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=34
04/26/2022 00:04:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=39
04/26/2022 00:04:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=44
04/26/2022 00:04:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=49
04/26/2022 00:04:46 - INFO - __main__ - Global step 100 Train loss 0.84 ACC 0.1875 on epoch=49
04/26/2022 00:04:46 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=49, global_step=100
04/26/2022 00:04:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=54
04/26/2022 00:04:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
04/26/2022 00:04:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=64
04/26/2022 00:05:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=69
04/26/2022 00:05:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=74
04/26/2022 00:05:11 - INFO - __main__ - Global step 150 Train loss 0.59 ACC 0.1875 on epoch=74
04/26/2022 00:05:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
04/26/2022 00:05:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=84
04/26/2022 00:05:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=89
04/26/2022 00:05:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
04/26/2022 00:05:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
04/26/2022 00:05:37 - INFO - __main__ - Global step 200 Train loss 0.44 ACC 0.15625 on epoch=99
04/26/2022 00:05:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
04/26/2022 00:05:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
04/26/2022 00:05:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
04/26/2022 00:05:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
04/26/2022 00:05:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
04/26/2022 00:06:02 - INFO - __main__ - Global step 250 Train loss 0.40 ACC 0.15625 on epoch=124
04/26/2022 00:06:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
04/26/2022 00:06:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
04/26/2022 00:06:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
04/26/2022 00:06:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
04/26/2022 00:06:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
04/26/2022 00:06:27 - INFO - __main__ - Global step 300 Train loss 0.35 ACC 0.1875 on epoch=149
04/26/2022 00:06:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=154
04/26/2022 00:06:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=159
04/26/2022 00:06:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=164
04/26/2022 00:06:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
04/26/2022 00:06:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
04/26/2022 00:06:52 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.1875 on epoch=174
04/26/2022 00:06:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
04/26/2022 00:07:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
04/26/2022 00:07:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
04/26/2022 00:07:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
04/26/2022 00:07:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
04/26/2022 00:07:18 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.21875 on epoch=199
04/26/2022 00:07:18 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=199, global_step=400
04/26/2022 00:07:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
04/26/2022 00:07:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
04/26/2022 00:07:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=214
04/26/2022 00:07:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
04/26/2022 00:07:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
04/26/2022 00:07:43 - INFO - __main__ - Global step 450 Train loss 0.22 ACC 0.15625 on epoch=224
04/26/2022 00:07:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
04/26/2022 00:07:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
04/26/2022 00:07:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
04/26/2022 00:08:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
04/26/2022 00:08:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
04/26/2022 00:08:08 - INFO - __main__ - Global step 500 Train loss 0.20 ACC 0.125 on epoch=249
04/26/2022 00:08:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
04/26/2022 00:08:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
04/26/2022 00:08:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
04/26/2022 00:08:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
04/26/2022 00:08:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
04/26/2022 00:08:33 - INFO - __main__ - Global step 550 Train loss 0.17 ACC 0.21875 on epoch=274
04/26/2022 00:08:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
04/26/2022 00:08:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
04/26/2022 00:08:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
04/26/2022 00:08:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
04/26/2022 00:08:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=299
04/26/2022 00:08:59 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.25 on epoch=299
04/26/2022 00:08:59 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=299, global_step=600
04/26/2022 00:09:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
04/26/2022 00:09:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
04/26/2022 00:09:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
04/26/2022 00:09:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
04/26/2022 00:09:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
04/26/2022 00:09:24 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.15625 on epoch=324
04/26/2022 00:09:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
04/26/2022 00:09:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
04/26/2022 00:09:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
04/26/2022 00:09:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
04/26/2022 00:09:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
04/26/2022 00:09:50 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.0625 on epoch=349
04/26/2022 00:09:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
04/26/2022 00:09:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
04/26/2022 00:10:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
04/26/2022 00:10:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
04/26/2022 00:10:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=374
04/26/2022 00:10:15 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.21875 on epoch=374
04/26/2022 00:10:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
04/26/2022 00:10:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
04/26/2022 00:10:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
04/26/2022 00:10:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
04/26/2022 00:10:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
04/26/2022 00:10:40 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.1875 on epoch=399
04/26/2022 00:10:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
04/26/2022 00:10:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/26/2022 00:10:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
04/26/2022 00:10:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
04/26/2022 00:11:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
04/26/2022 00:11:06 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.25 on epoch=424
04/26/2022 00:11:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
04/26/2022 00:11:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
04/26/2022 00:11:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
04/26/2022 00:11:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/26/2022 00:11:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
04/26/2022 00:11:31 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.15625 on epoch=449
04/26/2022 00:11:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
04/26/2022 00:11:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
04/26/2022 00:11:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
04/26/2022 00:11:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
04/26/2022 00:11:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
04/26/2022 00:11:57 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.1875 on epoch=474
04/26/2022 00:12:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
04/26/2022 00:12:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/26/2022 00:12:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
04/26/2022 00:12:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/26/2022 00:12:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
04/26/2022 00:12:23 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.21875 on epoch=499
04/26/2022 00:12:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
04/26/2022 00:12:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
04/26/2022 00:12:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/26/2022 00:12:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
04/26/2022 00:12:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
04/26/2022 00:12:48 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.15625 on epoch=524
04/26/2022 00:12:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
04/26/2022 00:12:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/26/2022 00:13:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
04/26/2022 00:13:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/26/2022 00:13:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
04/26/2022 00:13:14 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.21875 on epoch=549
04/26/2022 00:13:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/26/2022 00:13:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
04/26/2022 00:13:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
04/26/2022 00:13:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
04/26/2022 00:13:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
04/26/2022 00:13:40 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.21875 on epoch=574
04/26/2022 00:13:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
04/26/2022 00:13:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/26/2022 00:13:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/26/2022 00:13:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/26/2022 00:14:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
04/26/2022 00:14:05 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.1875 on epoch=599
04/26/2022 00:14:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/26/2022 00:14:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=609
04/26/2022 00:14:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/26/2022 00:14:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
04/26/2022 00:14:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
04/26/2022 00:14:31 - INFO - __main__ - Global step 1250 Train loss 0.06 ACC 0.21875 on epoch=624
04/26/2022 00:14:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
04/26/2022 00:14:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
04/26/2022 00:14:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/26/2022 00:14:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
04/26/2022 00:14:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/26/2022 00:14:56 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.21875 on epoch=649
04/26/2022 00:15:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
04/26/2022 00:15:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/26/2022 00:15:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/26/2022 00:15:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
04/26/2022 00:15:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
04/26/2022 00:15:22 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.1875 on epoch=674
04/26/2022 00:15:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/26/2022 00:15:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=684
04/26/2022 00:15:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
04/26/2022 00:15:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
04/26/2022 00:15:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/26/2022 00:15:47 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.21875 on epoch=699
04/26/2022 00:15:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
04/26/2022 00:15:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/26/2022 00:16:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/26/2022 00:16:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
04/26/2022 00:16:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/26/2022 00:16:13 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.21875 on epoch=724
04/26/2022 00:16:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/26/2022 00:16:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
04/26/2022 00:16:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
04/26/2022 00:16:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
04/26/2022 00:16:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
04/26/2022 00:16:38 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.1875 on epoch=749
04/26/2022 00:16:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/26/2022 00:16:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/26/2022 00:16:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
04/26/2022 00:16:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/26/2022 00:17:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
04/26/2022 00:17:04 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.25 on epoch=774
04/26/2022 00:17:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/26/2022 00:17:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/26/2022 00:17:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/26/2022 00:17:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/26/2022 00:17:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
04/26/2022 00:17:29 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.25 on epoch=799
04/26/2022 00:17:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
04/26/2022 00:17:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
04/26/2022 00:17:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
04/26/2022 00:17:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/26/2022 00:17:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
04/26/2022 00:17:55 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.1875 on epoch=824
04/26/2022 00:18:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/26/2022 00:18:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/26/2022 00:18:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
04/26/2022 00:18:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/26/2022 00:18:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
04/26/2022 00:18:21 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.21875 on epoch=849
04/26/2022 00:18:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/26/2022 00:18:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/26/2022 00:18:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/26/2022 00:18:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/26/2022 00:18:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
04/26/2022 00:18:46 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.21875 on epoch=874
04/26/2022 00:18:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/26/2022 00:18:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/26/2022 00:19:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/26/2022 00:19:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/26/2022 00:19:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/26/2022 00:19:12 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.28125 on epoch=899
04/26/2022 00:19:12 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=899, global_step=1800
04/26/2022 00:19:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/26/2022 00:19:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/26/2022 00:19:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/26/2022 00:19:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
04/26/2022 00:19:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/26/2022 00:19:38 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.25 on epoch=924
04/26/2022 00:19:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/26/2022 00:19:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/26/2022 00:19:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
04/26/2022 00:19:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/26/2022 00:20:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
04/26/2022 00:20:03 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.25 on epoch=949
04/26/2022 00:20:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/26/2022 00:20:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/26/2022 00:20:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
04/26/2022 00:20:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/26/2022 00:20:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/26/2022 00:20:29 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.25 on epoch=974
04/26/2022 00:20:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
04/26/2022 00:20:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/26/2022 00:20:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/26/2022 00:20:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/26/2022 00:20:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
04/26/2022 00:20:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:20:53 - INFO - __main__ - Printing 3 examples
04/26/2022 00:20:53 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:20:53 - INFO - __main__ - ['it was not installed']
04/26/2022 00:20:53 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:20:53 - INFO - __main__ - ['a few more months']
04/26/2022 00:20:53 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:20:53 - INFO - __main__ - ['still in the same position']
04/26/2022 00:20:53 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:20:53 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:20:53 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:20:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:20:53 - INFO - __main__ - Printing 3 examples
04/26/2022 00:20:53 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:20:53 - INFO - __main__ - ['not enough information']
04/26/2022 00:20:53 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:20:53 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:20:53 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:20:53 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:20:53 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:20:53 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:20:53 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:20:55 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.28125 on epoch=999
04/26/2022 00:20:55 - INFO - __main__ - save last model!
04/26/2022 00:20:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/26/2022 00:20:55 - INFO - __main__ - Start tokenizing ... 1000 instances
04/26/2022 00:20:55 - INFO - __main__ - Printing 3 examples
04/26/2022 00:20:55 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:20:55 - INFO - __main__ - ['about 10 minutes']
04/26/2022 00:20:55 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:20:55 - INFO - __main__ - ['Larry likes sex.']
04/26/2022 00:20:55 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:20:55 - INFO - __main__ - ['That Candy had all her teeth']
04/26/2022 00:20:55 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:20:56 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:20:57 - INFO - __main__ - Loaded 1000 examples from test data
04/26/2022 00:21:12 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:21:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:21:12 - INFO - __main__ - Starting training!
04/26/2022 00:22:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.2_8_predictions.txt
04/26/2022 00:22:39 - INFO - __main__ - ACC on test data: 0.2340
04/26/2022 00:22:40 - INFO - __main__ - prefix=quail_32_100, lr=0.2, bsz=8, dev_performance=0.28125, test_performance=0.234
04/26/2022 00:22:40 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.5, bsz=8 ...
04/26/2022 00:22:41 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:22:41 - INFO - __main__ - Printing 3 examples
04/26/2022 00:22:41 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:22:41 - INFO - __main__ - ['it was not installed']
04/26/2022 00:22:41 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:22:41 - INFO - __main__ - ['a few more months']
04/26/2022 00:22:41 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:22:41 - INFO - __main__ - ['still in the same position']
04/26/2022 00:22:41 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:22:41 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:22:41 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:22:41 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:22:41 - INFO - __main__ - Printing 3 examples
04/26/2022 00:22:41 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:22:41 - INFO - __main__ - ['not enough information']
04/26/2022 00:22:41 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:22:41 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:22:41 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:22:41 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:22:41 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:22:41 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:22:41 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:22:56 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:22:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:22:57 - INFO - __main__ - Starting training!
04/26/2022 00:23:02 - INFO - __main__ - Step 10 Global step 10 Train loss 1.95 on epoch=4
04/26/2022 00:23:06 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
04/26/2022 00:23:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
04/26/2022 00:23:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.53 on epoch=19
04/26/2022 00:23:19 - INFO - __main__ - Step 50 Global step 50 Train loss 3.93 on epoch=24
04/26/2022 00:23:35 - INFO - __main__ - Global step 50 Train loss 1.88 ACC 0.0 on epoch=24
04/26/2022 00:23:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
04/26/2022 00:23:40 - INFO - __main__ - Step 60 Global step 60 Train loss 4.98 on epoch=29
04/26/2022 00:23:44 - INFO - __main__ - Step 70 Global step 70 Train loss 2.01 on epoch=34
04/26/2022 00:23:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=39
04/26/2022 00:23:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=44
04/26/2022 00:23:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.82 on epoch=49
04/26/2022 00:24:00 - INFO - __main__ - Global step 100 Train loss 1.95 ACC 0.15625 on epoch=49
04/26/2022 00:24:00 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=49, global_step=100
04/26/2022 00:24:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=54
04/26/2022 00:24:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=59
04/26/2022 00:24:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=64
04/26/2022 00:24:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=69
04/26/2022 00:24:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=74
04/26/2022 00:24:24 - INFO - __main__ - Global step 150 Train loss 0.77 ACC 0.15625 on epoch=74
04/26/2022 00:24:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.73 on epoch=79
04/26/2022 00:24:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.70 on epoch=84
04/26/2022 00:24:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=89
04/26/2022 00:24:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=94
04/26/2022 00:24:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=99
04/26/2022 00:24:48 - INFO - __main__ - Global step 200 Train loss 0.71 ACC 0.15625 on epoch=99
04/26/2022 00:24:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=104
04/26/2022 00:24:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=109
04/26/2022 00:25:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.64 on epoch=114
04/26/2022 00:25:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=119
04/26/2022 00:25:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=124
04/26/2022 00:25:13 - INFO - __main__ - Global step 250 Train loss 0.66 ACC 0.15625 on epoch=124
04/26/2022 00:25:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=129
04/26/2022 00:25:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=134
04/26/2022 00:25:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=139
04/26/2022 00:25:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=144
04/26/2022 00:25:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=149
04/26/2022 00:25:37 - INFO - __main__ - Global step 300 Train loss 0.62 ACC 0.15625 on epoch=149
04/26/2022 00:25:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=154
04/26/2022 00:25:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=159
04/26/2022 00:25:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=164
04/26/2022 00:25:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=169
04/26/2022 00:25:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=174
04/26/2022 00:26:02 - INFO - __main__ - Global step 350 Train loss 0.55 ACC 0.1875 on epoch=174
04/26/2022 00:26:02 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=174, global_step=350
04/26/2022 00:26:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=179
04/26/2022 00:26:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.51 on epoch=184
04/26/2022 00:26:15 - INFO - __main__ - Step 380 Global step 380 Train loss 4.58 on epoch=189
04/26/2022 00:26:19 - INFO - __main__ - Step 390 Global step 390 Train loss 5.21 on epoch=194
04/26/2022 00:26:24 - INFO - __main__ - Step 400 Global step 400 Train loss 4.35 on epoch=199
04/26/2022 00:26:26 - INFO - __main__ - Global step 400 Train loss 3.25 ACC 0.03125 on epoch=199
04/26/2022 00:26:30 - INFO - __main__ - Step 410 Global step 410 Train loss 5.18 on epoch=204
04/26/2022 00:26:35 - INFO - __main__ - Step 420 Global step 420 Train loss 5.49 on epoch=209
04/26/2022 00:26:39 - INFO - __main__ - Step 430 Global step 430 Train loss 4.94 on epoch=214
04/26/2022 00:26:44 - INFO - __main__ - Step 440 Global step 440 Train loss 4.34 on epoch=219
04/26/2022 00:26:48 - INFO - __main__ - Step 450 Global step 450 Train loss 2.98 on epoch=224
04/26/2022 00:26:51 - INFO - __main__ - Global step 450 Train loss 4.59 ACC 0.03125 on epoch=224
04/26/2022 00:26:56 - INFO - __main__ - Step 460 Global step 460 Train loss 2.93 on epoch=229
04/26/2022 00:27:00 - INFO - __main__ - Step 470 Global step 470 Train loss 2.68 on epoch=234
04/26/2022 00:27:04 - INFO - __main__ - Step 480 Global step 480 Train loss 2.32 on epoch=239
04/26/2022 00:27:09 - INFO - __main__ - Step 490 Global step 490 Train loss 2.33 on epoch=244
04/26/2022 00:27:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.84 on epoch=249
04/26/2022 00:27:20 - INFO - __main__ - Global step 500 Train loss 2.42 ACC 0.125 on epoch=249
04/26/2022 00:27:24 - INFO - __main__ - Step 510 Global step 510 Train loss 1.94 on epoch=254
04/26/2022 00:27:29 - INFO - __main__ - Step 520 Global step 520 Train loss 1.63 on epoch=259
04/26/2022 00:27:33 - INFO - __main__ - Step 530 Global step 530 Train loss 1.31 on epoch=264
04/26/2022 00:27:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
04/26/2022 00:27:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.81 on epoch=274
04/26/2022 00:27:45 - INFO - __main__ - Global step 550 Train loss 1.58 ACC 0.21875 on epoch=274
04/26/2022 00:27:45 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=274, global_step=550
04/26/2022 00:27:50 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
04/26/2022 00:27:54 - INFO - __main__ - Step 570 Global step 570 Train loss 1.16 on epoch=284
04/26/2022 00:27:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=289
04/26/2022 00:28:03 - INFO - __main__ - Step 590 Global step 590 Train loss 1.79 on epoch=294
04/26/2022 00:28:08 - INFO - __main__ - Step 600 Global step 600 Train loss 1.29 on epoch=299
04/26/2022 00:28:10 - INFO - __main__ - Global step 600 Train loss 1.32 ACC 0.125 on epoch=299
04/26/2022 00:28:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=304
04/26/2022 00:28:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.71 on epoch=309
04/26/2022 00:28:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=314
04/26/2022 00:28:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=319
04/26/2022 00:28:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.94 on epoch=324
04/26/2022 00:28:34 - INFO - __main__ - Global step 650 Train loss 0.81 ACC 0.15625 on epoch=324
04/26/2022 00:28:39 - INFO - __main__ - Step 660 Global step 660 Train loss 1.22 on epoch=329
04/26/2022 00:28:43 - INFO - __main__ - Step 670 Global step 670 Train loss 1.57 on epoch=334
04/26/2022 00:28:48 - INFO - __main__ - Step 680 Global step 680 Train loss 1.28 on epoch=339
04/26/2022 00:28:52 - INFO - __main__ - Step 690 Global step 690 Train loss 1.15 on epoch=344
04/26/2022 00:28:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.25 on epoch=349
04/26/2022 00:28:59 - INFO - __main__ - Global step 700 Train loss 1.29 ACC 0.09375 on epoch=349
04/26/2022 00:29:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.97 on epoch=354
04/26/2022 00:29:08 - INFO - __main__ - Step 720 Global step 720 Train loss 1.07 on epoch=359
04/26/2022 00:29:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.93 on epoch=364
04/26/2022 00:29:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
04/26/2022 00:29:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.99 on epoch=374
04/26/2022 00:29:24 - INFO - __main__ - Global step 750 Train loss 0.98 ACC 0.0625 on epoch=374
04/26/2022 00:29:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.83 on epoch=379
04/26/2022 00:29:32 - INFO - __main__ - Step 770 Global step 770 Train loss 1.01 on epoch=384
04/26/2022 00:29:37 - INFO - __main__ - Step 780 Global step 780 Train loss 1.43 on epoch=389
04/26/2022 00:29:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.96 on epoch=394
04/26/2022 00:29:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=399
04/26/2022 00:29:48 - INFO - __main__ - Global step 800 Train loss 1.01 ACC 0.125 on epoch=399
04/26/2022 00:29:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.89 on epoch=404
04/26/2022 00:29:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
04/26/2022 00:30:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.78 on epoch=414
04/26/2022 00:30:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=419
04/26/2022 00:30:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.73 on epoch=424
04/26/2022 00:30:13 - INFO - __main__ - Global step 850 Train loss 0.78 ACC 0.15625 on epoch=424
04/26/2022 00:30:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.72 on epoch=429
04/26/2022 00:30:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=434
04/26/2022 00:30:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.70 on epoch=439
04/26/2022 00:30:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.74 on epoch=444
04/26/2022 00:30:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
04/26/2022 00:30:37 - INFO - __main__ - Global step 900 Train loss 0.74 ACC 0.15625 on epoch=449
04/26/2022 00:30:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=454
04/26/2022 00:30:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
04/26/2022 00:30:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.72 on epoch=464
04/26/2022 00:30:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.63 on epoch=469
04/26/2022 00:31:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.60 on epoch=474
04/26/2022 00:31:02 - INFO - __main__ - Global step 950 Train loss 0.69 ACC 0.09375 on epoch=474
04/26/2022 00:31:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.73 on epoch=479
04/26/2022 00:31:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.63 on epoch=484
04/26/2022 00:31:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.60 on epoch=489
04/26/2022 00:31:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.67 on epoch=494
04/26/2022 00:31:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
04/26/2022 00:31:27 - INFO - __main__ - Global step 1000 Train loss 0.66 ACC 0.125 on epoch=499
04/26/2022 00:31:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=504
04/26/2022 00:31:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.56 on epoch=509
04/26/2022 00:31:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=514
04/26/2022 00:31:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
04/26/2022 00:31:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.61 on epoch=524
04/26/2022 00:31:51 - INFO - __main__ - Global step 1050 Train loss 0.61 ACC 0.09375 on epoch=524
04/26/2022 00:31:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
04/26/2022 00:32:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.59 on epoch=534
04/26/2022 00:32:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.58 on epoch=539
04/26/2022 00:32:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.56 on epoch=544
04/26/2022 00:32:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
04/26/2022 00:32:16 - INFO - __main__ - Global step 1100 Train loss 0.58 ACC 0.125 on epoch=549
04/26/2022 00:32:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
04/26/2022 00:32:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
04/26/2022 00:32:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
04/26/2022 00:32:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.57 on epoch=569
04/26/2022 00:32:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=574
04/26/2022 00:32:40 - INFO - __main__ - Global step 1150 Train loss 0.55 ACC 0.125 on epoch=574
04/26/2022 00:32:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
04/26/2022 00:32:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.53 on epoch=584
04/26/2022 00:32:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.57 on epoch=589
04/26/2022 00:32:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
04/26/2022 00:33:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.60 on epoch=599
04/26/2022 00:33:05 - INFO - __main__ - Global step 1200 Train loss 0.55 ACC 0.125 on epoch=599
04/26/2022 00:33:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.53 on epoch=604
04/26/2022 00:33:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
04/26/2022 00:33:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=614
04/26/2022 00:33:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.57 on epoch=619
04/26/2022 00:33:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=624
04/26/2022 00:33:29 - INFO - __main__ - Global step 1250 Train loss 0.52 ACC 0.125 on epoch=624
04/26/2022 00:33:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.56 on epoch=629
04/26/2022 00:33:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
04/26/2022 00:33:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=639
04/26/2022 00:33:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
04/26/2022 00:33:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=649
04/26/2022 00:33:54 - INFO - __main__ - Global step 1300 Train loss 0.52 ACC 0.125 on epoch=649
04/26/2022 00:33:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=654
04/26/2022 00:34:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
04/26/2022 00:34:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=664
04/26/2022 00:34:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
04/26/2022 00:34:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
04/26/2022 00:34:18 - INFO - __main__ - Global step 1350 Train loss 0.50 ACC 0.125 on epoch=674
04/26/2022 00:34:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
04/26/2022 00:34:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
04/26/2022 00:34:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
04/26/2022 00:34:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
04/26/2022 00:34:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
04/26/2022 00:34:43 - INFO - __main__ - Global step 1400 Train loss 0.50 ACC 0.125 on epoch=699
04/26/2022 00:34:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=704
04/26/2022 00:34:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
04/26/2022 00:34:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.52 on epoch=714
04/26/2022 00:35:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
04/26/2022 00:35:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=724
04/26/2022 00:35:07 - INFO - __main__ - Global step 1450 Train loss 0.47 ACC 0.09375 on epoch=724
04/26/2022 00:35:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=729
04/26/2022 00:35:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
04/26/2022 00:35:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=739
04/26/2022 00:35:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
04/26/2022 00:35:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
04/26/2022 00:35:32 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.09375 on epoch=749
04/26/2022 00:35:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
04/26/2022 00:35:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
04/26/2022 00:35:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
04/26/2022 00:35:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=769
04/26/2022 00:35:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
04/26/2022 00:35:57 - INFO - __main__ - Global step 1550 Train loss 0.48 ACC 0.09375 on epoch=774
04/26/2022 00:36:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
04/26/2022 00:36:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
04/26/2022 00:36:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
04/26/2022 00:36:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
04/26/2022 00:36:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=799
04/26/2022 00:36:21 - INFO - __main__ - Global step 1600 Train loss 0.42 ACC 0.09375 on epoch=799
04/26/2022 00:36:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
04/26/2022 00:36:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=809
04/26/2022 00:36:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=814
04/26/2022 00:36:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
04/26/2022 00:36:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
04/26/2022 00:36:46 - INFO - __main__ - Global step 1650 Train loss 0.40 ACC 0.09375 on epoch=824
04/26/2022 00:36:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
04/26/2022 00:36:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
04/26/2022 00:36:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=839
04/26/2022 00:37:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
04/26/2022 00:37:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
04/26/2022 00:37:10 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.09375 on epoch=849
04/26/2022 00:37:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
04/26/2022 00:37:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
04/26/2022 00:37:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
04/26/2022 00:37:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
04/26/2022 00:37:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
04/26/2022 00:37:35 - INFO - __main__ - Global step 1750 Train loss 0.39 ACC 0.09375 on epoch=874
04/26/2022 00:37:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
04/26/2022 00:37:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=884
04/26/2022 00:37:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
04/26/2022 00:37:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=894
04/26/2022 00:37:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=899
04/26/2022 00:37:59 - INFO - __main__ - Global step 1800 Train loss 0.40 ACC 0.09375 on epoch=899
04/26/2022 00:38:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
04/26/2022 00:38:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
04/26/2022 00:38:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
04/26/2022 00:38:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=919
04/26/2022 00:38:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
04/26/2022 00:38:24 - INFO - __main__ - Global step 1850 Train loss 0.36 ACC 0.09375 on epoch=924
04/26/2022 00:38:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=929
04/26/2022 00:38:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
04/26/2022 00:38:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
04/26/2022 00:38:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
04/26/2022 00:38:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
04/26/2022 00:38:48 - INFO - __main__ - Global step 1900 Train loss 0.37 ACC 0.125 on epoch=949
04/26/2022 00:38:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
04/26/2022 00:38:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
04/26/2022 00:39:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
04/26/2022 00:39:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
04/26/2022 00:39:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
04/26/2022 00:39:13 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.125 on epoch=974
04/26/2022 00:39:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
04/26/2022 00:39:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=984
04/26/2022 00:39:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
04/26/2022 00:39:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
04/26/2022 00:39:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
04/26/2022 00:39:36 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:39:36 - INFO - __main__ - Printing 3 examples
04/26/2022 00:39:36 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:39:36 - INFO - __main__ - ['it was not installed']
04/26/2022 00:39:36 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:39:36 - INFO - __main__ - ['a few more months']
04/26/2022 00:39:36 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:39:36 - INFO - __main__ - ['still in the same position']
04/26/2022 00:39:36 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:39:37 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:39:37 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:39:37 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:39:37 - INFO - __main__ - Printing 3 examples
04/26/2022 00:39:37 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:39:37 - INFO - __main__ - ['not enough information']
04/26/2022 00:39:37 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:39:37 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:39:37 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:39:37 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:39:37 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:39:37 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:39:37 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:39:37 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.125 on epoch=999
04/26/2022 00:39:37 - INFO - __main__ - save last model!
04/26/2022 00:39:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/26/2022 00:39:37 - INFO - __main__ - Start tokenizing ... 1000 instances
04/26/2022 00:39:37 - INFO - __main__ - Printing 3 examples
04/26/2022 00:39:37 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:39:37 - INFO - __main__ - ['about 10 minutes']
04/26/2022 00:39:37 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:39:37 - INFO - __main__ - ['Larry likes sex.']
04/26/2022 00:39:37 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:39:37 - INFO - __main__ - ['That Candy had all her teeth']
04/26/2022 00:39:37 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:39:39 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:39:40 - INFO - __main__ - Loaded 1000 examples from test data
04/26/2022 00:39:55 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:39:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:39:56 - INFO - __main__ - Starting training!
04/26/2022 00:40:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.5_8_predictions.txt
04/26/2022 00:40:58 - INFO - __main__ - ACC on test data: 0.2090
04/26/2022 00:40:58 - INFO - __main__ - prefix=quail_32_13, lr=0.5, bsz=8, dev_performance=0.21875, test_performance=0.209
04/26/2022 00:40:58 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.4, bsz=8 ...
04/26/2022 00:40:59 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:40:59 - INFO - __main__ - Printing 3 examples
04/26/2022 00:40:59 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:40:59 - INFO - __main__ - ['it was not installed']
04/26/2022 00:40:59 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:40:59 - INFO - __main__ - ['a few more months']
04/26/2022 00:40:59 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:40:59 - INFO - __main__ - ['still in the same position']
04/26/2022 00:40:59 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:40:59 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:40:59 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:40:59 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:40:59 - INFO - __main__ - Printing 3 examples
04/26/2022 00:40:59 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:40:59 - INFO - __main__ - ['not enough information']
04/26/2022 00:40:59 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:40:59 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:40:59 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:40:59 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:40:59 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:40:59 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:40:59 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:41:18 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:41:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:41:19 - INFO - __main__ - Starting training!
04/26/2022 00:41:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.78 on epoch=4
04/26/2022 00:41:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.21 on epoch=9
04/26/2022 00:41:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=14
04/26/2022 00:41:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=19
04/26/2022 00:41:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
04/26/2022 00:41:45 - INFO - __main__ - Global step 50 Train loss 1.02 ACC 0.125 on epoch=24
04/26/2022 00:41:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
04/26/2022 00:41:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=29
04/26/2022 00:41:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
04/26/2022 00:41:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
04/26/2022 00:42:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
04/26/2022 00:42:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
04/26/2022 00:42:10 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.09375 on epoch=49
04/26/2022 00:42:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
04/26/2022 00:42:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
04/26/2022 00:42:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
04/26/2022 00:42:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
04/26/2022 00:42:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
04/26/2022 00:42:34 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.125 on epoch=74
04/26/2022 00:42:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
04/26/2022 00:42:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
04/26/2022 00:42:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
04/26/2022 00:42:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
04/26/2022 00:42:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
04/26/2022 00:42:59 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.15625 on epoch=99
04/26/2022 00:42:59 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.15625 on epoch=99, global_step=200
04/26/2022 00:43:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/26/2022 00:43:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/26/2022 00:43:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
04/26/2022 00:43:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
04/26/2022 00:43:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
04/26/2022 00:43:23 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.21875 on epoch=124
04/26/2022 00:43:23 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=124, global_step=250
04/26/2022 00:43:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
04/26/2022 00:43:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
04/26/2022 00:43:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
04/26/2022 00:43:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
04/26/2022 00:43:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
04/26/2022 00:43:48 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.1875 on epoch=149
04/26/2022 00:43:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
04/26/2022 00:43:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
04/26/2022 00:44:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
04/26/2022 00:44:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
04/26/2022 00:44:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/26/2022 00:44:12 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.1875 on epoch=174
04/26/2022 00:44:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
04/26/2022 00:44:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
04/26/2022 00:44:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/26/2022 00:44:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/26/2022 00:44:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/26/2022 00:44:37 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.15625 on epoch=199
04/26/2022 00:44:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
04/26/2022 00:44:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
04/26/2022 00:44:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
04/26/2022 00:44:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/26/2022 00:44:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
04/26/2022 00:45:01 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.15625 on epoch=224
04/26/2022 00:45:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/26/2022 00:45:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
04/26/2022 00:45:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
04/26/2022 00:45:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
04/26/2022 00:45:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
04/26/2022 00:45:26 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.1875 on epoch=249
04/26/2022 00:45:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
04/26/2022 00:45:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
04/26/2022 00:45:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
04/26/2022 00:45:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/26/2022 00:45:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
04/26/2022 00:45:51 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.1875 on epoch=274
04/26/2022 00:45:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=279
04/26/2022 00:45:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/26/2022 00:46:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
04/26/2022 00:46:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
04/26/2022 00:46:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=299
04/26/2022 00:46:15 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.21875 on epoch=299
04/26/2022 00:46:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/26/2022 00:46:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/26/2022 00:46:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/26/2022 00:46:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
04/26/2022 00:46:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/26/2022 00:46:40 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.09375 on epoch=324
04/26/2022 00:46:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
04/26/2022 00:46:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/26/2022 00:46:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/26/2022 00:46:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/26/2022 00:47:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/26/2022 00:47:04 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.25 on epoch=349
04/26/2022 00:47:04 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=349, global_step=700
04/26/2022 00:47:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
04/26/2022 00:47:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/26/2022 00:47:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
04/26/2022 00:47:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/26/2022 00:47:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/26/2022 00:47:29 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.21875 on epoch=374
04/26/2022 00:47:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
04/26/2022 00:47:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/26/2022 00:47:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
04/26/2022 00:47:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/26/2022 00:47:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/26/2022 00:47:53 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.15625 on epoch=399
04/26/2022 00:47:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/26/2022 00:48:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/26/2022 00:48:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/26/2022 00:48:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/26/2022 00:48:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
04/26/2022 00:48:18 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.15625 on epoch=424
04/26/2022 00:48:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/26/2022 00:48:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/26/2022 00:48:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
04/26/2022 00:48:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/26/2022 00:48:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
04/26/2022 00:48:43 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.125 on epoch=449
04/26/2022 00:48:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/26/2022 00:48:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/26/2022 00:48:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
04/26/2022 00:49:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/26/2022 00:49:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/26/2022 00:49:08 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.15625 on epoch=474
04/26/2022 00:49:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/26/2022 00:49:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/26/2022 00:49:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
04/26/2022 00:49:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/26/2022 00:49:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/26/2022 00:49:33 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.15625 on epoch=499
04/26/2022 00:49:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/26/2022 00:49:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/26/2022 00:49:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/26/2022 00:49:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/26/2022 00:49:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/26/2022 00:49:58 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.1875 on epoch=524
04/26/2022 00:50:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/26/2022 00:50:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/26/2022 00:50:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/26/2022 00:50:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
04/26/2022 00:50:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
04/26/2022 00:50:23 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.125 on epoch=549
04/26/2022 00:50:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/26/2022 00:50:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/26/2022 00:50:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/26/2022 00:50:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/26/2022 00:50:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/26/2022 00:50:47 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
04/26/2022 00:50:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/26/2022 00:50:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/26/2022 00:51:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/26/2022 00:51:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/26/2022 00:51:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/26/2022 00:51:12 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.25 on epoch=599
04/26/2022 00:51:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/26/2022 00:51:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/26/2022 00:51:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/26/2022 00:51:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
04/26/2022 00:51:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/26/2022 00:51:37 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.1875 on epoch=624
04/26/2022 00:51:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/26/2022 00:51:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/26/2022 00:51:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/26/2022 00:51:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/26/2022 00:51:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/26/2022 00:52:02 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.15625 on epoch=649
04/26/2022 00:52:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/26/2022 00:52:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/26/2022 00:52:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/26/2022 00:52:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/26/2022 00:52:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
04/26/2022 00:52:27 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.21875 on epoch=674
04/26/2022 00:52:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/26/2022 00:52:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
04/26/2022 00:52:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/26/2022 00:52:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/26/2022 00:52:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/26/2022 00:52:52 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.25 on epoch=699
04/26/2022 00:52:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/26/2022 00:53:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/26/2022 00:53:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/26/2022 00:53:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
04/26/2022 00:53:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/26/2022 00:53:17 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.15625 on epoch=724
04/26/2022 00:53:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/26/2022 00:53:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/26/2022 00:53:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/26/2022 00:53:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/26/2022 00:53:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/26/2022 00:53:42 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
04/26/2022 00:53:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/26/2022 00:53:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/26/2022 00:53:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/26/2022 00:54:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
04/26/2022 00:54:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/26/2022 00:54:06 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.15625 on epoch=774
04/26/2022 00:54:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/26/2022 00:54:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/26/2022 00:54:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/26/2022 00:54:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/26/2022 00:54:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/26/2022 00:54:31 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.28125 on epoch=799
04/26/2022 00:54:31 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=799, global_step=1600
04/26/2022 00:54:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/26/2022 00:54:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/26/2022 00:54:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/26/2022 00:54:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
04/26/2022 00:54:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
04/26/2022 00:54:56 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.28125 on epoch=824
04/26/2022 00:55:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
04/26/2022 00:55:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
04/26/2022 00:55:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/26/2022 00:55:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/26/2022 00:55:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/26/2022 00:55:21 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.21875 on epoch=849
04/26/2022 00:55:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/26/2022 00:55:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/26/2022 00:55:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/26/2022 00:55:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/26/2022 00:55:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/26/2022 00:55:46 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.1875 on epoch=874
04/26/2022 00:55:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/26/2022 00:55:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/26/2022 00:55:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/26/2022 00:56:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/26/2022 00:56:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/26/2022 00:56:11 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.28125 on epoch=899
04/26/2022 00:56:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/26/2022 00:56:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/26/2022 00:56:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
04/26/2022 00:56:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/26/2022 00:56:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/26/2022 00:56:35 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
04/26/2022 00:56:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
04/26/2022 00:56:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/26/2022 00:56:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/26/2022 00:56:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/26/2022 00:56:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/26/2022 00:57:00 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.28125 on epoch=949
04/26/2022 00:57:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/26/2022 00:57:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/26/2022 00:57:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
04/26/2022 00:57:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/26/2022 00:57:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/26/2022 00:57:24 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
04/26/2022 00:57:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/26/2022 00:57:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/26/2022 00:57:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/26/2022 00:57:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/26/2022 00:57:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/26/2022 00:57:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:57:49 - INFO - __main__ - Printing 3 examples
04/26/2022 00:57:49 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:57:49 - INFO - __main__ - ['it was not installed']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:57:49 - INFO - __main__ - ['a few more months']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:57:49 - INFO - __main__ - ['still in the same position']
04/26/2022 00:57:49 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:57:49 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:57:49 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:57:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:57:49 - INFO - __main__ - Printing 3 examples
04/26/2022 00:57:49 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:57:49 - INFO - __main__ - ['not enough information']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:57:49 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:57:49 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:57:49 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:57:49 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.375 on epoch=999
04/26/2022 00:57:49 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=999, global_step=2000
04/26/2022 00:57:49 - INFO - __main__ - save last model!
04/26/2022 00:57:49 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:57:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/26/2022 00:57:49 - INFO - __main__ - Start tokenizing ... 1000 instances
04/26/2022 00:57:49 - INFO - __main__ - Printing 3 examples
04/26/2022 00:57:49 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:57:49 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:57:49 - INFO - __main__ - ['about 10 minutes']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:57:49 - INFO - __main__ - ['Larry likes sex.']
04/26/2022 00:57:49 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 00:57:49 - INFO - __main__ - ['That Candy had all her teeth']
04/26/2022 00:57:49 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:57:51 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:57:52 - INFO - __main__ - Loaded 1000 examples from test data
04/26/2022 00:58:07 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:58:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:58:08 - INFO - __main__ - Starting training!
04/26/2022 00:59:18 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.4_8_predictions.txt
04/26/2022 00:59:18 - INFO - __main__ - ACC on test data: 0.2750
04/26/2022 00:59:19 - INFO - __main__ - prefix=quail_32_13, lr=0.4, bsz=8, dev_performance=0.375, test_performance=0.275
04/26/2022 00:59:19 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.3, bsz=8 ...
04/26/2022 00:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:59:20 - INFO - __main__ - Printing 3 examples
04/26/2022 00:59:20 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 00:59:20 - INFO - __main__ - ['it was not installed']
04/26/2022 00:59:20 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 00:59:20 - INFO - __main__ - ['a few more months']
04/26/2022 00:59:20 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 00:59:20 - INFO - __main__ - ['still in the same position']
04/26/2022 00:59:20 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:59:20 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:59:20 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 00:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 00:59:20 - INFO - __main__ - Printing 3 examples
04/26/2022 00:59:20 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 00:59:20 - INFO - __main__ - ['not enough information']
04/26/2022 00:59:20 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 00:59:20 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 00:59:20 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 00:59:20 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 00:59:20 - INFO - __main__ - Tokenizing Input ...
04/26/2022 00:59:20 - INFO - __main__ - Tokenizing Output ...
04/26/2022 00:59:20 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 00:59:36 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 00:59:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 00:59:37 - INFO - __main__ - Starting training!
04/26/2022 00:59:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.91 on epoch=4
04/26/2022 00:59:46 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=9
04/26/2022 00:59:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=14
04/26/2022 00:59:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
04/26/2022 00:59:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.70 on epoch=24
04/26/2022 01:00:02 - INFO - __main__ - Global step 50 Train loss 1.12 ACC 0.1875 on epoch=24
04/26/2022 01:00:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/26/2022 01:00:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
04/26/2022 01:00:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=34
04/26/2022 01:00:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
04/26/2022 01:00:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
04/26/2022 01:00:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
04/26/2022 01:00:26 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.125 on epoch=49
04/26/2022 01:00:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=54
04/26/2022 01:00:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
04/26/2022 01:00:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
04/26/2022 01:00:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
04/26/2022 01:00:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
04/26/2022 01:00:51 - INFO - __main__ - Global step 150 Train loss 0.38 ACC 0.1875 on epoch=74
04/26/2022 01:00:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
04/26/2022 01:00:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
04/26/2022 01:01:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
04/26/2022 01:01:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
04/26/2022 01:01:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=99
04/26/2022 01:01:15 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.15625 on epoch=99
04/26/2022 01:01:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
04/26/2022 01:01:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
04/26/2022 01:01:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
04/26/2022 01:01:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
04/26/2022 01:01:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
04/26/2022 01:01:40 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.25 on epoch=124
04/26/2022 01:01:40 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=124, global_step=250
04/26/2022 01:01:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
04/26/2022 01:01:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
04/26/2022 01:01:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
04/26/2022 01:01:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
04/26/2022 01:02:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
04/26/2022 01:02:04 - INFO - __main__ - Global step 300 Train loss 0.20 ACC 0.125 on epoch=149
04/26/2022 01:02:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
04/26/2022 01:02:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
04/26/2022 01:02:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
04/26/2022 01:02:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
04/26/2022 01:02:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
04/26/2022 01:02:29 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.125 on epoch=174
04/26/2022 01:02:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
04/26/2022 01:02:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
04/26/2022 01:02:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
04/26/2022 01:02:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
04/26/2022 01:02:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=199
04/26/2022 01:02:54 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.15625 on epoch=199
04/26/2022 01:02:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
04/26/2022 01:03:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
04/26/2022 01:03:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
04/26/2022 01:03:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/26/2022 01:03:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/26/2022 01:03:18 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.21875 on epoch=224
04/26/2022 01:03:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
04/26/2022 01:03:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
04/26/2022 01:03:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
04/26/2022 01:03:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/26/2022 01:03:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
04/26/2022 01:03:43 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.1875 on epoch=249
04/26/2022 01:03:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
04/26/2022 01:03:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
04/26/2022 01:03:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
04/26/2022 01:04:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
04/26/2022 01:04:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
04/26/2022 01:04:07 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.1875 on epoch=274
04/26/2022 01:04:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
04/26/2022 01:04:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
04/26/2022 01:04:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/26/2022 01:04:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
04/26/2022 01:04:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/26/2022 01:04:32 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.15625 on epoch=299
04/26/2022 01:04:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
04/26/2022 01:04:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
04/26/2022 01:04:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
04/26/2022 01:04:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/26/2022 01:04:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
04/26/2022 01:04:57 - INFO - __main__ - Global step 650 Train loss 0.07 ACC 0.25 on epoch=324
04/26/2022 01:05:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
04/26/2022 01:05:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
04/26/2022 01:05:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/26/2022 01:05:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/26/2022 01:05:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
04/26/2022 01:05:21 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.1875 on epoch=349
04/26/2022 01:05:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
04/26/2022 01:05:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/26/2022 01:05:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/26/2022 01:05:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
04/26/2022 01:05:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
04/26/2022 01:05:46 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.15625 on epoch=374
04/26/2022 01:05:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
04/26/2022 01:05:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
04/26/2022 01:05:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
04/26/2022 01:06:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
04/26/2022 01:06:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/26/2022 01:06:10 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.1875 on epoch=399
04/26/2022 01:06:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/26/2022 01:06:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/26/2022 01:06:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
04/26/2022 01:06:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/26/2022 01:06:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/26/2022 01:06:35 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.25 on epoch=424
04/26/2022 01:06:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/26/2022 01:06:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/26/2022 01:06:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
04/26/2022 01:06:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/26/2022 01:06:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
04/26/2022 01:06:59 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.125 on epoch=449
04/26/2022 01:07:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
04/26/2022 01:07:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
04/26/2022 01:07:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/26/2022 01:07:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/26/2022 01:07:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
04/26/2022 01:07:24 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.1875 on epoch=474
04/26/2022 01:07:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/26/2022 01:07:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/26/2022 01:07:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/26/2022 01:07:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/26/2022 01:07:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
04/26/2022 01:07:48 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.1875 on epoch=499
04/26/2022 01:07:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
04/26/2022 01:07:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
04/26/2022 01:08:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/26/2022 01:08:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/26/2022 01:08:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/26/2022 01:08:13 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.125 on epoch=524
04/26/2022 01:08:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/26/2022 01:08:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/26/2022 01:08:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/26/2022 01:08:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/26/2022 01:08:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/26/2022 01:08:37 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.125 on epoch=549
04/26/2022 01:08:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
04/26/2022 01:08:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
04/26/2022 01:08:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
04/26/2022 01:08:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
04/26/2022 01:09:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/26/2022 01:09:02 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.125 on epoch=574
04/26/2022 01:09:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
04/26/2022 01:09:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
04/26/2022 01:09:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/26/2022 01:09:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
04/26/2022 01:09:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/26/2022 01:09:26 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.09375 on epoch=599
04/26/2022 01:09:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/26/2022 01:09:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/26/2022 01:09:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/26/2022 01:09:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/26/2022 01:09:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/26/2022 01:09:51 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.09375 on epoch=624
04/26/2022 01:09:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/26/2022 01:10:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
04/26/2022 01:10:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/26/2022 01:10:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/26/2022 01:10:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
04/26/2022 01:10:16 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.21875 on epoch=649
04/26/2022 01:10:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
04/26/2022 01:10:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
04/26/2022 01:10:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/26/2022 01:10:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/26/2022 01:10:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/26/2022 01:10:40 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.1875 on epoch=674
04/26/2022 01:10:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
04/26/2022 01:10:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
04/26/2022 01:10:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/26/2022 01:10:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
04/26/2022 01:11:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
04/26/2022 01:11:05 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.1875 on epoch=699
04/26/2022 01:11:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/26/2022 01:11:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/26/2022 01:11:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/26/2022 01:11:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/26/2022 01:11:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/26/2022 01:11:29 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.125 on epoch=724
04/26/2022 01:11:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/26/2022 01:11:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
04/26/2022 01:11:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/26/2022 01:11:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/26/2022 01:11:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
04/26/2022 01:11:54 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
04/26/2022 01:11:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/26/2022 01:12:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/26/2022 01:12:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/26/2022 01:12:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/26/2022 01:12:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/26/2022 01:12:18 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.0625 on epoch=774
04/26/2022 01:12:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/26/2022 01:12:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/26/2022 01:12:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/26/2022 01:12:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/26/2022 01:12:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/26/2022 01:12:43 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.09375 on epoch=799
04/26/2022 01:12:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
04/26/2022 01:12:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/26/2022 01:12:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/26/2022 01:13:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/26/2022 01:13:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/26/2022 01:13:07 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.0625 on epoch=824
04/26/2022 01:13:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/26/2022 01:13:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/26/2022 01:13:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/26/2022 01:13:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/26/2022 01:13:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/26/2022 01:13:32 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.125 on epoch=849
04/26/2022 01:13:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/26/2022 01:13:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
04/26/2022 01:13:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
04/26/2022 01:13:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/26/2022 01:13:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/26/2022 01:13:56 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.125 on epoch=874
04/26/2022 01:14:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/26/2022 01:14:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/26/2022 01:14:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/26/2022 01:14:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/26/2022 01:14:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/26/2022 01:14:21 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.125 on epoch=899
04/26/2022 01:14:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/26/2022 01:14:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/26/2022 01:14:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
04/26/2022 01:14:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/26/2022 01:14:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/26/2022 01:14:46 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
04/26/2022 01:14:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/26/2022 01:14:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/26/2022 01:14:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/26/2022 01:15:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
04/26/2022 01:15:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/26/2022 01:15:10 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.125 on epoch=949
04/26/2022 01:15:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
04/26/2022 01:15:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/26/2022 01:15:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
04/26/2022 01:15:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/26/2022 01:15:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/26/2022 01:15:35 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.0625 on epoch=974
04/26/2022 01:15:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
04/26/2022 01:15:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/26/2022 01:15:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
04/26/2022 01:15:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/26/2022 01:15:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/26/2022 01:15:59 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 01:15:59 - INFO - __main__ - Printing 3 examples
04/26/2022 01:15:59 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 01:15:59 - INFO - __main__ - ['it was not installed']
04/26/2022 01:15:59 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 01:15:59 - INFO - __main__ - ['a few more months']
04/26/2022 01:15:59 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 01:15:59 - INFO - __main__ - ['still in the same position']
04/26/2022 01:15:59 - INFO - __main__ - Tokenizing Input ...
04/26/2022 01:15:59 - INFO - __main__ - Tokenizing Output ...
04/26/2022 01:15:59 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 01:15:59 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 01:15:59 - INFO - __main__ - Printing 3 examples
04/26/2022 01:15:59 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 01:15:59 - INFO - __main__ - ['not enough information']
04/26/2022 01:15:59 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 01:15:59 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 01:15:59 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 01:15:59 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 01:15:59 - INFO - __main__ - Tokenizing Input ...
04/26/2022 01:15:59 - INFO - __main__ - Tokenizing Output ...
04/26/2022 01:15:59 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 01:16:00 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.09375 on epoch=999
04/26/2022 01:16:00 - INFO - __main__ - save last model!
04/26/2022 01:16:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/26/2022 01:16:00 - INFO - __main__ - Start tokenizing ... 1000 instances
04/26/2022 01:16:00 - INFO - __main__ - Printing 3 examples
04/26/2022 01:16:00 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 01:16:00 - INFO - __main__ - ['about 10 minutes']
04/26/2022 01:16:00 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 01:16:00 - INFO - __main__ - ['Larry likes sex.']
04/26/2022 01:16:00 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
04/26/2022 01:16:00 - INFO - __main__ - ['That Candy had all her teeth']
04/26/2022 01:16:00 - INFO - __main__ - Tokenizing Input ...
04/26/2022 01:16:01 - INFO - __main__ - Tokenizing Output ...
04/26/2022 01:16:02 - INFO - __main__ - Loaded 1000 examples from test data
04/26/2022 01:16:14 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 01:16:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 01:16:15 - INFO - __main__ - Starting training!
04/26/2022 01:17:31 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.3_8_predictions.txt
04/26/2022 01:17:32 - INFO - __main__ - ACC on test data: 0.2550
04/26/2022 01:17:32 - INFO - __main__ - prefix=quail_32_13, lr=0.3, bsz=8, dev_performance=0.25, test_performance=0.255
04/26/2022 01:17:32 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.2, bsz=8 ...
04/26/2022 01:17:33 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 01:17:33 - INFO - __main__ - Printing 3 examples
04/26/2022 01:17:33 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
04/26/2022 01:17:33 - INFO - __main__ - ['it was not installed']
04/26/2022 01:17:33 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
04/26/2022 01:17:33 - INFO - __main__ - ['a few more months']
04/26/2022 01:17:33 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
04/26/2022 01:17:33 - INFO - __main__ - ['still in the same position']
04/26/2022 01:17:33 - INFO - __main__ - Tokenizing Input ...
04/26/2022 01:17:33 - INFO - __main__ - Tokenizing Output ...
04/26/2022 01:17:33 - INFO - __main__ - Loaded 32 examples from train data
04/26/2022 01:17:33 - INFO - __main__ - Start tokenizing ... 32 instances
04/26/2022 01:17:33 - INFO - __main__ - Printing 3 examples
04/26/2022 01:17:33 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
04/26/2022 01:17:33 - INFO - __main__ - ['not enough information']
04/26/2022 01:17:33 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
04/26/2022 01:17:33 - INFO - __main__ - ['something including carbohydrates']
04/26/2022 01:17:33 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
04/26/2022 01:17:33 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
04/26/2022 01:17:33 - INFO - __main__ - Tokenizing Input ...
04/26/2022 01:17:33 - INFO - __main__ - Tokenizing Output ...
04/26/2022 01:17:33 - INFO - __main__ - Loaded 32 examples from dev data
04/26/2022 01:17:52 - INFO - __main__ - load prompt embedding from ckpt
04/26/2022 01:17:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/26/2022 01:17:52 - INFO - __main__ - Starting training!
04/26/2022 01:17:58 - INFO - __main__ - Step 10 Global step 10 Train loss 2.08 on epoch=4
04/26/2022 01:18:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.58 on epoch=9
04/26/2022 01:18:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
04/26/2022 01:18:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=19
04/26/2022 01:18:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=24
04/26/2022 01:18:18 - INFO - __main__ - Global step 50 Train loss 1.33 ACC 0.1875 on epoch=24
04/26/2022 01:18:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/26/2022 01:18:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
04/26/2022 01:18:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=34
04/26/2022 01:18:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
04/26/2022 01:18:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=44
04/26/2022 01:18:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
04/26/2022 01:18:42 - INFO - __main__ - Global step 100 Train loss 0.66 ACC 0.125 on epoch=49
04/26/2022 01:18:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=54
04/26/2022 01:18:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=59
04/26/2022 01:18:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
04/26/2022 01:19:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
04/26/2022 01:19:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=74
04/26/2022 01:19:07 - INFO - __main__ - Global step 150 Train loss 0.50 ACC 0.125 on epoch=74
04/26/2022 01:19:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
04/26/2022 01:19:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
04/26/2022 01:19:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
04/26/2022 01:19:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
04/26/2022 01:19:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
04/26/2022 01:19:32 - INFO - __main__ - Global step 200 Train loss 0.39 ACC 0.125 on epoch=99
04/26/2022 01:19:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
04/26/2022 01:19:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
04/26/2022 01:19:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
04/26/2022 01:19:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
04/26/2022 01:19:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
04/26/2022 01:19:56 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.09375 on epoch=124
04/26/2022 01:20:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
04/26/2022 01:20:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=134
04/26/2022 01:20:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
04/26/2022 01:20:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
04/26/2022 01:20:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
04/26/2022 01:20:21 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.125 on epoch=149
04/26/2022 01:20:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
04/26/2022 01:20:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
04/26/2022 01:20:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
04/26/2022 01:20:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
04/26/2022 01:20:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
04/26/2022 01:20:45 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.15625 on epoch=174
04/26/2022 01:20:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
04/26/2022 01:20:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
04/26/2022 01:20:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
04/26/2022 01:21:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
04/26/2022 01:21:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
04/26/2022 01:21:10 - INFO - __main__ - Global step 400 Train loss 0.23 ACC 0.15625 on epoch=199
04/26/2022 01:21:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
04/26/2022 01:21:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
04/26/2022 01:21:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
04/26/2022 01:21:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
04/26/2022 01:21:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
04/26/2022 01:21:36 - INFO - __main__ - Global step 450 Train loss 0.19 ACC 0.15625 on epoch=224
04/26/2022 01:21:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=229
04/26/2022 01:21:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
04/26/2022 01:21:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=239
04/26/2022 01:21:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=244
04/26/2022 01:21:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
04/26/2022 01:22:00 - INFO - __main__ - Global step 500 Train loss 0.18 ACC 0.1875 on epoch=249
04/26/2022 01:22:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
04/26/2022 01:22:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
04/26/2022 01:22:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
04/26/2022 01:22:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
04/26/2022 01:22:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
04/26/2022 01:22:25 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.1875 on epoch=274
04/26/2022 01:22:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
04/26/2022 01:22:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=284
04/26/2022 01:22:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/26/2022 01:22:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=294
04/26/2022 01:22:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
04/26/2022 01:22:50 - INFO - __main__ - Global step 600 Train loss 0.14 ACC 0.1875 on epoch=299
04/26/2022 01:22:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
04/26/2022 01:22:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
04/26/2022 01:23:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
04/26/2022 01:23:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
04/26/2022 01:23:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
04/26/2022 01:23:14 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.21875 on epoch=324
04/26/2022 01:23:14 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=324, global_step=650
04/26/2022 01:23:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
04/26/2022 01:23:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
04/26/2022 01:23:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
04/26/2022 01:23:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
04/26/2022 01:23:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
04/26/2022 01:23:39 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.25 on epoch=349
04/26/2022 01:23:39 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=349, global_step=700
04/26/2022 01:23:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
04/26/2022 01:23:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=359
04/26/2022 01:23:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
04/26/2022 01:23:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
04/26/2022 01:24:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/26/2022 01:24:03 - INFO - __main__ - Global step 750 Train loss 0.08 ACC 0.1875 on epoch=374
04/26/2022 01:24:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
04/26/2022 01:24:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
04/26/2022 01:24:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
04/26/2022 01:24:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
04/26/2022 01:24:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=399
04/26/2022 01:24:28 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.21875 on epoch=399
04/26/2022 01:24:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/26/2022 01:24:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
04/26/2022 01:24:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=414
04/26/2022 01:24:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
04/26/2022 01:24:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
04/26/2022 01:24:53 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.1875 on epoch=424
04/26/2022 01:24:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
04/26/2022 01:25:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
04/26/2022 01:25:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
04/26/2022 01:25:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
04/26/2022 01:25:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
04/26/2022 01:25:17 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.1875 on epoch=449
04/26/2022 01:25:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
04/26/2022 01:25:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
04/26/2022 01:25:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
04/26/2022 01:25:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
04/26/2022 01:25:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
04/26/2022 01:25:42 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.1875 on epoch=474
04/26/2022 01:25:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
04/26/2022 01:25:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/26/2022 01:25:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
04/26/2022 01:26:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
04/26/2022 01:26:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
04/26/2022 01:26:07 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.15625 on epoch=499
04/26/2022 01:26:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
04/26/2022 01:26:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
04/26/2022 01:26:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
04/26/2022 01:26:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
04/26/2022 01:26:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
04/26/2022 01:26:32 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.15625 on epoch=524
04/26/2022 01:26:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
04/26/2022 01:26:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/26/2022 01:26:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
04/26/2022 01:26:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/26/2022 01:26:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/26/2022 01:26:56 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.15625 on epoch=549
04/26/2022 01:27:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
04/26/2022 01:27:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
04/26/2022 01:27:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.64 on epoch=564
04/26/2022 01:27:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=569
04/26/2022 01:27:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=574
04/26/2022 01:27:21 - INFO - __main__ - Global step 1150 Train loss 0.21 ACC 0.21875 on epoch=574
04/26/2022 01:27:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
04/26/2022 01:27:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
04/26/2022 01:27:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
04/26/2022 01:27:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
04/26/2022 01:27:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
04/26/2022 01:27:46 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.15625 on epoch=599
04/26/2022 01:27:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
04/26/2022 01:27:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
04/26/2022 01:27:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
04/26/2022 01:28:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
04/26/2022 01:28:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
04/26/2022 01:28:10 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.1875 on epoch=624
04/26/2022 01:28:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
04/26/2022 01:28:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
04/26/2022 01:28:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
04/26/2022 01:28:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
04/26/2022 01:28:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
04/26/2022 01:28:35 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.21875 on epoch=649
04/26/2022 01:28:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=654
04/26/2022 01:28:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=659
04/26/2022 01:28:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
04/26/2022 01:28:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
04/26/2022 01:28:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
04/26/2022 01:29:00 - INFO - __main__ - Global step 1350 Train loss 0.07 ACC 0.1875 on epoch=674
05/12/2022 15:45:34 - INFO - __main__ - Namespace(task_dir='data/quail/', task_name='quail', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/12/2022 15:45:34 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail
05/12/2022 15:45:34 - INFO - __main__ - Namespace(task_dir='data/quail/', task_name='quail', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/12/2022 15:45:34 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail
05/12/2022 15:45:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/12/2022 15:45:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/12/2022 15:45:35 - INFO - __main__ - args.device: cuda:0
05/12/2022 15:45:35 - INFO - __main__ - args.device: cuda:1
05/12/2022 15:45:35 - INFO - __main__ - Using 2 gpus
05/12/2022 15:45:35 - INFO - __main__ - Using 2 gpus
05/12/2022 15:45:35 - INFO - __main__ - Fine-tuning the following samples: ['quail_32_100', 'quail_32_13', 'quail_32_21', 'quail_32_42', 'quail_32_87']
05/12/2022 15:45:35 - INFO - __main__ - Fine-tuning the following samples: ['quail_32_100', 'quail_32_13', 'quail_32_21', 'quail_32_42', 'quail_32_87']
05/12/2022 15:45:40 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.5, bsz=8 ...
05/12/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 15:45:41 - INFO - __main__ - Printing 3 examples
05/12/2022 15:45:41 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 15:45:41 - INFO - __main__ - ['Looking stupid']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 15:45:41 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 15:45:41 - INFO - __main__ - ['For a couple weeks']
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 15:45:41 - INFO - __main__ - Printing 3 examples
05/12/2022 15:45:41 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 15:45:41 - INFO - __main__ - ['Looking stupid']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 15:45:41 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 15:45:41 - INFO - __main__ - ['For a couple weeks']
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 15:45:41 - INFO - __main__ - Printing 3 examples
05/12/2022 15:45:41 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 15:45:41 - INFO - __main__ - ['5 minutes']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 15:45:41 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 15:45:41 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 15:45:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 15:45:41 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 15:45:41 - INFO - __main__ - Printing 3 examples
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 15:45:41 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 15:45:41 - INFO - __main__ - ['5 minutes']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 15:45:41 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 15:45:41 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 15:45:41 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 15:45:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 15:45:41 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 15:45:59 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 15:46:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 15:46:00 - INFO - __main__ - Starting training!
05/12/2022 15:46:02 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 15:46:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 15:46:08 - INFO - __main__ - Starting training!
05/12/2022 15:46:13 - INFO - __main__ - Step 10 Global step 10 Train loss 1.96 on epoch=4
05/12/2022 15:46:18 - INFO - __main__ - Step 20 Global step 20 Train loss 1.49 on epoch=9
05/12/2022 15:46:22 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=14
05/12/2022 15:46:27 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=19
05/12/2022 15:46:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.77 on epoch=24
05/12/2022 15:46:34 - INFO - __main__ - Global step 50 Train loss 1.29 ACC 0.15625 on epoch=24
05/12/2022 15:46:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 15:46:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
05/12/2022 15:46:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
05/12/2022 15:46:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
05/12/2022 15:46:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
05/12/2022 15:46:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
05/12/2022 15:46:59 - INFO - __main__ - Global step 100 Train loss 0.54 ACC 0.1875 on epoch=49
05/12/2022 15:46:59 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=49, global_step=100
05/12/2022 15:47:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
05/12/2022 15:47:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=59
05/12/2022 15:47:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.35 on epoch=64
05/12/2022 15:47:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
05/12/2022 15:47:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
05/12/2022 15:47:25 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.3125 on epoch=74
05/12/2022 15:47:25 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.3125 on epoch=74, global_step=150
05/12/2022 15:47:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
05/12/2022 15:47:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
05/12/2022 15:47:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
05/12/2022 15:47:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
05/12/2022 15:47:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
05/12/2022 15:47:50 - INFO - __main__ - Global step 200 Train loss 0.32 ACC 0.21875 on epoch=99
05/12/2022 15:47:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
05/12/2022 15:47:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=109
05/12/2022 15:48:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
05/12/2022 15:48:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
05/12/2022 15:48:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
05/12/2022 15:48:16 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.21875 on epoch=124
05/12/2022 15:48:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
05/12/2022 15:48:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
05/12/2022 15:48:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
05/12/2022 15:48:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
05/12/2022 15:48:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
05/12/2022 15:48:41 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.1875 on epoch=149
05/12/2022 15:48:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
05/12/2022 15:48:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=159
05/12/2022 15:48:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
05/12/2022 15:48:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
05/12/2022 15:49:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
05/12/2022 15:49:06 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.15625 on epoch=174
05/12/2022 15:49:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
05/12/2022 15:49:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
05/12/2022 15:49:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
05/12/2022 15:49:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
05/12/2022 15:49:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
05/12/2022 15:49:32 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.15625 on epoch=199
05/12/2022 15:49:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
05/12/2022 15:49:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
05/12/2022 15:49:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
05/12/2022 15:49:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
05/12/2022 15:49:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
05/12/2022 15:49:57 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.1875 on epoch=224
05/12/2022 15:50:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
05/12/2022 15:50:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
05/12/2022 15:50:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
05/12/2022 15:50:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
05/12/2022 15:50:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
05/12/2022 15:50:23 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.21875 on epoch=249
05/12/2022 15:50:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
05/12/2022 15:50:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
05/12/2022 15:50:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
05/12/2022 15:50:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
05/12/2022 15:50:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/12/2022 15:50:48 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.15625 on epoch=274
05/12/2022 15:50:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
05/12/2022 15:50:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
05/12/2022 15:51:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
05/12/2022 15:51:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
05/12/2022 15:51:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
05/12/2022 15:51:13 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.21875 on epoch=299
05/12/2022 15:51:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
05/12/2022 15:51:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
05/12/2022 15:51:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
05/12/2022 15:51:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
05/12/2022 15:51:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/12/2022 15:51:38 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.15625 on epoch=324
05/12/2022 15:51:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/12/2022 15:51:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
05/12/2022 15:51:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
05/12/2022 15:51:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
05/12/2022 15:52:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
05/12/2022 15:52:04 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.15625 on epoch=349
05/12/2022 15:52:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
05/12/2022 15:52:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
05/12/2022 15:52:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
05/12/2022 15:52:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/12/2022 15:52:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
05/12/2022 15:52:29 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.25 on epoch=374
05/12/2022 15:52:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
05/12/2022 15:52:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
05/12/2022 15:52:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
05/12/2022 15:52:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
05/12/2022 15:52:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
05/12/2022 15:52:54 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.28125 on epoch=399
05/12/2022 15:52:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
05/12/2022 15:53:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
05/12/2022 15:53:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
05/12/2022 15:53:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
05/12/2022 15:53:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
05/12/2022 15:53:19 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.1875 on epoch=424
05/12/2022 15:53:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
05/12/2022 15:53:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
05/12/2022 15:53:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
05/12/2022 15:53:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 15:53:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
05/12/2022 15:53:44 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.25 on epoch=449
05/12/2022 15:53:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
05/12/2022 15:53:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
05/12/2022 15:53:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 15:54:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
05/12/2022 15:54:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
05/12/2022 15:54:09 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.21875 on epoch=474
05/12/2022 15:54:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
05/12/2022 15:54:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
05/12/2022 15:54:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
05/12/2022 15:54:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
05/12/2022 15:54:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
05/12/2022 15:54:35 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.1875 on epoch=499
05/12/2022 15:54:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
05/12/2022 15:54:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
05/12/2022 15:54:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
05/12/2022 15:54:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
05/12/2022 15:54:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
05/12/2022 15:55:00 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.21875 on epoch=524
05/12/2022 15:55:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
05/12/2022 15:55:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/12/2022 15:55:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
05/12/2022 15:55:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
05/12/2022 15:55:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
05/12/2022 15:55:25 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.1875 on epoch=549
05/12/2022 15:55:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 15:55:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
05/12/2022 15:55:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 15:55:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
05/12/2022 15:55:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/12/2022 15:55:50 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
05/12/2022 15:55:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
05/12/2022 15:55:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
05/12/2022 15:56:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
05/12/2022 15:56:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
05/12/2022 15:56:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
05/12/2022 15:56:15 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.15625 on epoch=599
05/12/2022 15:56:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 15:56:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
05/12/2022 15:56:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 15:56:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
05/12/2022 15:56:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
05/12/2022 15:56:40 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
05/12/2022 15:56:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
05/12/2022 15:56:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 15:56:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 15:56:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
05/12/2022 15:57:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/12/2022 15:57:05 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.1875 on epoch=649
05/12/2022 15:57:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
05/12/2022 15:57:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
05/12/2022 15:57:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
05/12/2022 15:57:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
05/12/2022 15:57:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
05/12/2022 15:57:30 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.25 on epoch=674
05/12/2022 15:57:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
05/12/2022 15:57:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
05/12/2022 15:57:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/12/2022 15:57:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
05/12/2022 15:57:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
05/12/2022 15:57:55 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.1875 on epoch=699
05/12/2022 15:58:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
05/12/2022 15:58:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/12/2022 15:58:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
05/12/2022 15:58:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 15:58:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 15:58:21 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.1875 on epoch=724
05/12/2022 15:58:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 15:58:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/12/2022 15:58:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
05/12/2022 15:58:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
05/12/2022 15:58:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
05/12/2022 15:58:46 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
05/12/2022 15:58:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 15:58:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
05/12/2022 15:58:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
05/12/2022 15:59:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 15:59:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 15:59:11 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.1875 on epoch=774
05/12/2022 15:59:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
05/12/2022 15:59:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
05/12/2022 15:59:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/12/2022 15:59:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
05/12/2022 15:59:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 15:59:36 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.21875 on epoch=799
05/12/2022 15:59:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
05/12/2022 15:59:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
05/12/2022 15:59:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
05/12/2022 15:59:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/12/2022 15:59:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 16:00:02 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
05/12/2022 16:00:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 16:00:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 16:00:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 16:00:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 16:00:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
05/12/2022 16:00:27 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.1875 on epoch=849
05/12/2022 16:00:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 16:00:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 16:00:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
05/12/2022 16:00:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
05/12/2022 16:00:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
05/12/2022 16:00:52 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.21875 on epoch=874
05/12/2022 16:00:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/12/2022 16:01:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
05/12/2022 16:01:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/12/2022 16:01:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/12/2022 16:01:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 16:01:17 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
05/12/2022 16:01:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 16:01:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 16:01:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
05/12/2022 16:01:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 16:01:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
05/12/2022 16:01:42 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.15625 on epoch=924
05/12/2022 16:01:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 16:01:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
05/12/2022 16:01:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/12/2022 16:02:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
05/12/2022 16:02:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 16:02:08 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.3125 on epoch=949
05/12/2022 16:02:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 16:02:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 16:02:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 16:02:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 16:02:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
05/12/2022 16:02:33 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.1875 on epoch=974
05/12/2022 16:02:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 16:02:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 16:02:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 16:02:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 16:02:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 16:02:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:02:56 - INFO - __main__ - Printing 3 examples
05/12/2022 16:02:56 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:02:56 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:02:56 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:02:56 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:02:56 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:02:56 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:02:56 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:02:56 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:02:56 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:02:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:02:56 - INFO - __main__ - Printing 3 examples
05/12/2022 16:02:56 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:02:56 - INFO - __main__ - ['5 minutes']
05/12/2022 16:02:56 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:02:56 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:02:56 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:02:56 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:02:56 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:02:56 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:02:56 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:02:58 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.28125 on epoch=999
05/12/2022 16:02:58 - INFO - __main__ - save last model!
05/12/2022 16:02:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 16:02:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 16:02:58 - INFO - __main__ - Printing 3 examples
05/12/2022 16:02:58 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:02:58 - INFO - __main__ - ['about 10 minutes']
05/12/2022 16:02:58 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:02:58 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 16:02:58 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:02:58 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 16:02:58 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:03:00 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:03:01 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 16:03:14 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:03:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:03:15 - INFO - __main__ - Starting training!
05/12/2022 16:04:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.5_8_predictions.txt
05/12/2022 16:04:33 - INFO - __main__ - ACC on test data: 0.2600
05/12/2022 16:04:33 - INFO - __main__ - prefix=quail_32_100, lr=0.5, bsz=8, dev_performance=0.3125, test_performance=0.26
05/12/2022 16:04:33 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.4, bsz=8 ...
05/12/2022 16:04:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:04:34 - INFO - __main__ - Printing 3 examples
05/12/2022 16:04:34 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:04:34 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:04:34 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:04:34 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:04:34 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:04:34 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:04:34 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:04:34 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:04:34 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:04:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:04:34 - INFO - __main__ - Printing 3 examples
05/12/2022 16:04:34 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:04:34 - INFO - __main__ - ['5 minutes']
05/12/2022 16:04:34 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:04:34 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:04:34 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:04:34 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:04:34 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:04:34 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:04:34 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:04:49 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:04:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:04:50 - INFO - __main__ - Starting training!
05/12/2022 16:04:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.08 on epoch=4
05/12/2022 16:05:00 - INFO - __main__ - Step 20 Global step 20 Train loss 1.62 on epoch=9
05/12/2022 16:05:04 - INFO - __main__ - Step 30 Global step 30 Train loss 2.11 on epoch=14
05/12/2022 16:05:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.80 on epoch=19
05/12/2022 16:05:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.30 on epoch=24
05/12/2022 16:05:16 - INFO - __main__ - Global step 50 Train loss 1.78 ACC 0.15625 on epoch=24
05/12/2022 16:05:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 16:05:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.24 on epoch=29
05/12/2022 16:05:25 - INFO - __main__ - Step 70 Global step 70 Train loss 1.20 on epoch=34
05/12/2022 16:05:29 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=39
05/12/2022 16:05:34 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=44
05/12/2022 16:05:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=49
05/12/2022 16:05:41 - INFO - __main__ - Global step 100 Train loss 1.08 ACC 0.21875 on epoch=49
05/12/2022 16:05:42 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=49, global_step=100
05/12/2022 16:05:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=54
05/12/2022 16:05:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=59
05/12/2022 16:05:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=64
05/12/2022 16:05:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.74 on epoch=69
05/12/2022 16:06:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=74
05/12/2022 16:06:07 - INFO - __main__ - Global step 150 Train loss 0.79 ACC 0.125 on epoch=74
05/12/2022 16:06:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=79
05/12/2022 16:06:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=84
05/12/2022 16:06:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=89
05/12/2022 16:06:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=94
05/12/2022 16:06:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=99
05/12/2022 16:06:32 - INFO - __main__ - Global step 200 Train loss 0.65 ACC 0.1875 on epoch=99
05/12/2022 16:06:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=104
05/12/2022 16:06:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=109
05/12/2022 16:06:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=114
05/12/2022 16:06:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=119
05/12/2022 16:06:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
05/12/2022 16:06:57 - INFO - __main__ - Global step 250 Train loss 0.50 ACC 0.15625 on epoch=124
05/12/2022 16:07:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=129
05/12/2022 16:07:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=134
05/12/2022 16:07:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
05/12/2022 16:07:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
05/12/2022 16:07:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
05/12/2022 16:07:23 - INFO - __main__ - Global step 300 Train loss 0.43 ACC 0.21875 on epoch=149
05/12/2022 16:07:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
05/12/2022 16:07:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
05/12/2022 16:07:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
05/12/2022 16:07:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=169
05/12/2022 16:07:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
05/12/2022 16:07:49 - INFO - __main__ - Global step 350 Train loss 0.36 ACC 0.25 on epoch=174
05/12/2022 16:07:49 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=174, global_step=350
05/12/2022 16:07:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
05/12/2022 16:07:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
05/12/2022 16:08:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
05/12/2022 16:08:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
05/12/2022 16:08:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
05/12/2022 16:08:14 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.21875 on epoch=199
05/12/2022 16:08:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
05/12/2022 16:08:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
05/12/2022 16:08:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
05/12/2022 16:08:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
05/12/2022 16:08:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=224
05/12/2022 16:08:40 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.1875 on epoch=224
05/12/2022 16:08:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
05/12/2022 16:08:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
05/12/2022 16:08:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
05/12/2022 16:08:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
05/12/2022 16:09:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
05/12/2022 16:09:06 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.21875 on epoch=249
05/12/2022 16:09:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
05/12/2022 16:09:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=259
05/12/2022 16:09:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=264
05/12/2022 16:09:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
05/12/2022 16:09:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=274
05/12/2022 16:09:31 - INFO - __main__ - Global step 550 Train loss 0.19 ACC 0.15625 on epoch=274
05/12/2022 16:09:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
05/12/2022 16:09:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
05/12/2022 16:09:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
05/12/2022 16:09:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
05/12/2022 16:09:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
05/12/2022 16:09:57 - INFO - __main__ - Global step 600 Train loss 0.15 ACC 0.21875 on epoch=299
05/12/2022 16:10:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
05/12/2022 16:10:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
05/12/2022 16:10:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=314
05/12/2022 16:10:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
05/12/2022 16:10:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
05/12/2022 16:10:22 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.15625 on epoch=324
05/12/2022 16:10:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
05/12/2022 16:10:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
05/12/2022 16:10:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
05/12/2022 16:10:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
05/12/2022 16:10:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
05/12/2022 16:10:48 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.1875 on epoch=349
05/12/2022 16:10:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
05/12/2022 16:10:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=359
05/12/2022 16:11:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
05/12/2022 16:11:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
05/12/2022 16:11:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
05/12/2022 16:11:13 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.21875 on epoch=374
05/12/2022 16:11:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
05/12/2022 16:11:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
05/12/2022 16:11:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
05/12/2022 16:11:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=394
05/12/2022 16:11:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
05/12/2022 16:11:38 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.1875 on epoch=399
05/12/2022 16:11:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
05/12/2022 16:11:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
05/12/2022 16:11:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
05/12/2022 16:11:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
05/12/2022 16:12:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
05/12/2022 16:12:04 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.21875 on epoch=424
05/12/2022 16:12:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 16:12:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
05/12/2022 16:12:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=439
05/12/2022 16:12:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
05/12/2022 16:12:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
05/12/2022 16:12:29 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.1875 on epoch=449
05/12/2022 16:12:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
05/12/2022 16:12:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
05/12/2022 16:12:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 16:12:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
05/12/2022 16:12:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
05/12/2022 16:12:54 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.21875 on epoch=474
05/12/2022 16:12:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
05/12/2022 16:13:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/12/2022 16:13:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
05/12/2022 16:13:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
05/12/2022 16:13:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=499
05/12/2022 16:13:20 - INFO - __main__ - Global step 1000 Train loss 0.21 ACC 0.25 on epoch=499
05/12/2022 16:13:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
05/12/2022 16:13:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/12/2022 16:13:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
05/12/2022 16:13:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
05/12/2022 16:13:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
05/12/2022 16:13:46 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.1875 on epoch=524
05/12/2022 16:13:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
05/12/2022 16:13:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
05/12/2022 16:13:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
05/12/2022 16:14:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
05/12/2022 16:14:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
05/12/2022 16:14:11 - INFO - __main__ - Global step 1100 Train loss 0.06 ACC 0.25 on epoch=549
05/12/2022 16:14:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 16:14:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
05/12/2022 16:14:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 16:14:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
05/12/2022 16:14:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
05/12/2022 16:14:37 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.25 on epoch=574
05/12/2022 16:14:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
05/12/2022 16:14:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
05/12/2022 16:14:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
05/12/2022 16:14:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 16:15:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
05/12/2022 16:15:03 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.1875 on epoch=599
05/12/2022 16:15:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
05/12/2022 16:15:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
05/12/2022 16:15:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
05/12/2022 16:15:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
05/12/2022 16:15:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
05/12/2022 16:15:28 - INFO - __main__ - Global step 1250 Train loss 0.06 ACC 0.15625 on epoch=624
05/12/2022 16:15:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 16:15:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 16:15:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
05/12/2022 16:15:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
05/12/2022 16:15:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
05/12/2022 16:15:54 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.21875 on epoch=649
05/12/2022 16:15:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/12/2022 16:16:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
05/12/2022 16:16:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
05/12/2022 16:16:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
05/12/2022 16:16:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
05/12/2022 16:16:19 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.1875 on epoch=674
05/12/2022 16:16:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
05/12/2022 16:16:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
05/12/2022 16:16:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
05/12/2022 16:16:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
05/12/2022 16:16:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
05/12/2022 16:16:45 - INFO - __main__ - Global step 1400 Train loss 0.05 ACC 0.28125 on epoch=699
05/12/2022 16:16:45 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=699, global_step=1400
05/12/2022 16:16:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
05/12/2022 16:16:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 16:16:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
05/12/2022 16:17:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
05/12/2022 16:17:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 16:17:11 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.25 on epoch=724
05/12/2022 16:17:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
05/12/2022 16:17:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
05/12/2022 16:17:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/12/2022 16:17:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
05/12/2022 16:17:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 16:17:36 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.21875 on epoch=749
05/12/2022 16:17:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
05/12/2022 16:17:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 16:17:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
05/12/2022 16:17:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 16:17:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=774
05/12/2022 16:18:02 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.21875 on epoch=774
05/12/2022 16:18:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
05/12/2022 16:18:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 16:18:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
05/12/2022 16:18:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 16:18:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 16:18:28 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.28125 on epoch=799
05/12/2022 16:18:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
05/12/2022 16:18:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
05/12/2022 16:18:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 16:18:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 16:18:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
05/12/2022 16:18:53 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.1875 on epoch=824
05/12/2022 16:18:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 16:19:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 16:19:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 16:19:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
05/12/2022 16:19:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
05/12/2022 16:19:19 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.21875 on epoch=849
05/12/2022 16:19:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 16:19:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 16:19:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
05/12/2022 16:19:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 16:19:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 16:19:44 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.25 on epoch=874
05/12/2022 16:19:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
05/12/2022 16:19:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
05/12/2022 16:19:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
05/12/2022 16:20:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 16:20:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/12/2022 16:20:10 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.1875 on epoch=899
05/12/2022 16:20:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 16:20:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
05/12/2022 16:20:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
05/12/2022 16:20:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
05/12/2022 16:20:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
05/12/2022 16:20:35 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.21875 on epoch=924
05/12/2022 16:20:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 16:20:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
05/12/2022 16:20:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
05/12/2022 16:20:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 16:20:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
05/12/2022 16:21:01 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.25 on epoch=949
05/12/2022 16:21:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 16:21:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
05/12/2022 16:21:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
05/12/2022 16:21:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/12/2022 16:21:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
05/12/2022 16:21:26 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.1875 on epoch=974
05/12/2022 16:21:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 16:21:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 16:21:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 16:21:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 16:21:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 16:21:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:21:51 - INFO - __main__ - Printing 3 examples
05/12/2022 16:21:51 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:21:51 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:21:51 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:21:51 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:21:51 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:21:51 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:21:51 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:21:51 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:21:51 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:21:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:21:51 - INFO - __main__ - Printing 3 examples
05/12/2022 16:21:51 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:21:51 - INFO - __main__ - ['5 minutes']
05/12/2022 16:21:51 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:21:51 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:21:51 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:21:51 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:21:51 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:21:51 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:21:51 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:21:52 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.21875 on epoch=999
05/12/2022 16:21:52 - INFO - __main__ - save last model!
05/12/2022 16:21:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 16:21:52 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 16:21:52 - INFO - __main__ - Printing 3 examples
05/12/2022 16:21:52 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:21:52 - INFO - __main__ - ['about 10 minutes']
05/12/2022 16:21:52 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:21:52 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 16:21:52 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:21:52 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 16:21:52 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:21:54 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:21:55 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 16:22:09 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:22:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:22:10 - INFO - __main__ - Starting training!
05/12/2022 16:23:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.4_8_predictions.txt
05/12/2022 16:23:25 - INFO - __main__ - ACC on test data: 0.2720
05/12/2022 16:23:25 - INFO - __main__ - prefix=quail_32_100, lr=0.4, bsz=8, dev_performance=0.28125, test_performance=0.272
05/12/2022 16:23:25 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.3, bsz=8 ...
05/12/2022 16:23:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:23:26 - INFO - __main__ - Printing 3 examples
05/12/2022 16:23:26 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:23:26 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:23:26 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:23:26 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:23:26 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:23:26 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:23:26 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:23:26 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:23:26 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:23:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:23:26 - INFO - __main__ - Printing 3 examples
05/12/2022 16:23:26 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:23:26 - INFO - __main__ - ['5 minutes']
05/12/2022 16:23:26 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:23:26 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:23:26 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:23:26 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:23:26 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:23:26 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:23:26 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:23:41 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:23:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:23:42 - INFO - __main__ - Starting training!
05/12/2022 16:23:47 - INFO - __main__ - Step 10 Global step 10 Train loss 1.96 on epoch=4
05/12/2022 16:23:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.53 on epoch=9
05/12/2022 16:23:56 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=14
05/12/2022 16:24:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=19
05/12/2022 16:24:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=24
05/12/2022 16:24:07 - INFO - __main__ - Global step 50 Train loss 1.36 ACC 0.09375 on epoch=24
05/12/2022 16:24:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.09375 on epoch=24, global_step=50
05/12/2022 16:24:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=29
05/12/2022 16:24:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
05/12/2022 16:24:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
05/12/2022 16:24:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=44
05/12/2022 16:24:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=49
05/12/2022 16:24:32 - INFO - __main__ - Global step 100 Train loss 0.65 ACC 0.1875 on epoch=49
05/12/2022 16:24:32 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.1875 on epoch=49, global_step=100
05/12/2022 16:24:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
05/12/2022 16:24:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
05/12/2022 16:24:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
05/12/2022 16:24:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
05/12/2022 16:24:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
05/12/2022 16:24:57 - INFO - __main__ - Global step 150 Train loss 0.45 ACC 0.1875 on epoch=74
05/12/2022 16:25:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
05/12/2022 16:25:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
05/12/2022 16:25:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
05/12/2022 16:25:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
05/12/2022 16:25:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
05/12/2022 16:25:22 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.125 on epoch=99
05/12/2022 16:25:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
05/12/2022 16:25:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
05/12/2022 16:25:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
05/12/2022 16:25:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
05/12/2022 16:25:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
05/12/2022 16:25:47 - INFO - __main__ - Global step 250 Train loss 0.33 ACC 0.125 on epoch=124
05/12/2022 16:25:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
05/12/2022 16:25:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
05/12/2022 16:26:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
05/12/2022 16:26:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=144
05/12/2022 16:26:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
05/12/2022 16:26:12 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.1875 on epoch=149
05/12/2022 16:26:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
05/12/2022 16:26:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
05/12/2022 16:26:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
05/12/2022 16:26:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
05/12/2022 16:26:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=174
05/12/2022 16:26:37 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.125 on epoch=174
05/12/2022 16:26:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=179
05/12/2022 16:26:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
05/12/2022 16:26:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
05/12/2022 16:26:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
05/12/2022 16:26:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
05/12/2022 16:27:02 - INFO - __main__ - Global step 400 Train loss 0.18 ACC 0.1875 on epoch=199
05/12/2022 16:27:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=204
05/12/2022 16:27:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
05/12/2022 16:27:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
05/12/2022 16:27:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
05/12/2022 16:27:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
05/12/2022 16:27:27 - INFO - __main__ - Global step 450 Train loss 0.15 ACC 0.1875 on epoch=224
05/12/2022 16:27:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
05/12/2022 16:27:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
05/12/2022 16:27:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
05/12/2022 16:27:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
05/12/2022 16:27:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
05/12/2022 16:27:52 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.15625 on epoch=249
05/12/2022 16:27:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
05/12/2022 16:28:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
05/12/2022 16:28:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
05/12/2022 16:28:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
05/12/2022 16:28:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
05/12/2022 16:28:17 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.125 on epoch=274
05/12/2022 16:28:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
05/12/2022 16:28:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
05/12/2022 16:28:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
05/12/2022 16:28:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
05/12/2022 16:28:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
05/12/2022 16:28:43 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.1875 on epoch=299
05/12/2022 16:28:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
05/12/2022 16:28:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
05/12/2022 16:28:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
05/12/2022 16:29:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
05/12/2022 16:29:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
05/12/2022 16:29:08 - INFO - __main__ - Global step 650 Train loss 0.08 ACC 0.15625 on epoch=324
05/12/2022 16:29:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
05/12/2022 16:29:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
05/12/2022 16:29:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
05/12/2022 16:29:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
05/12/2022 16:29:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
05/12/2022 16:29:33 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.15625 on epoch=349
05/12/2022 16:29:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
05/12/2022 16:29:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
05/12/2022 16:29:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
05/12/2022 16:29:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
05/12/2022 16:29:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
05/12/2022 16:29:59 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.1875 on epoch=374
05/12/2022 16:30:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
05/12/2022 16:30:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
05/12/2022 16:30:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
05/12/2022 16:30:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
05/12/2022 16:30:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
05/12/2022 16:30:24 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.21875 on epoch=399
05/12/2022 16:30:24 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=399, global_step=800
05/12/2022 16:30:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
05/12/2022 16:30:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
05/12/2022 16:30:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/12/2022 16:30:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
05/12/2022 16:30:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
05/12/2022 16:30:49 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.28125 on epoch=424
05/12/2022 16:30:49 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=424, global_step=850
05/12/2022 16:30:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/12/2022 16:30:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
05/12/2022 16:31:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
05/12/2022 16:31:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
05/12/2022 16:31:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
05/12/2022 16:31:15 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.1875 on epoch=449
05/12/2022 16:31:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
05/12/2022 16:31:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
05/12/2022 16:31:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
05/12/2022 16:31:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
05/12/2022 16:31:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
05/12/2022 16:31:40 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.25 on epoch=474
05/12/2022 16:31:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
05/12/2022 16:31:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
05/12/2022 16:31:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
05/12/2022 16:31:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 16:32:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
05/12/2022 16:32:05 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.25 on epoch=499
05/12/2022 16:32:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/12/2022 16:32:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
05/12/2022 16:32:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
05/12/2022 16:32:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
05/12/2022 16:32:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
05/12/2022 16:32:30 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.21875 on epoch=524
05/12/2022 16:32:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/12/2022 16:32:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
05/12/2022 16:32:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
05/12/2022 16:32:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
05/12/2022 16:32:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/12/2022 16:32:55 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.25 on epoch=549
05/12/2022 16:33:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 16:33:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
05/12/2022 16:33:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 16:33:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
05/12/2022 16:33:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
05/12/2022 16:33:21 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.1875 on epoch=574
05/12/2022 16:33:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
05/12/2022 16:33:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 16:33:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
05/12/2022 16:33:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 16:33:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
05/12/2022 16:33:46 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.25 on epoch=599
05/12/2022 16:33:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
05/12/2022 16:33:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
05/12/2022 16:33:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
05/12/2022 16:34:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
05/12/2022 16:34:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
05/12/2022 16:34:11 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
05/12/2022 16:34:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
05/12/2022 16:34:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 16:34:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
05/12/2022 16:34:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
05/12/2022 16:34:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/12/2022 16:34:36 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.15625 on epoch=649
05/12/2022 16:34:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/12/2022 16:34:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
05/12/2022 16:34:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 16:34:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
05/12/2022 16:34:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 16:35:01 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.21875 on epoch=674
05/12/2022 16:35:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
05/12/2022 16:35:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/12/2022 16:35:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
05/12/2022 16:35:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
05/12/2022 16:35:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 16:35:26 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.25 on epoch=699
05/12/2022 16:35:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
05/12/2022 16:35:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/12/2022 16:35:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
05/12/2022 16:35:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/12/2022 16:35:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 16:35:52 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
05/12/2022 16:35:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 16:36:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 16:36:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/12/2022 16:36:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 16:36:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
05/12/2022 16:36:17 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.21875 on epoch=749
05/12/2022 16:36:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 16:36:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 16:36:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
05/12/2022 16:36:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
05/12/2022 16:36:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 16:36:42 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.25 on epoch=774
05/12/2022 16:36:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 16:36:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 16:36:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 16:37:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/12/2022 16:37:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
05/12/2022 16:37:07 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
05/12/2022 16:37:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 16:37:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
05/12/2022 16:37:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 16:37:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/12/2022 16:37:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
05/12/2022 16:37:32 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.21875 on epoch=824
05/12/2022 16:37:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 16:37:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 16:37:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
05/12/2022 16:37:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 16:37:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 16:37:58 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.25 on epoch=849
05/12/2022 16:38:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 16:38:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 16:38:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
05/12/2022 16:38:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 16:38:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
05/12/2022 16:38:23 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.15625 on epoch=874
05/12/2022 16:38:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
05/12/2022 16:38:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 16:38:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 16:38:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
05/12/2022 16:38:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
05/12/2022 16:38:48 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.21875 on epoch=899
05/12/2022 16:38:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 16:38:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 16:39:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
05/12/2022 16:39:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 16:39:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 16:39:13 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
05/12/2022 16:39:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 16:39:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
05/12/2022 16:39:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
05/12/2022 16:39:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
05/12/2022 16:39:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 16:39:38 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.1875 on epoch=949
05/12/2022 16:39:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 16:39:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 16:39:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 16:39:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
05/12/2022 16:40:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 16:40:03 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.21875 on epoch=974
05/12/2022 16:40:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 16:40:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 16:40:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 16:40:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 16:40:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
05/12/2022 16:40:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:40:27 - INFO - __main__ - Printing 3 examples
05/12/2022 16:40:27 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:40:27 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:40:27 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:40:27 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:40:27 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:40:27 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:40:27 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:40:27 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:40:27 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:40:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:40:27 - INFO - __main__ - Printing 3 examples
05/12/2022 16:40:27 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:40:27 - INFO - __main__ - ['5 minutes']
05/12/2022 16:40:27 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:40:27 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:40:27 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:40:27 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:40:27 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:40:27 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:40:27 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:40:29 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.21875 on epoch=999
05/12/2022 16:40:29 - INFO - __main__ - save last model!
05/12/2022 16:40:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 16:40:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 16:40:29 - INFO - __main__ - Printing 3 examples
05/12/2022 16:40:29 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:40:29 - INFO - __main__ - ['about 10 minutes']
05/12/2022 16:40:29 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:40:29 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 16:40:29 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:40:29 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 16:40:29 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:40:30 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:40:31 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 16:40:45 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:40:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:40:45 - INFO - __main__ - Starting training!
05/12/2022 16:42:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.3_8_predictions.txt
05/12/2022 16:42:06 - INFO - __main__ - ACC on test data: 0.2810
05/12/2022 16:42:07 - INFO - __main__ - prefix=quail_32_100, lr=0.3, bsz=8, dev_performance=0.28125, test_performance=0.281
05/12/2022 16:42:07 - INFO - __main__ - Running ... prefix=quail_32_100, lr=0.2, bsz=8 ...
05/12/2022 16:42:08 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:42:08 - INFO - __main__ - Printing 3 examples
05/12/2022 16:42:08 - INFO - __main__ -  [quail] What does Tommy probably not like?(A)Looking stupid(B)Dogs(C)not enough information(D)Treasure [SEP] "If you can answer three questions," the dog said, "you can wear the magic shoes." Tommy looked up and down the deserted street. "Did you ... say something?" "That's right. Didn't you hear me?" It was a gruff voice, with just a trace of an English accent, and it was definitely coming out of the dog. "You're a dog." In fact it was a huge, fat bulldog, with big flaps of skin hanging off the sides of its face. From where it sat, on the front steps of the abandoned motel, it looked Tommy straight in the eye. "That's correct," the dog said. Tommy stared hard at the dusty windows of the motel office. "This is a trick, right? There's a TV camera back there and you want to make me look stupid." "No tricks, Tommy. Just three questions." "C'mon," Tommy said. He deepened his voice. "Sit up." The dog stared at him. "Roll over. Play dead." "Cut the crap, Tommy. Do you want the shoes or not?" "Let me see 'em." The dog shifted its weight to one side, revealing a battered pair of red Converse All-Stars. "Yuck," Tommy said. "Those are gross." "Maybe," the dog said, "but they're magic." "What are the questions?" "Which of the following presidents died in office? Lincoln, McKinley, F.D.R.?" "C'mon. They all did. That's the same dumb question they use when they're trying to sell you a free portrait on the telephone." "Which weighs more, a pound of feathers or a pound of lead?" "They both weigh a pound. This is stupid. Next you're going to ask me who's buried in Grant's Tomb." The dog narrowed its eyes. "Have you done this before?" "Ulysses S. Grant," Tommy said. "Lemme see the shoes." They were just his size and felt pretty good, even though they were scuffed up and the metal things were gone out of the side vents. "I don't feel any different," Tommy said. "You need the shoes to look for the treasure," the dog said. "What treasure?" "When you're wearing the shoes, you can open the doors of the motel rooms."
05/12/2022 16:42:08 - INFO - __main__ - ['Looking stupid']
05/12/2022 16:42:08 - INFO - __main__ -  [quail] Trumps recorded endorsement for Roy Moore likely lasted:(A)not enough information(B)Around 30 minutes.(C)Around 45 minutes.(D)Under 10 minutes. [SEP] The southern state of Alabama is the center of the U.S. political universe this week as voters on Tuesday choose a senator to replace Jeff Sessions, who left the Senate to become attorney general. The race pits controversial Republican Roy Moore, who is battling sexual harassment allegations, against Democrat Doug Jones, a former prosecutor. The outcome of the race could have national implications for both political parties and for President Donald Trump. Moore has denied several allegations of sexual misconduct when he was in his 30s involving women who were teenagers at the time, including one who was 14. "I do not know them. I had no encounter with them. I never molested anyone," Moore said in a televised interview Sunday with the Voice of Alabama Politics.  Jones says the accusations make Moore unfit to serve in the Senate. "It is crystal clear that these women are telling the truth and Roy Moore is not!" Jones said. Trump recorded a get-out-the-vote phone message for Moore and spoke on his behalf at a rally in neighboring Florida on Friday. "And we want jobs, jobs, jobs. So get out and vote for Roy Moore. Do it. Do it," he said. Trump held off on endorsing Moore for several weeks in the wake of the sexual misconduct allegations, but now says electing Moore is a priority for him. "We certainly don't want to have a liberal Democrat who is controlled by Nancy Pelosi and controlled by Chuck Schumer. We don't want to have that for Alabama," Trump said. In the final days of the campaign, Moore is highlighting his support for the president's agenda. "We are going to see if the people of Alabama will support the president and support his agenda in Washington by electing somebody that is not part of the establishment there," Moore said.  Democrat Jones told supporters that Moore's character is the issue. "We know who we are, Alabama, we know who we are. This is an election to tell the world who we are and what we stand for."
05/12/2022 16:42:08 - INFO - __main__ - ['Under 10 minutes.']
05/12/2022 16:42:08 - INFO - __main__ -  [quail] How long did the writer probably use the Mild Shampoo and Conditioner?(A)A few days(B)Under a month(C)For a couple weeks(D)not enough information [SEP] Those of you who are regular readers of Beauty Best Friend will know that I suffer from a very sensitive, itchy scalp and I am constantly on the hunt for haircare products that are natural and non-irritating but that also treat my hair well and leave it feeling soft, shiny and clean. So far my experience has generally shown me that natural, SLS-free shampoos and conditioners do not irritate the scalp as much as their chemical filled cousins, but that they do not always clean the hair as well and can leave it looking and feeling greasy, sad and lifeless. One of the first SLS-free shampoo and conditioners that I tried, back in 2013, was Mild Shampoo and Gentle Conditioner from Naked. The relief that I got from my itchy scalp was almost instant, but I did find that it didn’t remove grease and oil from my hair too well, and I had to wash my hair a lot more often. Since then I’ve tried lots of different SLS-free haircare products, all of which have had their benefits and downfalls. For the past month I have been using Rescue Intensive Care Shampoo & Conditioner from Naked, aimed at frizzy, dry and damaged hair. As I had found such relief from my itchy scalp when using Naked products previously I wanted to try out another variant to see if it cleaned my hair any better. Prior to using the Rescue duo I had been having a really hard time with my scalp, but after just the first use of these natural products the itching had subsided about by 75%. Both the shampoo and conditioner have a lovely rich almond scent which stays on the hair after it is dry. The conditioner is a thick, rich cream and it feels like it is giving dry hair a real treat. Unfortunately these Naked products still don’t clean my hair as well as some other products, and I still feel that my hair can look greasy and lank the day after I’ve washed it. I have tried the ‘reverse poo’ method which helps a bit – this means conditioning your hair first, then shampooing it second – but my hair can get very tangled after the shampooing stage.
05/12/2022 16:42:08 - INFO - __main__ - ['For a couple weeks']
05/12/2022 16:42:08 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:42:08 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:42:08 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:42:08 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:42:08 - INFO - __main__ - Printing 3 examples
05/12/2022 16:42:08 - INFO - __main__ -  [quail] How long was Carnie in the house before she slammed Judy:(A)20 minutes(B)5 minutes(C)not enough information(D)1 hour [SEP] "It's about to come on. Hurry." "I'm coming." Nurse Judy delivered Martha's tray just in time.  It was a frozen dinner, but Judy always transferred it to a fancy plate and prepared a small salad and a bowl of applesauce to go alongside it. "Looks great, Judy.  Now sit down and let's eat." Nurse Judy sat down in the recliner next to Martha's bed. The meal she made for herself was similar to Martha's. "Didn't we just see this one a few days ago?" "I don't remember. But you know it doesn't matter. I love Jessica Fletcher." It was the only good thing about her failing memory. She could watch reruns of Murder She Wrote over and over again. They were all new to her. The doorbell rang. "Whoever it is, just get rid of them. It couldn't be friends or family. They know better than to interrupt my show." Nurse Judy walked down the hallway to the front door. It was a nurse. "May I help you?" "The agency sent me." "No, there must be some mistake. I've been caring for Mrs. Mason for a couple of months now." "Oh, great. Why do they keep doing this to me? Mind if I come in and use the phone?" "Don't you have a cell phone?" "Yeah, but it's dead. I forgot to charge it last night." "I hate when I do that. Sure, come on in. What's your name?" "Carnie." "Good to meet you, Carnie. I'm Judy. You can use the house phone." Judy led her to the phone. Carnie picked up the receiver and began to dial. But as Judy turned to walk away, Carnie slammed the phone across the back of her head. Nurse Judy collapsed to the floor, unconscious. Martha's blaring TV masked the noise.
05/12/2022 16:42:08 - INFO - __main__ - ['5 minutes']
05/12/2022 16:42:08 - INFO - __main__ -  [quail] When did the man find his true diagnosis?(A)before talking to Dr. Douche(B)after going to Mayo Clinic(C)not enough information(D)after talking to Dr. Douche [SEP] I have a chronic illness, and so I received quite a few sterling gems in the months between onset and accurate diagnosis. I had one GP — let’s call him Dr Douche. I promise, it’s the kindest way I could describe him. “The jacket means I know things.” He came up with wilder and wilder theories as to why I was sick, and kept getting sicker. It should be said beforehand that few of these theories were embodied. He was sure it was something in my ladybrains that was preventing proper function of the rest of me. Dr Douche said so much weird, wild, and just-plain-unbelievable crap over the course of my diagnosis that someday I may create a novel that incorporates it all. But this here has to be the blue ribbon winner. I was describing a symptom to him: every time I got up in the morning, when I first stood, my heart would pound, my heart rate would accelerate, and I’d feel dizzy. After a few probing questions — (only in the morning? — no, but it’s worst in the morning)… “Maybe,” he said sensitively, “you’re afraid.” “Of standing?” I asked, just to be sure. “You think I’m afraid of… standing up.” Maybe he meant there was some kind of existential fear, like, we’re all afraid, it’s a big old universe out there, and he wanted some comfort and reassurance. Nope. The man genuinely thought I had such a profound fear of verticality, that I was having a near-panic in response to being upright. POTS, folks. It was POTS. Literally THE most common sign/symptom/syndrome of autonomic dysfunction. The most common one. He could’ve confirmed right there in the office with a poor man’s tilt table test, if he knew to… Mayo Clinic had to confirm with expensive instruments because he’d never heard of a disease that affects 1/100 teenagers and between 1–3 million people in the United States! Would’ve done better with this Doctor.
05/12/2022 16:42:08 - INFO - __main__ - ['after going to Mayo Clinic']
05/12/2022 16:42:08 - INFO - __main__ -  [quail] Why is a realtor helpful when moving?(A)the realtor is handy with repairs(B)not enough information(C)the realtor is inexpensive(D)the realtor can do the work for you so you can keep your home show ready [SEP] Moving can either be out of necessity or an act of desire. Whatever the case, moving rarely comes at a convenient time and involves so many variables that it is difficult to exhale until all of the documents are signed. Even then, that point in the process instantaneously starts a whole new chapter in the book of moving. No matter how long it takes to sell (or not sell) your previous home, whether you’re able to find your dream kitchen or settle for less, if you’re moving down the street, across the country, or around the world, the act of moving can be completely overwhelming. Long story short: Moving is a process. In the midst of all the uncertainties, there are a few ways to stay organized while moving. This short list is meant to guide you through steps to keep you on track during a stressful time. The largest piece of advice I can give you is to start by decluttering your current living space. Having less to deal with will help you feel more in control of the situation. Finding a realtor that you trust and feel comfortable working with will put your mind at ease (most of the time) about the process of listing your house and finding a buyer. Let your realtor do the work for you so you can concentrate on keeping your home “show ready” and begin to look at options on the other side of the move, like where you’ll be working, spending your time, and researching schools or neighborhoods. Make a list of people you’ll need to update contact information with once you leave. This should include any and all businesses you frequent or subscribe to, like pest control services, lawn maintenance, even all of your neighborhood loyal buyer programs you participate in. Do not overlook your banks, investment companies, retirement funds, healthcare providers for you and your family members, and even you pets. And, of course, family and friends.
05/12/2022 16:42:08 - INFO - __main__ - ['the realtor can do the work for you so you can keep your home show ready']
05/12/2022 16:42:08 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:42:08 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:42:08 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:42:25 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:42:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:42:26 - INFO - __main__ - Starting training!
05/12/2022 16:42:31 - INFO - __main__ - Step 10 Global step 10 Train loss 2.14 on epoch=4
05/12/2022 16:42:35 - INFO - __main__ - Step 20 Global step 20 Train loss 1.78 on epoch=9
05/12/2022 16:42:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=14
05/12/2022 16:42:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.33 on epoch=19
05/12/2022 16:42:49 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=24
05/12/2022 16:42:53 - INFO - __main__ - Global step 50 Train loss 1.58 ACC 0.15625 on epoch=24
05/12/2022 16:42:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 16:42:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=29
05/12/2022 16:43:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=34
05/12/2022 16:43:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=39
05/12/2022 16:43:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=44
05/12/2022 16:43:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=49
05/12/2022 16:43:18 - INFO - __main__ - Global step 100 Train loss 0.84 ACC 0.1875 on epoch=49
05/12/2022 16:43:18 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=49, global_step=100
05/12/2022 16:43:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=54
05/12/2022 16:43:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
05/12/2022 16:43:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=64
05/12/2022 16:43:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=69
05/12/2022 16:43:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=74
05/12/2022 16:43:43 - INFO - __main__ - Global step 150 Train loss 0.59 ACC 0.1875 on epoch=74
05/12/2022 16:43:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
05/12/2022 16:43:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=84
05/12/2022 16:43:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=89
05/12/2022 16:44:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
05/12/2022 16:44:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
05/12/2022 16:44:08 - INFO - __main__ - Global step 200 Train loss 0.44 ACC 0.15625 on epoch=99
05/12/2022 16:44:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
05/12/2022 16:44:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
05/12/2022 16:44:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
05/12/2022 16:44:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
05/12/2022 16:44:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
05/12/2022 16:44:33 - INFO - __main__ - Global step 250 Train loss 0.40 ACC 0.15625 on epoch=124
05/12/2022 16:44:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
05/12/2022 16:44:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
05/12/2022 16:44:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
05/12/2022 16:44:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
05/12/2022 16:44:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
05/12/2022 16:44:58 - INFO - __main__ - Global step 300 Train loss 0.35 ACC 0.1875 on epoch=149
05/12/2022 16:45:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=154
05/12/2022 16:45:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=159
05/12/2022 16:45:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=164
05/12/2022 16:45:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
05/12/2022 16:45:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
05/12/2022 16:45:23 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.1875 on epoch=174
05/12/2022 16:45:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
05/12/2022 16:45:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
05/12/2022 16:45:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
05/12/2022 16:45:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
05/12/2022 16:45:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
05/12/2022 16:45:49 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.21875 on epoch=199
05/12/2022 16:45:49 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=199, global_step=400
05/12/2022 16:45:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
05/12/2022 16:45:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
05/12/2022 16:46:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=214
05/12/2022 16:46:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
05/12/2022 16:46:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
05/12/2022 16:46:14 - INFO - __main__ - Global step 450 Train loss 0.22 ACC 0.15625 on epoch=224
05/12/2022 16:46:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
05/12/2022 16:46:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
05/12/2022 16:46:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
05/12/2022 16:46:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
05/12/2022 16:46:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
05/12/2022 16:46:39 - INFO - __main__ - Global step 500 Train loss 0.20 ACC 0.125 on epoch=249
05/12/2022 16:46:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
05/12/2022 16:46:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
05/12/2022 16:46:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
05/12/2022 16:46:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
05/12/2022 16:47:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
05/12/2022 16:47:04 - INFO - __main__ - Global step 550 Train loss 0.17 ACC 0.21875 on epoch=274
05/12/2022 16:47:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
05/12/2022 16:47:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
05/12/2022 16:47:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
05/12/2022 16:47:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
05/12/2022 16:47:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=299
05/12/2022 16:47:29 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.25 on epoch=299
05/12/2022 16:47:29 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=299, global_step=600
05/12/2022 16:47:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
05/12/2022 16:47:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
05/12/2022 16:47:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
05/12/2022 16:47:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
05/12/2022 16:47:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
05/12/2022 16:47:54 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.15625 on epoch=324
05/12/2022 16:47:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
05/12/2022 16:48:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
05/12/2022 16:48:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
05/12/2022 16:48:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
05/12/2022 16:48:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
05/12/2022 16:48:19 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.0625 on epoch=349
05/12/2022 16:48:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
05/12/2022 16:48:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
05/12/2022 16:48:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
05/12/2022 16:48:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
05/12/2022 16:48:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=374
05/12/2022 16:48:44 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.21875 on epoch=374
05/12/2022 16:48:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
05/12/2022 16:48:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
05/12/2022 16:48:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
05/12/2022 16:49:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
05/12/2022 16:49:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
05/12/2022 16:49:09 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.1875 on epoch=399
05/12/2022 16:49:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
05/12/2022 16:49:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
05/12/2022 16:49:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
05/12/2022 16:49:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
05/12/2022 16:49:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
05/12/2022 16:49:35 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.25 on epoch=424
05/12/2022 16:49:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 16:49:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
05/12/2022 16:49:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
05/12/2022 16:49:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 16:49:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/12/2022 16:50:00 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.15625 on epoch=449
05/12/2022 16:50:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
05/12/2022 16:50:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
05/12/2022 16:50:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
05/12/2022 16:50:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 16:50:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
05/12/2022 16:50:26 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.1875 on epoch=474
05/12/2022 16:50:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
05/12/2022 16:50:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/12/2022 16:50:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
05/12/2022 16:50:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 16:50:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
05/12/2022 16:50:52 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.21875 on epoch=499
05/12/2022 16:50:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
05/12/2022 16:51:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
05/12/2022 16:51:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
05/12/2022 16:51:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
05/12/2022 16:51:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
05/12/2022 16:51:17 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.15625 on epoch=524
05/12/2022 16:51:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
05/12/2022 16:51:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/12/2022 16:51:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
05/12/2022 16:51:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
05/12/2022 16:51:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
05/12/2022 16:51:42 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.21875 on epoch=549
05/12/2022 16:51:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/12/2022 16:51:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
05/12/2022 16:51:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
05/12/2022 16:52:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
05/12/2022 16:52:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
05/12/2022 16:52:08 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.21875 on epoch=574
05/12/2022 16:52:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
05/12/2022 16:52:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 16:52:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
05/12/2022 16:52:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 16:52:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
05/12/2022 16:52:33 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.1875 on epoch=599
05/12/2022 16:52:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 16:52:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=609
05/12/2022 16:52:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 16:52:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
05/12/2022 16:52:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
05/12/2022 16:52:58 - INFO - __main__ - Global step 1250 Train loss 0.06 ACC 0.21875 on epoch=624
05/12/2022 16:53:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
05/12/2022 16:53:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
05/12/2022 16:53:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
05/12/2022 16:53:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 16:53:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
05/12/2022 16:53:23 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.21875 on epoch=649
05/12/2022 16:53:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
05/12/2022 16:53:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
05/12/2022 16:53:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 16:53:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
05/12/2022 16:53:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
05/12/2022 16:53:49 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.1875 on epoch=674
05/12/2022 16:53:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
05/12/2022 16:53:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=684
05/12/2022 16:54:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
05/12/2022 16:54:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
05/12/2022 16:54:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
05/12/2022 16:54:14 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.21875 on epoch=699
05/12/2022 16:54:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
05/12/2022 16:54:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 16:54:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
05/12/2022 16:54:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
05/12/2022 16:54:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 16:54:39 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.21875 on epoch=724
05/12/2022 16:54:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
05/12/2022 16:54:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
05/12/2022 16:54:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
05/12/2022 16:54:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
05/12/2022 16:55:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
05/12/2022 16:55:04 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.1875 on epoch=749
05/12/2022 16:55:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
05/12/2022 16:55:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 16:55:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
05/12/2022 16:55:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 16:55:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 16:55:30 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.25 on epoch=774
05/12/2022 16:55:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
05/12/2022 16:55:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 16:55:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 16:55:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 16:55:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
05/12/2022 16:55:55 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.25 on epoch=799
05/12/2022 16:56:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
05/12/2022 16:56:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
05/12/2022 16:56:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 16:56:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 16:56:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
05/12/2022 16:56:21 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.1875 on epoch=824
05/12/2022 16:56:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 16:56:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 16:56:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
05/12/2022 16:56:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
05/12/2022 16:56:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
05/12/2022 16:56:46 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.21875 on epoch=849
05/12/2022 16:56:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 16:56:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 16:56:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 16:57:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 16:57:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
05/12/2022 16:57:11 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.21875 on epoch=874
05/12/2022 16:57:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
05/12/2022 16:57:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 16:57:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 16:57:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/12/2022 16:57:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 16:57:37 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.28125 on epoch=899
05/12/2022 16:57:37 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=899, global_step=1800
05/12/2022 16:57:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 16:57:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 16:57:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
05/12/2022 16:57:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 16:57:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 16:58:02 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.25 on epoch=924
05/12/2022 16:58:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 16:58:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
05/12/2022 16:58:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
05/12/2022 16:58:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 16:58:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
05/12/2022 16:58:28 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.25 on epoch=949
05/12/2022 16:58:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 16:58:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 16:58:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 16:58:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/12/2022 16:58:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 16:58:53 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.25 on epoch=974
05/12/2022 16:58:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
05/12/2022 16:59:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 16:59:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 16:59:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 16:59:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
05/12/2022 16:59:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:59:18 - INFO - __main__ - Printing 3 examples
05/12/2022 16:59:18 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 16:59:18 - INFO - __main__ - ['it was not installed']
05/12/2022 16:59:18 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 16:59:18 - INFO - __main__ - ['a few more months']
05/12/2022 16:59:18 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 16:59:18 - INFO - __main__ - ['still in the same position']
05/12/2022 16:59:18 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:59:18 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:59:18 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 16:59:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 16:59:18 - INFO - __main__ - Printing 3 examples
05/12/2022 16:59:18 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 16:59:18 - INFO - __main__ - ['not enough information']
05/12/2022 16:59:18 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 16:59:18 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 16:59:18 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 16:59:18 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 16:59:18 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:59:18 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:59:18 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 16:59:19 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.28125 on epoch=999
05/12/2022 16:59:19 - INFO - __main__ - save last model!
05/12/2022 16:59:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 16:59:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 16:59:19 - INFO - __main__ - Printing 3 examples
05/12/2022 16:59:19 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:59:19 - INFO - __main__ - ['about 10 minutes']
05/12/2022 16:59:19 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:59:19 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 16:59:19 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 16:59:19 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 16:59:19 - INFO - __main__ - Tokenizing Input ...
05/12/2022 16:59:21 - INFO - __main__ - Tokenizing Output ...
05/12/2022 16:59:22 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 16:59:34 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 16:59:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 16:59:35 - INFO - __main__ - Starting training!
05/12/2022 17:01:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_100_0.2_8_predictions.txt
05/12/2022 17:01:06 - INFO - __main__ - ACC on test data: 0.2340
05/12/2022 17:01:06 - INFO - __main__ - prefix=quail_32_100, lr=0.2, bsz=8, dev_performance=0.28125, test_performance=0.234
05/12/2022 17:01:06 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.5, bsz=8 ...
05/12/2022 17:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:01:07 - INFO - __main__ - Printing 3 examples
05/12/2022 17:01:07 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:01:07 - INFO - __main__ - ['it was not installed']
05/12/2022 17:01:07 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:01:07 - INFO - __main__ - ['a few more months']
05/12/2022 17:01:07 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:01:07 - INFO - __main__ - ['still in the same position']
05/12/2022 17:01:07 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:01:07 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:01:07 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:01:07 - INFO - __main__ - Printing 3 examples
05/12/2022 17:01:07 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:01:07 - INFO - __main__ - ['not enough information']
05/12/2022 17:01:07 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:01:07 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:01:07 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:01:07 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:01:07 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:01:07 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:01:07 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:01:23 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:01:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:01:23 - INFO - __main__ - Starting training!
05/12/2022 17:01:28 - INFO - __main__ - Step 10 Global step 10 Train loss 1.95 on epoch=4
05/12/2022 17:01:33 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
05/12/2022 17:01:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
05/12/2022 17:01:42 - INFO - __main__ - Step 40 Global step 40 Train loss 1.53 on epoch=19
05/12/2022 17:01:46 - INFO - __main__ - Step 50 Global step 50 Train loss 3.93 on epoch=24
05/12/2022 17:02:03 - INFO - __main__ - Global step 50 Train loss 1.88 ACC 0.0 on epoch=24
05/12/2022 17:02:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
05/12/2022 17:02:07 - INFO - __main__ - Step 60 Global step 60 Train loss 4.98 on epoch=29
05/12/2022 17:02:12 - INFO - __main__ - Step 70 Global step 70 Train loss 2.01 on epoch=34
05/12/2022 17:02:16 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=39
05/12/2022 17:02:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=44
05/12/2022 17:02:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.82 on epoch=49
05/12/2022 17:02:27 - INFO - __main__ - Global step 100 Train loss 1.95 ACC 0.15625 on epoch=49
05/12/2022 17:02:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=49, global_step=100
05/12/2022 17:02:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=54
05/12/2022 17:02:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=59
05/12/2022 17:02:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=64
05/12/2022 17:02:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=69
05/12/2022 17:02:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=74
05/12/2022 17:02:51 - INFO - __main__ - Global step 150 Train loss 0.77 ACC 0.15625 on epoch=74
05/12/2022 17:02:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.73 on epoch=79
05/12/2022 17:03:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.70 on epoch=84
05/12/2022 17:03:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=89
05/12/2022 17:03:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=94
05/12/2022 17:03:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=99
05/12/2022 17:03:16 - INFO - __main__ - Global step 200 Train loss 0.71 ACC 0.15625 on epoch=99
05/12/2022 17:03:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=104
05/12/2022 17:03:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=109
05/12/2022 17:03:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.64 on epoch=114
05/12/2022 17:03:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=119
05/12/2022 17:03:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=124
05/12/2022 17:03:41 - INFO - __main__ - Global step 250 Train loss 0.66 ACC 0.15625 on epoch=124
05/12/2022 17:03:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=129
05/12/2022 17:03:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=134
05/12/2022 17:03:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=139
05/12/2022 17:03:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=144
05/12/2022 17:04:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=149
05/12/2022 17:04:06 - INFO - __main__ - Global step 300 Train loss 0.62 ACC 0.15625 on epoch=149
05/12/2022 17:04:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=154
05/12/2022 17:04:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=159
05/12/2022 17:04:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=164
05/12/2022 17:04:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=169
05/12/2022 17:04:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=174
05/12/2022 17:04:30 - INFO - __main__ - Global step 350 Train loss 0.55 ACC 0.1875 on epoch=174
05/12/2022 17:04:30 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=174, global_step=350
05/12/2022 17:04:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=179
05/12/2022 17:04:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.51 on epoch=184
05/12/2022 17:04:44 - INFO - __main__ - Step 380 Global step 380 Train loss 4.58 on epoch=189
05/12/2022 17:04:48 - INFO - __main__ - Step 390 Global step 390 Train loss 5.21 on epoch=194
05/12/2022 17:04:52 - INFO - __main__ - Step 400 Global step 400 Train loss 4.35 on epoch=199
05/12/2022 17:04:54 - INFO - __main__ - Global step 400 Train loss 3.25 ACC 0.03125 on epoch=199
05/12/2022 17:04:59 - INFO - __main__ - Step 410 Global step 410 Train loss 5.18 on epoch=204
05/12/2022 17:05:03 - INFO - __main__ - Step 420 Global step 420 Train loss 5.49 on epoch=209
05/12/2022 17:05:08 - INFO - __main__ - Step 430 Global step 430 Train loss 4.94 on epoch=214
05/12/2022 17:05:12 - INFO - __main__ - Step 440 Global step 440 Train loss 4.34 on epoch=219
05/12/2022 17:05:17 - INFO - __main__ - Step 450 Global step 450 Train loss 2.98 on epoch=224
05/12/2022 17:05:20 - INFO - __main__ - Global step 450 Train loss 4.59 ACC 0.03125 on epoch=224
05/12/2022 17:05:24 - INFO - __main__ - Step 460 Global step 460 Train loss 2.93 on epoch=229
05/12/2022 17:05:29 - INFO - __main__ - Step 470 Global step 470 Train loss 2.68 on epoch=234
05/12/2022 17:05:33 - INFO - __main__ - Step 480 Global step 480 Train loss 2.32 on epoch=239
05/12/2022 17:05:38 - INFO - __main__ - Step 490 Global step 490 Train loss 2.33 on epoch=244
05/12/2022 17:05:42 - INFO - __main__ - Step 500 Global step 500 Train loss 1.84 on epoch=249
05/12/2022 17:05:48 - INFO - __main__ - Global step 500 Train loss 2.42 ACC 0.125 on epoch=249
05/12/2022 17:05:53 - INFO - __main__ - Step 510 Global step 510 Train loss 1.94 on epoch=254
05/12/2022 17:05:57 - INFO - __main__ - Step 520 Global step 520 Train loss 1.63 on epoch=259
05/12/2022 17:06:02 - INFO - __main__ - Step 530 Global step 530 Train loss 1.31 on epoch=264
05/12/2022 17:06:06 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
05/12/2022 17:06:11 - INFO - __main__ - Step 550 Global step 550 Train loss 1.81 on epoch=274
05/12/2022 17:06:14 - INFO - __main__ - Global step 550 Train loss 1.58 ACC 0.21875 on epoch=274
05/12/2022 17:06:14 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=274, global_step=550
05/12/2022 17:06:19 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
05/12/2022 17:06:23 - INFO - __main__ - Step 570 Global step 570 Train loss 1.16 on epoch=284
05/12/2022 17:06:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=289
05/12/2022 17:06:32 - INFO - __main__ - Step 590 Global step 590 Train loss 1.79 on epoch=294
05/12/2022 17:06:37 - INFO - __main__ - Step 600 Global step 600 Train loss 1.29 on epoch=299
05/12/2022 17:06:39 - INFO - __main__ - Global step 600 Train loss 1.32 ACC 0.125 on epoch=299
05/12/2022 17:06:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=304
05/12/2022 17:06:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.71 on epoch=309
05/12/2022 17:06:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=314
05/12/2022 17:06:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=319
05/12/2022 17:07:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.94 on epoch=324
05/12/2022 17:07:04 - INFO - __main__ - Global step 650 Train loss 0.81 ACC 0.15625 on epoch=324
05/12/2022 17:07:08 - INFO - __main__ - Step 660 Global step 660 Train loss 1.22 on epoch=329
05/12/2022 17:07:12 - INFO - __main__ - Step 670 Global step 670 Train loss 1.57 on epoch=334
05/12/2022 17:07:17 - INFO - __main__ - Step 680 Global step 680 Train loss 1.28 on epoch=339
05/12/2022 17:07:21 - INFO - __main__ - Step 690 Global step 690 Train loss 1.15 on epoch=344
05/12/2022 17:07:26 - INFO - __main__ - Step 700 Global step 700 Train loss 1.25 on epoch=349
05/12/2022 17:07:28 - INFO - __main__ - Global step 700 Train loss 1.29 ACC 0.09375 on epoch=349
05/12/2022 17:07:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.97 on epoch=354
05/12/2022 17:07:37 - INFO - __main__ - Step 720 Global step 720 Train loss 1.07 on epoch=359
05/12/2022 17:07:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.93 on epoch=364
05/12/2022 17:07:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
05/12/2022 17:07:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.99 on epoch=374
05/12/2022 17:07:53 - INFO - __main__ - Global step 750 Train loss 0.98 ACC 0.0625 on epoch=374
05/12/2022 17:07:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.83 on epoch=379
05/12/2022 17:08:02 - INFO - __main__ - Step 770 Global step 770 Train loss 1.01 on epoch=384
05/12/2022 17:08:06 - INFO - __main__ - Step 780 Global step 780 Train loss 1.43 on epoch=389
05/12/2022 17:08:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.96 on epoch=394
05/12/2022 17:08:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=399
05/12/2022 17:08:17 - INFO - __main__ - Global step 800 Train loss 1.01 ACC 0.125 on epoch=399
05/12/2022 17:08:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.89 on epoch=404
05/12/2022 17:08:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
05/12/2022 17:08:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.78 on epoch=414
05/12/2022 17:08:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=419
05/12/2022 17:08:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.73 on epoch=424
05/12/2022 17:08:42 - INFO - __main__ - Global step 850 Train loss 0.78 ACC 0.15625 on epoch=424
05/12/2022 17:08:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.72 on epoch=429
05/12/2022 17:08:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=434
05/12/2022 17:08:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.70 on epoch=439
05/12/2022 17:09:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.74 on epoch=444
05/12/2022 17:09:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
05/12/2022 17:09:06 - INFO - __main__ - Global step 900 Train loss 0.74 ACC 0.15625 on epoch=449
05/12/2022 17:09:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=454
05/12/2022 17:09:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
05/12/2022 17:09:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.72 on epoch=464
05/12/2022 17:09:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.63 on epoch=469
05/12/2022 17:09:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.60 on epoch=474
05/12/2022 17:09:31 - INFO - __main__ - Global step 950 Train loss 0.69 ACC 0.09375 on epoch=474
05/12/2022 17:09:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.73 on epoch=479
05/12/2022 17:09:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.63 on epoch=484
05/12/2022 17:09:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.60 on epoch=489
05/12/2022 17:09:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.67 on epoch=494
05/12/2022 17:09:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
05/12/2022 17:09:55 - INFO - __main__ - Global step 1000 Train loss 0.66 ACC 0.125 on epoch=499
05/12/2022 17:10:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=504
05/12/2022 17:10:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.56 on epoch=509
05/12/2022 17:10:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=514
05/12/2022 17:10:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
05/12/2022 17:10:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.61 on epoch=524
05/12/2022 17:10:20 - INFO - __main__ - Global step 1050 Train loss 0.61 ACC 0.09375 on epoch=524
05/12/2022 17:10:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
05/12/2022 17:10:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.59 on epoch=534
05/12/2022 17:10:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.58 on epoch=539
05/12/2022 17:10:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.56 on epoch=544
05/12/2022 17:10:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
05/12/2022 17:10:45 - INFO - __main__ - Global step 1100 Train loss 0.58 ACC 0.125 on epoch=549
05/12/2022 17:10:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
05/12/2022 17:10:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
05/12/2022 17:10:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
05/12/2022 17:11:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.57 on epoch=569
05/12/2022 17:11:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=574
05/12/2022 17:11:09 - INFO - __main__ - Global step 1150 Train loss 0.55 ACC 0.125 on epoch=574
05/12/2022 17:11:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
05/12/2022 17:11:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.53 on epoch=584
05/12/2022 17:11:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.57 on epoch=589
05/12/2022 17:11:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
05/12/2022 17:11:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.60 on epoch=599
05/12/2022 17:11:33 - INFO - __main__ - Global step 1200 Train loss 0.55 ACC 0.125 on epoch=599
05/12/2022 17:11:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.53 on epoch=604
05/12/2022 17:11:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
05/12/2022 17:11:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=614
05/12/2022 17:11:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.57 on epoch=619
05/12/2022 17:11:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=624
05/12/2022 17:11:58 - INFO - __main__ - Global step 1250 Train loss 0.52 ACC 0.125 on epoch=624
05/12/2022 17:12:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.56 on epoch=629
05/12/2022 17:12:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
05/12/2022 17:12:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=639
05/12/2022 17:12:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
05/12/2022 17:12:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=649
05/12/2022 17:12:22 - INFO - __main__ - Global step 1300 Train loss 0.52 ACC 0.125 on epoch=649
05/12/2022 17:12:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=654
05/12/2022 17:12:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
05/12/2022 17:12:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=664
05/12/2022 17:12:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
05/12/2022 17:12:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
05/12/2022 17:12:47 - INFO - __main__ - Global step 1350 Train loss 0.50 ACC 0.125 on epoch=674
05/12/2022 17:12:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
05/12/2022 17:12:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
05/12/2022 17:13:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
05/12/2022 17:13:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
05/12/2022 17:13:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
05/12/2022 17:13:11 - INFO - __main__ - Global step 1400 Train loss 0.50 ACC 0.125 on epoch=699
05/12/2022 17:13:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=704
05/12/2022 17:13:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
05/12/2022 17:13:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.52 on epoch=714
05/12/2022 17:13:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
05/12/2022 17:13:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=724
05/12/2022 17:13:36 - INFO - __main__ - Global step 1450 Train loss 0.47 ACC 0.09375 on epoch=724
05/12/2022 17:13:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=729
05/12/2022 17:13:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/12/2022 17:13:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=739
05/12/2022 17:13:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
05/12/2022 17:13:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
05/12/2022 17:14:00 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.09375 on epoch=749
05/12/2022 17:14:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
05/12/2022 17:14:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
05/12/2022 17:14:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
05/12/2022 17:14:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=769
05/12/2022 17:14:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
05/12/2022 17:14:25 - INFO - __main__ - Global step 1550 Train loss 0.48 ACC 0.09375 on epoch=774
05/12/2022 17:14:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/12/2022 17:14:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
05/12/2022 17:14:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
05/12/2022 17:14:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
05/12/2022 17:14:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=799
05/12/2022 17:14:49 - INFO - __main__ - Global step 1600 Train loss 0.42 ACC 0.09375 on epoch=799
05/12/2022 17:14:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/12/2022 17:14:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=809
05/12/2022 17:15:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=814
05/12/2022 17:15:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
05/12/2022 17:15:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
05/12/2022 17:15:13 - INFO - __main__ - Global step 1650 Train loss 0.40 ACC 0.09375 on epoch=824
05/12/2022 17:15:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/12/2022 17:15:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
05/12/2022 17:15:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=839
05/12/2022 17:15:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
05/12/2022 17:15:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
05/12/2022 17:15:38 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.09375 on epoch=849
05/12/2022 17:15:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
05/12/2022 17:15:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
05/12/2022 17:15:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
05/12/2022 17:15:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
05/12/2022 17:16:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
05/12/2022 17:16:02 - INFO - __main__ - Global step 1750 Train loss 0.39 ACC 0.09375 on epoch=874
05/12/2022 17:16:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
05/12/2022 17:16:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=884
05/12/2022 17:16:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/12/2022 17:16:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=894
05/12/2022 17:16:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=899
05/12/2022 17:16:27 - INFO - __main__ - Global step 1800 Train loss 0.40 ACC 0.09375 on epoch=899
05/12/2022 17:16:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
05/12/2022 17:16:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/12/2022 17:16:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
05/12/2022 17:16:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=919
05/12/2022 17:16:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/12/2022 17:16:51 - INFO - __main__ - Global step 1850 Train loss 0.36 ACC 0.09375 on epoch=924
05/12/2022 17:16:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=929
05/12/2022 17:17:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
05/12/2022 17:17:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
05/12/2022 17:17:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
05/12/2022 17:17:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
05/12/2022 17:17:15 - INFO - __main__ - Global step 1900 Train loss 0.37 ACC 0.125 on epoch=949
05/12/2022 17:17:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/12/2022 17:17:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/12/2022 17:17:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
05/12/2022 17:17:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/12/2022 17:17:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
05/12/2022 17:17:40 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.125 on epoch=974
05/12/2022 17:17:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/12/2022 17:17:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=984
05/12/2022 17:17:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/12/2022 17:17:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
05/12/2022 17:18:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
05/12/2022 17:18:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:18:03 - INFO - __main__ - Printing 3 examples
05/12/2022 17:18:03 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:18:03 - INFO - __main__ - ['it was not installed']
05/12/2022 17:18:03 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:18:03 - INFO - __main__ - ['a few more months']
05/12/2022 17:18:03 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:18:03 - INFO - __main__ - ['still in the same position']
05/12/2022 17:18:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:18:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:18:03 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:18:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:18:03 - INFO - __main__ - Printing 3 examples
05/12/2022 17:18:03 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:18:03 - INFO - __main__ - ['not enough information']
05/12/2022 17:18:03 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:18:03 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:18:03 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:18:03 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:18:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:18:04 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:18:04 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:18:04 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.125 on epoch=999
05/12/2022 17:18:04 - INFO - __main__ - save last model!
05/12/2022 17:18:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 17:18:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 17:18:04 - INFO - __main__ - Printing 3 examples
05/12/2022 17:18:04 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:18:04 - INFO - __main__ - ['about 10 minutes']
05/12/2022 17:18:04 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:18:04 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 17:18:04 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:18:04 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 17:18:04 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:18:06 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:18:07 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 17:18:19 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:18:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:18:20 - INFO - __main__ - Starting training!
05/12/2022 17:19:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.5_8_predictions.txt
05/12/2022 17:19:24 - INFO - __main__ - ACC on test data: 0.2090
05/12/2022 17:19:25 - INFO - __main__ - prefix=quail_32_13, lr=0.5, bsz=8, dev_performance=0.21875, test_performance=0.209
05/12/2022 17:19:25 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.4, bsz=8 ...
05/12/2022 17:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:19:26 - INFO - __main__ - Printing 3 examples
05/12/2022 17:19:26 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:19:26 - INFO - __main__ - ['it was not installed']
05/12/2022 17:19:26 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:19:26 - INFO - __main__ - ['a few more months']
05/12/2022 17:19:26 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:19:26 - INFO - __main__ - ['still in the same position']
05/12/2022 17:19:26 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:19:26 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:19:26 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:19:26 - INFO - __main__ - Printing 3 examples
05/12/2022 17:19:26 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:19:26 - INFO - __main__ - ['not enough information']
05/12/2022 17:19:26 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:19:26 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:19:26 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:19:26 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:19:26 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:19:26 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:19:26 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:19:45 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:19:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:19:45 - INFO - __main__ - Starting training!
05/12/2022 17:19:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.78 on epoch=4
05/12/2022 17:19:58 - INFO - __main__ - Step 20 Global step 20 Train loss 1.21 on epoch=9
05/12/2022 17:20:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=14
05/12/2022 17:20:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=19
05/12/2022 17:20:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
05/12/2022 17:20:14 - INFO - __main__ - Global step 50 Train loss 1.02 ACC 0.125 on epoch=24
05/12/2022 17:20:14 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
05/12/2022 17:20:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=29
05/12/2022 17:20:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
05/12/2022 17:20:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
05/12/2022 17:20:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
05/12/2022 17:20:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
05/12/2022 17:20:38 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.09375 on epoch=49
05/12/2022 17:20:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
05/12/2022 17:20:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
05/12/2022 17:20:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
05/12/2022 17:20:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
05/12/2022 17:21:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
05/12/2022 17:21:03 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.125 on epoch=74
05/12/2022 17:21:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
05/12/2022 17:21:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
05/12/2022 17:21:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
05/12/2022 17:21:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
05/12/2022 17:21:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/12/2022 17:21:28 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.15625 on epoch=99
05/12/2022 17:21:28 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.15625 on epoch=99, global_step=200
05/12/2022 17:21:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
05/12/2022 17:21:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
05/12/2022 17:21:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
05/12/2022 17:21:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
05/12/2022 17:21:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
05/12/2022 17:21:52 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.21875 on epoch=124
05/12/2022 17:21:52 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=124, global_step=250
05/12/2022 17:21:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
05/12/2022 17:22:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
05/12/2022 17:22:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
05/12/2022 17:22:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
05/12/2022 17:22:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
05/12/2022 17:22:17 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.1875 on epoch=149
05/12/2022 17:22:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
05/12/2022 17:22:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
05/12/2022 17:22:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
05/12/2022 17:22:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
05/12/2022 17:22:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
05/12/2022 17:22:41 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.1875 on epoch=174
05/12/2022 17:22:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
05/12/2022 17:22:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
05/12/2022 17:22:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
05/12/2022 17:22:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
05/12/2022 17:23:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
05/12/2022 17:23:06 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.15625 on epoch=199
05/12/2022 17:23:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
05/12/2022 17:23:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
05/12/2022 17:23:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
05/12/2022 17:23:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
05/12/2022 17:23:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
05/12/2022 17:23:30 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.15625 on epoch=224
05/12/2022 17:23:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
05/12/2022 17:23:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
05/12/2022 17:23:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
05/12/2022 17:23:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
05/12/2022 17:23:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
05/12/2022 17:23:55 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.1875 on epoch=249
05/12/2022 17:23:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
05/12/2022 17:24:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
05/12/2022 17:24:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
05/12/2022 17:24:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
05/12/2022 17:24:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/12/2022 17:24:20 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.1875 on epoch=274
05/12/2022 17:24:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=279
05/12/2022 17:24:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
05/12/2022 17:24:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
05/12/2022 17:24:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
05/12/2022 17:24:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=299
05/12/2022 17:24:44 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.21875 on epoch=299
05/12/2022 17:24:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
05/12/2022 17:24:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
05/12/2022 17:24:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
05/12/2022 17:25:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
05/12/2022 17:25:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/12/2022 17:25:09 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.09375 on epoch=324
05/12/2022 17:25:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
05/12/2022 17:25:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
05/12/2022 17:25:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
05/12/2022 17:25:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
05/12/2022 17:25:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
05/12/2022 17:25:33 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.25 on epoch=349
05/12/2022 17:25:33 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=349, global_step=700
05/12/2022 17:25:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
05/12/2022 17:25:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
05/12/2022 17:25:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
05/12/2022 17:25:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
05/12/2022 17:25:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
05/12/2022 17:25:58 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.21875 on epoch=374
05/12/2022 17:26:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
05/12/2022 17:26:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
05/12/2022 17:26:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
05/12/2022 17:26:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
05/12/2022 17:26:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
05/12/2022 17:26:22 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.15625 on epoch=399
05/12/2022 17:26:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
05/12/2022 17:26:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
05/12/2022 17:26:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/12/2022 17:26:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
05/12/2022 17:26:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
05/12/2022 17:26:47 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.15625 on epoch=424
05/12/2022 17:26:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/12/2022 17:26:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
05/12/2022 17:27:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
05/12/2022 17:27:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 17:27:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
05/12/2022 17:27:11 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.125 on epoch=449
05/12/2022 17:27:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
05/12/2022 17:27:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
05/12/2022 17:27:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
05/12/2022 17:27:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
05/12/2022 17:27:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
05/12/2022 17:27:37 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.15625 on epoch=474
05/12/2022 17:27:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
05/12/2022 17:27:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
05/12/2022 17:27:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
05/12/2022 17:27:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 17:27:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
05/12/2022 17:28:02 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.15625 on epoch=499
05/12/2022 17:28:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
05/12/2022 17:28:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
05/12/2022 17:28:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
05/12/2022 17:28:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
05/12/2022 17:28:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
05/12/2022 17:28:26 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.1875 on epoch=524
05/12/2022 17:28:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/12/2022 17:28:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
05/12/2022 17:28:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/12/2022 17:28:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
05/12/2022 17:28:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
05/12/2022 17:28:52 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.125 on epoch=549
05/12/2022 17:28:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 17:29:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
05/12/2022 17:29:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
05/12/2022 17:29:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
05/12/2022 17:29:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
05/12/2022 17:29:16 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
05/12/2022 17:29:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
05/12/2022 17:29:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 17:29:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
05/12/2022 17:29:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 17:29:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
05/12/2022 17:29:41 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.25 on epoch=599
05/12/2022 17:29:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
05/12/2022 17:29:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
05/12/2022 17:29:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 17:29:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
05/12/2022 17:30:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/12/2022 17:30:05 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.1875 on epoch=624
05/12/2022 17:30:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 17:30:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
05/12/2022 17:30:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
05/12/2022 17:30:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
05/12/2022 17:30:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
05/12/2022 17:30:31 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.15625 on epoch=649
05/12/2022 17:30:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/12/2022 17:30:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
05/12/2022 17:30:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 17:30:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
05/12/2022 17:30:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
05/12/2022 17:30:56 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.21875 on epoch=674
05/12/2022 17:31:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
05/12/2022 17:31:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
05/12/2022 17:31:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/12/2022 17:31:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
05/12/2022 17:31:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 17:31:21 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.25 on epoch=699
05/12/2022 17:31:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
05/12/2022 17:31:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/12/2022 17:31:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
05/12/2022 17:31:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
05/12/2022 17:31:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/12/2022 17:31:45 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.15625 on epoch=724
05/12/2022 17:31:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
05/12/2022 17:31:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
05/12/2022 17:31:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/12/2022 17:32:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
05/12/2022 17:32:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 17:32:10 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
05/12/2022 17:32:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
05/12/2022 17:32:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 17:32:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 17:32:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
05/12/2022 17:32:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
05/12/2022 17:32:34 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.15625 on epoch=774
05/12/2022 17:32:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 17:32:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 17:32:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
05/12/2022 17:32:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/12/2022 17:32:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 17:32:59 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.28125 on epoch=799
05/12/2022 17:32:59 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=799, global_step=1600
05/12/2022 17:33:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
05/12/2022 17:33:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
05/12/2022 17:33:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 17:33:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
05/12/2022 17:33:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
05/12/2022 17:33:24 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.28125 on epoch=824
05/12/2022 17:33:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 17:33:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
05/12/2022 17:33:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
05/12/2022 17:33:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 17:33:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
05/12/2022 17:33:49 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.21875 on epoch=849
05/12/2022 17:33:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
05/12/2022 17:33:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 17:34:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
05/12/2022 17:34:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 17:34:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 17:34:14 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.1875 on epoch=874
05/12/2022 17:34:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 17:34:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 17:34:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/12/2022 17:34:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 17:34:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
05/12/2022 17:34:39 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.28125 on epoch=899
05/12/2022 17:34:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 17:34:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
05/12/2022 17:34:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 17:34:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 17:35:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 17:35:03 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
05/12/2022 17:35:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
05/12/2022 17:35:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 17:35:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/12/2022 17:35:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 17:35:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
05/12/2022 17:35:28 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.28125 on epoch=949
05/12/2022 17:35:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 17:35:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 17:35:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 17:35:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/12/2022 17:35:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
05/12/2022 17:35:52 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
05/12/2022 17:35:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 17:36:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 17:36:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 17:36:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 17:36:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 17:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:36:16 - INFO - __main__ - Printing 3 examples
05/12/2022 17:36:16 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:36:16 - INFO - __main__ - ['it was not installed']
05/12/2022 17:36:16 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:36:16 - INFO - __main__ - ['a few more months']
05/12/2022 17:36:16 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:36:16 - INFO - __main__ - ['still in the same position']
05/12/2022 17:36:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:36:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:36:16 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:36:16 - INFO - __main__ - Printing 3 examples
05/12/2022 17:36:16 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:36:16 - INFO - __main__ - ['not enough information']
05/12/2022 17:36:16 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:36:16 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:36:16 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:36:16 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:36:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:36:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:36:16 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:36:17 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.375 on epoch=999
05/12/2022 17:36:17 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=999, global_step=2000
05/12/2022 17:36:17 - INFO - __main__ - save last model!
05/12/2022 17:36:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 17:36:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 17:36:17 - INFO - __main__ - Printing 3 examples
05/12/2022 17:36:17 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:36:17 - INFO - __main__ - ['about 10 minutes']
05/12/2022 17:36:17 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:36:17 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 17:36:17 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:36:17 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 17:36:17 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:36:19 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:36:20 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 17:36:34 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:36:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:36:35 - INFO - __main__ - Starting training!
05/12/2022 17:37:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.4_8_predictions.txt
05/12/2022 17:37:46 - INFO - __main__ - ACC on test data: 0.2750
05/12/2022 17:37:48 - INFO - __main__ - prefix=quail_32_13, lr=0.4, bsz=8, dev_performance=0.375, test_performance=0.275
05/12/2022 17:37:48 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.3, bsz=8 ...
05/12/2022 17:37:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:37:49 - INFO - __main__ - Printing 3 examples
05/12/2022 17:37:49 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:37:49 - INFO - __main__ - ['it was not installed']
05/12/2022 17:37:49 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:37:49 - INFO - __main__ - ['a few more months']
05/12/2022 17:37:49 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:37:49 - INFO - __main__ - ['still in the same position']
05/12/2022 17:37:49 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:37:49 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:37:49 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:37:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:37:49 - INFO - __main__ - Printing 3 examples
05/12/2022 17:37:49 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:37:49 - INFO - __main__ - ['not enough information']
05/12/2022 17:37:49 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:37:49 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:37:49 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:37:49 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:37:49 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:37:49 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:37:49 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:38:08 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:38:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:38:09 - INFO - __main__ - Starting training!
05/12/2022 17:38:14 - INFO - __main__ - Step 10 Global step 10 Train loss 1.91 on epoch=4
05/12/2022 17:38:18 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=9
05/12/2022 17:38:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=14
05/12/2022 17:38:27 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
05/12/2022 17:38:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.70 on epoch=24
05/12/2022 17:38:34 - INFO - __main__ - Global step 50 Train loss 1.12 ACC 0.1875 on epoch=24
05/12/2022 17:38:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
05/12/2022 17:38:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
05/12/2022 17:38:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=34
05/12/2022 17:38:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
05/12/2022 17:38:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
05/12/2022 17:38:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
05/12/2022 17:38:58 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.125 on epoch=49
05/12/2022 17:39:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=54
05/12/2022 17:39:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
05/12/2022 17:39:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
05/12/2022 17:39:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
05/12/2022 17:39:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
05/12/2022 17:39:23 - INFO - __main__ - Global step 150 Train loss 0.38 ACC 0.1875 on epoch=74
05/12/2022 17:39:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
05/12/2022 17:39:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
05/12/2022 17:39:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
05/12/2022 17:39:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
05/12/2022 17:39:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=99
05/12/2022 17:39:47 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.15625 on epoch=99
05/12/2022 17:39:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
05/12/2022 17:39:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
05/12/2022 17:40:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
05/12/2022 17:40:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
05/12/2022 17:40:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
05/12/2022 17:40:12 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.25 on epoch=124
05/12/2022 17:40:12 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=124, global_step=250
05/12/2022 17:40:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
05/12/2022 17:40:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
05/12/2022 17:40:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
05/12/2022 17:40:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
05/12/2022 17:40:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
05/12/2022 17:40:36 - INFO - __main__ - Global step 300 Train loss 0.20 ACC 0.125 on epoch=149
05/12/2022 17:40:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
05/12/2022 17:40:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
05/12/2022 17:40:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
05/12/2022 17:40:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
05/12/2022 17:40:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
05/12/2022 17:41:01 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.125 on epoch=174
05/12/2022 17:41:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
05/12/2022 17:41:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
05/12/2022 17:41:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
05/12/2022 17:41:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
05/12/2022 17:41:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=199
05/12/2022 17:41:26 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.15625 on epoch=199
05/12/2022 17:41:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
05/12/2022 17:41:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
05/12/2022 17:41:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
05/12/2022 17:41:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
05/12/2022 17:41:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
05/12/2022 17:41:50 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.21875 on epoch=224
05/12/2022 17:41:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
05/12/2022 17:41:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
05/12/2022 17:42:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
05/12/2022 17:42:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
05/12/2022 17:42:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
05/12/2022 17:42:15 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.1875 on epoch=249
05/12/2022 17:42:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
05/12/2022 17:42:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
05/12/2022 17:42:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
05/12/2022 17:42:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
05/12/2022 17:42:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
05/12/2022 17:42:39 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.1875 on epoch=274
05/12/2022 17:42:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
05/12/2022 17:42:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
05/12/2022 17:42:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
05/12/2022 17:42:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
05/12/2022 17:43:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
05/12/2022 17:43:04 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.15625 on epoch=299
05/12/2022 17:43:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
05/12/2022 17:43:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
05/12/2022 17:43:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
05/12/2022 17:43:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
05/12/2022 17:43:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
05/12/2022 17:43:28 - INFO - __main__ - Global step 650 Train loss 0.07 ACC 0.25 on epoch=324
05/12/2022 17:43:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
05/12/2022 17:43:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
05/12/2022 17:43:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
05/12/2022 17:43:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
05/12/2022 17:43:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
05/12/2022 17:43:53 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.1875 on epoch=349
05/12/2022 17:43:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
05/12/2022 17:44:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
05/12/2022 17:44:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
05/12/2022 17:44:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
05/12/2022 17:44:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
05/12/2022 17:44:17 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.15625 on epoch=374
05/12/2022 17:44:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
05/12/2022 17:44:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/12/2022 17:44:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
05/12/2022 17:44:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
05/12/2022 17:44:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
05/12/2022 17:44:42 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.1875 on epoch=399
05/12/2022 17:44:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
05/12/2022 17:44:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
05/12/2022 17:44:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
05/12/2022 17:45:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
05/12/2022 17:45:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
05/12/2022 17:45:06 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.25 on epoch=424
05/12/2022 17:45:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/12/2022 17:45:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
05/12/2022 17:45:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
05/12/2022 17:45:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 17:45:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/12/2022 17:45:31 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.125 on epoch=449
05/12/2022 17:45:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
05/12/2022 17:45:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
05/12/2022 17:45:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 17:45:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
05/12/2022 17:45:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
05/12/2022 17:45:55 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.1875 on epoch=474
05/12/2022 17:46:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
05/12/2022 17:46:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/12/2022 17:46:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
05/12/2022 17:46:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
05/12/2022 17:46:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
05/12/2022 17:46:20 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.1875 on epoch=499
05/12/2022 17:46:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/12/2022 17:46:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
05/12/2022 17:46:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
05/12/2022 17:46:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
05/12/2022 17:46:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
05/12/2022 17:46:44 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.125 on epoch=524
05/12/2022 17:46:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
05/12/2022 17:46:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/12/2022 17:46:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/12/2022 17:47:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
05/12/2022 17:47:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/12/2022 17:47:09 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.125 on epoch=549
05/12/2022 17:47:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
05/12/2022 17:47:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
05/12/2022 17:47:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
05/12/2022 17:47:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
05/12/2022 17:47:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
05/12/2022 17:47:34 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.125 on epoch=574
05/12/2022 17:47:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
05/12/2022 17:47:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
05/12/2022 17:47:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
05/12/2022 17:47:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
05/12/2022 17:47:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
05/12/2022 17:47:58 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.09375 on epoch=599
05/12/2022 17:48:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
05/12/2022 17:48:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
05/12/2022 17:48:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 17:48:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
05/12/2022 17:48:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/12/2022 17:48:23 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.09375 on epoch=624
05/12/2022 17:48:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 17:48:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
05/12/2022 17:48:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 17:48:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
05/12/2022 17:48:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
05/12/2022 17:48:48 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.21875 on epoch=649
05/12/2022 17:48:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
05/12/2022 17:48:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
05/12/2022 17:49:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 17:49:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
05/12/2022 17:49:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
05/12/2022 17:49:12 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.1875 on epoch=674
05/12/2022 17:49:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 17:49:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
05/12/2022 17:49:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/12/2022 17:49:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
05/12/2022 17:49:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
05/12/2022 17:49:37 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.1875 on epoch=699
05/12/2022 17:49:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 17:49:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 17:49:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
05/12/2022 17:49:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 17:49:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/12/2022 17:50:01 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.125 on epoch=724
05/12/2022 17:50:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
05/12/2022 17:50:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 17:50:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/12/2022 17:50:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 17:50:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
05/12/2022 17:50:25 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
05/12/2022 17:50:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
05/12/2022 17:50:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 17:50:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
05/12/2022 17:50:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 17:50:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 17:50:50 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.0625 on epoch=774
05/12/2022 17:50:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 17:50:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 17:51:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 17:51:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 17:51:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 17:51:14 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.09375 on epoch=799
05/12/2022 17:51:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
05/12/2022 17:51:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/12/2022 17:51:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
05/12/2022 17:51:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
05/12/2022 17:51:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 17:51:39 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.0625 on epoch=824
05/12/2022 17:51:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 17:51:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 17:51:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
05/12/2022 17:51:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
05/12/2022 17:52:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 17:52:03 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.125 on epoch=849
05/12/2022 17:52:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
05/12/2022 17:52:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
05/12/2022 17:52:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
05/12/2022 17:52:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
05/12/2022 17:52:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 17:52:27 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.125 on epoch=874
05/12/2022 17:52:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 17:52:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
05/12/2022 17:52:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 17:52:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 17:52:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
05/12/2022 17:52:52 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.125 on epoch=899
05/12/2022 17:52:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 17:53:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 17:53:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 17:53:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 17:53:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
05/12/2022 17:53:17 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
05/12/2022 17:53:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 17:53:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 17:53:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
05/12/2022 17:53:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
05/12/2022 17:53:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
05/12/2022 17:53:41 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.125 on epoch=949
05/12/2022 17:53:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
05/12/2022 17:53:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 17:53:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 17:53:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/12/2022 17:54:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
05/12/2022 17:54:06 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.0625 on epoch=974
05/12/2022 17:54:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
05/12/2022 17:54:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 17:54:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
05/12/2022 17:54:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 17:54:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 17:54:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:54:30 - INFO - __main__ - Printing 3 examples
05/12/2022 17:54:30 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:54:30 - INFO - __main__ - ['it was not installed']
05/12/2022 17:54:30 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:54:30 - INFO - __main__ - ['a few more months']
05/12/2022 17:54:30 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:54:30 - INFO - __main__ - ['still in the same position']
05/12/2022 17:54:30 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:54:30 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:54:30 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:54:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:54:30 - INFO - __main__ - Printing 3 examples
05/12/2022 17:54:30 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:54:30 - INFO - __main__ - ['not enough information']
05/12/2022 17:54:30 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:54:30 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:54:30 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:54:30 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:54:30 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:54:30 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:54:30 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:54:31 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.09375 on epoch=999
05/12/2022 17:54:31 - INFO - __main__ - save last model!
05/12/2022 17:54:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 17:54:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 17:54:31 - INFO - __main__ - Printing 3 examples
05/12/2022 17:54:31 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:54:31 - INFO - __main__ - ['about 10 minutes']
05/12/2022 17:54:31 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:54:31 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 17:54:31 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 17:54:31 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 17:54:31 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:54:32 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:54:33 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 17:54:48 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:54:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:54:48 - INFO - __main__ - Starting training!
05/12/2022 17:56:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.3_8_predictions.txt
05/12/2022 17:56:02 - INFO - __main__ - ACC on test data: 0.2550
05/12/2022 17:56:02 - INFO - __main__ - prefix=quail_32_13, lr=0.3, bsz=8, dev_performance=0.25, test_performance=0.255
05/12/2022 17:56:02 - INFO - __main__ - Running ... prefix=quail_32_13, lr=0.2, bsz=8 ...
05/12/2022 17:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:56:03 - INFO - __main__ - Printing 3 examples
05/12/2022 17:56:03 - INFO - __main__ -  [quail] Why was the remote start function ignored?(A)not enough information(B)it was not installed(C)it was not working(D)it was installed but not thought of [SEP] I saw one last night on an episode of Designated Survivor. To qualify this, I’m not normally bothered by product placements. Sometimes they can even add to the scene. But this one was bad, really bad. Agent is busy hunting for baddies. Finishes questioning one naughty person who climbs in his Ford F150 and speeds off. So far, so good - it is the sort of vehicle a country-living billionaire bad-guy would drive. The agent then pulls out her phone and… Move to close up of phone screen showing Ford app already loaded (and screen unlocked!). Agent slowly moves finger to app. Does something. Pause as app does fancy graphics. Cut to car interior. Shot of dash, showing Ford logo on steering wheel. It’s bright blue - are they not normally grey or more subtle? Pause for a second… Zoom in on dash. Dash light up. Car starts. Show pretty dash lights for a second or so. Cut to agent Agent walks to car, gets in drives off. Lingering shot of rear of car. It was just so clumsy. Massive halt to the flow of the scene to show it. In most films you never see anyone starting cars, putting on seatbelts or similar unless this is part of the plot because it’s unnecessary and not interesting to watch. I sort of expected the remote start function to have some sort of relevance later… but no, it was totally ignored. Added to that: There was no password or security on her phone - and this is an agent investigating super secret stuff. If you don’t show her unlocking the phone, why show her lovingly prodding the app? They are as relevant. She unlocked and started the car while she was 20–30 paces away, on the grounds of a suspect ranch. Someone could easily have jumped in the car. Not very security conscious.
05/12/2022 17:56:03 - INFO - __main__ - ['it was not installed']
05/12/2022 17:56:03 - INFO - __main__ -  [quail] The campaigns will probably last for:(A)not enough information(B)one year(C)a few more months(D)all day [SEP] WASHINGTON — Democratic presidential front-runner Hillary Clinton called Republican rival Donald Trump dangerous and unqualified for the presidency in a blistering foreign policy speech Thursday in San Diego, California. "He is temperamentally unfit to hold an office that requires knowledge, stability and immense responsibility," Clinton said. "This is not someone who should ever have the nuclear codes." Trump “doesn’t understand America, or the world,” she said. "It’s not hard to imagine Donald Trump leading us into a war just because somebody got under his very thin skin." In anticipation of the address, Trump attacked his Democratic opponent on Twitter. “Crooked Hillary Clinton, who I would love to call Lyin’ Hillary, is getting ready to totally misrepresent my foreign policy positions,” he tweeted. Clinton emphasized her own experience as first lady, senator and secretary of state, saying she would provide the steady diplomacy the country needs. “National security is the foundation of how we make sure our interests are pursued in the world,” said Louis Goodman, Emeritus Dean of International Relations at American University in an interview with VOA. With polls show terrorism is a major concern among Americans, Clinton targeted Trump's positions on the issue. Trump, the presumptive Republican presidential nominee, has promised to temporarily block Muslims from crossing U.S. borders. "The struggle against radical Islam also takes place in our homeland. There are scores of recent migrants inside our borders charged with terrorism. For every case known to the public, there are dozens more. We must stop importing extremism through senseless immigration policies," Trump said in a foreign policy speech in April. Trump's other anti-terrorism proposals include a pledge to torture and murder the families of suspected terrorists and target Islamic State. "I have a simple message for them," Trump said. "Their days are numbered. I won't tell them where and I won't tell them how. But they will be gone. And soon." But Clinton said Trump's presidency would have the opposite effect. “A Trump presidency would embolden ISIS,” she said referring to the group also known as Islamic State. The two presidential candidates have presented very different approaches to terrorism, which experts like Goodman believe would likely produce different results.
05/12/2022 17:56:03 - INFO - __main__ - ['a few more months']
05/12/2022 17:56:03 - INFO - __main__ -  [quail] After the end of the story, General Howerton probably is(A)retired(B)still in the same position(C)was promoted(D)not enough information [SEP] SEOUL — The annual U.S.–South Korea joint military exercises currently underway have historically been a very public demonstration of force intended, in part, to counter North Korea’s aggressive nuclear and missile tests. But with looming diplomatic summits aimed at reaching a deal to end the North’s nuclear program, media coverage of the drills has so far been limited. This week, the U.S. Second Infantry Division held a public tree planting ceremony at Camp Casey in South Korea, near the inter-Korean border. “It is my belief that this tree, like those that have come before it, shall symbolize the roots that bind us together, and to our past in an enduring way,” said General Jon Howerton, the deputy commander of the U.S. 2nd Infantry Division, at the event. The journalists attending the tree ceremony were also permitted to watch a shooting competition to determine the best marksmen in the participating U.S. and South Korean divisions. The Camp Casey event was one of the few joint military training activities that news organizations have, so far, been allowed to cover, and it illustrates how the United States and South Korea are downplaying the offensive component of this year’s combined exercises. “This is clearly in order to send a signal to North Korea that South Korea and the United States are not willing to escalate the situation any further,” said Go Myong-Hyun, a North Korea analyst with the Asan Institute for Policy Studies in Seoul. This year’s Foal Eagle and Key Resolve combined military exercises are similar in scale to past drills, with more than 23,000 U.S. troops and 300,000 South Korean forces engaging in joint battlefield maneuvers, and responding to computer-simulated attacks from North Korea. But the start of the drills was delayed this year at South Korea’s request to set a peaceful tone for the Pyeongchang Winter Olympics in the South and to facilitate the North’s participation in the games.
05/12/2022 17:56:03 - INFO - __main__ - ['still in the same position']
05/12/2022 17:56:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:56:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:56:03 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 17:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 17:56:03 - INFO - __main__ - Printing 3 examples
05/12/2022 17:56:03 - INFO - __main__ -  [quail] When will Aya be able to perform magic like Li Reiko?(A)Two years after the events in the story(B)not enough information(C)Three years after the events in the story(D)One year after the events in the story [SEP] Light dappled through the trees in the family courtyard, painting shadows on the paving stones. Li Reiko knelt by her son to look at his scraped knee. "I just scratched it." Nawi squirmed under her hands. Her daughter, Aya, leaned over her shoulder studying the healing. "Maybe Mama will show you her armor after she heals you." Nawi stopped wiggling. "Really?" Reiko shot Aya a warning look, but her little boy's dark eyes shone with excitement. Reiko smiled. "Really." What did tradition matter? "Now let me heal your knee." She laid her hand on the shallow wound. "Ow." "Shush." Reiko closed her eyes and rose in the dark space within her mind. In her mind's eye, Reiko took her time with the ritual, knowing it took less time than it appeared. In a heartbeat, green fire flared out to the walls of her mind. She dissolved into it as she focused on healing her son. When the wound closed beneath her hand, she sank to the surface of her mind. "There." She tousled Nawi's hair. "That wasn't bad, was it?" "It tickled." He wrinkled his nose. "Will you show me your armor now?" She sighed. She should not encourage his interest in the martial arts. His work would be with the histories that men kept, and yet... "Watch." Pulling the smooth black surface out of the ether, she manifested her armor. It sheathed her like silence in the night. Aya watched with obvious anticipation for the day when she earned her own armor. Nawi's face, full of sharp yearning for something he would never have, cut Reiko's heart like a new blade. "Can I see your sword?" She let her armor vanish back into thought. "No." Reiko brushed his hair from his eyes. "It's my turn to hide, right?"
05/12/2022 17:56:03 - INFO - __main__ - ['not enough information']
05/12/2022 17:56:03 - INFO - __main__ -  [quail] What did participants in the study eat?(A)something including sugar(B)something including carbohydrates(C)not enough information(D)something including fats [SEP] Called the PURE (Prospective Urban Rural Epidemiology) study, this was a very large observational study looking at the link between fat and carbohydrate consumption and longevity in 18 countries across 5 continents. Running for 7 years and involving over 135,000 people, this was a very big study so its conclusions are right to take notice of. The key finding from the work that attracted the most media attention was that a high carbohydrate diet was linked with a higher risk of earlier mortality, whereas total fat and individual types of fat were related to a lower risk of earlier mortality. Digging deeper into the study, the research team found that global diets consisted of 61 percent energy coming from carbohydrates and 24 percent energy from fats. And while those in the highest carbohydrate consumption group (a whopping 77 percent of energy) had a higher risk of earlier death, it wasn’t cardiovascular disease they were dying from. What those other causes of death were exactly is unclear. Perhaps getting hit by a car running for a Mars Bar was one of them as a recent commenter on my Facebook page theorised. A paradigm shift? Not quite Does this study turn on its head ‘everything we knew about nutrition?’ Not quite. And here’s why. Before the PURE study, there were many studies showing the opposite link between carbohydrates and longevity. So, when a conflicting study comes along, this grabs the media spotlight for the day. Here is just one example – a major systematic review and meta-analysis from 2013 involving 17 individual studies and over 242,000 people showing a higher risk of earlier mortality as carbohydrate intake decreased. And this is the problem at times with observational research in that two studies can give polar opposite results so the findings of the PURE study should be seen through this filter. I’m not going to pick apart the PURE study for its flaws. Such issues are consistent across all observational studies no matter if the conclusions support consensus views or not. What is of value to look at is the positive messages the study gave and how when you look at the full research field, it takes you back to some pretty sensible advice.
05/12/2022 17:56:03 - INFO - __main__ - ['something including carbohydrates']
05/12/2022 17:56:03 - INFO - __main__ -  [quail] When will a potential recipient of hand-me-downs decline an offer, according to the text?(A)if they conclude that they do not want the item being offered(B)if they do not have children(C)not enough information(D)if they think it is too expensive [SEP] I have been preparing our house for the market and in doing so, I have gotten rid of a lot of stuff! I am definitely the hoarder in our house. My husband could live out of two bags, use about five kitchen items each year, and doesn’t gather anything for future use or hang on to much for sentimental value. (Which probably means the items he has hung on to mean a whole lot more!) I am always tucking something away here or stashing materials there…all in preparation for “some day.” It’s also part of the teacher in me. Do you know many teachers that don’t have a ton of stuff or utilize every bit of storage available? But, over the last several years, I’ve been fairly good about going through things every six months and weeding out a little here and a little there. Today I’ll be sharing six simple ways to declutter your home and why you should! GIVE THINGS AWAY It’s nice to make money, but sometimes you come across something that you really think someone else could use and you don’t want to throw it away. If it’s the perfect fit for that person, they may adopt the item and add their own wear and tear! Anyone that’s had children knows that kids go through things so fast and it’s nice to save a little money by taking hand-me-downs from a friend or relative. If the receiver decides they don’t want the item, let it be. They’ll either get rid of it on their own or decline the offer. If they choose the latter, maybe the rest of this list will help. PACK If you know you don’t want to purge an item from your house AND you know that you will use it in the future, but it’s not an everyday use item, pack it up. We have several containers of things in our garage that are full of items we use once or twice each year. I have added close to 100 boxes of things to simply declutter our home while it’s on the market. I took a look at everything and kept the essentials (well, maybe even more than the essentials), and packed up the rest.
05/12/2022 17:56:03 - INFO - __main__ - ['if they conclude that they do not want the item being offered']
05/12/2022 17:56:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 17:56:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 17:56:04 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 17:56:22 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 17:56:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 17:56:23 - INFO - __main__ - Starting training!
05/12/2022 17:56:28 - INFO - __main__ - Step 10 Global step 10 Train loss 2.08 on epoch=4
05/12/2022 17:56:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.58 on epoch=9
05/12/2022 17:56:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
05/12/2022 17:56:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=19
05/12/2022 17:56:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=24
05/12/2022 17:56:48 - INFO - __main__ - Global step 50 Train loss 1.33 ACC 0.1875 on epoch=24
05/12/2022 17:56:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
05/12/2022 17:56:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
05/12/2022 17:56:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=34
05/12/2022 17:57:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
05/12/2022 17:57:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=44
05/12/2022 17:57:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
05/12/2022 17:57:13 - INFO - __main__ - Global step 100 Train loss 0.66 ACC 0.125 on epoch=49
05/12/2022 17:57:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=54
05/12/2022 17:57:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=59
05/12/2022 17:57:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
05/12/2022 17:57:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
05/12/2022 17:57:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=74
05/12/2022 17:57:37 - INFO - __main__ - Global step 150 Train loss 0.50 ACC 0.125 on epoch=74
05/12/2022 17:57:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
05/12/2022 17:57:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
05/12/2022 17:57:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
05/12/2022 17:57:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
05/12/2022 17:57:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
05/12/2022 17:58:02 - INFO - __main__ - Global step 200 Train loss 0.39 ACC 0.125 on epoch=99
05/12/2022 17:58:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
05/12/2022 17:58:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
05/12/2022 17:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
05/12/2022 17:58:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
05/12/2022 17:58:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
05/12/2022 17:58:26 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.09375 on epoch=124
05/12/2022 17:58:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
05/12/2022 17:58:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=134
05/12/2022 17:58:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
05/12/2022 17:58:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
05/12/2022 17:58:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
05/12/2022 17:58:51 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.125 on epoch=149
05/12/2022 17:58:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
05/12/2022 17:59:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
05/12/2022 17:59:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
05/12/2022 17:59:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
05/12/2022 17:59:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
05/12/2022 17:59:15 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.15625 on epoch=174
05/12/2022 17:59:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
05/12/2022 17:59:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
05/12/2022 17:59:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
05/12/2022 17:59:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
05/12/2022 17:59:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
05/12/2022 17:59:40 - INFO - __main__ - Global step 400 Train loss 0.23 ACC 0.15625 on epoch=199
05/12/2022 17:59:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
05/12/2022 17:59:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
05/12/2022 17:59:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
05/12/2022 17:59:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
05/12/2022 18:00:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
05/12/2022 18:00:06 - INFO - __main__ - Global step 450 Train loss 0.19 ACC 0.15625 on epoch=224
05/12/2022 18:00:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=229
05/12/2022 18:00:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
05/12/2022 18:00:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=239
05/12/2022 18:00:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=244
05/12/2022 18:00:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
05/12/2022 18:00:30 - INFO - __main__ - Global step 500 Train loss 0.18 ACC 0.1875 on epoch=249
05/12/2022 18:00:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
05/12/2022 18:00:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
05/12/2022 18:00:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
05/12/2022 18:00:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
05/12/2022 18:00:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
05/12/2022 18:00:55 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.1875 on epoch=274
05/12/2022 18:00:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
05/12/2022 18:01:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=284
05/12/2022 18:01:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
05/12/2022 18:01:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=294
05/12/2022 18:01:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
05/12/2022 18:01:19 - INFO - __main__ - Global step 600 Train loss 0.14 ACC 0.1875 on epoch=299
05/12/2022 18:01:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
05/12/2022 18:01:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
05/12/2022 18:01:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
05/12/2022 18:01:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
05/12/2022 18:01:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
05/12/2022 18:01:44 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.21875 on epoch=324
05/12/2022 18:01:44 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=324, global_step=650
05/12/2022 18:01:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
05/12/2022 18:01:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
05/12/2022 18:01:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
05/12/2022 18:02:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
05/12/2022 18:02:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
05/12/2022 18:02:08 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.25 on epoch=349
05/12/2022 18:02:08 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=349, global_step=700
05/12/2022 18:02:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
05/12/2022 18:02:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=359
05/12/2022 18:02:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
05/12/2022 18:02:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
05/12/2022 18:02:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
05/12/2022 18:02:33 - INFO - __main__ - Global step 750 Train loss 0.08 ACC 0.1875 on epoch=374
05/12/2022 18:02:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
05/12/2022 18:02:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
05/12/2022 18:02:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
05/12/2022 18:02:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
05/12/2022 18:02:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=399
05/12/2022 18:02:57 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.21875 on epoch=399
05/12/2022 18:03:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
05/12/2022 18:03:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
05/12/2022 18:03:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=414
05/12/2022 18:03:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
05/12/2022 18:03:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
05/12/2022 18:03:22 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.1875 on epoch=424
05/12/2022 18:03:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 18:03:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
05/12/2022 18:03:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
05/12/2022 18:03:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
05/12/2022 18:03:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
05/12/2022 18:03:46 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.1875 on epoch=449
05/12/2022 18:03:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
05/12/2022 18:03:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
05/12/2022 18:04:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
05/12/2022 18:04:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
05/12/2022 18:04:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
05/12/2022 18:04:11 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.1875 on epoch=474
05/12/2022 18:04:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
05/12/2022 18:04:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/12/2022 18:04:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
05/12/2022 18:04:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
05/12/2022 18:04:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
05/12/2022 18:04:35 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.15625 on epoch=499
05/12/2022 18:04:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
05/12/2022 18:04:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/12/2022 18:04:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
05/12/2022 18:04:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
05/12/2022 18:04:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
05/12/2022 18:05:00 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.15625 on epoch=524
05/12/2022 18:05:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
05/12/2022 18:05:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
05/12/2022 18:05:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
05/12/2022 18:05:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
05/12/2022 18:05:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
05/12/2022 18:05:24 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.15625 on epoch=549
05/12/2022 18:05:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
05/12/2022 18:05:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
05/12/2022 18:05:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.64 on epoch=564
05/12/2022 18:05:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=569
05/12/2022 18:05:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=574
05/12/2022 18:05:48 - INFO - __main__ - Global step 1150 Train loss 0.21 ACC 0.21875 on epoch=574
05/12/2022 18:05:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
05/12/2022 18:05:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
05/12/2022 18:06:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
05/12/2022 18:06:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
05/12/2022 18:06:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
05/12/2022 18:06:12 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.15625 on epoch=599
05/12/2022 18:06:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
05/12/2022 18:06:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
05/12/2022 18:06:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
05/12/2022 18:06:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
05/12/2022 18:06:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
05/12/2022 18:06:37 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.1875 on epoch=624
05/12/2022 18:06:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
05/12/2022 18:06:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
05/12/2022 18:06:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
05/12/2022 18:06:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
05/12/2022 18:06:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
05/12/2022 18:07:01 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.21875 on epoch=649
05/12/2022 18:07:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=654
05/12/2022 18:07:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=659
05/12/2022 18:07:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
05/12/2022 18:07:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
05/12/2022 18:07:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 18:07:25 - INFO - __main__ - Global step 1350 Train loss 0.07 ACC 0.1875 on epoch=674
05/12/2022 18:07:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
05/12/2022 18:07:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=684
05/12/2022 18:07:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
05/12/2022 18:07:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
05/12/2022 18:07:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 18:07:49 - INFO - __main__ - Global step 1400 Train loss 0.05 ACC 0.25 on epoch=699
05/12/2022 18:07:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 18:07:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=709
05/12/2022 18:08:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
05/12/2022 18:08:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
05/12/2022 18:08:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
05/12/2022 18:08:14 - INFO - __main__ - Global step 1450 Train loss 0.04 ACC 0.28125 on epoch=724
05/12/2022 18:08:14 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=724, global_step=1450
05/12/2022 18:08:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 18:08:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 18:08:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 18:08:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
05/12/2022 18:08:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
05/12/2022 18:08:38 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.3125 on epoch=749
05/12/2022 18:08:38 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=749, global_step=1500
05/12/2022 18:08:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
05/12/2022 18:08:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 18:08:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
05/12/2022 18:08:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
05/12/2022 18:09:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 18:09:03 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.15625 on epoch=774
05/12/2022 18:09:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 18:09:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
05/12/2022 18:09:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
05/12/2022 18:09:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
05/12/2022 18:09:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 18:09:27 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.21875 on epoch=799
05/12/2022 18:09:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
05/12/2022 18:09:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=809
05/12/2022 18:09:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 18:09:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
05/12/2022 18:09:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
05/12/2022 18:09:51 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.25 on epoch=824
05/12/2022 18:09:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=829
05/12/2022 18:10:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 18:10:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 18:10:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
05/12/2022 18:10:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
05/12/2022 18:10:15 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.1875 on epoch=849
05/12/2022 18:10:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
05/12/2022 18:10:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
05/12/2022 18:10:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 18:10:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 18:10:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
05/12/2022 18:10:40 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.25 on epoch=874
05/12/2022 18:10:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 18:10:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 18:10:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
05/12/2022 18:10:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 18:11:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
05/12/2022 18:11:04 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.25 on epoch=899
05/12/2022 18:11:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 18:11:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
05/12/2022 18:11:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
05/12/2022 18:11:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 18:11:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 18:11:29 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.25 on epoch=924
05/12/2022 18:11:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 18:11:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=934
05/12/2022 18:11:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
05/12/2022 18:11:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 18:11:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 18:11:53 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.25 on epoch=949
05/12/2022 18:11:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 18:12:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 18:12:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 18:12:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
05/12/2022 18:12:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
05/12/2022 18:12:17 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.28125 on epoch=974
05/12/2022 18:12:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
05/12/2022 18:12:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 18:12:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 18:12:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 18:12:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
05/12/2022 18:12:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:12:41 - INFO - __main__ - Printing 3 examples
05/12/2022 18:12:41 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:12:41 - INFO - __main__ - ['Cleveland']
05/12/2022 18:12:41 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:12:41 - INFO - __main__ - ['Frustration.']
05/12/2022 18:12:41 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:12:41 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:12:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:12:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:12:41 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:12:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:12:41 - INFO - __main__ - Printing 3 examples
05/12/2022 18:12:41 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:12:41 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:12:41 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:12:41 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:12:41 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:12:41 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:12:41 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:12:41 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:12:41 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:12:42 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.3125 on epoch=999
05/12/2022 18:12:42 - INFO - __main__ - save last model!
05/12/2022 18:12:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 18:12:42 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 18:12:42 - INFO - __main__ - Printing 3 examples
05/12/2022 18:12:42 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:12:42 - INFO - __main__ - ['about 10 minutes']
05/12/2022 18:12:42 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:12:42 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 18:12:42 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:12:42 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 18:12:42 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:12:44 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:12:45 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 18:12:59 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:13:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:13:00 - INFO - __main__ - Starting training!
05/12/2022 18:14:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_13_0.2_8_predictions.txt
05/12/2022 18:14:14 - INFO - __main__ - ACC on test data: 0.2720
05/12/2022 18:14:14 - INFO - __main__ - prefix=quail_32_13, lr=0.2, bsz=8, dev_performance=0.3125, test_performance=0.272
05/12/2022 18:14:15 - INFO - __main__ - Running ... prefix=quail_32_21, lr=0.5, bsz=8 ...
05/12/2022 18:14:15 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:14:15 - INFO - __main__ - Printing 3 examples
05/12/2022 18:14:15 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:14:15 - INFO - __main__ - ['Cleveland']
05/12/2022 18:14:15 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:14:15 - INFO - __main__ - ['Frustration.']
05/12/2022 18:14:15 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:14:15 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:14:15 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:14:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:14:16 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:14:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:14:16 - INFO - __main__ - Printing 3 examples
05/12/2022 18:14:16 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:14:16 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:14:16 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:14:16 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:14:16 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:14:16 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:14:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:14:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:14:16 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:14:31 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:14:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:14:32 - INFO - __main__ - Starting training!
05/12/2022 18:14:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.77 on epoch=4
05/12/2022 18:14:41 - INFO - __main__ - Step 20 Global step 20 Train loss 1.16 on epoch=9
05/12/2022 18:14:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.91 on epoch=14
05/12/2022 18:14:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.70 on epoch=19
05/12/2022 18:14:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
05/12/2022 18:14:57 - INFO - __main__ - Global step 50 Train loss 1.03 ACC 0.15625 on epoch=24
05/12/2022 18:14:57 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 18:15:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=29
05/12/2022 18:15:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.68 on epoch=34
05/12/2022 18:15:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.18 on epoch=39
05/12/2022 18:15:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=44
05/12/2022 18:15:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=49
05/12/2022 18:15:22 - INFO - __main__ - Global step 100 Train loss 0.83 ACC 0.21875 on epoch=49
05/12/2022 18:15:22 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=49, global_step=100
05/12/2022 18:15:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=54
05/12/2022 18:15:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=59
05/12/2022 18:15:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
05/12/2022 18:15:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=69
05/12/2022 18:15:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=74
05/12/2022 18:15:47 - INFO - __main__ - Global step 150 Train loss 0.53 ACC 0.15625 on epoch=74
05/12/2022 18:15:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=79
05/12/2022 18:15:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
05/12/2022 18:16:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
05/12/2022 18:16:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
05/12/2022 18:16:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
05/12/2022 18:16:11 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.21875 on epoch=99
05/12/2022 18:16:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
05/12/2022 18:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=109
05/12/2022 18:16:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
05/12/2022 18:16:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
05/12/2022 18:16:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
05/12/2022 18:16:36 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.15625 on epoch=124
05/12/2022 18:16:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
05/12/2022 18:16:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
05/12/2022 18:16:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
05/12/2022 18:16:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
05/12/2022 18:16:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
05/12/2022 18:17:00 - INFO - __main__ - Global step 300 Train loss 0.32 ACC 0.15625 on epoch=149
05/12/2022 18:17:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
05/12/2022 18:17:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=159
05/12/2022 18:17:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
05/12/2022 18:17:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
05/12/2022 18:17:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
05/12/2022 18:17:25 - INFO - __main__ - Global step 350 Train loss 0.31 ACC 0.15625 on epoch=174
05/12/2022 18:17:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
05/12/2022 18:17:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
05/12/2022 18:17:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
05/12/2022 18:17:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
05/12/2022 18:17:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
05/12/2022 18:17:49 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.15625 on epoch=199
05/12/2022 18:17:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
05/12/2022 18:17:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
05/12/2022 18:18:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
05/12/2022 18:18:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
05/12/2022 18:18:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
05/12/2022 18:18:14 - INFO - __main__ - Global step 450 Train loss 0.21 ACC 0.15625 on epoch=224
05/12/2022 18:18:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=229
05/12/2022 18:18:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=234
05/12/2022 18:18:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=239
05/12/2022 18:18:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=244
05/12/2022 18:18:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
05/12/2022 18:18:38 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.125 on epoch=249
05/12/2022 18:18:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
05/12/2022 18:18:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
05/12/2022 18:18:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
05/12/2022 18:18:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
05/12/2022 18:19:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=274
05/12/2022 18:19:03 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.21875 on epoch=274
05/12/2022 18:19:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
05/12/2022 18:19:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
05/12/2022 18:19:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=289
05/12/2022 18:19:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
05/12/2022 18:19:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=299
05/12/2022 18:19:28 - INFO - __main__ - Global step 600 Train loss 0.14 ACC 0.09375 on epoch=299
05/12/2022 18:19:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
05/12/2022 18:19:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
05/12/2022 18:19:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=314
05/12/2022 18:19:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=319
05/12/2022 18:19:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
05/12/2022 18:19:52 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.21875 on epoch=324
05/12/2022 18:19:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
05/12/2022 18:20:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
05/12/2022 18:20:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
05/12/2022 18:20:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
05/12/2022 18:20:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
05/12/2022 18:20:17 - INFO - __main__ - Global step 700 Train loss 0.09 ACC 0.15625 on epoch=349
05/12/2022 18:20:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
05/12/2022 18:20:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
05/12/2022 18:20:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
05/12/2022 18:20:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
05/12/2022 18:20:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=374
05/12/2022 18:20:41 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.21875 on epoch=374
05/12/2022 18:20:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
05/12/2022 18:20:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/12/2022 18:20:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
05/12/2022 18:20:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
05/12/2022 18:21:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
05/12/2022 18:21:06 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.21875 on epoch=399
05/12/2022 18:21:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=404
05/12/2022 18:21:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
05/12/2022 18:21:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
05/12/2022 18:21:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
05/12/2022 18:21:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
05/12/2022 18:21:31 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.21875 on epoch=424
05/12/2022 18:21:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 18:21:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
05/12/2022 18:21:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
05/12/2022 18:21:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 18:21:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
05/12/2022 18:21:55 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.125 on epoch=449
05/12/2022 18:22:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
05/12/2022 18:22:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
05/12/2022 18:22:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
05/12/2022 18:22:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
05/12/2022 18:22:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
05/12/2022 18:22:19 - INFO - __main__ - Global step 950 Train loss 0.06 ACC 0.125 on epoch=474
05/12/2022 18:22:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
05/12/2022 18:22:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
05/12/2022 18:22:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
05/12/2022 18:22:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 18:22:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
05/12/2022 18:22:44 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.28125 on epoch=499
05/12/2022 18:22:44 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=499, global_step=1000
05/12/2022 18:22:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/12/2022 18:22:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
05/12/2022 18:22:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
05/12/2022 18:23:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
05/12/2022 18:23:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
05/12/2022 18:23:09 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.15625 on epoch=524
05/12/2022 18:23:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
05/12/2022 18:23:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
05/12/2022 18:23:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
05/12/2022 18:23:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
05/12/2022 18:23:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
05/12/2022 18:23:34 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.15625 on epoch=549
05/12/2022 18:23:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 18:23:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/12/2022 18:23:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
05/12/2022 18:23:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
05/12/2022 18:23:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
05/12/2022 18:23:59 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.1875 on epoch=574
05/12/2022 18:24:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
05/12/2022 18:24:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
05/12/2022 18:24:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
05/12/2022 18:24:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
05/12/2022 18:24:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
05/12/2022 18:24:23 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.28125 on epoch=599
05/12/2022 18:24:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 18:24:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
05/12/2022 18:24:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
05/12/2022 18:24:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
05/12/2022 18:24:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=624
05/12/2022 18:24:48 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.28125 on epoch=624
05/12/2022 18:24:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
05/12/2022 18:24:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 18:25:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 18:25:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
05/12/2022 18:25:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
05/12/2022 18:25:13 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.09375 on epoch=649
05/12/2022 18:25:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
05/12/2022 18:25:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
05/12/2022 18:25:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
05/12/2022 18:25:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
05/12/2022 18:25:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
05/12/2022 18:25:37 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.1875 on epoch=674
05/12/2022 18:25:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
05/12/2022 18:25:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
05/12/2022 18:25:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
05/12/2022 18:25:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
05/12/2022 18:26:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 18:26:02 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.21875 on epoch=699
05/12/2022 18:26:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 18:26:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
05/12/2022 18:26:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
05/12/2022 18:26:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
05/12/2022 18:26:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
05/12/2022 18:26:27 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.21875 on epoch=724
05/12/2022 18:26:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
05/12/2022 18:26:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/12/2022 18:26:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 18:26:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 18:26:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 18:26:52 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.1875 on epoch=749
05/12/2022 18:26:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
05/12/2022 18:27:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
05/12/2022 18:27:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 18:27:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
05/12/2022 18:27:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 18:27:17 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.1875 on epoch=774
05/12/2022 18:27:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
05/12/2022 18:27:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 18:27:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 18:27:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/12/2022 18:27:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 18:27:41 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.3125 on epoch=799
05/12/2022 18:27:41 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=799, global_step=1600
05/12/2022 18:27:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 18:27:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/12/2022 18:27:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 18:27:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 18:28:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
05/12/2022 18:28:06 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.3125 on epoch=824
05/12/2022 18:28:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 18:28:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
05/12/2022 18:28:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
05/12/2022 18:28:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=844
05/12/2022 18:28:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 18:28:31 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.1875 on epoch=849
05/12/2022 18:28:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
05/12/2022 18:28:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 18:28:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
05/12/2022 18:28:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=869
05/12/2022 18:28:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 18:28:56 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.1875 on epoch=874
05/12/2022 18:29:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 18:29:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
05/12/2022 18:29:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
05/12/2022 18:29:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
05/12/2022 18:29:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
05/12/2022 18:29:21 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.3125 on epoch=899
05/12/2022 18:29:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 18:29:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
05/12/2022 18:29:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
05/12/2022 18:29:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 18:29:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 18:29:46 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.21875 on epoch=924
05/12/2022 18:29:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
05/12/2022 18:29:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 18:29:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/12/2022 18:30:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 18:30:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
05/12/2022 18:30:11 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.21875 on epoch=949
05/12/2022 18:30:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 18:30:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
05/12/2022 18:30:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 18:30:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 18:30:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 18:30:36 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.25 on epoch=974
05/12/2022 18:30:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 18:30:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 18:30:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 18:30:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 18:30:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
05/12/2022 18:31:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:31:00 - INFO - __main__ - Printing 3 examples
05/12/2022 18:31:00 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:31:00 - INFO - __main__ - ['Cleveland']
05/12/2022 18:31:00 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:31:00 - INFO - __main__ - ['Frustration.']
05/12/2022 18:31:00 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:31:00 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:31:00 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:31:00 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:31:00 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:31:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:31:00 - INFO - __main__ - Printing 3 examples
05/12/2022 18:31:00 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:31:00 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:31:00 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:31:00 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:31:00 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:31:00 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:31:00 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:31:00 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:31:00 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:31:01 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.25 on epoch=999
05/12/2022 18:31:01 - INFO - __main__ - save last model!
05/12/2022 18:31:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 18:31:01 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 18:31:01 - INFO - __main__ - Printing 3 examples
05/12/2022 18:31:01 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:31:01 - INFO - __main__ - ['about 10 minutes']
05/12/2022 18:31:01 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:31:01 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 18:31:01 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:31:01 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 18:31:01 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:31:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:31:04 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 18:31:18 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:31:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:31:19 - INFO - __main__ - Starting training!
05/12/2022 18:32:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_21_0.5_8_predictions.txt
05/12/2022 18:32:33 - INFO - __main__ - ACC on test data: 0.2340
05/12/2022 18:32:33 - INFO - __main__ - prefix=quail_32_21, lr=0.5, bsz=8, dev_performance=0.3125, test_performance=0.234
05/12/2022 18:32:33 - INFO - __main__ - Running ... prefix=quail_32_21, lr=0.4, bsz=8 ...
05/12/2022 18:32:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:32:34 - INFO - __main__ - Printing 3 examples
05/12/2022 18:32:34 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:32:34 - INFO - __main__ - ['Cleveland']
05/12/2022 18:32:34 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:32:34 - INFO - __main__ - ['Frustration.']
05/12/2022 18:32:34 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:32:34 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:32:34 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:32:34 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:32:34 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:32:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:32:34 - INFO - __main__ - Printing 3 examples
05/12/2022 18:32:34 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:32:34 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:32:34 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:32:34 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:32:34 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:32:34 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:32:34 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:32:34 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:32:34 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:32:53 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:32:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:32:54 - INFO - __main__ - Starting training!
05/12/2022 18:32:59 - INFO - __main__ - Step 10 Global step 10 Train loss 1.73 on epoch=4
05/12/2022 18:33:03 - INFO - __main__ - Step 20 Global step 20 Train loss 1.20 on epoch=9
05/12/2022 18:33:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=14
05/12/2022 18:33:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=19
05/12/2022 18:33:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
05/12/2022 18:33:20 - INFO - __main__ - Global step 50 Train loss 1.05 ACC 0.15625 on epoch=24
05/12/2022 18:33:20 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 18:33:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
05/12/2022 18:33:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
05/12/2022 18:33:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=39
05/12/2022 18:33:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
05/12/2022 18:33:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
05/12/2022 18:33:46 - INFO - __main__ - Global step 100 Train loss 0.49 ACC 0.125 on epoch=49
05/12/2022 18:33:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
05/12/2022 18:33:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=59
05/12/2022 18:33:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
05/12/2022 18:34:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
05/12/2022 18:34:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
05/12/2022 18:34:11 - INFO - __main__ - Global step 150 Train loss 0.32 ACC 0.15625 on epoch=74
05/12/2022 18:34:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
05/12/2022 18:34:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
05/12/2022 18:34:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
05/12/2022 18:34:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
05/12/2022 18:34:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/12/2022 18:34:36 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.125 on epoch=99
05/12/2022 18:34:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
05/12/2022 18:34:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.17 on epoch=109
05/12/2022 18:34:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
05/12/2022 18:34:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
05/12/2022 18:34:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
05/12/2022 18:35:01 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.1875 on epoch=124
05/12/2022 18:35:01 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=124, global_step=250
05/12/2022 18:35:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
05/12/2022 18:35:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
05/12/2022 18:35:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
05/12/2022 18:35:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
05/12/2022 18:35:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
05/12/2022 18:35:26 - INFO - __main__ - Global step 300 Train loss 0.16 ACC 0.28125 on epoch=149
05/12/2022 18:35:26 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.28125 on epoch=149, global_step=300
05/12/2022 18:35:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.12 on epoch=154
05/12/2022 18:35:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
05/12/2022 18:35:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
05/12/2022 18:35:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
05/12/2022 18:35:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
05/12/2022 18:35:51 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.1875 on epoch=174
05/12/2022 18:35:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
05/12/2022 18:36:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.11 on epoch=184
05/12/2022 18:36:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
05/12/2022 18:36:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
05/12/2022 18:36:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
05/12/2022 18:36:16 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.15625 on epoch=199
05/12/2022 18:36:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.08 on epoch=204
05/12/2022 18:36:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
05/12/2022 18:36:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
05/12/2022 18:36:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
05/12/2022 18:36:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
05/12/2022 18:36:41 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.15625 on epoch=224
05/12/2022 18:36:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
05/12/2022 18:36:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
05/12/2022 18:36:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
05/12/2022 18:36:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
05/12/2022 18:37:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
05/12/2022 18:37:07 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.1875 on epoch=249
05/12/2022 18:37:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
05/12/2022 18:37:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
05/12/2022 18:37:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
05/12/2022 18:37:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
05/12/2022 18:37:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
05/12/2022 18:37:32 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.21875 on epoch=274
05/12/2022 18:37:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
05/12/2022 18:37:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
05/12/2022 18:37:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
05/12/2022 18:37:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
05/12/2022 18:37:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
05/12/2022 18:37:57 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.15625 on epoch=299
05/12/2022 18:38:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
05/12/2022 18:38:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
05/12/2022 18:38:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=314
05/12/2022 18:38:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
05/12/2022 18:38:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/12/2022 18:38:23 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.1875 on epoch=324
05/12/2022 18:38:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
05/12/2022 18:38:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
05/12/2022 18:38:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
05/12/2022 18:38:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
05/12/2022 18:38:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
05/12/2022 18:38:48 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.21875 on epoch=349
05/12/2022 18:38:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
05/12/2022 18:38:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
05/12/2022 18:39:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
05/12/2022 18:39:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/12/2022 18:39:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
05/12/2022 18:39:13 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.25 on epoch=374
05/12/2022 18:39:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
05/12/2022 18:39:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
05/12/2022 18:39:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
05/12/2022 18:39:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
05/12/2022 18:39:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
05/12/2022 18:39:39 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.15625 on epoch=399
05/12/2022 18:39:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
05/12/2022 18:39:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
05/12/2022 18:39:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/12/2022 18:39:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
05/12/2022 18:40:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
05/12/2022 18:40:04 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.21875 on epoch=424
05/12/2022 18:40:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
05/12/2022 18:40:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
05/12/2022 18:40:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=439
05/12/2022 18:40:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
05/12/2022 18:40:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/12/2022 18:40:29 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.15625 on epoch=449
05/12/2022 18:40:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
05/12/2022 18:40:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/12/2022 18:40:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 18:40:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 18:40:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
05/12/2022 18:40:55 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.25 on epoch=474
05/12/2022 18:40:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
05/12/2022 18:41:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/12/2022 18:41:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
05/12/2022 18:41:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 18:41:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
05/12/2022 18:41:20 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.21875 on epoch=499
05/12/2022 18:41:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/12/2022 18:41:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
05/12/2022 18:41:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
05/12/2022 18:41:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
05/12/2022 18:41:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
05/12/2022 18:41:45 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.15625 on epoch=524
05/12/2022 18:41:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/12/2022 18:41:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/12/2022 18:41:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/12/2022 18:42:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
05/12/2022 18:42:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/12/2022 18:42:10 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.28125 on epoch=549
05/12/2022 18:42:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/12/2022 18:42:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
05/12/2022 18:42:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
05/12/2022 18:42:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
05/12/2022 18:42:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
05/12/2022 18:42:36 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.125 on epoch=574
05/12/2022 18:42:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
05/12/2022 18:42:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 18:42:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
05/12/2022 18:42:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 18:42:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
05/12/2022 18:43:02 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.21875 on epoch=599
05/12/2022 18:43:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
05/12/2022 18:43:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/12/2022 18:43:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
05/12/2022 18:43:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/12/2022 18:43:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/12/2022 18:43:27 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
05/12/2022 18:43:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 18:43:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
05/12/2022 18:43:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
05/12/2022 18:43:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
05/12/2022 18:43:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/12/2022 18:43:53 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
05/12/2022 18:43:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
05/12/2022 18:44:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
05/12/2022 18:44:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 18:44:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
05/12/2022 18:44:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
05/12/2022 18:44:18 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.25 on epoch=674
05/12/2022 18:44:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 18:44:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/12/2022 18:44:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
05/12/2022 18:44:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
05/12/2022 18:44:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
05/12/2022 18:44:44 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.28125 on epoch=699
05/12/2022 18:44:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
05/12/2022 18:44:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/12/2022 18:44:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
05/12/2022 18:45:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/12/2022 18:45:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/12/2022 18:45:09 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.1875 on epoch=724
05/12/2022 18:45:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
05/12/2022 18:45:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
05/12/2022 18:45:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
05/12/2022 18:45:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 18:45:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
05/12/2022 18:45:35 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.28125 on epoch=749
05/12/2022 18:45:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
05/12/2022 18:45:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
05/12/2022 18:45:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 18:45:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
05/12/2022 18:45:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 18:46:00 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.21875 on epoch=774
05/12/2022 18:46:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
05/12/2022 18:46:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
05/12/2022 18:46:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 18:46:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 18:46:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/12/2022 18:46:25 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
05/12/2022 18:46:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
05/12/2022 18:46:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
05/12/2022 18:46:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
05/12/2022 18:46:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 18:46:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
05/12/2022 18:46:51 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.15625 on epoch=824
05/12/2022 18:46:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 18:47:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
05/12/2022 18:47:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
05/12/2022 18:47:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
05/12/2022 18:47:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
05/12/2022 18:47:16 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.25 on epoch=849
05/12/2022 18:47:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 18:47:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 18:47:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
05/12/2022 18:47:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 18:47:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 18:47:42 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.25 on epoch=874
05/12/2022 18:47:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
05/12/2022 18:47:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 18:47:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 18:47:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/12/2022 18:48:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/12/2022 18:48:07 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.21875 on epoch=899
05/12/2022 18:48:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 18:48:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
05/12/2022 18:48:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 18:48:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 18:48:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
05/12/2022 18:48:32 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.21875 on epoch=924
05/12/2022 18:48:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 18:48:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
05/12/2022 18:48:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
05/12/2022 18:48:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 18:48:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 18:48:58 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.25 on epoch=949
05/12/2022 18:49:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 18:49:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 18:49:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 18:49:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 18:49:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 18:49:23 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.1875 on epoch=974
05/12/2022 18:49:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 18:49:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 18:49:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 18:49:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 18:49:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 18:49:48 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.1875 on epoch=999
05/12/2022 18:49:48 - INFO - __main__ - save last model!
05/12/2022 18:49:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 18:49:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 18:49:48 - INFO - __main__ - Printing 3 examples
05/12/2022 18:49:48 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:49:48 - INFO - __main__ - ['about 10 minutes']
05/12/2022 18:49:48 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:49:48 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 18:49:48 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 18:49:48 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 18:49:48 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:49:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:49:49 - INFO - __main__ - Printing 3 examples
05/12/2022 18:49:49 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:49:49 - INFO - __main__ - ['Cleveland']
05/12/2022 18:49:49 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:49:49 - INFO - __main__ - ['Frustration.']
05/12/2022 18:49:49 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:49:49 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:49:49 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:49:49 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:49:49 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:49:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:49:49 - INFO - __main__ - Printing 3 examples
05/12/2022 18:49:49 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:49:49 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:49:49 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:49:49 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:49:49 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:49:49 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:49:49 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:49:49 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:49:49 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:49:50 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:49:51 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 18:50:05 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:50:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:50:06 - INFO - __main__ - Starting training!
05/12/2022 18:51:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_21_0.4_8_predictions.txt
05/12/2022 18:51:21 - INFO - __main__ - ACC on test data: 0.2310
05/12/2022 18:51:21 - INFO - __main__ - prefix=quail_32_21, lr=0.4, bsz=8, dev_performance=0.28125, test_performance=0.231
05/12/2022 18:51:21 - INFO - __main__ - Running ... prefix=quail_32_21, lr=0.3, bsz=8 ...
05/12/2022 18:51:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:51:22 - INFO - __main__ - Printing 3 examples
05/12/2022 18:51:22 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 18:51:22 - INFO - __main__ - ['Cleveland']
05/12/2022 18:51:22 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 18:51:22 - INFO - __main__ - ['Frustration.']
05/12/2022 18:51:22 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 18:51:22 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 18:51:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:51:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:51:22 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 18:51:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 18:51:22 - INFO - __main__ - Printing 3 examples
05/12/2022 18:51:22 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 18:51:22 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 18:51:22 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 18:51:22 - INFO - __main__ - ["She's an educator."]
05/12/2022 18:51:22 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 18:51:22 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 18:51:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 18:51:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 18:51:22 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 18:51:41 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 18:51:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 18:51:41 - INFO - __main__ - Starting training!
05/12/2022 18:51:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.87 on epoch=4
05/12/2022 18:51:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.39 on epoch=9
05/12/2022 18:51:55 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=14
05/12/2022 18:52:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=19
05/12/2022 18:52:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=24
05/12/2022 18:52:07 - INFO - __main__ - Global step 50 Train loss 1.21 ACC 0.15625 on epoch=24
05/12/2022 18:52:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
05/12/2022 18:52:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.73 on epoch=29
05/12/2022 18:52:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=34
05/12/2022 18:52:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=39
05/12/2022 18:52:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.75 on epoch=44
05/12/2022 18:52:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=49
05/12/2022 18:52:32 - INFO - __main__ - Global step 100 Train loss 0.81 ACC 0.09375 on epoch=49
05/12/2022 18:52:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.82 on epoch=54
05/12/2022 18:52:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
05/12/2022 18:52:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=64
05/12/2022 18:52:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=69
05/12/2022 18:52:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=74
05/12/2022 18:52:56 - INFO - __main__ - Global step 150 Train loss 0.63 ACC 0.15625 on epoch=74
05/12/2022 18:53:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=79
05/12/2022 18:53:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=84
05/12/2022 18:53:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=89
05/12/2022 18:53:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=94
05/12/2022 18:53:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
05/12/2022 18:53:21 - INFO - __main__ - Global step 200 Train loss 0.52 ACC 0.1875 on epoch=99
05/12/2022 18:53:22 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=99, global_step=200
05/12/2022 18:53:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=104
05/12/2022 18:53:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=109
05/12/2022 18:53:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=114
05/12/2022 18:53:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
05/12/2022 18:53:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
05/12/2022 18:53:46 - INFO - __main__ - Global step 250 Train loss 0.46 ACC 0.15625 on epoch=124
05/12/2022 18:53:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
05/12/2022 18:53:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
05/12/2022 18:53:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
05/12/2022 18:54:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
05/12/2022 18:54:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
05/12/2022 18:54:11 - INFO - __main__ - Global step 300 Train loss 0.37 ACC 0.15625 on epoch=149
05/12/2022 18:54:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
05/12/2022 18:54:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
05/12/2022 18:54:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=164
05/12/2022 18:54:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=169
05/12/2022 18:54:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
05/12/2022 18:54:36 - INFO - __main__ - Global step 350 Train loss 0.35 ACC 0.1875 on epoch=174
05/12/2022 18:54:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
05/12/2022 18:54:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
05/12/2022 18:54:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
05/12/2022 18:54:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
05/12/2022 18:54:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
05/12/2022 18:55:00 - INFO - __main__ - Global step 400 Train loss 0.32 ACC 0.125 on epoch=199
05/12/2022 18:55:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
05/12/2022 18:55:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
05/12/2022 18:55:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
05/12/2022 18:55:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
05/12/2022 18:55:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
05/12/2022 18:55:25 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.125 on epoch=224
05/12/2022 18:55:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
05/12/2022 18:55:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
05/12/2022 18:55:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
05/12/2022 18:55:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
05/12/2022 18:55:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
05/12/2022 18:55:50 - INFO - __main__ - Global step 500 Train loss 0.23 ACC 0.125 on epoch=249
05/12/2022 18:55:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
05/12/2022 18:55:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
05/12/2022 18:56:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
05/12/2022 18:56:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
05/12/2022 18:56:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=274
05/12/2022 18:56:15 - INFO - __main__ - Global step 550 Train loss 0.20 ACC 0.125 on epoch=274
05/12/2022 18:56:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
05/12/2022 18:56:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=284
05/12/2022 18:56:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
05/12/2022 18:56:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
05/12/2022 18:56:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
05/12/2022 18:56:40 - INFO - __main__ - Global step 600 Train loss 0.16 ACC 0.125 on epoch=299
05/12/2022 18:56:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=304
05/12/2022 18:56:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
05/12/2022 18:56:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
05/12/2022 18:56:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
05/12/2022 18:57:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
05/12/2022 18:57:04 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.125 on epoch=324
05/12/2022 18:57:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=329
05/12/2022 18:57:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
05/12/2022 18:57:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=339
05/12/2022 18:57:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
05/12/2022 18:57:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
05/12/2022 18:57:29 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.09375 on epoch=349
05/12/2022 18:57:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
05/12/2022 18:57:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
05/12/2022 18:57:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
05/12/2022 18:57:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=369
05/12/2022 18:57:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
05/12/2022 18:57:54 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.09375 on epoch=374
05/12/2022 18:57:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=379
05/12/2022 18:58:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
05/12/2022 18:58:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
05/12/2022 18:58:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
05/12/2022 18:58:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
05/12/2022 18:58:19 - INFO - __main__ - Global step 800 Train loss 0.11 ACC 0.125 on epoch=399
05/12/2022 18:58:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
05/12/2022 18:58:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=409
05/12/2022 18:58:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
05/12/2022 18:58:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
05/12/2022 18:58:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
05/12/2022 18:58:44 - INFO - __main__ - Global step 850 Train loss 0.10 ACC 0.09375 on epoch=424
05/12/2022 18:58:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=429
05/12/2022 18:58:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
05/12/2022 18:58:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
05/12/2022 18:59:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
05/12/2022 18:59:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
05/12/2022 18:59:10 - INFO - __main__ - Global step 900 Train loss 0.09 ACC 0.09375 on epoch=449
05/12/2022 18:59:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
05/12/2022 18:59:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=459
05/12/2022 18:59:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
05/12/2022 18:59:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
05/12/2022 18:59:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
05/12/2022 18:59:35 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.15625 on epoch=474
05/12/2022 18:59:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
05/12/2022 18:59:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/12/2022 18:59:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
05/12/2022 18:59:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
05/12/2022 18:59:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
05/12/2022 19:00:00 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.0625 on epoch=499
05/12/2022 19:00:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
05/12/2022 19:00:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
05/12/2022 19:00:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
05/12/2022 19:00:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
05/12/2022 19:00:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
05/12/2022 19:00:24 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.125 on epoch=524
05/12/2022 19:00:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
05/12/2022 19:00:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/12/2022 19:00:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
05/12/2022 19:00:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
05/12/2022 19:00:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
05/12/2022 19:00:49 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.15625 on epoch=549
05/12/2022 19:00:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
05/12/2022 19:00:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
05/12/2022 19:01:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
05/12/2022 19:01:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
05/12/2022 19:01:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
05/12/2022 19:01:14 - INFO - __main__ - Global step 1150 Train loss 0.06 ACC 0.09375 on epoch=574
05/12/2022 19:01:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
05/12/2022 19:01:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
05/12/2022 19:01:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
05/12/2022 19:01:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
05/12/2022 19:01:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
05/12/2022 19:01:39 - INFO - __main__ - Global step 1200 Train loss 0.06 ACC 0.21875 on epoch=599
05/12/2022 19:01:39 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=599, global_step=1200
05/12/2022 19:01:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 19:01:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
05/12/2022 19:01:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 19:01:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
05/12/2022 19:02:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
05/12/2022 19:02:04 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.03125 on epoch=624
05/12/2022 19:02:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
05/12/2022 19:02:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
05/12/2022 19:02:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
05/12/2022 19:02:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 19:02:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
05/12/2022 19:02:29 - INFO - __main__ - Global step 1300 Train loss 0.05 ACC 0.09375 on epoch=649
05/12/2022 19:02:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
05/12/2022 19:02:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
05/12/2022 19:02:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
05/12/2022 19:02:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
05/12/2022 19:02:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
05/12/2022 19:02:55 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.125 on epoch=674
05/12/2022 19:02:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
05/12/2022 19:03:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/12/2022 19:03:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
05/12/2022 19:03:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
05/12/2022 19:03:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 19:03:20 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.09375 on epoch=699
05/12/2022 19:03:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 19:03:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=709
05/12/2022 19:03:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
05/12/2022 19:03:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
05/12/2022 19:03:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
05/12/2022 19:03:44 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.15625 on epoch=724
05/12/2022 19:03:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 19:03:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
05/12/2022 19:03:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
05/12/2022 19:04:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
05/12/2022 19:04:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 19:04:09 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.1875 on epoch=749
05/12/2022 19:04:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
05/12/2022 19:04:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
05/12/2022 19:04:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
05/12/2022 19:04:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
05/12/2022 19:04:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 19:04:34 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.125 on epoch=774
05/12/2022 19:04:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
05/12/2022 19:04:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
05/12/2022 19:04:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
05/12/2022 19:04:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 19:04:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 19:04:59 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.125 on epoch=799
05/12/2022 19:05:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 19:05:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
05/12/2022 19:05:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 19:05:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 19:05:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
05/12/2022 19:05:24 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.15625 on epoch=824
05/12/2022 19:05:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
05/12/2022 19:05:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
05/12/2022 19:05:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 19:05:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
05/12/2022 19:05:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
05/12/2022 19:05:49 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.1875 on epoch=849
05/12/2022 19:05:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
05/12/2022 19:05:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
05/12/2022 19:06:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 19:06:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
05/12/2022 19:06:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 19:06:14 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.1875 on epoch=874
05/12/2022 19:06:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 19:06:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
05/12/2022 19:06:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 19:06:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
05/12/2022 19:06:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 19:06:48 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.15625 on epoch=899
05/12/2022 19:06:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
05/12/2022 19:06:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 19:07:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 19:07:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 19:07:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
05/12/2022 19:07:13 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.125 on epoch=924
05/12/2022 19:07:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
05/12/2022 19:07:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
05/12/2022 19:07:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
05/12/2022 19:07:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
05/12/2022 19:07:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 19:07:38 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.125 on epoch=949
05/12/2022 19:07:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
05/12/2022 19:07:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 19:07:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 19:07:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/12/2022 19:08:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 19:08:03 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.21875 on epoch=974
05/12/2022 19:08:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
05/12/2022 19:08:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 19:08:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 19:08:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 19:08:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
05/12/2022 19:08:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:08:27 - INFO - __main__ - Printing 3 examples
05/12/2022 19:08:27 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 19:08:27 - INFO - __main__ - ['Cleveland']
05/12/2022 19:08:27 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 19:08:27 - INFO - __main__ - ['Frustration.']
05/12/2022 19:08:27 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 19:08:27 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 19:08:27 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:08:27 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:08:27 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:08:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:08:27 - INFO - __main__ - Printing 3 examples
05/12/2022 19:08:27 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 19:08:27 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 19:08:27 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 19:08:27 - INFO - __main__ - ["She's an educator."]
05/12/2022 19:08:27 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 19:08:27 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 19:08:27 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:08:27 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:08:27 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:08:29 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.1875 on epoch=999
05/12/2022 19:08:29 - INFO - __main__ - save last model!
05/12/2022 19:08:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 19:08:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 19:08:29 - INFO - __main__ - Printing 3 examples
05/12/2022 19:08:29 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:08:29 - INFO - __main__ - ['about 10 minutes']
05/12/2022 19:08:29 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:08:29 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 19:08:29 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:08:29 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 19:08:29 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:08:31 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:08:32 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 19:08:43 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:08:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:08:44 - INFO - __main__ - Starting training!
05/12/2022 19:10:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_21_0.3_8_predictions.txt
05/12/2022 19:10:10 - INFO - __main__ - ACC on test data: 0.2500
05/12/2022 19:10:10 - INFO - __main__ - prefix=quail_32_21, lr=0.3, bsz=8, dev_performance=0.21875, test_performance=0.25
05/12/2022 19:10:10 - INFO - __main__ - Running ... prefix=quail_32_21, lr=0.2, bsz=8 ...
05/12/2022 19:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:10:11 - INFO - __main__ - Printing 3 examples
05/12/2022 19:10:11 - INFO - __main__ -  [quail] Who was the biggest puppy?(A)not enough information(B)Spike(C)Cleveland(D)Lilo [SEP] The hardest thing was having to give up my three beautiful puppies due to my situation, the environment, and the people in that environment. I've mentioned this in another question. (Lilo, my best friend) (Cleveland, the biggest of the litter, he was chill like me) (Spike, the fluffiest, he was as fluffy as a cat, but clumsy to) What I did for these puppies was out of this world love. I never loved anything more in this world than these three right here. I raised them from birth to almost 11 weeks. While my mom wanted money, selling the others to anyone.(there was 11 in the litter) I cared for their safety and happiness and quality of life. They were my everything when I had nothing. I gave them away to a animal charity organization for free. My mom bitched at me for not getting money for them. At this time I was in severe depression, severe poverty, no chance of a job due to location, and wearing dirty clothes for months in her basement. I love animals to death, I love animals more than I love humans(but I'm no PETA activist). I loved these puppies, what I did was out of complete love and care for them and was seriously the hardest thing I have ever done in my entire life. It gets me very emotional thinking about this, I wish I was in a better position to give them a happy life. The black puppy, Lilo was my upmost favorite. She had the whine of a angel. She used it to always get my attention to give her more love. She always wanted to sleep with me every night and be with me every second of the day. Not a day passes that I hope they are getting love from a family in a great environment. I really want to get to see Lilo again. But of course the dog charity people changed their names. But she will also be Lilo to me♥️
05/12/2022 19:10:11 - INFO - __main__ - ['Cleveland']
05/12/2022 19:10:11 - INFO - __main__ -  [quail] What emotion was the author likely feeling afterwards?(A)Joy.(B)Happiness.(C)Frustration.(D)not enough information [SEP] Back in middle school, at least I think it was, I had art class. I didn’t particularly love it but it was interesting enough at the least to hold my attention. We got to class and the teacher pulled up a list of instructions on the projector. She had each of us follow the instructions and she gave us 20 minutes to complete all of them. It may even have been a bit of a race too, to see who finished it first. This becomes a lot more evident in a second. I can’t recall the exact details of the instructions, except that clearly it had to do with art such as having to draw a bunch of things. Anyways, we all went one by one down the instructions getting increasing nervous as to try to be the first one to finish. The class goes quiet while we work. Then I get to the last instruction which reads something along the lines of: “Step 15: Skip steps 1–14 and put your pencil down.” You gotta be freaking kidding me. We’re all so confused and mad at ourselves for not reading the whole of the instructions first. She turned this into a lesson for how we need to learn to read all the instructions first and then start, which was pretty clever. She got us! I remember this vividly to this day. But I look back now on this “prank” and I realize that she was kind of…wrong. Sure if the instructions are short like 15 steps it might be good to glance through before you start. But what about a whole load of instructions? Who the hell is going to read through the instructions for how to build their Death Star LEGO set before even starting to build it. Just a waste of time really. Note: I have never built a Death Star LEGO set.
05/12/2022 19:10:11 - INFO - __main__ - ['Frustration.']
05/12/2022 19:10:11 - INFO - __main__ -  [quail] Where did the pastor sit?(A)in his usual place on the podium(B)in the first pew(C)on the dias steps(D)not enough information [SEP] The pews were packed at First Baptist Church, Coreyville. As part-time music minister of the church, Greg Tenorly sat in his usual place on the podium, behind and slightly to the left of the pastor. He wondered why attendance was up. It was a perfect day--seventy degrees, sunny. That had to be part of the reason. And the sermon title was 'Forgiveness Fighters.' People would much rather hear a sermon about forgiveness than one about Hell. Everybody wanted to be forgiven. But when it came to forgiving others--many people fight it. The pastor said these folks were the Forgiveness Fighters. He read a scripture passage. Then came Peter to him, and said, Lord, how oft shall my brother sin against me, and I forgive him? till seven times? Jesus saith unto him, I say not unto thee, Until seven times: but, Until seventy times seven. When Greg heard these verses, which he knew by memory, it was like a slap in the face. How many times had he already forgiven his father? But he knew that 'seventy times seven' did not mean literally 490 times. The number 'seven' in the Bible symbolized completeness. It meant forgiving an unlimited number of times. But how could Greg ever forgive his father for killing his mother? Maybe if Greg had been there it wouldn't have happened. But he had moved out of the house during his first semester at Lamar University--even though it was only forty minutes away, in Beaumont. A fellow music major had been more than happy to let Greg share the little rent house and the expenses. Ralph Tenorly had sent his wife to the grocery store for more chips and dip. The big game was already starting, and there were no snacks in the house. But on her way back home, a pickup truck blew through a stop sign, crashing into the driver's side of the car. Barbara was killed instantly.
05/12/2022 19:10:11 - INFO - __main__ - ['in his usual place on the podium']
05/12/2022 19:10:11 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:10:11 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:10:11 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:10:11 - INFO - __main__ - Printing 3 examples
05/12/2022 19:10:11 - INFO - __main__ -  [quail] What is probably true about those supporting the bill?(A)They believe in freedom of speech(B)They are not mostly prejudiced(C)not enough information(D)They are mostly prejudiced against the Pres based on political beliefs [SEP] The U.S. Senate Judiciary Committee approved a bill Thursday that would protect from arbitrary dismissal the special counsel investigating Russian interference in the 2016 U.S. election. The measure, backed by 10 Democrats and four Republicans, would codify Justice Department regulations that the special counsel can only be fired by the attorney general or a designee for "misconduct, dereliction of duty, incapacity, conflict of interest, or other good cause." The proposal would give the special counsel 10 days to challenge a dismissal in court. If a court determines the firing was not for "good cause," the special counsel would be reinstated. The measure would also require the Justice Department to notify Congress when a special counsel is appointed and to report the findings of an investigation. While marking a strong show of support for Special Counsel Robert Mueller who is under frequent attack by President Donald Trump and some Republicans, the bill is unlikely to become law in the face of Republican opposition. Senate Majority Leader Mitch McConnell said last week that Trump will not fire Mueller and that there was no need to bring the measure to the Senate floor for a vote. House Speaker Paul Ryan has also opposed the idea. The legislation was introduced by four Senators earlier this month after Trump's sharp criticism of an FBI raid on his personal lawyer's home and office rekindled fears that Trump may fire Mueller and Deputy Attorney General Rod Rosenstein, who supervises Mueller. Mueller is heading the federal investigation into Russia's electoral interference and possible collusion with the Trump presidential campaign. Trump has said there was no collusion and repeatedly denounced the probe as a "witch hunt." Despite his harsh criticism of the Special Counsel and the Justice Department, Trump has dismissed reports that he's privately talked about firing Mueller. He told the cable show Fox and Friends on Thursday that he'll "try and stay away" from the Justice Department, but "at some point, I won't." Supporters of the Special Counsel bill hailed it as a victory for the rule of law and said it would send a message that the president doesn't have unfettered authority.
05/12/2022 19:10:11 - INFO - __main__ - ['They are mostly prejudiced against the Pres based on political beliefs']
05/12/2022 19:10:11 - INFO - __main__ -  [quail] What position is the lady that Julie asked the opinion of?(A)She's an educator.(B)not enough information(C)She's a therapist(D)She's a doctor. [SEP] “Please remember me to myself!” When sliding toward mania in 2016, I begged this of my therapist. He cocked his head. “What do you mean?” I had no idea—on a conscious level—what I was asking for, just that I felt in a desperate sense that the self I knew was slipping away. An inability to recognize myself either in my writing or in reflecting upon my own actions was taking over. And then…? I didn't care. I wanted to kiss girls. Which is fine—just not something I ordinarily do. I wanted to drink, smoke. Again, fine—but not something I ordinarily do. “Do I seem…okay?” I asked a colleague at work. A woman I barely knew. Insanely, I thought that since she worked with disabled students, many of whom struggled with mental illness, she would know mania when she saw it. This was a rather foolish assumption—especially since she's an educator, not a psychiatrist, especially with how skilled I am at wearing the mask of calm, the face of sanity. “You seem great, Julie. Cheery. Professional as always.” I let her reassurance placate me. Wanted to be placated. Yet, within months I lost my job. And not only that, but a chance at a scholarship, two really important long term friendships—relationships I'd enjoyed since high school. I was hospitalized three times between September and February. I lost my ability to trust myself, lost my mind. It is now a little over two years since I first felt that sliding. Twenty-six months since I knew that somewhere in the deepest parts of who I am, I was slipping. Nine seasons since I begged Dr. Poe, “Remember me to myself!” I'm still recuperating. Yes. I have felt like I wasn't me.
05/12/2022 19:10:11 - INFO - __main__ - ["She's an educator."]
05/12/2022 19:10:11 - INFO - __main__ -  [quail] What did Gilbert do as IBM's General Manager of Design?(A)Trained  thousands of employees(B)Organized thousands of employees to solve a problem(C)Hire a thousand designers as trainers for his staff(D)not enough information [SEP] A prospective client sent me a link to this in-depth article on IBM's design thinking revolution, where Phil Gilbert, IBM's General Manager of Design, has hired over 1000 designers into the firm, and pushed for over 8000 of its managers and staff to get 'trained' in design thinking. They have even created specific design centres across the firm, with design offices in most of its key locations, such as the one above. The goal is nothing short of beginning IBM's next phase of transformation, one of many in its 100+ year history. However, all is not rosy. Despite achieving a monumental success relative to the status quo, 8000 'recognised' design thinkers in a corporation of over 370,000 souls is barely a dent in terms of changing practice. If NoTosh were to effect change in only 2% of the teachers with whom we work, we'd have packed up our bags long ago. I'm not sure hiring 1000 designers in and of itself is the answer to any organisation trying to instil a different way of viewing the world. Here's why. Since design thinking really began to be a thing, back in the early 60s, the designer him or herself has consistently been at the centre of the design process. Even though we talk of 'user-centred design', the actual ideation and production of a solution, and in many cases the synthesis and definition of the problem to be solve, too, are all tasks undertaken by skilled 'designers', rather than the people in the organisation who have the scope, brand, or 'permission' to play in that space. Once the designers leave the project, so does the design thinking. There is a reason d.school sees its executive courses filled with repeat customers and firms like IDEO continue to thrive - they are resolving challenges in specific examples of services or products, but not necessarily transforming the firms and organisations who had the budget and desire to solve a problem in that specific area. Solving a problem costs money. Solving a problem and teaching the client how to do it again and again costs more than just money. That might be the greatest challenge of all.
05/12/2022 19:10:11 - INFO - __main__ - ['Hire a thousand designers as trainers for his staff']
05/12/2022 19:10:11 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:10:11 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:10:11 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:10:27 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:10:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:10:27 - INFO - __main__ - Starting training!
05/12/2022 19:10:33 - INFO - __main__ - Step 10 Global step 10 Train loss 1.97 on epoch=4
05/12/2022 19:10:37 - INFO - __main__ - Step 20 Global step 20 Train loss 1.51 on epoch=9
05/12/2022 19:10:41 - INFO - __main__ - Step 30 Global step 30 Train loss 1.25 on epoch=14
05/12/2022 19:10:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=19
05/12/2022 19:10:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=24
05/12/2022 19:10:53 - INFO - __main__ - Global step 50 Train loss 1.36 ACC 0.0625 on epoch=24
05/12/2022 19:10:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=24, global_step=50
05/12/2022 19:10:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.88 on epoch=29
05/12/2022 19:11:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=34
05/12/2022 19:11:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=39
05/12/2022 19:11:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.70 on epoch=44
05/12/2022 19:11:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=49
05/12/2022 19:11:18 - INFO - __main__ - Global step 100 Train loss 0.75 ACC 0.09375 on epoch=49
05/12/2022 19:11:18 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.09375 on epoch=49, global_step=100
05/12/2022 19:11:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=54
05/12/2022 19:11:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
05/12/2022 19:11:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=64
05/12/2022 19:11:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=69
05/12/2022 19:11:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=74
05/12/2022 19:11:42 - INFO - __main__ - Global step 150 Train loss 0.51 ACC 0.25 on epoch=74
05/12/2022 19:11:42 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.25 on epoch=74, global_step=150
05/12/2022 19:11:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
05/12/2022 19:11:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.34 on epoch=84
05/12/2022 19:11:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
05/12/2022 19:12:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
05/12/2022 19:12:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
05/12/2022 19:12:07 - INFO - __main__ - Global step 200 Train loss 0.37 ACC 0.15625 on epoch=99
05/12/2022 19:12:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
05/12/2022 19:12:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
05/12/2022 19:12:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
05/12/2022 19:12:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
05/12/2022 19:12:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
05/12/2022 19:12:32 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.1875 on epoch=124
05/12/2022 19:12:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
05/12/2022 19:12:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
05/12/2022 19:12:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
05/12/2022 19:12:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
05/12/2022 19:12:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=149
05/12/2022 19:12:57 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.21875 on epoch=149
05/12/2022 19:13:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
05/12/2022 19:13:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=159
05/12/2022 19:13:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
05/12/2022 19:13:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
05/12/2022 19:13:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=174
05/12/2022 19:13:21 - INFO - __main__ - Global step 350 Train loss 0.20 ACC 0.125 on epoch=174
05/12/2022 19:13:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=179
05/12/2022 19:13:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
05/12/2022 19:13:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=189
05/12/2022 19:13:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
05/12/2022 19:13:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
05/12/2022 19:13:46 - INFO - __main__ - Global step 400 Train loss 0.18 ACC 0.15625 on epoch=199
05/12/2022 19:13:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.17 on epoch=204
05/12/2022 19:13:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
05/12/2022 19:14:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
05/12/2022 19:14:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
05/12/2022 19:14:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
05/12/2022 19:14:11 - INFO - __main__ - Global step 450 Train loss 0.17 ACC 0.125 on epoch=224
05/12/2022 19:14:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=229
05/12/2022 19:14:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
05/12/2022 19:14:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
05/12/2022 19:14:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
05/12/2022 19:14:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
05/12/2022 19:14:35 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.15625 on epoch=249
05/12/2022 19:14:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
05/12/2022 19:14:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
05/12/2022 19:14:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
05/12/2022 19:14:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
05/12/2022 19:14:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
05/12/2022 19:15:00 - INFO - __main__ - Global step 550 Train loss 0.12 ACC 0.1875 on epoch=274
05/12/2022 19:15:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
05/12/2022 19:15:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
05/12/2022 19:15:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=289
05/12/2022 19:15:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
05/12/2022 19:15:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
05/12/2022 19:15:25 - INFO - __main__ - Global step 600 Train loss 0.09 ACC 0.09375 on epoch=299
05/12/2022 19:15:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
05/12/2022 19:15:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
05/12/2022 19:15:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
05/12/2022 19:15:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
05/12/2022 19:15:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
05/12/2022 19:15:50 - INFO - __main__ - Global step 650 Train loss 0.08 ACC 0.125 on epoch=324
05/12/2022 19:15:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
05/12/2022 19:15:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
05/12/2022 19:16:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
05/12/2022 19:16:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=344
05/12/2022 19:16:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
05/12/2022 19:16:15 - INFO - __main__ - Global step 700 Train loss 0.09 ACC 0.125 on epoch=349
05/12/2022 19:16:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
05/12/2022 19:16:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
05/12/2022 19:16:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
05/12/2022 19:16:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
05/12/2022 19:16:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
05/12/2022 19:16:40 - INFO - __main__ - Global step 750 Train loss 0.08 ACC 0.1875 on epoch=374
05/12/2022 19:16:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
05/12/2022 19:16:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/12/2022 19:16:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
05/12/2022 19:16:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=394
05/12/2022 19:17:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
05/12/2022 19:17:05 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.15625 on epoch=399
05/12/2022 19:17:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
05/12/2022 19:17:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
05/12/2022 19:17:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
05/12/2022 19:17:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
05/12/2022 19:17:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
05/12/2022 19:17:31 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.125 on epoch=424
05/12/2022 19:17:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 19:17:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
05/12/2022 19:17:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
05/12/2022 19:17:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 19:17:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
05/12/2022 19:17:56 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.1875 on epoch=449
05/12/2022 19:18:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
05/12/2022 19:18:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
05/12/2022 19:18:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
05/12/2022 19:18:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 19:18:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
05/12/2022 19:18:21 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.1875 on epoch=474
05/12/2022 19:18:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
05/12/2022 19:18:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/12/2022 19:18:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
05/12/2022 19:18:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
05/12/2022 19:18:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
05/12/2022 19:18:46 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.1875 on epoch=499
05/12/2022 19:18:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
05/12/2022 19:18:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
05/12/2022 19:18:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
05/12/2022 19:19:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
05/12/2022 19:19:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
05/12/2022 19:19:11 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.125 on epoch=524
05/12/2022 19:19:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
05/12/2022 19:19:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/12/2022 19:19:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
05/12/2022 19:19:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
05/12/2022 19:19:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
05/12/2022 19:19:36 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.1875 on epoch=549
05/12/2022 19:19:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 19:19:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
05/12/2022 19:19:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
05/12/2022 19:19:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
05/12/2022 19:19:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=574
05/12/2022 19:20:02 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.1875 on epoch=574
05/12/2022 19:20:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
05/12/2022 19:20:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 19:20:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
05/12/2022 19:20:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
05/12/2022 19:20:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
05/12/2022 19:20:28 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.15625 on epoch=599
05/12/2022 19:20:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 19:20:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/12/2022 19:20:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
05/12/2022 19:20:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
05/12/2022 19:20:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
05/12/2022 19:20:53 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.09375 on epoch=624
05/12/2022 19:20:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
05/12/2022 19:21:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 19:21:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
05/12/2022 19:21:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 19:21:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
05/12/2022 19:21:18 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.1875 on epoch=649
05/12/2022 19:21:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
05/12/2022 19:21:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
05/12/2022 19:21:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
05/12/2022 19:21:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
05/12/2022 19:21:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
05/12/2022 19:21:52 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.1875 on epoch=674
05/12/2022 19:21:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 19:22:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
05/12/2022 19:22:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
05/12/2022 19:22:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
05/12/2022 19:22:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 19:22:17 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.09375 on epoch=699
05/12/2022 19:22:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 19:22:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 19:22:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
05/12/2022 19:22:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 19:22:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 19:22:43 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.1875 on epoch=724
05/12/2022 19:22:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
05/12/2022 19:22:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 19:22:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 19:23:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 19:23:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
05/12/2022 19:23:08 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.1875 on epoch=749
05/12/2022 19:23:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
05/12/2022 19:23:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
05/12/2022 19:23:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 19:23:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
05/12/2022 19:23:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 19:23:34 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.1875 on epoch=774
05/12/2022 19:23:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 19:23:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 19:23:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 19:23:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
05/12/2022 19:23:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/12/2022 19:23:59 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
05/12/2022 19:24:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 19:24:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
05/12/2022 19:24:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 19:24:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 19:24:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 19:24:25 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.0625 on epoch=824
05/12/2022 19:24:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
05/12/2022 19:24:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 19:24:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 19:24:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 19:24:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 19:24:50 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.15625 on epoch=849
05/12/2022 19:24:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 19:24:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 19:25:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 19:25:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
05/12/2022 19:25:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
05/12/2022 19:25:16 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.1875 on epoch=874
05/12/2022 19:25:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 19:25:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 19:25:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/12/2022 19:25:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
05/12/2022 19:25:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
05/12/2022 19:25:42 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.125 on epoch=899
05/12/2022 19:25:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 19:25:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 19:25:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 19:25:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 19:26:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 19:26:07 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.15625 on epoch=924
05/12/2022 19:26:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 19:26:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 19:26:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
05/12/2022 19:26:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
05/12/2022 19:26:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
05/12/2022 19:26:33 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.21875 on epoch=949
05/12/2022 19:26:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
05/12/2022 19:26:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 19:26:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 19:26:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 19:26:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
05/12/2022 19:26:58 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.1875 on epoch=974
05/12/2022 19:27:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
05/12/2022 19:27:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 19:27:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
05/12/2022 19:27:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 19:27:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 19:27:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:27:22 - INFO - __main__ - Printing 3 examples
05/12/2022 19:27:22 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 19:27:22 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 19:27:22 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 19:27:22 - INFO - __main__ - ['we all have to do housework']
05/12/2022 19:27:22 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 19:27:22 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 19:27:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:27:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:27:22 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:27:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:27:22 - INFO - __main__ - Printing 3 examples
05/12/2022 19:27:22 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 19:27:22 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 19:27:22 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 19:27:22 - INFO - __main__ - ['Jason and Carly']
05/12/2022 19:27:22 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 19:27:22 - INFO - __main__ - ['not enough information']
05/12/2022 19:27:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:27:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:27:22 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:27:23 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.15625 on epoch=999
05/12/2022 19:27:24 - INFO - __main__ - save last model!
05/12/2022 19:27:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 19:27:24 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 19:27:24 - INFO - __main__ - Printing 3 examples
05/12/2022 19:27:24 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:27:24 - INFO - __main__ - ['about 10 minutes']
05/12/2022 19:27:24 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:27:24 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 19:27:24 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:27:24 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 19:27:24 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:27:25 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:27:26 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 19:27:38 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:27:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:27:39 - INFO - __main__ - Starting training!
05/12/2022 19:28:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_21_0.2_8_predictions.txt
05/12/2022 19:28:58 - INFO - __main__ - ACC on test data: 0.2380
05/12/2022 19:28:58 - INFO - __main__ - prefix=quail_32_21, lr=0.2, bsz=8, dev_performance=0.25, test_performance=0.238
05/12/2022 19:28:58 - INFO - __main__ - Running ... prefix=quail_32_42, lr=0.5, bsz=8 ...
05/12/2022 19:28:59 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:28:59 - INFO - __main__ - Printing 3 examples
05/12/2022 19:28:59 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 19:28:59 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 19:28:59 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 19:28:59 - INFO - __main__ - ['we all have to do housework']
05/12/2022 19:28:59 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 19:28:59 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 19:28:59 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:28:59 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:28:59 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:28:59 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:28:59 - INFO - __main__ - Printing 3 examples
05/12/2022 19:28:59 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 19:28:59 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 19:28:59 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 19:28:59 - INFO - __main__ - ['Jason and Carly']
05/12/2022 19:28:59 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 19:28:59 - INFO - __main__ - ['not enough information']
05/12/2022 19:28:59 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:28:59 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:28:59 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:29:14 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:29:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:29:15 - INFO - __main__ - Starting training!
05/12/2022 19:29:23 - INFO - __main__ - Step 10 Global step 10 Train loss 1.82 on epoch=4
05/12/2022 19:29:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.19 on epoch=9
05/12/2022 19:29:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.84 on epoch=14
05/12/2022 19:29:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.66 on epoch=19
05/12/2022 19:29:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=24
05/12/2022 19:29:44 - INFO - __main__ - Global step 50 Train loss 1.02 ACC 0.21875 on epoch=24
05/12/2022 19:29:44 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.21875 on epoch=24, global_step=50
05/12/2022 19:29:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=29
05/12/2022 19:29:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=34
05/12/2022 19:29:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=39
05/12/2022 19:30:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
05/12/2022 19:30:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
05/12/2022 19:30:08 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.15625 on epoch=49
05/12/2022 19:30:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
05/12/2022 19:30:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
05/12/2022 19:30:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
05/12/2022 19:30:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
05/12/2022 19:30:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
05/12/2022 19:30:34 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.3125 on epoch=74
05/12/2022 19:30:34 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.3125 on epoch=74, global_step=150
05/12/2022 19:30:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
05/12/2022 19:30:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
05/12/2022 19:30:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
05/12/2022 19:30:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
05/12/2022 19:30:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/12/2022 19:30:59 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.28125 on epoch=99
05/12/2022 19:31:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
05/12/2022 19:31:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
05/12/2022 19:31:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.16 on epoch=114
05/12/2022 19:31:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
05/12/2022 19:31:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.12 on epoch=124
05/12/2022 19:31:24 - INFO - __main__ - Global step 250 Train loss 0.17 ACC 0.34375 on epoch=124
05/12/2022 19:31:24 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=124, global_step=250
05/12/2022 19:31:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
05/12/2022 19:31:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
05/12/2022 19:31:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
05/12/2022 19:31:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
05/12/2022 19:31:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
05/12/2022 19:31:49 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.3125 on epoch=149
05/12/2022 19:31:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
05/12/2022 19:31:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
05/12/2022 19:32:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
05/12/2022 19:32:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
05/12/2022 19:32:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
05/12/2022 19:32:14 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.34375 on epoch=174
05/12/2022 19:32:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
05/12/2022 19:32:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.07 on epoch=184
05/12/2022 19:32:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
05/12/2022 19:32:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
05/12/2022 19:32:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
05/12/2022 19:32:39 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.28125 on epoch=199
05/12/2022 19:32:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
05/12/2022 19:32:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
05/12/2022 19:32:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
05/12/2022 19:32:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
05/12/2022 19:33:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
05/12/2022 19:33:04 - INFO - __main__ - Global step 450 Train loss 0.10 ACC 0.3125 on epoch=224
05/12/2022 19:33:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
05/12/2022 19:33:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
05/12/2022 19:33:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
05/12/2022 19:33:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
05/12/2022 19:33:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
05/12/2022 19:33:29 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.28125 on epoch=249
05/12/2022 19:33:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
05/12/2022 19:33:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
05/12/2022 19:33:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
05/12/2022 19:33:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
05/12/2022 19:33:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/12/2022 19:33:54 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.25 on epoch=274
05/12/2022 19:33:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
05/12/2022 19:34:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
05/12/2022 19:34:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
05/12/2022 19:34:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
05/12/2022 19:34:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
05/12/2022 19:34:19 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.3125 on epoch=299
05/12/2022 19:34:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
05/12/2022 19:34:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
05/12/2022 19:34:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
05/12/2022 19:34:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
05/12/2022 19:34:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/12/2022 19:34:44 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.28125 on epoch=324
05/12/2022 19:34:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/12/2022 19:34:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
05/12/2022 19:34:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
05/12/2022 19:35:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
05/12/2022 19:35:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
05/12/2022 19:35:10 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.3125 on epoch=349
05/12/2022 19:35:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
05/12/2022 19:35:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
05/12/2022 19:35:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
05/12/2022 19:35:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/12/2022 19:35:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
05/12/2022 19:35:37 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.34375 on epoch=374
05/12/2022 19:35:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
05/12/2022 19:35:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
05/12/2022 19:35:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
05/12/2022 19:35:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
05/12/2022 19:35:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
05/12/2022 19:36:02 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.28125 on epoch=399
05/12/2022 19:36:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
05/12/2022 19:36:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
05/12/2022 19:36:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/12/2022 19:36:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
05/12/2022 19:36:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
05/12/2022 19:36:28 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.1875 on epoch=424
05/12/2022 19:36:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
05/12/2022 19:36:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
05/12/2022 19:36:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
05/12/2022 19:36:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
05/12/2022 19:36:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/12/2022 19:36:53 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.21875 on epoch=449
05/12/2022 19:36:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
05/12/2022 19:37:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/12/2022 19:37:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
05/12/2022 19:37:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 19:37:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
05/12/2022 19:37:20 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.40625 on epoch=474
05/12/2022 19:37:20 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.40625 on epoch=474, global_step=950
05/12/2022 19:37:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
05/12/2022 19:37:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
05/12/2022 19:37:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
05/12/2022 19:37:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
05/12/2022 19:37:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
05/12/2022 19:37:45 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.34375 on epoch=499
05/12/2022 19:37:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
05/12/2022 19:37:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
05/12/2022 19:37:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
05/12/2022 19:38:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
05/12/2022 19:38:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
05/12/2022 19:38:10 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.34375 on epoch=524
05/12/2022 19:38:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
05/12/2022 19:38:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
05/12/2022 19:38:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
05/12/2022 19:38:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
05/12/2022 19:38:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
05/12/2022 19:38:35 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.3125 on epoch=549
05/12/2022 19:38:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/12/2022 19:38:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
05/12/2022 19:38:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 19:38:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
05/12/2022 19:38:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
05/12/2022 19:39:00 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.25 on epoch=574
05/12/2022 19:39:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=579
05/12/2022 19:39:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
05/12/2022 19:39:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
05/12/2022 19:39:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
05/12/2022 19:39:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
05/12/2022 19:39:25 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.25 on epoch=599
05/12/2022 19:39:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
05/12/2022 19:39:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/12/2022 19:39:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 19:39:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
05/12/2022 19:39:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=624
05/12/2022 19:39:50 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.25 on epoch=624
05/12/2022 19:39:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 19:39:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 19:40:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 19:40:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 19:40:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
05/12/2022 19:40:15 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.34375 on epoch=649
05/12/2022 19:40:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
05/12/2022 19:40:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
05/12/2022 19:40:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 19:40:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
05/12/2022 19:40:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
05/12/2022 19:40:41 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.3125 on epoch=674
05/12/2022 19:40:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
05/12/2022 19:40:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/12/2022 19:40:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/12/2022 19:40:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
05/12/2022 19:41:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
05/12/2022 19:41:06 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.3125 on epoch=699
05/12/2022 19:41:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 19:41:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/12/2022 19:41:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
05/12/2022 19:41:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 19:41:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 19:41:31 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.34375 on epoch=724
05/12/2022 19:41:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 19:41:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
05/12/2022 19:41:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/12/2022 19:41:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 19:41:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 19:41:57 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.28125 on epoch=749
05/12/2022 19:42:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 19:42:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 19:42:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 19:42:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 19:42:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 19:42:21 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.1875 on epoch=774
05/12/2022 19:42:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 19:42:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
05/12/2022 19:42:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 19:42:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 19:42:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
05/12/2022 19:42:46 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.21875 on epoch=799
05/12/2022 19:42:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 19:42:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
05/12/2022 19:42:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 19:43:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
05/12/2022 19:43:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
05/12/2022 19:43:14 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.3125 on epoch=824
05/12/2022 19:43:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
05/12/2022 19:43:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 19:43:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
05/12/2022 19:43:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
05/12/2022 19:43:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
05/12/2022 19:43:39 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.3125 on epoch=849
05/12/2022 19:43:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
05/12/2022 19:43:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 19:43:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 19:43:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/12/2022 19:44:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
05/12/2022 19:44:04 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.25 on epoch=874
05/12/2022 19:44:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 19:44:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
05/12/2022 19:44:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
05/12/2022 19:44:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/12/2022 19:44:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 19:44:29 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.28125 on epoch=899
05/12/2022 19:44:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
05/12/2022 19:44:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/12/2022 19:44:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
05/12/2022 19:44:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/12/2022 19:44:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 19:44:54 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.28125 on epoch=924
05/12/2022 19:44:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 19:45:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 19:45:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
05/12/2022 19:45:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
05/12/2022 19:45:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
05/12/2022 19:45:20 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.34375 on epoch=949
05/12/2022 19:45:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 19:45:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
05/12/2022 19:45:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 19:45:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 19:45:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
05/12/2022 19:45:46 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
05/12/2022 19:45:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/12/2022 19:45:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 19:45:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 19:46:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/12/2022 19:46:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
05/12/2022 19:46:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:46:09 - INFO - __main__ - Printing 3 examples
05/12/2022 19:46:09 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 19:46:09 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 19:46:09 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 19:46:09 - INFO - __main__ - ['we all have to do housework']
05/12/2022 19:46:09 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 19:46:09 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 19:46:09 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:46:09 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:46:09 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:46:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:46:09 - INFO - __main__ - Printing 3 examples
05/12/2022 19:46:09 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 19:46:09 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 19:46:09 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 19:46:09 - INFO - __main__ - ['Jason and Carly']
05/12/2022 19:46:09 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 19:46:09 - INFO - __main__ - ['not enough information']
05/12/2022 19:46:09 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:46:09 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:46:09 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:46:12 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.3125 on epoch=999
05/12/2022 19:46:12 - INFO - __main__ - save last model!
05/12/2022 19:46:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 19:46:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 19:46:12 - INFO - __main__ - Printing 3 examples
05/12/2022 19:46:12 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:46:12 - INFO - __main__ - ['about 10 minutes']
05/12/2022 19:46:12 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:46:12 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 19:46:12 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 19:46:12 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 19:46:12 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:46:13 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:46:14 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 19:46:25 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:46:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:46:26 - INFO - __main__ - Starting training!
05/12/2022 19:48:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_42_0.5_8_predictions.txt
05/12/2022 19:48:00 - INFO - __main__ - ACC on test data: 0.2680
05/12/2022 19:48:00 - INFO - __main__ - prefix=quail_32_42, lr=0.5, bsz=8, dev_performance=0.40625, test_performance=0.268
05/12/2022 19:48:00 - INFO - __main__ - Running ... prefix=quail_32_42, lr=0.4, bsz=8 ...
05/12/2022 19:48:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:48:01 - INFO - __main__ - Printing 3 examples
05/12/2022 19:48:01 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 19:48:01 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 19:48:01 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 19:48:01 - INFO - __main__ - ['we all have to do housework']
05/12/2022 19:48:01 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 19:48:01 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 19:48:01 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:48:02 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:48:02 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 19:48:02 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 19:48:02 - INFO - __main__ - Printing 3 examples
05/12/2022 19:48:02 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 19:48:02 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 19:48:02 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 19:48:02 - INFO - __main__ - ['Jason and Carly']
05/12/2022 19:48:02 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 19:48:02 - INFO - __main__ - ['not enough information']
05/12/2022 19:48:02 - INFO - __main__ - Tokenizing Input ...
05/12/2022 19:48:02 - INFO - __main__ - Tokenizing Output ...
05/12/2022 19:48:02 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 19:48:20 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 19:48:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 19:48:21 - INFO - __main__ - Starting training!
05/12/2022 19:48:26 - INFO - __main__ - Step 10 Global step 10 Train loss 1.95 on epoch=4
05/12/2022 19:48:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.41 on epoch=9
05/12/2022 19:48:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.67 on epoch=14
05/12/2022 19:48:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.83 on epoch=19
05/12/2022 19:48:44 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=24
05/12/2022 19:48:47 - INFO - __main__ - Global step 50 Train loss 1.62 ACC 0.0 on epoch=24
05/12/2022 19:48:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
05/12/2022 19:48:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=29
05/12/2022 19:48:56 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=34
05/12/2022 19:49:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=39
05/12/2022 19:49:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=44
05/12/2022 19:49:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=49
05/12/2022 19:49:12 - INFO - __main__ - Global step 100 Train loss 0.90 ACC 0.15625 on epoch=49
05/12/2022 19:49:12 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=49, global_step=100
05/12/2022 19:49:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=54
05/12/2022 19:49:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=59
05/12/2022 19:49:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=64
05/12/2022 19:49:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=69
05/12/2022 19:49:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=74
05/12/2022 19:49:37 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.09375 on epoch=74
05/12/2022 19:49:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=79
05/12/2022 19:49:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=84
05/12/2022 19:49:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
05/12/2022 19:49:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=94
05/12/2022 19:49:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
05/12/2022 19:50:02 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.28125 on epoch=99
05/12/2022 19:50:02 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.28125 on epoch=99, global_step=200
05/12/2022 19:50:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=104
05/12/2022 19:50:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
05/12/2022 19:50:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
05/12/2022 19:50:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
05/12/2022 19:50:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
05/12/2022 19:50:27 - INFO - __main__ - Global step 250 Train loss 0.39 ACC 0.3125 on epoch=124
05/12/2022 19:50:27 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=124, global_step=250
05/12/2022 19:50:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
05/12/2022 19:50:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=134
05/12/2022 19:50:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
05/12/2022 19:50:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
05/12/2022 19:50:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
05/12/2022 19:50:52 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.3125 on epoch=149
05/12/2022 19:50:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
05/12/2022 19:51:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
05/12/2022 19:51:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
05/12/2022 19:51:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
05/12/2022 19:51:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
05/12/2022 19:51:16 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.40625 on epoch=174
05/12/2022 19:51:16 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=174, global_step=350
05/12/2022 19:51:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=179
05/12/2022 19:51:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
05/12/2022 19:51:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
05/12/2022 19:51:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=194
05/12/2022 19:51:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
05/12/2022 19:51:41 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.375 on epoch=199
05/12/2022 19:51:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
05/12/2022 19:51:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
05/12/2022 19:51:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
05/12/2022 19:51:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=219
05/12/2022 19:52:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
05/12/2022 19:52:06 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.3125 on epoch=224
05/12/2022 19:52:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=229
05/12/2022 19:52:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
05/12/2022 19:52:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=239
05/12/2022 19:52:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
05/12/2022 19:52:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
05/12/2022 19:52:31 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.40625 on epoch=249
05/12/2022 19:52:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
05/12/2022 19:52:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=259
05/12/2022 19:52:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
05/12/2022 19:52:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
05/12/2022 19:52:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
05/12/2022 19:52:56 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.3125 on epoch=274
05/12/2022 19:53:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
05/12/2022 19:53:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
05/12/2022 19:53:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
05/12/2022 19:53:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=294
05/12/2022 19:53:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
05/12/2022 19:53:21 - INFO - __main__ - Global step 600 Train loss 0.20 ACC 0.28125 on epoch=299
05/12/2022 19:53:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
05/12/2022 19:53:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
05/12/2022 19:53:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
05/12/2022 19:53:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=319
05/12/2022 19:53:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=324
05/12/2022 19:53:46 - INFO - __main__ - Global step 650 Train loss 0.18 ACC 0.34375 on epoch=324
05/12/2022 19:53:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
05/12/2022 19:53:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
05/12/2022 19:53:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=339
05/12/2022 19:54:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
05/12/2022 19:54:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
05/12/2022 19:54:11 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.46875 on epoch=349
05/12/2022 19:54:11 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=349, global_step=700
05/12/2022 19:54:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
05/12/2022 19:54:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
05/12/2022 19:54:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
05/12/2022 19:54:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
05/12/2022 19:54:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
05/12/2022 19:54:36 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.3125 on epoch=374
05/12/2022 19:54:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=379
05/12/2022 19:54:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
05/12/2022 19:54:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
05/12/2022 19:54:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
05/12/2022 19:54:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
05/12/2022 19:55:01 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.40625 on epoch=399
05/12/2022 19:55:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=404
05/12/2022 19:55:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
05/12/2022 19:55:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
05/12/2022 19:55:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
05/12/2022 19:55:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
05/12/2022 19:55:26 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.40625 on epoch=424
05/12/2022 19:55:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
05/12/2022 19:55:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=434
05/12/2022 19:55:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
05/12/2022 19:55:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
05/12/2022 19:55:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
05/12/2022 19:55:52 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.3125 on epoch=449
05/12/2022 19:55:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
05/12/2022 19:56:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
05/12/2022 19:56:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 19:56:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
05/12/2022 19:56:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
05/12/2022 19:56:17 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.40625 on epoch=474
05/12/2022 19:56:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
05/12/2022 19:56:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/12/2022 19:56:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
05/12/2022 19:56:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
05/12/2022 19:56:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
05/12/2022 19:56:42 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.3125 on epoch=499
05/12/2022 19:56:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=504
05/12/2022 19:56:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/12/2022 19:56:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
05/12/2022 19:57:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
05/12/2022 19:57:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
05/12/2022 19:57:07 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.40625 on epoch=524
05/12/2022 19:57:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
05/12/2022 19:57:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/12/2022 19:57:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
05/12/2022 19:57:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
05/12/2022 19:57:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
05/12/2022 19:57:33 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.34375 on epoch=549
05/12/2022 19:57:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/12/2022 19:57:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
05/12/2022 19:57:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
05/12/2022 19:57:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
05/12/2022 19:57:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
05/12/2022 19:57:59 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.3125 on epoch=574
05/12/2022 19:58:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
05/12/2022 19:58:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
05/12/2022 19:58:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
05/12/2022 19:58:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 19:58:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
05/12/2022 19:58:24 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.375 on epoch=599
05/12/2022 19:58:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
05/12/2022 19:58:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
05/12/2022 19:58:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
05/12/2022 19:58:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
05/12/2022 19:58:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=624
05/12/2022 19:58:49 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.375 on epoch=624
05/12/2022 19:58:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/12/2022 19:58:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/12/2022 19:59:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
05/12/2022 19:59:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
05/12/2022 19:59:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
05/12/2022 19:59:16 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.34375 on epoch=649
05/12/2022 19:59:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/12/2022 19:59:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
05/12/2022 19:59:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
05/12/2022 19:59:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
05/12/2022 19:59:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 19:59:41 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.3125 on epoch=674
05/12/2022 19:59:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 19:59:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=684
05/12/2022 19:59:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
05/12/2022 19:59:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
05/12/2022 20:00:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
05/12/2022 20:00:07 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.34375 on epoch=699
05/12/2022 20:00:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
05/12/2022 20:00:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 20:00:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
05/12/2022 20:00:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 20:00:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
05/12/2022 20:00:33 - INFO - __main__ - Global step 1450 Train loss 0.04 ACC 0.28125 on epoch=724
05/12/2022 20:00:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 20:00:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 20:00:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 20:00:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
05/12/2022 20:00:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 20:00:58 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.34375 on epoch=749
05/12/2022 20:01:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 20:01:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 20:01:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 20:01:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 20:01:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 20:01:23 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.3125 on epoch=774
05/12/2022 20:01:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 20:01:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
05/12/2022 20:01:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 20:01:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
05/12/2022 20:01:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
05/12/2022 20:01:49 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.3125 on epoch=799
05/12/2022 20:01:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 20:01:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/12/2022 20:02:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 20:02:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/12/2022 20:02:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 20:02:15 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.28125 on epoch=824
05/12/2022 20:02:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 20:02:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
05/12/2022 20:02:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
05/12/2022 20:02:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
05/12/2022 20:02:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 20:02:40 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.3125 on epoch=849
05/12/2022 20:02:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 20:02:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/12/2022 20:02:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 20:02:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
05/12/2022 20:03:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
05/12/2022 20:03:06 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.375 on epoch=874
05/12/2022 20:03:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
05/12/2022 20:03:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
05/12/2022 20:03:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/12/2022 20:03:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 20:03:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/12/2022 20:03:32 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.375 on epoch=899
05/12/2022 20:03:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 20:03:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
05/12/2022 20:03:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
05/12/2022 20:03:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 20:03:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
05/12/2022 20:03:57 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.3125 on epoch=924
05/12/2022 20:04:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
05/12/2022 20:04:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
05/12/2022 20:04:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/12/2022 20:04:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
05/12/2022 20:04:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 20:04:22 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.34375 on epoch=949
05/12/2022 20:04:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 20:04:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
05/12/2022 20:04:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 20:04:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 20:04:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
05/12/2022 20:04:47 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.25 on epoch=974
05/12/2022 20:04:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
05/12/2022 20:04:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/12/2022 20:05:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 20:05:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
05/12/2022 20:05:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 20:05:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:05:10 - INFO - __main__ - Printing 3 examples
05/12/2022 20:05:10 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 20:05:10 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 20:05:10 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 20:05:10 - INFO - __main__ - ['we all have to do housework']
05/12/2022 20:05:10 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 20:05:10 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 20:05:10 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:05:10 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:05:10 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:05:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:05:10 - INFO - __main__ - Printing 3 examples
05/12/2022 20:05:10 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 20:05:10 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 20:05:10 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 20:05:10 - INFO - __main__ - ['Jason and Carly']
05/12/2022 20:05:10 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 20:05:10 - INFO - __main__ - ['not enough information']
05/12/2022 20:05:10 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:05:11 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:05:11 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:05:12 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.3125 on epoch=999
05/12/2022 20:05:12 - INFO - __main__ - save last model!
05/12/2022 20:05:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 20:05:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 20:05:12 - INFO - __main__ - Printing 3 examples
05/12/2022 20:05:12 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:05:12 - INFO - __main__ - ['about 10 minutes']
05/12/2022 20:05:12 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:05:12 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 20:05:12 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:05:12 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 20:05:12 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:05:13 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:05:14 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 20:05:29 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:05:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:05:29 - INFO - __main__ - Starting training!
05/12/2022 20:06:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_42_0.4_8_predictions.txt
05/12/2022 20:06:48 - INFO - __main__ - ACC on test data: 0.2620
05/12/2022 20:06:49 - INFO - __main__ - prefix=quail_32_42, lr=0.4, bsz=8, dev_performance=0.46875, test_performance=0.262
05/12/2022 20:06:49 - INFO - __main__ - Running ... prefix=quail_32_42, lr=0.3, bsz=8 ...
05/12/2022 20:06:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:06:50 - INFO - __main__ - Printing 3 examples
05/12/2022 20:06:50 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 20:06:50 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 20:06:50 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 20:06:50 - INFO - __main__ - ['we all have to do housework']
05/12/2022 20:06:50 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 20:06:50 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 20:06:50 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:06:50 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:06:50 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:06:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:06:50 - INFO - __main__ - Printing 3 examples
05/12/2022 20:06:50 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 20:06:50 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 20:06:50 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 20:06:50 - INFO - __main__ - ['Jason and Carly']
05/12/2022 20:06:50 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 20:06:50 - INFO - __main__ - ['not enough information']
05/12/2022 20:06:50 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:06:50 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:06:50 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:07:09 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:07:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:07:10 - INFO - __main__ - Starting training!
05/12/2022 20:07:15 - INFO - __main__ - Step 10 Global step 10 Train loss 1.88 on epoch=4
05/12/2022 20:07:19 - INFO - __main__ - Step 20 Global step 20 Train loss 1.37 on epoch=9
05/12/2022 20:07:24 - INFO - __main__ - Step 30 Global step 30 Train loss 1.08 on epoch=14
05/12/2022 20:07:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.86 on epoch=19
05/12/2022 20:07:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.79 on epoch=24
05/12/2022 20:07:36 - INFO - __main__ - Global step 50 Train loss 1.20 ACC 0.21875 on epoch=24
05/12/2022 20:07:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.21875 on epoch=24, global_step=50
05/12/2022 20:07:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=29
05/12/2022 20:07:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=34
05/12/2022 20:07:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=39
05/12/2022 20:07:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
05/12/2022 20:07:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=49
05/12/2022 20:08:01 - INFO - __main__ - Global step 100 Train loss 0.75 ACC 0.25 on epoch=49
05/12/2022 20:08:01 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=49, global_step=100
05/12/2022 20:08:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
05/12/2022 20:08:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
05/12/2022 20:08:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
05/12/2022 20:08:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
05/12/2022 20:08:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
05/12/2022 20:08:26 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.34375 on epoch=74
05/12/2022 20:08:26 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.34375 on epoch=74, global_step=150
05/12/2022 20:08:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
05/12/2022 20:08:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
05/12/2022 20:08:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
05/12/2022 20:08:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
05/12/2022 20:08:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
05/12/2022 20:08:52 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.40625 on epoch=99
05/12/2022 20:08:52 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.40625 on epoch=99, global_step=200
05/12/2022 20:08:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
05/12/2022 20:09:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
05/12/2022 20:09:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=114
05/12/2022 20:09:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=119
05/12/2022 20:09:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
05/12/2022 20:09:17 - INFO - __main__ - Global step 250 Train loss 0.41 ACC 0.40625 on epoch=124
05/12/2022 20:09:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
05/12/2022 20:09:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
05/12/2022 20:09:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=139
05/12/2022 20:09:35 - INFO - __main__ - Step 290 Global step 290 Train loss 2.16 on epoch=144
05/12/2022 20:09:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
05/12/2022 20:09:42 - INFO - __main__ - Global step 300 Train loss 0.79 ACC 0.3125 on epoch=149
05/12/2022 20:09:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=154
05/12/2022 20:09:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=159
05/12/2022 20:09:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=164
05/12/2022 20:10:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
05/12/2022 20:10:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
05/12/2022 20:10:08 - INFO - __main__ - Global step 350 Train loss 0.37 ACC 0.375 on epoch=174
05/12/2022 20:10:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
05/12/2022 20:10:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
05/12/2022 20:10:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
05/12/2022 20:10:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
05/12/2022 20:10:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
05/12/2022 20:10:33 - INFO - __main__ - Global step 400 Train loss 0.33 ACC 0.3125 on epoch=199
05/12/2022 20:10:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
05/12/2022 20:10:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
05/12/2022 20:10:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
05/12/2022 20:10:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
05/12/2022 20:10:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
05/12/2022 20:10:58 - INFO - __main__ - Global step 450 Train loss 0.31 ACC 0.25 on epoch=224
05/12/2022 20:11:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
05/12/2022 20:11:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
05/12/2022 20:11:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=239
05/12/2022 20:11:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
05/12/2022 20:11:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
05/12/2022 20:11:23 - INFO - __main__ - Global step 500 Train loss 0.31 ACC 0.28125 on epoch=249
05/12/2022 20:11:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
05/12/2022 20:11:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
05/12/2022 20:11:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=264
05/12/2022 20:11:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
05/12/2022 20:11:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=274
05/12/2022 20:11:49 - INFO - __main__ - Global step 550 Train loss 0.29 ACC 0.34375 on epoch=274
05/12/2022 20:11:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
05/12/2022 20:11:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
05/12/2022 20:12:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=289
05/12/2022 20:12:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
05/12/2022 20:12:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=299
05/12/2022 20:12:14 - INFO - __main__ - Global step 600 Train loss 0.28 ACC 0.34375 on epoch=299
05/12/2022 20:12:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=304
05/12/2022 20:12:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=309
05/12/2022 20:12:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
05/12/2022 20:12:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
05/12/2022 20:12:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
05/12/2022 20:12:39 - INFO - __main__ - Global step 650 Train loss 0.27 ACC 0.28125 on epoch=324
05/12/2022 20:12:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
05/12/2022 20:12:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
05/12/2022 20:12:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
05/12/2022 20:12:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
05/12/2022 20:13:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
05/12/2022 20:13:05 - INFO - __main__ - Global step 700 Train loss 0.23 ACC 0.34375 on epoch=349
05/12/2022 20:13:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
05/12/2022 20:13:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
05/12/2022 20:13:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=364
05/12/2022 20:13:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
05/12/2022 20:13:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
05/12/2022 20:13:30 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.375 on epoch=374
05/12/2022 20:13:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=379
05/12/2022 20:13:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
05/12/2022 20:13:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
05/12/2022 20:13:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
05/12/2022 20:13:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
05/12/2022 20:13:55 - INFO - __main__ - Global step 800 Train loss 0.23 ACC 0.3125 on epoch=399
05/12/2022 20:14:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
05/12/2022 20:14:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
05/12/2022 20:14:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
05/12/2022 20:14:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=419
05/12/2022 20:14:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
05/12/2022 20:14:21 - INFO - __main__ - Global step 850 Train loss 0.21 ACC 0.21875 on epoch=424
05/12/2022 20:14:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
05/12/2022 20:14:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
05/12/2022 20:14:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
05/12/2022 20:14:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=444
05/12/2022 20:14:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
05/12/2022 20:14:46 - INFO - __main__ - Global step 900 Train loss 0.20 ACC 0.28125 on epoch=449
05/12/2022 20:14:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
05/12/2022 20:14:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
05/12/2022 20:15:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
05/12/2022 20:15:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=469
05/12/2022 20:15:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=474
05/12/2022 20:15:11 - INFO - __main__ - Global step 950 Train loss 0.21 ACC 0.28125 on epoch=474
05/12/2022 20:15:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
05/12/2022 20:15:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=484
05/12/2022 20:15:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=489
05/12/2022 20:15:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
05/12/2022 20:15:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=499
05/12/2022 20:15:37 - INFO - __main__ - Global step 1000 Train loss 0.19 ACC 0.34375 on epoch=499
05/12/2022 20:15:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=504
05/12/2022 20:15:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=509
05/12/2022 20:15:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
05/12/2022 20:15:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
05/12/2022 20:15:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=524
05/12/2022 20:16:02 - INFO - __main__ - Global step 1050 Train loss 0.17 ACC 0.375 on epoch=524
05/12/2022 20:16:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=529
05/12/2022 20:16:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=534
05/12/2022 20:16:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
05/12/2022 20:16:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
05/12/2022 20:16:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=549
05/12/2022 20:16:28 - INFO - __main__ - Global step 1100 Train loss 0.16 ACC 0.34375 on epoch=549
05/12/2022 20:16:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
05/12/2022 20:16:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=559
05/12/2022 20:16:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=564
05/12/2022 20:16:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=569
05/12/2022 20:16:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=574
05/12/2022 20:16:53 - INFO - __main__ - Global step 1150 Train loss 0.17 ACC 0.3125 on epoch=574
05/12/2022 20:16:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=579
05/12/2022 20:17:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=584
05/12/2022 20:17:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
05/12/2022 20:17:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=594
05/12/2022 20:17:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
05/12/2022 20:17:19 - INFO - __main__ - Global step 1200 Train loss 0.14 ACC 0.34375 on epoch=599
05/12/2022 20:17:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=604
05/12/2022 20:17:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
05/12/2022 20:17:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=614
05/12/2022 20:17:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
05/12/2022 20:17:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=624
05/12/2022 20:17:44 - INFO - __main__ - Global step 1250 Train loss 0.14 ACC 0.3125 on epoch=624
05/12/2022 20:17:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=629
05/12/2022 20:17:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=634
05/12/2022 20:17:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=639
05/12/2022 20:18:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=644
05/12/2022 20:18:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=649
05/12/2022 20:18:10 - INFO - __main__ - Global step 1300 Train loss 0.11 ACC 0.3125 on epoch=649
05/12/2022 20:18:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=654
05/12/2022 20:18:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=659
05/12/2022 20:18:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
05/12/2022 20:18:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
05/12/2022 20:18:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=674
05/12/2022 20:18:35 - INFO - __main__ - Global step 1350 Train loss 0.13 ACC 0.3125 on epoch=674
05/12/2022 20:18:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=679
05/12/2022 20:18:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=684
05/12/2022 20:18:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=689
05/12/2022 20:18:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
05/12/2022 20:18:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=699
05/12/2022 20:19:00 - INFO - __main__ - Global step 1400 Train loss 0.10 ACC 0.25 on epoch=699
05/12/2022 20:19:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=704
05/12/2022 20:19:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=709
05/12/2022 20:19:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=714
05/12/2022 20:19:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=719
05/12/2022 20:19:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
05/12/2022 20:19:26 - INFO - __main__ - Global step 1450 Train loss 0.09 ACC 0.3125 on epoch=724
05/12/2022 20:19:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=729
05/12/2022 20:19:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=734
05/12/2022 20:19:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=739
05/12/2022 20:19:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
05/12/2022 20:19:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=749
05/12/2022 20:19:51 - INFO - __main__ - Global step 1500 Train loss 0.10 ACC 0.28125 on epoch=749
05/12/2022 20:19:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=754
05/12/2022 20:20:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
05/12/2022 20:20:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=764
05/12/2022 20:20:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=769
05/12/2022 20:20:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=774
05/12/2022 20:20:16 - INFO - __main__ - Global step 1550 Train loss 0.08 ACC 0.34375 on epoch=774
05/12/2022 20:20:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
05/12/2022 20:20:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=784
05/12/2022 20:20:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=789
05/12/2022 20:20:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=794
05/12/2022 20:20:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=799
05/12/2022 20:20:42 - INFO - __main__ - Global step 1600 Train loss 0.09 ACC 0.3125 on epoch=799
05/12/2022 20:20:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=804
05/12/2022 20:20:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=809
05/12/2022 20:20:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=814
05/12/2022 20:21:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
05/12/2022 20:21:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
05/12/2022 20:21:07 - INFO - __main__ - Global step 1650 Train loss 0.07 ACC 0.25 on epoch=824
05/12/2022 20:21:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
05/12/2022 20:21:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=834
05/12/2022 20:21:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
05/12/2022 20:21:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=844
05/12/2022 20:21:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=849
05/12/2022 20:21:32 - INFO - __main__ - Global step 1700 Train loss 0.10 ACC 0.3125 on epoch=849
05/12/2022 20:21:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=854
05/12/2022 20:21:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
05/12/2022 20:21:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=864
05/12/2022 20:21:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
05/12/2022 20:21:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
05/12/2022 20:21:58 - INFO - __main__ - Global step 1750 Train loss 0.07 ACC 0.25 on epoch=874
05/12/2022 20:22:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=879
05/12/2022 20:22:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
05/12/2022 20:22:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
05/12/2022 20:22:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=894
05/12/2022 20:22:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
05/12/2022 20:22:23 - INFO - __main__ - Global step 1800 Train loss 0.07 ACC 0.28125 on epoch=899
05/12/2022 20:22:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=904
05/12/2022 20:22:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
05/12/2022 20:22:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
05/12/2022 20:22:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=919
05/12/2022 20:22:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=924
05/12/2022 20:22:48 - INFO - __main__ - Global step 1850 Train loss 0.06 ACC 0.3125 on epoch=924
05/12/2022 20:22:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
05/12/2022 20:22:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=934
05/12/2022 20:23:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
05/12/2022 20:23:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=944
05/12/2022 20:23:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=949
05/12/2022 20:23:14 - INFO - __main__ - Global step 1900 Train loss 0.07 ACC 0.21875 on epoch=949
05/12/2022 20:23:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
05/12/2022 20:23:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=959
05/12/2022 20:23:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
05/12/2022 20:23:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
05/12/2022 20:23:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=974
05/12/2022 20:23:39 - INFO - __main__ - Global step 1950 Train loss 0.05 ACC 0.1875 on epoch=974
05/12/2022 20:23:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=979
05/12/2022 20:23:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
05/12/2022 20:23:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
05/12/2022 20:23:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
05/12/2022 20:24:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
05/12/2022 20:24:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:24:03 - INFO - __main__ - Printing 3 examples
05/12/2022 20:24:03 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 20:24:03 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 20:24:03 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 20:24:03 - INFO - __main__ - ['we all have to do housework']
05/12/2022 20:24:03 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 20:24:03 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 20:24:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:24:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:24:03 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:24:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:24:03 - INFO - __main__ - Printing 3 examples
05/12/2022 20:24:03 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 20:24:03 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 20:24:03 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 20:24:03 - INFO - __main__ - ['Jason and Carly']
05/12/2022 20:24:03 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 20:24:03 - INFO - __main__ - ['not enough information']
05/12/2022 20:24:03 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:24:03 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:24:03 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:24:05 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.28125 on epoch=999
05/12/2022 20:24:05 - INFO - __main__ - save last model!
05/12/2022 20:24:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 20:24:05 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 20:24:05 - INFO - __main__ - Printing 3 examples
05/12/2022 20:24:05 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:24:05 - INFO - __main__ - ['about 10 minutes']
05/12/2022 20:24:05 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:24:05 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 20:24:05 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:24:05 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 20:24:05 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:24:06 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:24:07 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 20:24:19 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:24:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:24:20 - INFO - __main__ - Starting training!
05/12/2022 20:25:37 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_42_0.3_8_predictions.txt
05/12/2022 20:25:37 - INFO - __main__ - ACC on test data: 0.2510
05/12/2022 20:25:37 - INFO - __main__ - prefix=quail_32_42, lr=0.3, bsz=8, dev_performance=0.40625, test_performance=0.251
05/12/2022 20:25:37 - INFO - __main__ - Running ... prefix=quail_32_42, lr=0.2, bsz=8 ...
05/12/2022 20:25:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:25:38 - INFO - __main__ - Printing 3 examples
05/12/2022 20:25:38 - INFO - __main__ -  [quail] What main issue does President Moon constantly want to address during his speeches?(A)Harsh U.S. led economic sanctions against North Korea(B)His desire to have a peaceful solution(C)Possible military action(D)not enough information [SEP] SEOUL, SOUTH KOREA — During his first year in office, South Korean President Moon Jae-in’s persistent pursuit of diplomacy, often working in parallel with U.S. President Donald Trump’s “maximum pressure” campaign, played a significant role in persuading North Korea to engage in talks to end its nuclear program. “Before the extreme measures might have been chosen by the United States, he gave diplomacy another chance to succeed,” said Bong Young-shik, a political analyst with the Yonsei University’s Institute for North Korean Studies in Seoul. A year ago this week, Moon, a former human rights lawyer and candidate for the progressive Democratic Party, won a special presidential election held after former conservative President Park Geun-hye was impeached for her involvement in a bribery and corruption scandal. This year, Park was sentenced to 24 years in prison during criminal trial related to the scandal in which she was charged with abuse of power, coercion and bribery. Moon assumed power at a time of increasing tensions, not just concerning North Korea’s accelerated nuclear and ballistic missile tests, but also China’s imposing of informal economic sanctions against South Korea for deploying the U.S. THAAD missile defense system, and the Trump administration’s talk of possible military action to end the North’s efforts to develop a nuclear armed intercontinental missile that could strike the U.S. mainland. In his inaugural address in May 2017, Moon promised to engage in shuttle diplomacy with Washington, Beijing and Pyongyang to work out a peaceful solution to the growing crisis. In a July speech in Berlin, President Moon laid out his vision for inter-Korean reconciliation that called for peaceful co-existence of the two Korean governments, but also said that denuclearization of the Korean Peninsula is “the absolute condition for peace.” But unlike past progressive South Korean governments that provided unconditional economic assistance, the Moon administration complied with economic sanctions in place and offered only cooperation on humanitarian aid and exchanges. Seoul also balanced its outreach efforts with maintaining strong support for the U.S. alliance, for joint military deterrence, and for imposing increasingly harsh U.S.-led economic sanctions against North Korea.
05/12/2022 20:25:38 - INFO - __main__ - ['His desire to have a peaceful solution']
05/12/2022 20:25:38 - INFO - __main__ -  [quail] Who has to do housework?(A)only a select few(B)we all have to do housework(C)only those who want to(D)not enough information [SEP] The two basic things you have to do to have a successful weight-loss journey is consume fewer calories (eat less) and exercise more.  It is a pleasure to report that it is possible to do both of these things without feeling deprived or overwhelmed. Just add these two little extras for help with weight-loss success. Add a Little Extra Exercise to Your Daily Routine A simple way to increase the level of exercise during your week is by adding a little more effort into your daily activities – the things you are going to do anyway. For example, we all have to do housework right? So, that is an easy one! Instead of taking your time and working at a leisurely pace, work faster. As you sweep and mop the kitchen floor, put some oomph into it. Work at a steady, brisk pace. Stretch your arms and use your muscles. This will definitely increase your heart rate, and burn more calories. When it’s time to clean the windows, reach up as high as you can – stretch to the top of the windows and be sure to get into the corners. Then, buff the glass to a high sheen, which will involve a little more elbow grease than normal. When you scrub out the tub and the shower, do it with energy and a song – make those tiles shine. Play fun music and dance when you sweep, mop or vacuum. Work for longer periods of time – 20 to 30 minutes of active cleaning is much better than 5 minutes here and 5 minutes there. It gives you more time to get your heart rate going and a better boost to your metabolism. These are things you can do to keep your house sparkling and build the new you. So, start throwing in an abundant amount of gusto for extra calorie burning and body shaping.
05/12/2022 20:25:38 - INFO - __main__ - ['we all have to do housework']
05/12/2022 20:25:38 - INFO - __main__ -  [quail] what is true about the mother(A)she likes to cook(B)not enough information(C)she likes to be the boss(D)she likes to be the leader [SEP] Thank you for the A2A ….. What has been your biggest heartbreak during your adult years? Had I answered this question 20 years ago, it would have been the death of my Daddy … Had it been 12 years ago, it would have been the death of my Mother … Had it been 10 years ago, it would have been when my son turned his back on God so far that he went into satanism … As it is now ~ I would have to say that the biggest heartbreak in my life was 3 years ago when I discovered (online) that my beautiful daughter had begun taking Testosterone injections. Believe me ~ I’ve been told time after time that her decision to do so is her own … to that fact I will agree. What I cannot agree with though, is that I have no right to feel hurt, upset, or any of the other assorted emotions that come with having a transgender child!! It’s not okay to be devastated by the choices my little girl makes … Its not okay to not agree with what she’s doing to herself … It’s not gonna have an impact on her family … It’s not something that is allowed to affect our relationship … The ONLY thing that matters ~ is the transgender person and how they feel!!!! One of the biggest things that bothers me is when I’m told that I can’t love my child ~ unless I accept that she wants to be a male and give her support with that decision. I can say that I do love my daughter ~ I always have ~ and I always will. A mother’s love doesn’t go away. ***To those who wish to attack me for what I have to say on the topic of transgenderism ~ I respect your right to tell me your opinion and why you think I’m wrong … but my feelings matter, too, and deserve that same respect from you.*** Wishing you the best!!
05/12/2022 20:25:38 - INFO - __main__ - ['she likes to be the boss']
05/12/2022 20:25:38 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:25:38 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:25:38 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:25:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:25:38 - INFO - __main__ - Printing 3 examples
05/12/2022 20:25:38 - INFO - __main__ -  [quail] What is currently making headlines?(A)Privacy invasions by Facebook(B)Facebook is listening to  consumer needs(C)Facebook following through on promises(D)not enough information [SEP] Facebook has a long track record and sordid history of abusing their users’ trust and privacy, including the most recent and egregious cases currently making headlines with Cambridge Analytica as well as election meddling in the US and Britain. As if that wasn’t enough, it then came to light that they have also been tracking and storing users’ phone call and text message data, including “who contacted whom, what time, and how long the conversation was.” This is nothing new, as we can see from this 2012 “study” in which they deliberately manipulated news feeds to influence users’ emotional state. And again with their settlement with the FCC back in 2011 (that they likely violated with their recent offenses) over deceiving consumers by telling them their information was private, but then repeatedly sharing it and making it public. And then there’s The Ugly Memo, in which a FB exec justifies growth at any cost in the name of connecting people, even if that results literally in people dying: We talk about the good and the bad of our work often. I want to talk about the ugly. We connect people. That can be bad if they make it negative. Maybe it costs a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is *de facto* good. And up until this most recent Cambridge Analytica scandal, Facebook was negotiating with various healthcare organizations to share patients’ health data with FB so they could match it up to your social circle to allegedly provide better health care. Um yeah right. Each time this stuff happens, they issue an apology (or a justification for their behavior) and promise they will take steps to better inform users and protect their privacy, but it’s clear that this is just lip service at this point.
05/12/2022 20:25:38 - INFO - __main__ - ['Privacy invasions by Facebook']
05/12/2022 20:25:38 - INFO - __main__ -  [quail] Who was beamed up?(A)the sheriff(B)not enough information(C)jason's mom(D)Jason and Carly [SEP] Jason Munt said him and Carly Furnish got beamed up by a bunch of aliens just after he'd boldly gone with her in the car park woods. It was the boldly going bit people thought was bullshit. Carly Furnish was a good God-loving girl. Trouble was, she'd gone missing. And Jason Munt had a weird crescent-shaped branding in his back, and was sticking to his story. Jason got hauled in by the cops and told he was in a whole heap of trouble. He reported blinding lights and a feeling like floating. He described being strapped to a table by little green men. A cop slapped the table and shouted, 'there's a frigging girl out there.' Jason said he knew how it sounded - the little green men, the whole thing - but it's true: they were little and green, just like out of the comic books. He volunteered tests for drink and drugs. He came back negative on both counts. They left him to stew. He said the last he saw of Carly was her being sucked up in some kind of light ray. He said, 'she seemed asleep - all peaceful, like.' Jason could not explain why he'd been beamed back down to earth, yet they'd seemingly taken Carly all the way off home with them to the Planet Zog. There were plenty of people willing to reckon it proved aliens had mighty good taste, but it wasn't the time nor the place to say it out loud. The cops released Jason after two days of questions. He stuck to his story throughout. The desk sergeant said, 'mark my words, there's a lot of hate out there.' Jason headed straight home. He lived in one of the straggle of council houses leading up to the tip. Carly Furnish and her folks lived two doors down. Supposedly they were distant relatives, but that's what everyone said about folks on that street.
05/12/2022 20:25:38 - INFO - __main__ - ['Jason and Carly']
05/12/2022 20:25:38 - INFO - __main__ -  [quail] What is Rod Rosenstein's favorite food?(A)Cheesecake(B)Pasta(C)not enough information(D)Pizza [SEP] WASHINGTON — U.S. President Donald Trump disclosed in a financial report filed with the government’s ethics watchdog Tuesday that he had reimbursed his personal lawyer more than $100,000 for unspecified expenses. In his annual financial disclosure form, which was released by the Office of Government Ethics (OGE) on Wednesday, Trump acknowledged that he had “fully reimbursed” his personal lawyer, Michael Cohen, in the range of $100,000 to $250,000 in 2016. Trump’s lawyers have previously said the president reimbursed Cohen for $130,000 Cohen paid to adult film star Stormy Daniels in the final weeks of the 2016 U.S. presidential campaign to keep her quiet about a sexual tryst she said she had with Trump 10 years earlier. Trump has denied the affair but recently confirmed reimbursing Cohen through a monthly retainer to stop “false and extortionist accusations” made by Daniels about an affair. Cohen has also acknowledged making the payment. The disclosure said that while the payment to Cohen was not a “reportable” liability, Trump chose to list it “in the interest of transparency.” It did not say why Trump had left it out of his 2017 financial disclosure documents, though one of the president’s lawyers, Rudy Giuliani, has said that Trump didn’t know about the payment when he reported his finances last year. The Office of Government Ethics, in a letter to Deputy Attorney General Rod Rosenstein, said it had determined that the payment to Cohen constituted a loan that should have been reported. However, it said the information Trump provided in his latest financial form met “the disclosure requirements for a reportable liability” under the Ethics in Government Act. Under the Ethics in Government Act, top government officials are required to report all debts in excess of $10,000 during the previous reporting period. “Knowingly or willfully” falsifying or failing to file reports carries civil and criminal penalties. Trump listed several hundred million dollars in liabilities in his financial report. Critics seized on the OGE’s letter to charge that Trump’s earlier omission of the payment could amount to a violation of federal laws on financial disclosures.
05/12/2022 20:25:38 - INFO - __main__ - ['not enough information']
05/12/2022 20:25:38 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:25:38 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:25:38 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:25:57 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:25:58 - INFO - __main__ - Starting training!
05/12/2022 20:26:03 - INFO - __main__ - Step 10 Global step 10 Train loss 2.01 on epoch=4
05/12/2022 20:26:08 - INFO - __main__ - Step 20 Global step 20 Train loss 1.54 on epoch=9
05/12/2022 20:26:12 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
05/12/2022 20:26:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.10 on epoch=19
05/12/2022 20:26:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=24
05/12/2022 20:26:24 - INFO - __main__ - Global step 50 Train loss 1.37 ACC 0.125 on epoch=24
05/12/2022 20:26:24 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
05/12/2022 20:26:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=29
05/12/2022 20:26:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=34
05/12/2022 20:26:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
05/12/2022 20:26:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
05/12/2022 20:26:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=49
05/12/2022 20:26:49 - INFO - __main__ - Global step 100 Train loss 0.65 ACC 0.1875 on epoch=49
05/12/2022 20:26:49 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.1875 on epoch=49, global_step=100
05/12/2022 20:26:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
05/12/2022 20:26:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=59
05/12/2022 20:27:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=64
05/12/2022 20:27:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
05/12/2022 20:27:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
05/12/2022 20:27:14 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.25 on epoch=74
05/12/2022 20:27:14 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=74, global_step=150
05/12/2022 20:27:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
05/12/2022 20:27:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
05/12/2022 20:27:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=89
05/12/2022 20:27:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=94
05/12/2022 20:27:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
05/12/2022 20:27:40 - INFO - __main__ - Global step 200 Train loss 0.36 ACC 0.25 on epoch=99
05/12/2022 20:27:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
05/12/2022 20:27:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
05/12/2022 20:27:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=114
05/12/2022 20:27:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
05/12/2022 20:28:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
05/12/2022 20:28:05 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.34375 on epoch=124
05/12/2022 20:28:05 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.34375 on epoch=124, global_step=250
05/12/2022 20:28:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
05/12/2022 20:28:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=134
05/12/2022 20:28:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
05/12/2022 20:28:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
05/12/2022 20:28:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
05/12/2022 20:28:30 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.34375 on epoch=149
05/12/2022 20:28:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
05/12/2022 20:28:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=159
05/12/2022 20:28:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=164
05/12/2022 20:28:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=169
05/12/2022 20:28:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
05/12/2022 20:28:56 - INFO - __main__ - Global step 350 Train loss 0.22 ACC 0.28125 on epoch=174
05/12/2022 20:29:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.16 on epoch=179
05/12/2022 20:29:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
05/12/2022 20:29:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=189
05/12/2022 20:29:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=194
05/12/2022 20:29:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
05/12/2022 20:29:21 - INFO - __main__ - Global step 400 Train loss 0.18 ACC 0.3125 on epoch=199
05/12/2022 20:29:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
05/12/2022 20:29:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=209
05/12/2022 20:29:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
05/12/2022 20:29:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
05/12/2022 20:29:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
05/12/2022 20:29:46 - INFO - __main__ - Global step 450 Train loss 0.17 ACC 0.28125 on epoch=224
05/12/2022 20:29:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
05/12/2022 20:29:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
05/12/2022 20:30:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.16 on epoch=239
05/12/2022 20:30:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=244
05/12/2022 20:30:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=249
05/12/2022 20:30:12 - INFO - __main__ - Global step 500 Train loss 0.17 ACC 0.28125 on epoch=249
05/12/2022 20:30:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
05/12/2022 20:30:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
05/12/2022 20:30:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
05/12/2022 20:30:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
05/12/2022 20:30:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
05/12/2022 20:30:37 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.28125 on epoch=274
05/12/2022 20:30:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
05/12/2022 20:30:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=284
05/12/2022 20:30:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=289
05/12/2022 20:30:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
05/12/2022 20:31:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
05/12/2022 20:31:03 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.375 on epoch=299
05/12/2022 20:31:03 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=299, global_step=600
05/12/2022 20:31:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
05/12/2022 20:31:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
05/12/2022 20:31:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
05/12/2022 20:31:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
05/12/2022 20:31:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
05/12/2022 20:31:28 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.34375 on epoch=324
05/12/2022 20:31:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
05/12/2022 20:31:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
05/12/2022 20:31:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=339
05/12/2022 20:31:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
05/12/2022 20:31:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
05/12/2022 20:31:54 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.375 on epoch=349
05/12/2022 20:31:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
05/12/2022 20:32:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
05/12/2022 20:32:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=364
05/12/2022 20:32:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
05/12/2022 20:32:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=374
05/12/2022 20:32:19 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.34375 on epoch=374
05/12/2022 20:32:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
05/12/2022 20:32:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=384
05/12/2022 20:32:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
05/12/2022 20:32:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
05/12/2022 20:32:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
05/12/2022 20:32:44 - INFO - __main__ - Global step 800 Train loss 0.09 ACC 0.3125 on epoch=399
05/12/2022 20:32:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
05/12/2022 20:32:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
05/12/2022 20:32:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
05/12/2022 20:33:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
05/12/2022 20:33:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
05/12/2022 20:33:10 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.3125 on epoch=424
05/12/2022 20:33:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
05/12/2022 20:33:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
05/12/2022 20:33:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
05/12/2022 20:33:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
05/12/2022 20:33:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
05/12/2022 20:33:35 - INFO - __main__ - Global step 900 Train loss 0.09 ACC 0.34375 on epoch=449
05/12/2022 20:33:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
05/12/2022 20:33:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
05/12/2022 20:33:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
05/12/2022 20:33:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 20:33:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
05/12/2022 20:34:01 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.40625 on epoch=474
05/12/2022 20:34:01 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=474, global_step=950
05/12/2022 20:34:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
05/12/2022 20:34:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/12/2022 20:34:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
05/12/2022 20:34:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
05/12/2022 20:34:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
05/12/2022 20:34:26 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.375 on epoch=499
05/12/2022 20:34:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=504
05/12/2022 20:34:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
05/12/2022 20:34:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
05/12/2022 20:34:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
05/12/2022 20:34:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
05/12/2022 20:34:52 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.34375 on epoch=524
05/12/2022 20:34:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=529
05/12/2022 20:35:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
05/12/2022 20:35:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=539
05/12/2022 20:35:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
05/12/2022 20:35:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
05/12/2022 20:35:17 - INFO - __main__ - Global step 1100 Train loss 0.07 ACC 0.3125 on epoch=549
05/12/2022 20:35:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=554
05/12/2022 20:35:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
05/12/2022 20:35:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
05/12/2022 20:35:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
05/12/2022 20:35:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
05/12/2022 20:35:42 - INFO - __main__ - Global step 1150 Train loss 0.06 ACC 0.34375 on epoch=574
05/12/2022 20:35:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
05/12/2022 20:35:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
05/12/2022 20:35:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
05/12/2022 20:36:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
05/12/2022 20:36:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
05/12/2022 20:36:08 - INFO - __main__ - Global step 1200 Train loss 0.06 ACC 0.40625 on epoch=599
05/12/2022 20:36:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
05/12/2022 20:36:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
05/12/2022 20:36:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
05/12/2022 20:36:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=619
05/12/2022 20:36:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=624
05/12/2022 20:36:33 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.28125 on epoch=624
05/12/2022 20:36:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
05/12/2022 20:36:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
05/12/2022 20:36:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
05/12/2022 20:36:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
05/12/2022 20:36:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
05/12/2022 20:36:59 - INFO - __main__ - Global step 1300 Train loss 0.05 ACC 0.34375 on epoch=649
05/12/2022 20:37:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
05/12/2022 20:37:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
05/12/2022 20:37:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
05/12/2022 20:37:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
05/12/2022 20:37:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 20:37:24 - INFO - __main__ - Global step 1350 Train loss 0.05 ACC 0.375 on epoch=674
05/12/2022 20:37:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 20:37:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
05/12/2022 20:37:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
05/12/2022 20:37:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
05/12/2022 20:37:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
05/12/2022 20:37:50 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.40625 on epoch=699
05/12/2022 20:37:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
05/12/2022 20:37:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
05/12/2022 20:38:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
05/12/2022 20:38:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 20:38:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
05/12/2022 20:38:15 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.34375 on epoch=724
05/12/2022 20:38:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
05/12/2022 20:38:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
05/12/2022 20:38:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 20:38:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
05/12/2022 20:38:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=749
05/12/2022 20:38:41 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.40625 on epoch=749
05/12/2022 20:38:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
05/12/2022 20:38:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
05/12/2022 20:38:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
05/12/2022 20:38:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
05/12/2022 20:39:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
05/12/2022 20:39:06 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.3125 on epoch=774
05/12/2022 20:39:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
05/12/2022 20:39:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
05/12/2022 20:39:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
05/12/2022 20:39:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
05/12/2022 20:39:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
05/12/2022 20:39:32 - INFO - __main__ - Global step 1600 Train loss 0.04 ACC 0.40625 on epoch=799
05/12/2022 20:39:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
05/12/2022 20:39:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
05/12/2022 20:39:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 20:39:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 20:39:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
05/12/2022 20:39:57 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.4375 on epoch=824
05/12/2022 20:39:57 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.4375 on epoch=824, global_step=1650
05/12/2022 20:40:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
05/12/2022 20:40:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
05/12/2022 20:40:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
05/12/2022 20:40:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
05/12/2022 20:40:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/12/2022 20:40:23 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.40625 on epoch=849
05/12/2022 20:40:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/12/2022 20:40:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
05/12/2022 20:40:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 20:40:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 20:40:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
05/12/2022 20:40:48 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.40625 on epoch=874
05/12/2022 20:40:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 20:40:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
05/12/2022 20:41:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=889
05/12/2022 20:41:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 20:41:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/12/2022 20:41:14 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.3125 on epoch=899
05/12/2022 20:41:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
05/12/2022 20:41:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
05/12/2022 20:41:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
05/12/2022 20:41:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
05/12/2022 20:41:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
05/12/2022 20:41:39 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.28125 on epoch=924
05/12/2022 20:41:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
05/12/2022 20:41:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
05/12/2022 20:41:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
05/12/2022 20:41:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
05/12/2022 20:42:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
05/12/2022 20:42:05 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.375 on epoch=949
05/12/2022 20:42:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
05/12/2022 20:42:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 20:42:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
05/12/2022 20:42:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
05/12/2022 20:42:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
05/12/2022 20:42:30 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.4375 on epoch=974
05/12/2022 20:42:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
05/12/2022 20:42:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
05/12/2022 20:42:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/12/2022 20:42:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
05/12/2022 20:42:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
05/12/2022 20:42:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:42:54 - INFO - __main__ - Printing 3 examples
05/12/2022 20:42:54 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 20:42:54 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 20:42:54 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 20:42:54 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 20:42:54 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 20:42:54 - INFO - __main__ - ['A few hours']
05/12/2022 20:42:54 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:42:54 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:42:54 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:42:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:42:54 - INFO - __main__ - Printing 3 examples
05/12/2022 20:42:54 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 20:42:54 - INFO - __main__ - ['inspired']
05/12/2022 20:42:54 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 20:42:54 - INFO - __main__ - ['not enough information']
05/12/2022 20:42:54 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 20:42:54 - INFO - __main__ - ['he is sensitive']
05/12/2022 20:42:54 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:42:55 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:42:55 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:42:56 - INFO - __main__ - Global step 2000 Train loss 0.03 ACC 0.40625 on epoch=999
05/12/2022 20:42:56 - INFO - __main__ - save last model!
05/12/2022 20:42:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 20:42:56 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 20:42:56 - INFO - __main__ - Printing 3 examples
05/12/2022 20:42:56 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:42:56 - INFO - __main__ - ['about 10 minutes']
05/12/2022 20:42:56 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:42:56 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 20:42:56 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 20:42:56 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 20:42:56 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:42:58 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:42:59 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 20:43:12 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:43:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:43:13 - INFO - __main__ - Starting training!
05/12/2022 20:44:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_42_0.2_8_predictions.txt
05/12/2022 20:44:28 - INFO - __main__ - ACC on test data: 0.2780
05/12/2022 20:44:28 - INFO - __main__ - prefix=quail_32_42, lr=0.2, bsz=8, dev_performance=0.4375, test_performance=0.278
05/12/2022 20:44:28 - INFO - __main__ - Running ... prefix=quail_32_87, lr=0.5, bsz=8 ...
05/12/2022 20:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:44:29 - INFO - __main__ - Printing 3 examples
05/12/2022 20:44:29 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 20:44:29 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 20:44:29 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 20:44:29 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 20:44:29 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 20:44:29 - INFO - __main__ - ['A few hours']
05/12/2022 20:44:29 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:44:29 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:44:29 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 20:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 20:44:29 - INFO - __main__ - Printing 3 examples
05/12/2022 20:44:29 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 20:44:29 - INFO - __main__ - ['inspired']
05/12/2022 20:44:29 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 20:44:29 - INFO - __main__ - ['not enough information']
05/12/2022 20:44:29 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 20:44:29 - INFO - __main__ - ['he is sensitive']
05/12/2022 20:44:29 - INFO - __main__ - Tokenizing Input ...
05/12/2022 20:44:30 - INFO - __main__ - Tokenizing Output ...
05/12/2022 20:44:30 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 20:44:48 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 20:44:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 20:44:49 - INFO - __main__ - Starting training!
05/12/2022 20:44:56 - INFO - __main__ - Step 10 Global step 10 Train loss 1.62 on epoch=4
05/12/2022 20:45:01 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
05/12/2022 20:45:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.77 on epoch=14
05/12/2022 20:45:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=19
05/12/2022 20:45:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=24
05/12/2022 20:45:17 - INFO - __main__ - Global step 50 Train loss 0.95 ACC 0.25 on epoch=24
05/12/2022 20:45:17 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=24, global_step=50
05/12/2022 20:45:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=29
05/12/2022 20:45:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=34
05/12/2022 20:45:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
05/12/2022 20:45:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
05/12/2022 20:45:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
05/12/2022 20:45:43 - INFO - __main__ - Global step 100 Train loss 0.47 ACC 0.3125 on epoch=49
05/12/2022 20:45:43 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.3125 on epoch=49, global_step=100
05/12/2022 20:45:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=54
05/12/2022 20:45:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
05/12/2022 20:45:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
05/12/2022 20:46:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
05/12/2022 20:46:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
05/12/2022 20:46:08 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.34375 on epoch=74
05/12/2022 20:46:08 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=74, global_step=150
05/12/2022 20:46:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
05/12/2022 20:46:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
05/12/2022 20:46:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
05/12/2022 20:46:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=94
05/12/2022 20:46:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
05/12/2022 20:46:33 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.375 on epoch=99
05/12/2022 20:46:33 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=99, global_step=200
05/12/2022 20:46:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.21 on epoch=104
05/12/2022 20:46:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
05/12/2022 20:46:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
05/12/2022 20:46:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.17 on epoch=119
05/12/2022 20:46:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
05/12/2022 20:46:59 - INFO - __main__ - Global step 250 Train loss 0.19 ACC 0.40625 on epoch=124
05/12/2022 20:46:59 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=124, global_step=250
05/12/2022 20:47:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=129
05/12/2022 20:47:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
05/12/2022 20:47:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
05/12/2022 20:47:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
05/12/2022 20:47:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
05/12/2022 20:47:24 - INFO - __main__ - Global step 300 Train loss 0.17 ACC 0.28125 on epoch=149
05/12/2022 20:47:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
05/12/2022 20:47:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
05/12/2022 20:47:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
05/12/2022 20:47:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
05/12/2022 20:47:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.10 on epoch=174
05/12/2022 20:47:49 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.4375 on epoch=174
05/12/2022 20:47:49 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.4375 on epoch=174, global_step=350
05/12/2022 20:47:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
05/12/2022 20:47:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
05/12/2022 20:48:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
05/12/2022 20:48:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
05/12/2022 20:48:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
05/12/2022 20:48:14 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.375 on epoch=199
05/12/2022 20:48:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
05/12/2022 20:48:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
05/12/2022 20:48:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
05/12/2022 20:48:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
05/12/2022 20:48:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
05/12/2022 20:48:40 - INFO - __main__ - Global step 450 Train loss 0.08 ACC 0.46875 on epoch=224
05/12/2022 20:48:40 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=224, global_step=450
05/12/2022 20:48:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
05/12/2022 20:48:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
05/12/2022 20:48:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
05/12/2022 20:48:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
05/12/2022 20:49:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
05/12/2022 20:49:05 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.40625 on epoch=249
05/12/2022 20:49:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
05/12/2022 20:49:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
05/12/2022 20:49:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
05/12/2022 20:49:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
05/12/2022 20:49:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
05/12/2022 20:49:30 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.4375 on epoch=274
05/12/2022 20:49:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
05/12/2022 20:49:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
05/12/2022 20:49:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
05/12/2022 20:49:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
05/12/2022 20:49:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
05/12/2022 20:49:55 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.53125 on epoch=299
05/12/2022 20:49:55 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=299, global_step=600
05/12/2022 20:49:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
05/12/2022 20:50:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
05/12/2022 20:50:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
05/12/2022 20:50:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
05/12/2022 20:50:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
05/12/2022 20:50:20 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.53125 on epoch=324
05/12/2022 20:50:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/12/2022 20:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
05/12/2022 20:50:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
05/12/2022 20:50:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
05/12/2022 20:50:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
05/12/2022 20:50:45 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.53125 on epoch=349
05/12/2022 20:50:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
05/12/2022 20:50:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
05/12/2022 20:50:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
05/12/2022 20:51:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/12/2022 20:51:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
05/12/2022 20:51:10 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.46875 on epoch=374
05/12/2022 20:51:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
05/12/2022 20:51:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
05/12/2022 20:51:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
05/12/2022 20:51:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
05/12/2022 20:51:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
05/12/2022 20:51:35 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.40625 on epoch=399
05/12/2022 20:51:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
05/12/2022 20:51:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
05/12/2022 20:51:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
05/12/2022 20:51:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
05/12/2022 20:51:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
05/12/2022 20:52:01 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.46875 on epoch=424
05/12/2022 20:52:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/12/2022 20:52:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
05/12/2022 20:52:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
05/12/2022 20:52:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
05/12/2022 20:52:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/12/2022 20:52:26 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.5 on epoch=449
05/12/2022 20:52:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
05/12/2022 20:52:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
05/12/2022 20:52:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
05/12/2022 20:52:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/12/2022 20:52:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
05/12/2022 20:52:51 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.5 on epoch=474
05/12/2022 20:52:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
05/12/2022 20:53:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
05/12/2022 20:53:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
05/12/2022 20:53:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
05/12/2022 20:53:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
05/12/2022 20:53:16 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.46875 on epoch=499
05/12/2022 20:53:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
05/12/2022 20:53:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
05/12/2022 20:53:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
05/12/2022 20:53:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
05/12/2022 20:53:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
05/12/2022 20:53:41 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.59375 on epoch=524
05/12/2022 20:53:41 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=524, global_step=1050
05/12/2022 20:53:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/12/2022 20:53:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/12/2022 20:53:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
05/12/2022 20:53:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
05/12/2022 20:54:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
05/12/2022 20:54:06 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.5625 on epoch=549
05/12/2022 20:54:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/12/2022 20:54:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/12/2022 20:54:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
05/12/2022 20:54:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
05/12/2022 20:54:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
05/12/2022 20:54:31 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.4375 on epoch=574
05/12/2022 20:54:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
05/12/2022 20:54:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
05/12/2022 20:54:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
05/12/2022 20:54:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
05/12/2022 20:54:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
05/12/2022 20:54:56 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.5625 on epoch=599
05/12/2022 20:55:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
05/12/2022 20:55:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/12/2022 20:55:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
05/12/2022 20:55:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/12/2022 20:55:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/12/2022 20:55:22 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.53125 on epoch=624
05/12/2022 20:55:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
05/12/2022 20:55:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
05/12/2022 20:55:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 20:55:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 20:55:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
05/12/2022 20:55:47 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.4375 on epoch=649
05/12/2022 20:55:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
05/12/2022 20:55:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
05/12/2022 20:56:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/12/2022 20:56:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
05/12/2022 20:56:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
05/12/2022 20:56:13 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.53125 on epoch=674
05/12/2022 20:56:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
05/12/2022 20:56:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/12/2022 20:56:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/12/2022 20:56:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
05/12/2022 20:56:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 20:56:38 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.4375 on epoch=699
05/12/2022 20:56:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 20:56:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
05/12/2022 20:56:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
05/12/2022 20:56:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/12/2022 20:57:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/12/2022 20:57:03 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.46875 on epoch=724
05/12/2022 20:57:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
05/12/2022 20:57:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
05/12/2022 20:57:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
05/12/2022 20:57:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/12/2022 20:57:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
05/12/2022 20:57:28 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.53125 on epoch=749
05/12/2022 20:57:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 20:57:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
05/12/2022 20:57:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
05/12/2022 20:57:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 20:57:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
05/12/2022 20:57:53 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.53125 on epoch=774
05/12/2022 20:57:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 20:58:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
05/12/2022 20:58:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
05/12/2022 20:58:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 20:58:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
05/12/2022 20:58:19 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.59375 on epoch=799
05/12/2022 20:58:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
05/12/2022 20:58:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/12/2022 20:58:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
05/12/2022 20:58:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/12/2022 20:58:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 20:58:44 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=824
05/12/2022 20:58:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
05/12/2022 20:58:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
05/12/2022 20:58:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
05/12/2022 20:59:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 20:59:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
05/12/2022 20:59:09 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.46875 on epoch=849
05/12/2022 20:59:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
05/12/2022 20:59:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 20:59:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 20:59:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
05/12/2022 20:59:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 20:59:34 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.46875 on epoch=874
05/12/2022 20:59:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
05/12/2022 20:59:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
05/12/2022 20:59:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 20:59:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/12/2022 20:59:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 20:59:59 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.46875 on epoch=899
05/12/2022 21:00:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/12/2022 21:00:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
05/12/2022 21:00:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
05/12/2022 21:00:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 21:00:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 21:00:24 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.5625 on epoch=924
05/12/2022 21:00:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 21:00:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/12/2022 21:00:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/12/2022 21:00:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
05/12/2022 21:00:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
05/12/2022 21:00:50 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5 on epoch=949
05/12/2022 21:00:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 21:00:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
05/12/2022 21:01:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 21:01:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 21:01:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
05/12/2022 21:01:15 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.53125 on epoch=974
05/12/2022 21:01:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
05/12/2022 21:01:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 21:01:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 21:01:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
05/12/2022 21:01:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/12/2022 21:01:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:01:38 - INFO - __main__ - Printing 3 examples
05/12/2022 21:01:38 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:01:38 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:01:38 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:01:38 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:01:38 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:01:38 - INFO - __main__ - ['A few hours']
05/12/2022 21:01:38 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:01:38 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:01:38 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:01:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:01:38 - INFO - __main__ - Printing 3 examples
05/12/2022 21:01:38 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:01:38 - INFO - __main__ - ['inspired']
05/12/2022 21:01:38 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:01:38 - INFO - __main__ - ['not enough information']
05/12/2022 21:01:38 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:01:38 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:01:38 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:01:38 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:01:38 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:01:40 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.53125 on epoch=999
05/12/2022 21:01:40 - INFO - __main__ - save last model!
05/12/2022 21:01:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 21:01:40 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 21:01:40 - INFO - __main__ - Printing 3 examples
05/12/2022 21:01:40 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:01:40 - INFO - __main__ - ['about 10 minutes']
05/12/2022 21:01:40 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:01:40 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 21:01:40 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:01:40 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 21:01:40 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:01:42 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:01:43 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 21:01:55 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:01:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:01:55 - INFO - __main__ - Starting training!
05/12/2022 21:03:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_87_0.5_8_predictions.txt
05/12/2022 21:03:19 - INFO - __main__ - ACC on test data: 0.2850
05/12/2022 21:03:21 - INFO - __main__ - prefix=quail_32_87, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.285
05/12/2022 21:03:21 - INFO - __main__ - Running ... prefix=quail_32_87, lr=0.4, bsz=8 ...
05/12/2022 21:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:03:22 - INFO - __main__ - Printing 3 examples
05/12/2022 21:03:22 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:03:22 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:03:22 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:03:22 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:03:22 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:03:22 - INFO - __main__ - ['A few hours']
05/12/2022 21:03:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:03:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:03:22 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:03:22 - INFO - __main__ - Printing 3 examples
05/12/2022 21:03:22 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:03:22 - INFO - __main__ - ['inspired']
05/12/2022 21:03:22 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:03:22 - INFO - __main__ - ['not enough information']
05/12/2022 21:03:22 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:03:22 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:03:22 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:03:22 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:03:22 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:03:37 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:03:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:03:38 - INFO - __main__ - Starting training!
05/12/2022 21:03:43 - INFO - __main__ - Step 10 Global step 10 Train loss 1.84 on epoch=4
05/12/2022 21:03:48 - INFO - __main__ - Step 20 Global step 20 Train loss 1.37 on epoch=9
05/12/2022 21:03:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.97 on epoch=14
05/12/2022 21:03:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.71 on epoch=19
05/12/2022 21:04:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=24
05/12/2022 21:04:04 - INFO - __main__ - Global step 50 Train loss 1.10 ACC 0.28125 on epoch=24
05/12/2022 21:04:04 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
05/12/2022 21:04:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
05/12/2022 21:04:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
05/12/2022 21:04:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=39
05/12/2022 21:04:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
05/12/2022 21:04:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=49
05/12/2022 21:04:29 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.34375 on epoch=49
05/12/2022 21:04:29 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.34375 on epoch=49, global_step=100
05/12/2022 21:04:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
05/12/2022 21:04:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=59
05/12/2022 21:04:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
05/12/2022 21:04:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
05/12/2022 21:04:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
05/12/2022 21:04:53 - INFO - __main__ - Global step 150 Train loss 0.37 ACC 0.34375 on epoch=74
05/12/2022 21:04:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=79
05/12/2022 21:05:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
05/12/2022 21:05:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
05/12/2022 21:05:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
05/12/2022 21:05:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
05/12/2022 21:05:18 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.3125 on epoch=99
05/12/2022 21:05:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
05/12/2022 21:05:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=109
05/12/2022 21:05:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=114
05/12/2022 21:05:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
05/12/2022 21:05:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
05/12/2022 21:05:43 - INFO - __main__ - Global step 250 Train loss 0.22 ACC 0.28125 on epoch=124
05/12/2022 21:05:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
05/12/2022 21:05:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
05/12/2022 21:05:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
05/12/2022 21:06:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
05/12/2022 21:06:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
05/12/2022 21:06:07 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.375 on epoch=149
05/12/2022 21:06:07 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=149, global_step=300
05/12/2022 21:06:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
05/12/2022 21:06:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
05/12/2022 21:06:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
05/12/2022 21:06:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
05/12/2022 21:06:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.17 on epoch=174
05/12/2022 21:06:32 - INFO - __main__ - Global step 350 Train loss 0.16 ACC 0.34375 on epoch=174
05/12/2022 21:06:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=179
05/12/2022 21:06:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
05/12/2022 21:06:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
05/12/2022 21:06:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=194
05/12/2022 21:06:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=199
05/12/2022 21:06:56 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.25 on epoch=199
05/12/2022 21:07:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
05/12/2022 21:07:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
05/12/2022 21:07:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
05/12/2022 21:07:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
05/12/2022 21:07:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
05/12/2022 21:07:21 - INFO - __main__ - Global step 450 Train loss 0.11 ACC 0.28125 on epoch=224
05/12/2022 21:07:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
05/12/2022 21:07:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=234
05/12/2022 21:07:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
05/12/2022 21:07:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
05/12/2022 21:07:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
05/12/2022 21:07:46 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.28125 on epoch=249
05/12/2022 21:07:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
05/12/2022 21:07:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
05/12/2022 21:07:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
05/12/2022 21:08:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
05/12/2022 21:08:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
05/12/2022 21:08:10 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.3125 on epoch=274
05/12/2022 21:08:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
05/12/2022 21:08:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
05/12/2022 21:08:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=289
05/12/2022 21:08:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
05/12/2022 21:08:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
05/12/2022 21:08:35 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.40625 on epoch=299
05/12/2022 21:08:35 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=299, global_step=600
05/12/2022 21:08:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
05/12/2022 21:08:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=309
05/12/2022 21:08:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
05/12/2022 21:08:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
05/12/2022 21:08:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
05/12/2022 21:09:00 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.375 on epoch=324
05/12/2022 21:09:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=329
05/12/2022 21:09:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
05/12/2022 21:09:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=339
05/12/2022 21:09:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
05/12/2022 21:09:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
05/12/2022 21:09:24 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.40625 on epoch=349
05/12/2022 21:09:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
05/12/2022 21:09:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
05/12/2022 21:09:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
05/12/2022 21:09:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/12/2022 21:09:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
05/12/2022 21:09:49 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.34375 on epoch=374
05/12/2022 21:09:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
05/12/2022 21:09:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/12/2022 21:10:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
05/12/2022 21:10:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
05/12/2022 21:10:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
05/12/2022 21:10:14 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.46875 on epoch=399
05/12/2022 21:10:14 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=399, global_step=800
05/12/2022 21:10:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
05/12/2022 21:10:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
05/12/2022 21:10:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/12/2022 21:10:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
05/12/2022 21:10:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
05/12/2022 21:10:38 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.40625 on epoch=424
05/12/2022 21:10:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
05/12/2022 21:10:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
05/12/2022 21:10:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=439
05/12/2022 21:10:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
05/12/2022 21:11:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
05/12/2022 21:11:03 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.4375 on epoch=449
05/12/2022 21:11:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
05/12/2022 21:11:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
05/12/2022 21:11:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
05/12/2022 21:11:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
05/12/2022 21:11:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
05/12/2022 21:11:29 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.34375 on epoch=474
05/12/2022 21:11:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
05/12/2022 21:11:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/12/2022 21:11:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
05/12/2022 21:11:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
05/12/2022 21:11:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
05/12/2022 21:11:54 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.4375 on epoch=499
05/12/2022 21:11:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
05/12/2022 21:12:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/12/2022 21:12:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
05/12/2022 21:12:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
05/12/2022 21:12:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
05/12/2022 21:12:18 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.375 on epoch=524
05/12/2022 21:12:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/12/2022 21:12:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
05/12/2022 21:12:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
05/12/2022 21:12:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
05/12/2022 21:12:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
05/12/2022 21:12:44 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.40625 on epoch=549
05/12/2022 21:12:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
05/12/2022 21:12:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
05/12/2022 21:12:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 21:13:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
05/12/2022 21:13:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
05/12/2022 21:13:09 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.40625 on epoch=574
05/12/2022 21:13:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
05/12/2022 21:13:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
05/12/2022 21:13:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
05/12/2022 21:13:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
05/12/2022 21:13:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
05/12/2022 21:13:34 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.34375 on epoch=599
05/12/2022 21:13:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
05/12/2022 21:13:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
05/12/2022 21:13:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
05/12/2022 21:13:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
05/12/2022 21:13:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
05/12/2022 21:13:59 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.4375 on epoch=624
05/12/2022 21:14:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
05/12/2022 21:14:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
05/12/2022 21:14:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/12/2022 21:14:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
05/12/2022 21:14:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/12/2022 21:14:24 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.40625 on epoch=649
05/12/2022 21:14:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/12/2022 21:14:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
05/12/2022 21:14:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
05/12/2022 21:14:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
05/12/2022 21:14:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 21:14:49 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.375 on epoch=674
05/12/2022 21:14:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 21:14:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
05/12/2022 21:15:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
05/12/2022 21:15:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
05/12/2022 21:15:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
05/12/2022 21:15:14 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.375 on epoch=699
05/12/2022 21:15:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/12/2022 21:15:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
05/12/2022 21:15:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
05/12/2022 21:15:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
05/12/2022 21:15:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
05/12/2022 21:15:39 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.40625 on epoch=724
05/12/2022 21:15:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
05/12/2022 21:15:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
05/12/2022 21:15:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 21:15:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
05/12/2022 21:16:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/12/2022 21:16:04 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.40625 on epoch=749
05/12/2022 21:16:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/12/2022 21:16:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/12/2022 21:16:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 21:16:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
05/12/2022 21:16:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 21:16:29 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.3125 on epoch=774
05/12/2022 21:16:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/12/2022 21:16:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 21:16:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
05/12/2022 21:16:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/12/2022 21:16:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/12/2022 21:16:55 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.25 on epoch=799
05/12/2022 21:16:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
05/12/2022 21:17:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
05/12/2022 21:17:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 21:17:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
05/12/2022 21:17:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/12/2022 21:17:20 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.40625 on epoch=824
05/12/2022 21:17:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 21:17:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
05/12/2022 21:17:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
05/12/2022 21:17:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/12/2022 21:17:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
05/12/2022 21:17:45 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.375 on epoch=849
05/12/2022 21:17:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
05/12/2022 21:17:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
05/12/2022 21:17:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/12/2022 21:18:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=869
05/12/2022 21:18:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
05/12/2022 21:18:10 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.375 on epoch=874
05/12/2022 21:18:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/12/2022 21:18:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
05/12/2022 21:18:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 21:18:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 21:18:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/12/2022 21:18:35 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.34375 on epoch=899
05/12/2022 21:18:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
05/12/2022 21:18:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
05/12/2022 21:18:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
05/12/2022 21:18:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/12/2022 21:18:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/12/2022 21:19:00 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.375 on epoch=924
05/12/2022 21:19:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/12/2022 21:19:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
05/12/2022 21:19:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
05/12/2022 21:19:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
05/12/2022 21:19:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 21:19:26 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.46875 on epoch=949
05/12/2022 21:19:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
05/12/2022 21:19:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
05/12/2022 21:19:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/12/2022 21:19:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/12/2022 21:19:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 21:19:51 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.34375 on epoch=974
05/12/2022 21:19:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/12/2022 21:20:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 21:20:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/12/2022 21:20:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 21:20:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
05/12/2022 21:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:20:14 - INFO - __main__ - Printing 3 examples
05/12/2022 21:20:14 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:20:14 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:20:14 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:20:14 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:20:14 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:20:14 - INFO - __main__ - ['A few hours']
05/12/2022 21:20:14 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:20:14 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:20:14 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:20:14 - INFO - __main__ - Printing 3 examples
05/12/2022 21:20:14 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:20:14 - INFO - __main__ - ['inspired']
05/12/2022 21:20:14 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:20:14 - INFO - __main__ - ['not enough information']
05/12/2022 21:20:14 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:20:14 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:20:14 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:20:14 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:20:15 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:20:16 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.3125 on epoch=999
05/12/2022 21:20:16 - INFO - __main__ - save last model!
05/12/2022 21:20:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 21:20:16 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 21:20:16 - INFO - __main__ - Printing 3 examples
05/12/2022 21:20:16 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:20:16 - INFO - __main__ - ['about 10 minutes']
05/12/2022 21:20:16 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:20:16 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 21:20:16 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:20:16 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 21:20:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:20:18 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:20:19 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 21:20:30 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:20:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:20:31 - INFO - __main__ - Starting training!
05/12/2022 21:21:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_87_0.4_8_predictions.txt
05/12/2022 21:21:57 - INFO - __main__ - ACC on test data: 0.2990
05/12/2022 21:21:57 - INFO - __main__ - prefix=quail_32_87, lr=0.4, bsz=8, dev_performance=0.46875, test_performance=0.299
05/12/2022 21:21:57 - INFO - __main__ - Running ... prefix=quail_32_87, lr=0.3, bsz=8 ...
05/12/2022 21:21:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:21:58 - INFO - __main__ - Printing 3 examples
05/12/2022 21:21:58 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:21:58 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:21:58 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:21:58 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:21:58 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:21:58 - INFO - __main__ - ['A few hours']
05/12/2022 21:21:58 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:21:58 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:21:58 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:21:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:21:58 - INFO - __main__ - Printing 3 examples
05/12/2022 21:21:58 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:21:58 - INFO - __main__ - ['inspired']
05/12/2022 21:21:58 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:21:58 - INFO - __main__ - ['not enough information']
05/12/2022 21:21:58 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:21:58 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:21:58 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:21:58 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:21:58 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:22:17 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:22:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:22:18 - INFO - __main__ - Starting training!
05/12/2022 21:22:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.83 on epoch=4
05/12/2022 21:22:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.20 on epoch=9
05/12/2022 21:22:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.94 on epoch=14
05/12/2022 21:22:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=19
05/12/2022 21:22:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=24
05/12/2022 21:22:46 - INFO - __main__ - Global step 50 Train loss 1.07 ACC 0.3125 on epoch=24
05/12/2022 21:22:46 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.3125 on epoch=24, global_step=50
05/12/2022 21:22:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
05/12/2022 21:22:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
05/12/2022 21:22:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=39
05/12/2022 21:23:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
05/12/2022 21:23:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
05/12/2022 21:23:11 - INFO - __main__ - Global step 100 Train loss 0.52 ACC 0.28125 on epoch=49
05/12/2022 21:23:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
05/12/2022 21:23:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
05/12/2022 21:23:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
05/12/2022 21:23:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
05/12/2022 21:23:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
05/12/2022 21:23:35 - INFO - __main__ - Global step 150 Train loss 0.38 ACC 0.34375 on epoch=74
05/12/2022 21:23:35 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=74, global_step=150
05/12/2022 21:23:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
05/12/2022 21:23:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
05/12/2022 21:23:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=89
05/12/2022 21:23:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
05/12/2022 21:23:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
05/12/2022 21:24:00 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.25 on epoch=99
05/12/2022 21:24:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=104
05/12/2022 21:24:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
05/12/2022 21:24:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
05/12/2022 21:24:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
05/12/2022 21:24:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
05/12/2022 21:24:26 - INFO - __main__ - Global step 250 Train loss 0.24 ACC 0.28125 on epoch=124
05/12/2022 21:24:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
05/12/2022 21:24:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
05/12/2022 21:24:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
05/12/2022 21:24:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
05/12/2022 21:24:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
05/12/2022 21:24:51 - INFO - __main__ - Global step 300 Train loss 0.22 ACC 0.40625 on epoch=149
05/12/2022 21:24:51 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.40625 on epoch=149, global_step=300
05/12/2022 21:24:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=154
05/12/2022 21:25:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
05/12/2022 21:25:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
05/12/2022 21:25:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
05/12/2022 21:25:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
05/12/2022 21:25:16 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.28125 on epoch=174
05/12/2022 21:25:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
05/12/2022 21:25:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
05/12/2022 21:25:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
05/12/2022 21:25:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
05/12/2022 21:25:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
05/12/2022 21:25:41 - INFO - __main__ - Global step 400 Train loss 0.16 ACC 0.34375 on epoch=199
05/12/2022 21:25:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
05/12/2022 21:25:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
05/12/2022 21:25:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
05/12/2022 21:25:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
05/12/2022 21:26:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
05/12/2022 21:26:16 - INFO - __main__ - Global step 450 Train loss 0.17 ACC 0.125 on epoch=224
05/12/2022 21:26:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.07 on epoch=229
05/12/2022 21:26:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=234
05/12/2022 21:26:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
05/12/2022 21:26:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
05/12/2022 21:26:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
05/12/2022 21:26:41 - INFO - __main__ - Global step 500 Train loss 0.32 ACC 0.375 on epoch=249
05/12/2022 21:26:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=254
05/12/2022 21:26:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
05/12/2022 21:26:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
05/12/2022 21:26:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
05/12/2022 21:27:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
05/12/2022 21:27:07 - INFO - __main__ - Global step 550 Train loss 0.11 ACC 0.375 on epoch=274
05/12/2022 21:27:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
05/12/2022 21:27:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
05/12/2022 21:27:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
05/12/2022 21:27:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
05/12/2022 21:27:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
05/12/2022 21:27:32 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.4375 on epoch=299
05/12/2022 21:27:32 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.4375 on epoch=299, global_step=600
05/12/2022 21:27:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
05/12/2022 21:27:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
05/12/2022 21:27:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
05/12/2022 21:27:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=319
05/12/2022 21:27:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
05/12/2022 21:27:57 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.40625 on epoch=324
05/12/2022 21:28:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
05/12/2022 21:28:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=334
05/12/2022 21:28:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=339
05/12/2022 21:28:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=344
05/12/2022 21:28:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
05/12/2022 21:28:22 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.4375 on epoch=349
05/12/2022 21:28:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
05/12/2022 21:28:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
05/12/2022 21:28:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
05/12/2022 21:28:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
05/12/2022 21:28:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
05/12/2022 21:28:47 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.375 on epoch=374
05/12/2022 21:28:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
05/12/2022 21:28:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
05/12/2022 21:29:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
05/12/2022 21:29:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=394
05/12/2022 21:29:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
05/12/2022 21:29:11 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.34375 on epoch=399
05/12/2022 21:29:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
05/12/2022 21:29:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
05/12/2022 21:29:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
05/12/2022 21:29:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
05/12/2022 21:29:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
05/12/2022 21:29:37 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.40625 on epoch=424
05/12/2022 21:29:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
05/12/2022 21:29:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
05/12/2022 21:29:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
05/12/2022 21:29:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
05/12/2022 21:29:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
05/12/2022 21:30:02 - INFO - __main__ - Global step 900 Train loss 0.08 ACC 0.40625 on epoch=449
05/12/2022 21:30:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
05/12/2022 21:30:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
05/12/2022 21:30:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
05/12/2022 21:30:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
05/12/2022 21:30:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
05/12/2022 21:30:27 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.34375 on epoch=474
05/12/2022 21:30:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
05/12/2022 21:30:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
05/12/2022 21:30:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
05/12/2022 21:30:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
05/12/2022 21:30:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
05/12/2022 21:30:52 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.34375 on epoch=499
05/12/2022 21:30:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/12/2022 21:31:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/12/2022 21:31:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
05/12/2022 21:31:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
05/12/2022 21:31:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
05/12/2022 21:31:17 - INFO - __main__ - Global step 1050 Train loss 0.07 ACC 0.375 on epoch=524
05/12/2022 21:31:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
05/12/2022 21:31:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
05/12/2022 21:31:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
05/12/2022 21:31:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
05/12/2022 21:31:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
05/12/2022 21:31:42 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.3125 on epoch=549
05/12/2022 21:31:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
05/12/2022 21:31:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
05/12/2022 21:31:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/12/2022 21:32:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
05/12/2022 21:32:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
05/12/2022 21:32:07 - INFO - __main__ - Global step 1150 Train loss 0.06 ACC 0.5 on epoch=574
05/12/2022 21:32:07 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=574, global_step=1150
05/12/2022 21:32:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
05/12/2022 21:32:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
05/12/2022 21:32:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
05/12/2022 21:32:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
05/12/2022 21:32:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
05/12/2022 21:32:32 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.40625 on epoch=599
05/12/2022 21:32:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
05/12/2022 21:32:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/12/2022 21:32:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
05/12/2022 21:32:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
05/12/2022 21:32:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
05/12/2022 21:32:57 - INFO - __main__ - Global step 1250 Train loss 0.05 ACC 0.375 on epoch=624
05/12/2022 21:33:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
05/12/2022 21:33:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
05/12/2022 21:33:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
05/12/2022 21:33:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
05/12/2022 21:33:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
05/12/2022 21:33:22 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.34375 on epoch=649
05/12/2022 21:33:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
05/12/2022 21:33:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=659
05/12/2022 21:33:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
05/12/2022 21:33:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
05/12/2022 21:33:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
05/12/2022 21:33:47 - INFO - __main__ - Global step 1350 Train loss 0.05 ACC 0.40625 on epoch=674
05/12/2022 21:33:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/12/2022 21:33:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
05/12/2022 21:34:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
05/12/2022 21:34:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
05/12/2022 21:34:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
05/12/2022 21:34:12 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.5 on epoch=699
05/12/2022 21:34:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
05/12/2022 21:34:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/12/2022 21:34:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
05/12/2022 21:34:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
05/12/2022 21:34:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
05/12/2022 21:34:37 - INFO - __main__ - Global step 1450 Train loss 0.04 ACC 0.4375 on epoch=724
05/12/2022 21:34:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=729
05/12/2022 21:34:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
05/12/2022 21:34:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
05/12/2022 21:34:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
05/12/2022 21:34:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
05/12/2022 21:35:02 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.34375 on epoch=749
05/12/2022 21:35:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
05/12/2022 21:35:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
05/12/2022 21:35:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/12/2022 21:35:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
05/12/2022 21:35:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/12/2022 21:35:27 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.4375 on epoch=774
05/12/2022 21:35:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
05/12/2022 21:35:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
05/12/2022 21:35:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
05/12/2022 21:35:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=794
05/12/2022 21:35:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/12/2022 21:35:52 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.40625 on epoch=799
05/12/2022 21:35:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
05/12/2022 21:36:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
05/12/2022 21:36:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
05/12/2022 21:36:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
05/12/2022 21:36:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
05/12/2022 21:36:17 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.34375 on epoch=824
05/12/2022 21:36:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/12/2022 21:36:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
05/12/2022 21:36:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
05/12/2022 21:36:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=844
05/12/2022 21:36:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
05/12/2022 21:36:42 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.40625 on epoch=849
05/12/2022 21:36:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
05/12/2022 21:36:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
05/12/2022 21:36:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
05/12/2022 21:37:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
05/12/2022 21:37:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
05/12/2022 21:37:07 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.4375 on epoch=874
05/12/2022 21:37:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
05/12/2022 21:37:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
05/12/2022 21:37:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
05/12/2022 21:37:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/12/2022 21:37:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/12/2022 21:37:32 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.4375 on epoch=899
05/12/2022 21:37:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
05/12/2022 21:37:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
05/12/2022 21:37:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=914
05/12/2022 21:37:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
05/12/2022 21:37:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
05/12/2022 21:37:57 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.5 on epoch=924
05/12/2022 21:38:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
05/12/2022 21:38:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
05/12/2022 21:38:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
05/12/2022 21:38:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
05/12/2022 21:38:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/12/2022 21:38:23 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.46875 on epoch=949
05/12/2022 21:38:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
05/12/2022 21:38:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
05/12/2022 21:38:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/12/2022 21:38:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/12/2022 21:38:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
05/12/2022 21:38:52 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.46875 on epoch=974
05/12/2022 21:38:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
05/12/2022 21:39:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/12/2022 21:39:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
05/12/2022 21:39:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/12/2022 21:39:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
05/12/2022 21:39:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:39:16 - INFO - __main__ - Printing 3 examples
05/12/2022 21:39:16 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:39:16 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:39:16 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:39:16 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:39:16 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:39:16 - INFO - __main__ - ['A few hours']
05/12/2022 21:39:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:39:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:39:16 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:39:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:39:16 - INFO - __main__ - Printing 3 examples
05/12/2022 21:39:16 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:39:16 - INFO - __main__ - ['inspired']
05/12/2022 21:39:16 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:39:16 - INFO - __main__ - ['not enough information']
05/12/2022 21:39:16 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:39:16 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:39:16 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:39:16 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:39:16 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:39:17 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.40625 on epoch=999
05/12/2022 21:39:17 - INFO - __main__ - save last model!
05/12/2022 21:39:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 21:39:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 21:39:17 - INFO - __main__ - Printing 3 examples
05/12/2022 21:39:17 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:39:17 - INFO - __main__ - ['about 10 minutes']
05/12/2022 21:39:17 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:39:17 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 21:39:17 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:39:17 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 21:39:17 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:39:19 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:39:20 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 21:39:33 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:39:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:39:34 - INFO - __main__ - Starting training!
05/12/2022 21:40:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_87_0.3_8_predictions.txt
05/12/2022 21:40:50 - INFO - __main__ - ACC on test data: 0.2740
05/12/2022 21:40:51 - INFO - __main__ - prefix=quail_32_87, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.274
05/12/2022 21:40:51 - INFO - __main__ - Running ... prefix=quail_32_87, lr=0.2, bsz=8 ...
05/12/2022 21:40:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:40:51 - INFO - __main__ - Printing 3 examples
05/12/2022 21:40:51 - INFO - __main__ -  [quail] Why did Silver apologize?(A)not enough information(B)Patch learned to speak bird(C)Patch crawled into the robin's nest(D)Patch shouted at the bluejay [SEP] It was not entirely true that Patch knew there was food in the mountains. He had never been to the mountains. No squirrel in all the Center Kingdom, as far as he knew, had ever been to the mountains. For between the kingdom and the mountains, surrounding it on all sides like a moat around a castle, there lay a blasted concrete wasteland, as wide as fifty squirrels laid nose to tail, and horrific death machines roared up and down this wasteland at terrifying speeds, all day and night. What's more, humans and dogs often crossed between the mountains and the kingdoms. And sometimes the dogs were not leashed. A squirrel would have to be very desperate indeed to dare the wastelands. It was Toro who had told Patch about the food in the mountains. Toro was Patch's friend. And that itself was extraordinary. Patch had always talked to birds. The drey he had grown up in -- Silver's old drey, before she became leader of the Seeker clan -- had been only a few branches away from a nest of robins. Once, in early spring when he was still a baby, Patch had crawled out of Silver's drey and into the robin's nest, and had spent a whole day among the chicks before Silver returned home and retrieved him. The robin mother had been unamused by Silver's profound apologies, and even less amused when Patch had returned to her nest the very next day. Eventually Silver taught Patch to leave the robins alone, but not before he had learned how to speak Bird. Most squirrels of the Center Kingdom could say and understand a few simple things in Bird, but Patch could actually hold conversations. And so, one autumn day when a bluejay swooped past and stole an acorn out of Patch's paws, Patch shouted angrily at the thief in Bird to bring it back; and the thief, intrigued, wheeled around in midair, perched on a branch above Patch, and looked curiously down at the irate squirrel. "Thieving feather-brained no-nose hawkbait!" Patch shouted up. "Stupid blind furry groundworm!" the bluejay retorted, and began to peck at the acorn.
05/12/2022 21:40:51 - INFO - __main__ - ["Patch crawled into the robin's nest"]
05/12/2022 21:40:52 - INFO - __main__ -  [quail] After a full day, how did the blogger probably feel about her day trip to the city?(A)not enough information(B)Hungry(C)Exhausted from a busy day walking around(D)Rushed to get back to the city [SEP] The Transylvanian city of Brasov is usually just a day trip from Bucharest. People spend maybe an hour or so to wandering around the town square, and buy a drink before hopping back on their transport back to the capital city of Romania. But, thanks to an epic tip-off from the travel ninja Shikha at Why Waste Annual Leave and a few other Brasov devotees, instead of following the crowds, we booked a couple of leisurely nights there. At the end of our day trip, instead of returning to Bucharest and getting stuck in the Sunday evening traffic, we checked into our hotel and waved our guide off.  Yes, you can easily ‘do’ Brasov in an afternoon – it’s a popular stop off at the end of a day trip after visiting the “home” of Dracula – Bran castle, and the beautiful Peles castle – but having the leisure to linger was exquisite. In addition to trying local dishes, we also ate lunch at McDonalds. Yes, yes, as a blogger we shouldn’t pretend to eat anything other than caviar washed down with champagne and unicorn tears, but we were hungry and couldn’t be bothered to find somewhere. (For the record, we both opted for a Big Mac and a diet coke.) For the first half day, we wandered around whilst the skies were beautifully blue… …learned a lot of local history, admired the Hollywood style sign in the hills… …slipped into a couple of churches (not to mention dodged giggling kids racing around on miniature cars)… …some hidden, and some hidden in plain sight… …and admired the sign from our cosy hotel, as it lit up at twilight Our only full day dawned fairly murky for the most part, it was a Sunday so a lot of things were closed, so after hitting 20,000 steps and getting lost a few times in the less pretty corners of the city…
05/12/2022 21:40:52 - INFO - __main__ - ['Exhausted from a busy day walking around']
05/12/2022 21:40:52 - INFO - __main__ -  [quail] How long was Trump's White House meeting with U.S. Governors?(A)More than a week(B)Ten minutes(C)A few hours(D)not enough information [SEP] The White House says China's proposal to abolish presidential term limits — a move that could make Xi Jinping president for life — is an internal matter for Beijing. "I believe that's a decision for China to make about what's best for their country," press secretary Sarah Huckabee Sanders said at a Monday press briefing. Term limits, Sanders said, are something Trump "supports here in the United States, but that's a decision that would be up to China." The Chinese Communist Party proposed removing the presidential two-term limit from China's constitution, state media reported Sunday. The move would be a further consolidation of power for Xi, who is already seen as one of China's most powerful leaders in decades.  On Monday, U.S. President Donald Trump praised Xi, saying he has a "very good relationship" and "great respect" for the Chinese leader. "I think that President Xi is unique. He's helping us with North Korea," Trump said during a White House meeting with U.S. governors. Trump has not specifically addressed the issue of China removing term limits. To some, Sanders' comments are the latest evidence of a break in the long-standing U.S. tradition of encouraging democracy in China, and reflect an unwillingness to criticize undemocratic regimes. "In effect, she is saying that the U.S. is OK with Xi Jinping simply asserting that he will remain in power indefinitely," said Sophie Richardson, China director at Human Rights Watch. "Does she realize China isn't a democracy?" During the presidential campaign, Trump regularly slammed China and its trade policies. But since becoming president, Trump has toned down the criticism. Instead, Trump has prioritized working with China to address North Korea's nuclear and ballistic missile programs. However, several reports suggest the White House could soon announce trade decisions, such as tariffs on Chinese imports, that could strain the U.S.-China relationship.
05/12/2022 21:40:52 - INFO - __main__ - ['A few hours']
05/12/2022 21:40:52 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:40:52 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:40:52 - INFO - __main__ - Loaded 32 examples from train data
05/12/2022 21:40:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/12/2022 21:40:52 - INFO - __main__ - Printing 3 examples
05/12/2022 21:40:52 - INFO - __main__ -  [quail] How does the author feel after reading the cookbook?(A)not enough information(B)uninterested(C)inspired(D)doubtful [SEP] I have known, and admired, Julia Busuttil Nishimura, for many years now and always felt connected through our love of Italian food, Tuscany (Julia lived in Florence and in Orbetello, just 10 minutes away from where we lived in Porto Ercole while I was writing Acquacotta) and Japan. So I have been eagerly awaiting her debut cookbook, Ostro: The Pleasure that Comes From Slowing Down and Cooking with Simple Ingredients, and it is a beauty — it is full of food I want to make and eat. It’s also a joy to look at, as it is presented so beautifully with photographs of Julia (and often her darling little boy Haruki) at home and tables full with food that look Caravaggesque. I have already made her everyday banana loaf, several times, after years of searching for my ideal everyday cake, it is now my go-to banana loaf. I tested her ricciarelli (soft almond cookies from Siena) and pork braised in milk (a classic Italian dish that I have always loved) when she was writing the book and I have long-known her absolutely wonderful, incredibly moist lemon olive oil cake (which is also divine with blood oranges). I absolutely love her homemade approach to everything, including all the wonderful handmade pasta like trofie and orecchiette (look at those beautiful dishes below), but also things like classic tiramisu — homemade savoiardi, homemade mascarpone (yes and yes, I absolutely believe that for the freshest, most delicious mascarpone, it needs to be homemade especially if you live outside of Italy). Yes, these are all things you can also buy in a packet, but Julia’s point is, it is a pleasure to make these things by hand, and the ingredients are so simple — water, flour, cream, eggs, — you probably have them all in your house already. She is a woman after my own heart. Her book inspires you to give it a go.
05/12/2022 21:40:52 - INFO - __main__ - ['inspired']
05/12/2022 21:40:52 - INFO - __main__ -  [quail] What was the name of the writer's wife?(A)Jeanette.(B)Marcia.(C)Gloria.(D)not enough information [SEP] April 2-May 2 is the worst 30 day period for me. The reason is… April 2, 2014 was the day my 53 year old wife found out that she had metastasized lung cancer. A large tumor had formed on the bottom of one lung, they found 4 tumors in her brain, another large one in her stomach and several more throughout her intestinal track. She rarely complained about physical problems and I knew something was wrong when she said she needed to go to the hospital ER. She died exactly 30 days later. I spent virtually every minute of that time with her. I went with her to radiation appointments. I stayed in the hospital with her because it seemed like every week I would have to take her in for something that required a 3 or 4 day stay. And the final trip to the ER was May 1st. An MRI showed that one of the tumors in her intestines had torn a hole in her bowel. The ER surgeon told us straight up that she was beyond any medical care that could help her and to use the next 24 hours to say goodbye to family and friends. And almost exactly 24 hours later, she was gone. I was devastated. 30.5 years of Happily Ever After turned into god fucking damn it all to hell! And when April 2 rolls around every year, I begin living those last 30 days of her life over again. I thought that after the third time in 2017 that I was about ready to let it go. I started really living again instead of just existing. But when April 2 came this year, I found that I was back in my memories. It was easier to deal with this time because of the way I had changed over the last year. I hung around with more friends and they helped distract me and one actually helped me turn May 2 into a celebration. And I love her for that. That's my story.
05/12/2022 21:40:52 - INFO - __main__ - ['not enough information']
05/12/2022 21:40:52 - INFO - __main__ -  [quail] what is probably true about Bruce?(A)not enough information(B)he is sensitive(C)he is thick skinned(D)he will find a new profession [SEP] When I first started out in the field, I was working at a private hospital in the locked psychiatric unit, and there was a call from one of the med floors requiring someone from our unit to assist with a problem of a young man who had bitten through his cotton restraints on his wrists. They were requesting he be restrained with leather ones. I had not done any restraining and was asked to assist a woman counselor to show me the ropes (no pun intended). When we arrived in the patients room, I was shocked to see a man in his early 20’s flailing around spasmodically on the bed with his gums bleeding in between his teeth. The nurse informed us that the patient was dying of a liver disease and his body and brain had become toxic, causing delirium. The counselor instructed me to hold down his arms and she placed him in leather restraints which were fastened to the side of the bed. The most upsetting thing which to this day I can not remove from memory was this man literally snapping his jaws at me like a mad animal with traumatized bloody gums from biting so hard on the cloth restraints. There was nothing I, or anyone could do, except keep him restrained from hurting himself or others. It is the kind of witness to suffering I had never experienced before, and I felt almost relieved when the nurse quietly said, “ He will die before morning. “ I suppose what is so traumatic about witnessing such situations is the utter sense of helplessness. I was a trained therapist, and in this situation, I was unable to do anything, and was rendered powerless. I recall being plagued by this young man’s face for some time when I tried to go to sleep at night. Bruce Kugler
05/12/2022 21:40:52 - INFO - __main__ - ['he is sensitive']
05/12/2022 21:40:52 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:40:52 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:40:52 - INFO - __main__ - Loaded 32 examples from dev data
05/12/2022 21:41:10 - INFO - __main__ - load prompt embedding from ckpt
05/12/2022 21:41:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/12/2022 21:41:11 - INFO - __main__ - Starting training!
05/12/2022 21:41:16 - INFO - __main__ - Step 10 Global step 10 Train loss 1.84 on epoch=4
05/12/2022 21:41:21 - INFO - __main__ - Step 20 Global step 20 Train loss 1.41 on epoch=9
05/12/2022 21:41:25 - INFO - __main__ - Step 30 Global step 30 Train loss 1.09 on epoch=14
05/12/2022 21:41:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.85 on epoch=19
05/12/2022 21:41:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=24
05/12/2022 21:41:37 - INFO - __main__ - Global step 50 Train loss 1.19 ACC 0.25 on epoch=24
05/12/2022 21:41:37 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=24, global_step=50
05/12/2022 21:41:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.71 on epoch=29
05/12/2022 21:41:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=34
05/12/2022 21:41:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=39
05/12/2022 21:41:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
05/12/2022 21:41:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=49
05/12/2022 21:42:02 - INFO - __main__ - Global step 100 Train loss 0.62 ACC 0.25 on epoch=49
05/12/2022 21:42:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=54
05/12/2022 21:42:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
05/12/2022 21:42:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
05/12/2022 21:42:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=69
05/12/2022 21:42:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
05/12/2022 21:42:27 - INFO - __main__ - Global step 150 Train loss 0.47 ACC 0.28125 on epoch=74
05/12/2022 21:42:27 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=74, global_step=150
05/12/2022 21:42:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
05/12/2022 21:42:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
05/12/2022 21:42:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
05/12/2022 21:42:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
05/12/2022 21:42:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
05/12/2022 21:42:52 - INFO - __main__ - Global step 200 Train loss 0.38 ACC 0.34375 on epoch=99
05/12/2022 21:42:52 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.34375 on epoch=99, global_step=200
05/12/2022 21:42:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
05/12/2022 21:43:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
05/12/2022 21:43:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
05/12/2022 21:43:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
05/12/2022 21:43:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=124
05/12/2022 21:43:17 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.40625 on epoch=124
05/12/2022 21:43:17 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.40625 on epoch=124, global_step=250
05/12/2022 21:43:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
05/12/2022 21:43:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=134
05/12/2022 21:43:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
05/12/2022 21:43:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
05/12/2022 21:43:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
05/12/2022 21:43:43 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.3125 on epoch=149
05/12/2022 21:43:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
05/12/2022 21:43:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
05/12/2022 21:43:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=164
05/12/2022 21:44:01 - INFO - __main__ - Step 340 Global step 340 Train loss 1.18 on epoch=169
05/12/2022 21:44:05 - INFO - __main__ - Step 350 Global step 350 Train loss 3.24 on epoch=174
05/12/2022 21:44:11 - INFO - __main__ - Global step 350 Train loss 1.03 ACC 0.09375 on epoch=174
05/12/2022 21:44:16 - INFO - __main__ - Step 360 Global step 360 Train loss 3.24 on epoch=179
05/12/2022 21:44:20 - INFO - __main__ - Step 370 Global step 370 Train loss 1.47 on epoch=184
05/12/2022 21:44:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
05/12/2022 21:44:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
05/12/2022 21:44:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
05/12/2022 21:44:36 - INFO - __main__ - Global step 400 Train loss 1.09 ACC 0.40625 on epoch=199
05/12/2022 21:44:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
05/12/2022 21:44:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
05/12/2022 21:44:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
05/12/2022 21:44:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
05/12/2022 21:44:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
05/12/2022 21:45:02 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.40625 on epoch=224
05/12/2022 21:45:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
05/12/2022 21:45:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
05/12/2022 21:45:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
05/12/2022 21:45:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
05/12/2022 21:45:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
05/12/2022 21:45:27 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.4375 on epoch=249
05/12/2022 21:45:27 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.4375 on epoch=249, global_step=500
05/12/2022 21:45:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
05/12/2022 21:45:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
05/12/2022 21:45:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=264
05/12/2022 21:45:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
05/12/2022 21:45:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
05/12/2022 21:45:52 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.4375 on epoch=274
05/12/2022 21:45:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=279
05/12/2022 21:46:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
05/12/2022 21:46:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
05/12/2022 21:46:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
05/12/2022 21:46:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=299
05/12/2022 21:46:18 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.4375 on epoch=299
05/12/2022 21:46:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
05/12/2022 21:46:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
05/12/2022 21:46:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
05/12/2022 21:46:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=319
05/12/2022 21:46:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=324
05/12/2022 21:46:43 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.40625 on epoch=324
05/12/2022 21:46:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
05/12/2022 21:46:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
05/12/2022 21:46:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
05/12/2022 21:47:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
05/12/2022 21:47:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
05/12/2022 21:47:08 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.40625 on epoch=349
05/12/2022 21:47:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
05/12/2022 21:47:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=359
05/12/2022 21:47:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=364
05/12/2022 21:47:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
05/12/2022 21:47:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=374
05/12/2022 21:47:34 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.375 on epoch=374
05/12/2022 21:47:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=379
05/12/2022 21:47:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
05/12/2022 21:47:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
05/12/2022 21:47:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
05/12/2022 21:47:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
05/12/2022 21:47:59 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.40625 on epoch=399
05/12/2022 21:48:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
05/12/2022 21:48:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
05/12/2022 21:48:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
05/12/2022 21:48:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
05/12/2022 21:48:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
05/12/2022 21:48:24 - INFO - __main__ - Global step 850 Train loss 0.23 ACC 0.34375 on epoch=424
05/12/2022 21:48:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
05/12/2022 21:48:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
05/12/2022 21:48:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
05/12/2022 21:48:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=444
05/12/2022 21:48:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
05/12/2022 21:48:50 - INFO - __main__ - Global step 900 Train loss 0.21 ACC 0.34375 on epoch=449
05/12/2022 21:48:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=454
05/12/2022 21:48:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
05/12/2022 21:49:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
05/12/2022 21:49:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
05/12/2022 21:49:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
05/12/2022 21:49:15 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.375 on epoch=474
05/12/2022 21:49:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=479
05/12/2022 21:49:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
05/12/2022 21:49:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
05/12/2022 21:49:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=494
05/12/2022 21:49:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
05/12/2022 21:49:40 - INFO - __main__ - Global step 1000 Train loss 0.22 ACC 0.375 on epoch=499
05/12/2022 21:49:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=504
05/12/2022 21:49:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
05/12/2022 21:49:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=514
05/12/2022 21:49:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
05/12/2022 21:50:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
05/12/2022 21:50:06 - INFO - __main__ - Global step 1050 Train loss 0.21 ACC 0.375 on epoch=524
05/12/2022 21:50:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
05/12/2022 21:50:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=534
05/12/2022 21:50:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=539
05/12/2022 21:50:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
05/12/2022 21:50:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=549
05/12/2022 21:50:31 - INFO - __main__ - Global step 1100 Train loss 0.20 ACC 0.375 on epoch=549
05/12/2022 21:50:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
05/12/2022 21:50:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
05/12/2022 21:50:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=564
05/12/2022 21:50:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
05/12/2022 21:50:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
05/12/2022 21:50:56 - INFO - __main__ - Global step 1150 Train loss 0.22 ACC 0.34375 on epoch=574
05/12/2022 21:51:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
05/12/2022 21:51:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=584
05/12/2022 21:51:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=589
05/12/2022 21:51:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=594
05/12/2022 21:51:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
05/12/2022 21:51:21 - INFO - __main__ - Global step 1200 Train loss 0.21 ACC 0.375 on epoch=599
05/12/2022 21:51:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=604
05/12/2022 21:51:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
05/12/2022 21:51:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=614
05/12/2022 21:51:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=619
05/12/2022 21:51:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
05/12/2022 21:51:47 - INFO - __main__ - Global step 1250 Train loss 0.19 ACC 0.34375 on epoch=624
05/12/2022 21:51:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=629
05/12/2022 21:51:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
05/12/2022 21:52:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=639
05/12/2022 21:52:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=644
05/12/2022 21:52:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=649
05/12/2022 21:52:12 - INFO - __main__ - Global step 1300 Train loss 0.18 ACC 0.40625 on epoch=649
05/12/2022 21:52:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=654
05/12/2022 21:52:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
05/12/2022 21:52:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
05/12/2022 21:52:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=669
05/12/2022 21:52:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=674
05/12/2022 21:52:37 - INFO - __main__ - Global step 1350 Train loss 0.18 ACC 0.375 on epoch=674
05/12/2022 21:52:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=679
05/12/2022 21:52:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
05/12/2022 21:52:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
05/12/2022 21:52:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=694
05/12/2022 21:52:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
05/12/2022 21:53:02 - INFO - __main__ - Global step 1400 Train loss 0.17 ACC 0.375 on epoch=699
05/12/2022 21:53:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=704
05/12/2022 21:53:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
05/12/2022 21:53:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=714
05/12/2022 21:53:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
05/12/2022 21:53:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
05/12/2022 21:53:27 - INFO - __main__ - Global step 1450 Train loss 0.16 ACC 0.375 on epoch=724
05/12/2022 21:53:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=729
05/12/2022 21:53:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=734
05/12/2022 21:53:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
05/12/2022 21:53:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
05/12/2022 21:53:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
05/12/2022 21:53:52 - INFO - __main__ - Global step 1500 Train loss 0.15 ACC 0.375 on epoch=749
05/12/2022 21:53:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
05/12/2022 21:54:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=759
05/12/2022 21:54:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
05/12/2022 21:54:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=769
05/12/2022 21:54:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
05/12/2022 21:54:18 - INFO - __main__ - Global step 1550 Train loss 0.18 ACC 0.46875 on epoch=774
05/12/2022 21:54:18 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=774, global_step=1550
05/12/2022 21:54:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=779
05/12/2022 21:54:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=784
05/12/2022 21:54:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=789
05/12/2022 21:54:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
05/12/2022 21:54:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=799
05/12/2022 21:54:43 - INFO - __main__ - Global step 1600 Train loss 0.17 ACC 0.4375 on epoch=799
05/12/2022 21:54:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
05/12/2022 21:54:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=809
05/12/2022 21:54:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=814
05/12/2022 21:55:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=819
05/12/2022 21:55:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=824
05/12/2022 21:55:08 - INFO - __main__ - Global step 1650 Train loss 0.16 ACC 0.40625 on epoch=824
05/12/2022 21:55:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
05/12/2022 21:55:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
05/12/2022 21:55:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
05/12/2022 21:55:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=844
05/12/2022 21:55:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=849
05/12/2022 21:55:33 - INFO - __main__ - Global step 1700 Train loss 0.15 ACC 0.375 on epoch=849
05/12/2022 21:55:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=854
05/12/2022 21:55:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=859
05/12/2022 21:55:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=864
05/12/2022 21:55:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
05/12/2022 21:55:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=874
05/12/2022 21:55:58 - INFO - __main__ - Global step 1750 Train loss 0.15 ACC 0.40625 on epoch=874
05/12/2022 21:56:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=879
05/12/2022 21:56:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=884
05/12/2022 21:56:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
05/12/2022 21:56:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=894
05/12/2022 21:56:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
05/12/2022 21:56:23 - INFO - __main__ - Global step 1800 Train loss 0.13 ACC 0.46875 on epoch=899
05/12/2022 21:56:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
05/12/2022 21:56:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
05/12/2022 21:56:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=914
05/12/2022 21:56:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=919
05/12/2022 21:56:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
05/12/2022 21:56:48 - INFO - __main__ - Global step 1850 Train loss 0.14 ACC 0.40625 on epoch=924
05/12/2022 21:56:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=929
05/12/2022 21:56:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
05/12/2022 21:57:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=939
05/12/2022 21:57:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=944
05/12/2022 21:57:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=949
05/12/2022 21:57:14 - INFO - __main__ - Global step 1900 Train loss 0.12 ACC 0.40625 on epoch=949
05/12/2022 21:57:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=954
05/12/2022 21:57:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
05/12/2022 21:57:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=964
05/12/2022 21:57:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
05/12/2022 21:57:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
05/12/2022 21:57:39 - INFO - __main__ - Global step 1950 Train loss 0.11 ACC 0.4375 on epoch=974
05/12/2022 21:57:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
05/12/2022 21:57:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
05/12/2022 21:57:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
05/12/2022 21:57:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=994
05/12/2022 21:58:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
05/12/2022 21:58:04 - INFO - __main__ - Global step 2000 Train loss 0.10 ACC 0.4375 on epoch=999
05/12/2022 21:58:04 - INFO - __main__ - save last model!
05/12/2022 21:58:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/12/2022 21:58:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/12/2022 21:58:04 - INFO - __main__ - Printing 3 examples
05/12/2022 21:58:04 - INFO - __main__ -  [quail] How long was Candy trying to seduce Larry?(A)about 10 minutes(B)about 2 hours(C)not enough information(D)All day [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:58:04 - INFO - __main__ - ['about 10 minutes']
05/12/2022 21:58:04 - INFO - __main__ -  [quail] What is probably true about Larry.(A)Larry does not have enough money for a prostitute.(B)not enough information(C)Larry likes sex.(D)Larry likes paying for sex. [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:58:04 - INFO - __main__ - ['Larry likes sex.']
05/12/2022 21:58:04 - INFO - __main__ -  [quail] What was Larry impressed with?(A)How windy it was outside(B)His own car(C)not enough information(D)That Candy had all her teeth [SEP] Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke. A chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off. She wrote down the license number as she circled around to the side of the expensive vehicle. He'll have a big wad of cash, she thought. Larry Luzor had just stepped out of the car, when she said, "Nice car, Honey." "Uh, thanks." "I'm Candy. You got a sweet tooth tonight?" He gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5'8". The long legs were very nice. Larry had never used a prostitute. He'd always thought of it as revolting. The idea of having sex with a woman who'd been with hundreds of men did not appeal to him. But this didn't seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn't. He knew she had to be just as skanky as the rest of them. Still--if he hadn't been in the middle of something important he might have been more than willing to buy what she was selling. "So, what do you say? Want to get it on?" She smiled seductively. He was impressed that she had all her teeth, and that they looked white. "How can I resist?" He grinned at her and winked.
05/12/2022 21:58:04 - INFO - __main__ - ['That Candy had all her teeth']
05/12/2022 21:58:04 - INFO - __main__ - Tokenizing Input ...
05/12/2022 21:58:05 - INFO - __main__ - Tokenizing Output ...
05/12/2022 21:58:07 - INFO - __main__ - Loaded 1000 examples from test data
05/12/2022 21:59:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-quail/quail_32_87_0.2_8_predictions.txt
05/12/2022 21:59:36 - INFO - __main__ - ACC on test data: 0.2830
05/12/2022 21:59:36 - INFO - __main__ - prefix=quail_32_87, lr=0.2, bsz=8, dev_performance=0.46875, test_performance=0.283
