nohup: ignoring input
Task: superglue-cb, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/18/2022 15:28:11 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 15:28:11 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-superglue-cb
03/18/2022 15:28:11 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 15:28:11 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-superglue-cb
03/18/2022 15:28:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 15:28:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 15:28:11 - INFO - __main__ - args.device: cuda:0
03/18/2022 15:28:11 - INFO - __main__ - Using 2 gpus
03/18/2022 15:28:11 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/18/2022 15:28:11 - INFO - __main__ - args.device: cuda:1
03/18/2022 15:28:11 - INFO - __main__ - Using 2 gpus
03/18/2022 15:28:11 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/18/2022 15:28:17 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0005, bsz=8 ...
03/18/2022 15:28:18 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:28:18 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:28:18 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:18 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:18 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:28:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:28:18 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:28:18 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:28:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:28:18 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:28:18 - INFO - __main__ - ['contradiction']
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:18 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:28:18 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:28:31 - INFO - __main__ - Starting training!
03/18/2022 15:28:31 - INFO - __main__ - Starting training!
03/18/2022 15:28:36 - INFO - __main__ - Step 10 Global step 10 Train loss 24.380905 on epoch=3
03/18/2022 15:28:40 - INFO - __main__ - Step 20 Global step 20 Train loss 16.753994 on epoch=6
03/18/2022 15:28:45 - INFO - __main__ - Step 30 Global step 30 Train loss 12.093635 on epoch=9
03/18/2022 15:28:50 - INFO - __main__ - Step 40 Global step 40 Train loss 9.559935 on epoch=13
03/18/2022 15:28:56 - INFO - __main__ - Step 50 Global step 50 Train loss 8.371121 on epoch=16
03/18/2022 15:28:56 - INFO - __main__ - Global step 50 Train loss 14.231916 ACC 0.0 on epoch=16
03/18/2022 15:29:03 - INFO - __main__ - Step 60 Global step 60 Train loss 7.926928 on epoch=19
03/18/2022 15:29:08 - INFO - __main__ - Step 70 Global step 70 Train loss 6.556070 on epoch=23
03/18/2022 15:29:13 - INFO - __main__ - Step 80 Global step 80 Train loss 5.325913 on epoch=26
03/18/2022 15:29:18 - INFO - __main__ - Step 90 Global step 90 Train loss 3.157629 on epoch=29
03/18/2022 15:29:23 - INFO - __main__ - Step 100 Global step 100 Train loss 2.824237 on epoch=33
03/18/2022 15:29:24 - INFO - __main__ - Global step 100 Train loss 5.158155 ACC 0.0 on epoch=33
03/18/2022 15:29:29 - INFO - __main__ - Step 110 Global step 110 Train loss 2.510618 on epoch=36
03/18/2022 15:29:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.845361 on epoch=39
03/18/2022 15:29:39 - INFO - __main__ - Step 130 Global step 130 Train loss 2.219876 on epoch=43
03/18/2022 15:29:44 - INFO - __main__ - Step 140 Global step 140 Train loss 2.520507 on epoch=46
03/18/2022 15:29:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.534117 on epoch=49
03/18/2022 15:29:50 - INFO - __main__ - Global step 150 Train loss 2.126096 ACC 0.5 on epoch=49
03/18/2022 15:29:57 - INFO - __main__ - Step 160 Global step 160 Train loss 1.634218 on epoch=53
03/18/2022 15:30:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.554612 on epoch=56
03/18/2022 15:30:07 - INFO - __main__ - Step 180 Global step 180 Train loss 1.780053 on epoch=59
03/18/2022 15:30:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.341440 on epoch=63
03/18/2022 15:30:18 - INFO - __main__ - Step 200 Global step 200 Train loss 1.579530 on epoch=66
03/18/2022 15:30:18 - INFO - __main__ - Global step 200 Train loss 1.577971 ACC 0.0 on epoch=66
03/18/2022 15:30:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.938030 on epoch=69
03/18/2022 15:30:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.154967 on epoch=73
03/18/2022 15:30:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.186437 on epoch=76
03/18/2022 15:30:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.010934 on epoch=79
03/18/2022 15:30:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.743167 on epoch=83
03/18/2022 15:30:44 - INFO - __main__ - Global step 250 Train loss 1.006707 ACC 0.0 on epoch=83
03/18/2022 15:30:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.681652 on epoch=86
03/18/2022 15:30:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.606585 on epoch=89
03/18/2022 15:30:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.608881 on epoch=93
03/18/2022 15:31:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.607002 on epoch=96
03/18/2022 15:31:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.599670 on epoch=99
03/18/2022 15:31:10 - INFO - __main__ - Global step 300 Train loss 0.620758 ACC 0.5625 on epoch=99
03/18/2022 15:31:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.787495 on epoch=103
03/18/2022 15:31:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.542470 on epoch=106
03/18/2022 15:31:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.472408 on epoch=109
03/18/2022 15:31:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.635988 on epoch=113
03/18/2022 15:31:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.363669 on epoch=116
03/18/2022 15:31:37 - INFO - __main__ - Global step 350 Train loss 0.560406 ACC 0.65625 on epoch=116
03/18/2022 15:31:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.339850 on epoch=119
03/18/2022 15:31:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.666020 on epoch=123
03/18/2022 15:31:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.297661 on epoch=126
03/18/2022 15:31:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.107465 on epoch=129
03/18/2022 15:32:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.030229 on epoch=133
03/18/2022 15:32:04 - INFO - __main__ - Global step 400 Train loss 0.288245 ACC 0.65625 on epoch=133
03/18/2022 15:32:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.103377 on epoch=136
03/18/2022 15:32:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003524 on epoch=139
03/18/2022 15:32:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001357 on epoch=143
03/18/2022 15:32:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003165 on epoch=146
03/18/2022 15:32:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005681 on epoch=149
03/18/2022 15:32:30 - INFO - __main__ - Global step 450 Train loss 0.023421 ACC 0.5 on epoch=149
03/18/2022 15:32:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.010055 on epoch=153
03/18/2022 15:32:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001451 on epoch=156
03/18/2022 15:32:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001600 on epoch=159
03/18/2022 15:32:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000902 on epoch=163
03/18/2022 15:32:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001211 on epoch=166
03/18/2022 15:32:56 - INFO - __main__ - Global step 500 Train loss 0.003044 ACC 0.71875 on epoch=166
03/18/2022 15:33:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001640 on epoch=169
03/18/2022 15:33:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000683 on epoch=173
03/18/2022 15:33:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000883 on epoch=176
03/18/2022 15:33:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000994 on epoch=179
03/18/2022 15:33:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000334 on epoch=183
03/18/2022 15:33:23 - INFO - __main__ - Global step 550 Train loss 0.000907 ACC 0.625 on epoch=183
03/18/2022 15:33:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000380 on epoch=186
03/18/2022 15:33:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000185 on epoch=189
03/18/2022 15:33:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000425 on epoch=193
03/18/2022 15:33:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000395 on epoch=196
03/18/2022 15:33:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000202 on epoch=199
03/18/2022 15:33:48 - INFO - __main__ - Global step 600 Train loss 0.000318 ACC 0.65625 on epoch=199
03/18/2022 15:33:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000636 on epoch=203
03/18/2022 15:33:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000304 on epoch=206
03/18/2022 15:34:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000413 on epoch=209
03/18/2022 15:34:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000155 on epoch=213
03/18/2022 15:34:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001116 on epoch=216
03/18/2022 15:34:14 - INFO - __main__ - Global step 650 Train loss 0.000525 ACC 0.71875 on epoch=216
03/18/2022 15:34:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000120 on epoch=219
03/18/2022 15:34:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000085 on epoch=223
03/18/2022 15:34:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000210 on epoch=226
03/18/2022 15:34:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000056 on epoch=229
03/18/2022 15:34:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000217 on epoch=233
03/18/2022 15:34:40 - INFO - __main__ - Global step 700 Train loss 0.000137 ACC 0.71875 on epoch=233
03/18/2022 15:34:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.017662 on epoch=236
03/18/2022 15:34:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001963 on epoch=239
03/18/2022 15:34:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000157 on epoch=243
03/18/2022 15:35:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000188 on epoch=246
03/18/2022 15:35:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000452 on epoch=249
03/18/2022 15:35:06 - INFO - __main__ - Global step 750 Train loss 0.004084 ACC 0.84375 on epoch=249
03/18/2022 15:35:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000073 on epoch=253
03/18/2022 15:35:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000211 on epoch=256
03/18/2022 15:35:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000632 on epoch=259
03/18/2022 15:35:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000166 on epoch=263
03/18/2022 15:35:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000308 on epoch=266
03/18/2022 15:35:32 - INFO - __main__ - Global step 800 Train loss 0.000278 ACC 0.5625 on epoch=266
03/18/2022 15:35:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000355 on epoch=269
03/18/2022 15:35:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000382 on epoch=273
03/18/2022 15:35:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000057 on epoch=276
03/18/2022 15:35:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000276 on epoch=279
03/18/2022 15:35:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000282 on epoch=283
03/18/2022 15:35:58 - INFO - __main__ - Global step 850 Train loss 0.000271 ACC 0.625 on epoch=283
03/18/2022 15:36:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000062 on epoch=286
03/18/2022 15:36:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000089 on epoch=289
03/18/2022 15:36:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000084 on epoch=293
03/18/2022 15:36:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000103 on epoch=296
03/18/2022 15:36:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000059 on epoch=299
03/18/2022 15:36:24 - INFO - __main__ - Global step 900 Train loss 0.000080 ACC 0.53125 on epoch=299
03/18/2022 15:36:24 - INFO - __main__ - save last model!
03/18/2022 15:36:25 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:36:25 - INFO - __main__ - Printing 3 examples
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 15:36:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:36:25 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:36:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:36:25 - INFO - __main__ - Printing 3 examples
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:36:25 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:25 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:36:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:36:25 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:36:32 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 15:36:32 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 15:36:32 - INFO - __main__ - Printing 3 examples
03/18/2022 15:36:32 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 15:36:32 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:32 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 15:36:32 - INFO - __main__ - ['neutral']
03/18/2022 15:36:32 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 15:36:32 - INFO - __main__ - ['entailment']
03/18/2022 15:36:32 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:36:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:36:32 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 15:36:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0005_8_predictions.txt
03/18/2022 15:36:33 - INFO - __main__ - ACC on test data: 0.8393
03/18/2022 15:36:34 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0005, bsz=8, dev_performance=0.84375, test_performance=0.8392857142857143
03/18/2022 15:36:34 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0003, bsz=8 ...
03/18/2022 15:36:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:36:35 - INFO - __main__ - Printing 3 examples
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 15:36:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:36:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:36:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:36:35 - INFO - __main__ - Printing 3 examples
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:36:35 - INFO - __main__ - ['contradiction']
03/18/2022 15:36:35 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:36:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:36:35 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:36:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:36:37 - INFO - __main__ - Starting training!
03/18/2022 15:36:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:36:48 - INFO - __main__ - Starting training!
03/18/2022 15:36:53 - INFO - __main__ - Step 10 Global step 10 Train loss 24.546253 on epoch=3
03/18/2022 15:36:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.695787 on epoch=6
03/18/2022 15:37:03 - INFO - __main__ - Step 30 Global step 30 Train loss 13.136694 on epoch=9
03/18/2022 15:37:09 - INFO - __main__ - Step 40 Global step 40 Train loss 11.001385 on epoch=13
03/18/2022 15:37:14 - INFO - __main__ - Step 50 Global step 50 Train loss 9.978685 on epoch=16
03/18/2022 15:37:15 - INFO - __main__ - Global step 50 Train loss 15.471762 ACC 0.03125 on epoch=16
03/18/2022 15:37:20 - INFO - __main__ - Step 60 Global step 60 Train loss 9.480143 on epoch=19
03/18/2022 15:37:25 - INFO - __main__ - Step 70 Global step 70 Train loss 8.663375 on epoch=23
03/18/2022 15:37:30 - INFO - __main__ - Step 80 Global step 80 Train loss 8.263922 on epoch=26
03/18/2022 15:37:36 - INFO - __main__ - Step 90 Global step 90 Train loss 7.879115 on epoch=29
03/18/2022 15:37:41 - INFO - __main__ - Step 100 Global step 100 Train loss 6.625957 on epoch=33
03/18/2022 15:37:42 - INFO - __main__ - Global step 100 Train loss 8.182503 ACC 0.0 on epoch=33
03/18/2022 15:37:47 - INFO - __main__ - Step 110 Global step 110 Train loss 6.256316 on epoch=36
03/18/2022 15:37:52 - INFO - __main__ - Step 120 Global step 120 Train loss 4.642182 on epoch=39
03/18/2022 15:37:57 - INFO - __main__ - Step 130 Global step 130 Train loss 4.170035 on epoch=43
03/18/2022 15:38:02 - INFO - __main__ - Step 140 Global step 140 Train loss 3.694360 on epoch=46
03/18/2022 15:38:07 - INFO - __main__ - Step 150 Global step 150 Train loss 3.193352 on epoch=49
03/18/2022 15:38:08 - INFO - __main__ - Global step 150 Train loss 4.391249 ACC 0.0 on epoch=49
03/18/2022 15:38:13 - INFO - __main__ - Step 160 Global step 160 Train loss 2.228550 on epoch=53
03/18/2022 15:38:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.460831 on epoch=56
03/18/2022 15:38:23 - INFO - __main__ - Step 180 Global step 180 Train loss 2.358396 on epoch=59
03/18/2022 15:38:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.915632 on epoch=63
03/18/2022 15:38:33 - INFO - __main__ - Step 200 Global step 200 Train loss 2.289541 on epoch=66
03/18/2022 15:38:34 - INFO - __main__ - Global step 200 Train loss 2.250590 ACC 0.0 on epoch=66
03/18/2022 15:38:39 - INFO - __main__ - Step 210 Global step 210 Train loss 2.497721 on epoch=69
03/18/2022 15:38:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.664180 on epoch=73
03/18/2022 15:38:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.767030 on epoch=76
03/18/2022 15:38:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.815750 on epoch=79
03/18/2022 15:38:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.665439 on epoch=83
03/18/2022 15:39:00 - INFO - __main__ - Global step 250 Train loss 1.882024 ACC 0.0 on epoch=83
03/18/2022 15:39:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.621156 on epoch=86
03/18/2022 15:39:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.303544 on epoch=89
03/18/2022 15:39:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.539524 on epoch=93
03/18/2022 15:39:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.357677 on epoch=96
03/18/2022 15:39:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.271180 on epoch=99
03/18/2022 15:39:26 - INFO - __main__ - Global step 300 Train loss 1.418616 ACC 0.0 on epoch=99
03/18/2022 15:39:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.618559 on epoch=103
03/18/2022 15:39:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.070789 on epoch=106
03/18/2022 15:39:41 - INFO - __main__ - Step 330 Global step 330 Train loss 1.361260 on epoch=109
03/18/2022 15:39:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.007833 on epoch=113
03/18/2022 15:39:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.982800 on epoch=116
03/18/2022 15:39:52 - INFO - __main__ - Global step 350 Train loss 1.208248 ACC 0.1875 on epoch=116
03/18/2022 15:39:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.037290 on epoch=119
03/18/2022 15:40:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.068884 on epoch=123
03/18/2022 15:40:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.952599 on epoch=126
03/18/2022 15:40:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.945661 on epoch=129
03/18/2022 15:40:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.690442 on epoch=133
03/18/2022 15:40:19 - INFO - __main__ - Global step 400 Train loss 0.938975 ACC 0.0 on epoch=133
03/18/2022 15:40:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.758368 on epoch=136
03/18/2022 15:40:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.876154 on epoch=139
03/18/2022 15:40:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.710333 on epoch=143
03/18/2022 15:40:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.749593 on epoch=146
03/18/2022 15:40:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.726680 on epoch=149
03/18/2022 15:40:45 - INFO - __main__ - Global step 450 Train loss 0.764226 ACC 0.5 on epoch=149
03/18/2022 15:40:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.812917 on epoch=153
03/18/2022 15:40:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.675196 on epoch=156
03/18/2022 15:41:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.522434 on epoch=159
03/18/2022 15:41:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.665663 on epoch=163
03/18/2022 15:41:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.523488 on epoch=166
03/18/2022 15:41:12 - INFO - __main__ - Global step 500 Train loss 0.639939 ACC 0.5 on epoch=166
03/18/2022 15:41:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.645667 on epoch=169
03/18/2022 15:41:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.489207 on epoch=173
03/18/2022 15:41:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.396230 on epoch=176
03/18/2022 15:41:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.307521 on epoch=179
03/18/2022 15:41:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.278488 on epoch=183
03/18/2022 15:41:38 - INFO - __main__ - Global step 550 Train loss 0.423423 ACC 0.75 on epoch=183
03/18/2022 15:41:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.223544 on epoch=186
03/18/2022 15:41:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.185897 on epoch=189
03/18/2022 15:41:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.141353 on epoch=193
03/18/2022 15:41:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.052436 on epoch=196
03/18/2022 15:42:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.034071 on epoch=199
03/18/2022 15:42:05 - INFO - __main__ - Global step 600 Train loss 0.127460 ACC 0.53125 on epoch=199
03/18/2022 15:42:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.008210 on epoch=203
03/18/2022 15:42:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.018158 on epoch=206
03/18/2022 15:42:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.005757 on epoch=209
03/18/2022 15:42:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001928 on epoch=213
03/18/2022 15:42:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001223 on epoch=216
03/18/2022 15:42:31 - INFO - __main__ - Global step 650 Train loss 0.007055 ACC 0.46875 on epoch=216
03/18/2022 15:42:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.009153 on epoch=219
03/18/2022 15:42:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.017225 on epoch=223
03/18/2022 15:42:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.014658 on epoch=226
03/18/2022 15:42:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004799 on epoch=229
03/18/2022 15:42:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001248 on epoch=233
03/18/2022 15:42:58 - INFO - __main__ - Global step 700 Train loss 0.009417 ACC 0.5625 on epoch=233
03/18/2022 15:43:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.027451 on epoch=236
03/18/2022 15:43:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001673 on epoch=239
03/18/2022 15:43:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.009124 on epoch=243
03/18/2022 15:43:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002447 on epoch=246
03/18/2022 15:43:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.004176 on epoch=249
03/18/2022 15:43:24 - INFO - __main__ - Global step 750 Train loss 0.008974 ACC 0.3125 on epoch=249
03/18/2022 15:43:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001199 on epoch=253
03/18/2022 15:43:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000348 on epoch=256
03/18/2022 15:43:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000434 on epoch=259
03/18/2022 15:43:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000117 on epoch=263
03/18/2022 15:43:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000976 on epoch=266
03/18/2022 15:43:50 - INFO - __main__ - Global step 800 Train loss 0.000615 ACC 0.59375 on epoch=266
03/18/2022 15:43:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000330 on epoch=269
03/18/2022 15:44:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000100 on epoch=273
03/18/2022 15:44:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000278 on epoch=276
03/18/2022 15:44:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000087 on epoch=279
03/18/2022 15:44:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000260 on epoch=283
03/18/2022 15:44:16 - INFO - __main__ - Global step 850 Train loss 0.000211 ACC 0.59375 on epoch=283
03/18/2022 15:44:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000092 on epoch=286
03/18/2022 15:44:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000312 on epoch=289
03/18/2022 15:44:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000139 on epoch=293
03/18/2022 15:44:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.029754 on epoch=296
03/18/2022 15:44:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000112 on epoch=299
03/18/2022 15:44:42 - INFO - __main__ - Global step 900 Train loss 0.006082 ACC 0.65625 on epoch=299
03/18/2022 15:44:42 - INFO - __main__ - save last model!
03/18/2022 15:44:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:44:43 - INFO - __main__ - Printing 3 examples
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 15:44:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:44:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:44:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:44:43 - INFO - __main__ - Printing 3 examples
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:44:43 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:43 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:44:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:44:43 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:44:49 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 15:44:50 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 15:44:50 - INFO - __main__ - Printing 3 examples
03/18/2022 15:44:50 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 15:44:50 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:50 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 15:44:50 - INFO - __main__ - ['neutral']
03/18/2022 15:44:50 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 15:44:50 - INFO - __main__ - ['entailment']
03/18/2022 15:44:50 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:44:50 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:44:50 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 15:44:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0003_8_predictions.txt
03/18/2022 15:44:51 - INFO - __main__ - ACC on test data: 0.7143
03/18/2022 15:44:52 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0003, bsz=8, dev_performance=0.75, test_performance=0.7142857142857143
03/18/2022 15:44:52 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0002, bsz=8 ...
03/18/2022 15:44:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:44:53 - INFO - __main__ - Printing 3 examples
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 15:44:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:44:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:44:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:44:53 - INFO - __main__ - Printing 3 examples
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:44:53 - INFO - __main__ - ['contradiction']
03/18/2022 15:44:53 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:44:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:44:53 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:44:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:44:53 - INFO - __main__ - Starting training!
03/18/2022 15:45:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:45:05 - INFO - __main__ - Starting training!
03/18/2022 15:45:09 - INFO - __main__ - Step 10 Global step 10 Train loss 24.200710 on epoch=3
03/18/2022 15:45:14 - INFO - __main__ - Step 20 Global step 20 Train loss 19.640779 on epoch=6
03/18/2022 15:45:19 - INFO - __main__ - Step 30 Global step 30 Train loss 12.323061 on epoch=9
03/18/2022 15:45:24 - INFO - __main__ - Step 40 Global step 40 Train loss 11.299921 on epoch=13
03/18/2022 15:45:29 - INFO - __main__ - Step 50 Global step 50 Train loss 10.552020 on epoch=16
03/18/2022 15:45:30 - INFO - __main__ - Global step 50 Train loss 15.603297 ACC 0.03125 on epoch=16
03/18/2022 15:45:36 - INFO - __main__ - Step 60 Global step 60 Train loss 10.306559 on epoch=19
03/18/2022 15:45:41 - INFO - __main__ - Step 70 Global step 70 Train loss 10.113131 on epoch=23
03/18/2022 15:45:46 - INFO - __main__ - Step 80 Global step 80 Train loss 9.263752 on epoch=26
03/18/2022 15:45:51 - INFO - __main__ - Step 90 Global step 90 Train loss 9.436083 on epoch=29
03/18/2022 15:45:56 - INFO - __main__ - Step 100 Global step 100 Train loss 8.320772 on epoch=33
03/18/2022 15:45:57 - INFO - __main__ - Global step 100 Train loss 9.488059 ACC 0.03125 on epoch=33
03/18/2022 15:46:02 - INFO - __main__ - Step 110 Global step 110 Train loss 8.399812 on epoch=36
03/18/2022 15:46:07 - INFO - __main__ - Step 120 Global step 120 Train loss 8.294911 on epoch=39
03/18/2022 15:46:12 - INFO - __main__ - Step 130 Global step 130 Train loss 7.296575 on epoch=43
03/18/2022 15:46:17 - INFO - __main__ - Step 140 Global step 140 Train loss 6.975546 on epoch=46
03/18/2022 15:46:22 - INFO - __main__ - Step 150 Global step 150 Train loss 6.168410 on epoch=49
03/18/2022 15:46:23 - INFO - __main__ - Global step 150 Train loss 7.427051 ACC 0.0 on epoch=49
03/18/2022 15:46:28 - INFO - __main__ - Step 160 Global step 160 Train loss 5.790554 on epoch=53
03/18/2022 15:46:33 - INFO - __main__ - Step 170 Global step 170 Train loss 5.086205 on epoch=56
03/18/2022 15:46:38 - INFO - __main__ - Step 180 Global step 180 Train loss 4.538135 on epoch=59
03/18/2022 15:46:43 - INFO - __main__ - Step 190 Global step 190 Train loss 3.816820 on epoch=63
03/18/2022 15:46:48 - INFO - __main__ - Step 200 Global step 200 Train loss 2.830945 on epoch=66
03/18/2022 15:46:49 - INFO - __main__ - Global step 200 Train loss 4.412532 ACC 0.03125 on epoch=66
03/18/2022 15:46:54 - INFO - __main__ - Step 210 Global step 210 Train loss 1.575397 on epoch=69
03/18/2022 15:46:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.882794 on epoch=73
03/18/2022 15:47:04 - INFO - __main__ - Step 230 Global step 230 Train loss 1.083695 on epoch=76
03/18/2022 15:47:09 - INFO - __main__ - Step 240 Global step 240 Train loss 3.354898 on epoch=79
03/18/2022 15:47:14 - INFO - __main__ - Step 250 Global step 250 Train loss 2.522397 on epoch=83
03/18/2022 15:47:15 - INFO - __main__ - Global step 250 Train loss 1.883836 ACC 0.03125 on epoch=83
03/18/2022 15:47:20 - INFO - __main__ - Step 260 Global step 260 Train loss 2.316615 on epoch=86
03/18/2022 15:47:25 - INFO - __main__ - Step 270 Global step 270 Train loss 2.395977 on epoch=89
03/18/2022 15:47:30 - INFO - __main__ - Step 280 Global step 280 Train loss 2.501704 on epoch=93
03/18/2022 15:47:36 - INFO - __main__ - Step 290 Global step 290 Train loss 2.270956 on epoch=96
03/18/2022 15:47:41 - INFO - __main__ - Step 300 Global step 300 Train loss 2.135233 on epoch=99
03/18/2022 15:47:41 - INFO - __main__ - Global step 300 Train loss 2.324097 ACC 0.0 on epoch=99
03/18/2022 15:47:46 - INFO - __main__ - Step 310 Global step 310 Train loss 2.424004 on epoch=103
03/18/2022 15:47:51 - INFO - __main__ - Step 320 Global step 320 Train loss 2.531080 on epoch=106
03/18/2022 15:47:56 - INFO - __main__ - Step 330 Global step 330 Train loss 1.566653 on epoch=109
03/18/2022 15:48:01 - INFO - __main__ - Step 340 Global step 340 Train loss 2.035227 on epoch=113
03/18/2022 15:48:07 - INFO - __main__ - Step 350 Global step 350 Train loss 1.909627 on epoch=116
03/18/2022 15:48:07 - INFO - __main__ - Global step 350 Train loss 2.093318 ACC 0.15625 on epoch=116
03/18/2022 15:48:14 - INFO - __main__ - Step 360 Global step 360 Train loss 2.233635 on epoch=119
03/18/2022 15:48:19 - INFO - __main__ - Step 370 Global step 370 Train loss 2.098013 on epoch=123
03/18/2022 15:48:24 - INFO - __main__ - Step 380 Global step 380 Train loss 1.876738 on epoch=126
03/18/2022 15:48:29 - INFO - __main__ - Step 390 Global step 390 Train loss 1.405152 on epoch=129
03/18/2022 15:48:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.899005 on epoch=133
03/18/2022 15:48:34 - INFO - __main__ - Global step 400 Train loss 1.902509 ACC 0.0 on epoch=133
03/18/2022 15:48:40 - INFO - __main__ - Step 410 Global step 410 Train loss 1.885217 on epoch=136
03/18/2022 15:48:45 - INFO - __main__ - Step 420 Global step 420 Train loss 1.957924 on epoch=139
03/18/2022 15:48:50 - INFO - __main__ - Step 430 Global step 430 Train loss 1.630125 on epoch=143
03/18/2022 15:48:55 - INFO - __main__ - Step 440 Global step 440 Train loss 1.584318 on epoch=146
03/18/2022 15:49:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.565595 on epoch=149
03/18/2022 15:49:00 - INFO - __main__ - Global step 450 Train loss 1.724636 ACC 0.0 on epoch=149
03/18/2022 15:49:06 - INFO - __main__ - Step 460 Global step 460 Train loss 1.349315 on epoch=153
03/18/2022 15:49:11 - INFO - __main__ - Step 470 Global step 470 Train loss 1.552324 on epoch=156
03/18/2022 15:49:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.239469 on epoch=159
03/18/2022 15:49:21 - INFO - __main__ - Step 490 Global step 490 Train loss 1.464395 on epoch=163
03/18/2022 15:49:26 - INFO - __main__ - Step 500 Global step 500 Train loss 1.389025 on epoch=166
03/18/2022 15:49:26 - INFO - __main__ - Global step 500 Train loss 1.398906 ACC 0.5 on epoch=166
03/18/2022 15:49:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.245452 on epoch=169
03/18/2022 15:49:37 - INFO - __main__ - Step 520 Global step 520 Train loss 1.326840 on epoch=173
03/18/2022 15:49:42 - INFO - __main__ - Step 530 Global step 530 Train loss 1.141451 on epoch=176
03/18/2022 15:49:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.917864 on epoch=179
03/18/2022 15:49:53 - INFO - __main__ - Step 550 Global step 550 Train loss 1.302205 on epoch=183
03/18/2022 15:49:53 - INFO - __main__ - Global step 550 Train loss 1.186762 ACC 0.5 on epoch=183
03/18/2022 15:49:58 - INFO - __main__ - Step 560 Global step 560 Train loss 1.430276 on epoch=186
03/18/2022 15:50:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.976617 on epoch=189
03/18/2022 15:50:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.302009 on epoch=193
03/18/2022 15:50:13 - INFO - __main__ - Step 590 Global step 590 Train loss 1.150055 on epoch=196
03/18/2022 15:50:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.727735 on epoch=199
03/18/2022 15:50:19 - INFO - __main__ - Global step 600 Train loss 1.117339 ACC 0.5 on epoch=199
03/18/2022 15:50:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.863096 on epoch=203
03/18/2022 15:50:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.719625 on epoch=206
03/18/2022 15:50:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.972363 on epoch=209
03/18/2022 15:50:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.873664 on epoch=213
03/18/2022 15:50:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.905786 on epoch=216
03/18/2022 15:50:45 - INFO - __main__ - Global step 650 Train loss 0.866907 ACC 0.46875 on epoch=216
03/18/2022 15:50:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.850101 on epoch=219
03/18/2022 15:50:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.859690 on epoch=223
03/18/2022 15:51:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.692038 on epoch=226
03/18/2022 15:51:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.720729 on epoch=229
03/18/2022 15:51:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.875555 on epoch=233
03/18/2022 15:51:11 - INFO - __main__ - Global step 700 Train loss 0.799622 ACC 0.40625 on epoch=233
03/18/2022 15:51:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.760769 on epoch=236
03/18/2022 15:51:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.831518 on epoch=239
03/18/2022 15:51:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.784667 on epoch=243
03/18/2022 15:51:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.590817 on epoch=246
03/18/2022 15:51:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.740486 on epoch=249
03/18/2022 15:51:37 - INFO - __main__ - Global step 750 Train loss 0.741651 ACC 0.5 on epoch=249
03/18/2022 15:51:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.689925 on epoch=253
03/18/2022 15:51:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.601212 on epoch=256
03/18/2022 15:51:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.608448 on epoch=259
03/18/2022 15:51:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.547607 on epoch=263
03/18/2022 15:52:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.677112 on epoch=266
03/18/2022 15:52:03 - INFO - __main__ - Global step 800 Train loss 0.624861 ACC 0.5625 on epoch=266
03/18/2022 15:52:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.578871 on epoch=269
03/18/2022 15:52:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.631381 on epoch=273
03/18/2022 15:52:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.526973 on epoch=276
03/18/2022 15:52:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.608802 on epoch=279
03/18/2022 15:52:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.578178 on epoch=283
03/18/2022 15:52:30 - INFO - __main__ - Global step 850 Train loss 0.584841 ACC 0.5 on epoch=283
03/18/2022 15:52:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.596599 on epoch=286
03/18/2022 15:52:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.563668 on epoch=289
03/18/2022 15:52:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.713520 on epoch=293
03/18/2022 15:52:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.502729 on epoch=296
03/18/2022 15:52:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.570606 on epoch=299
03/18/2022 15:52:56 - INFO - __main__ - Global step 900 Train loss 0.589424 ACC 0.65625 on epoch=299
03/18/2022 15:52:57 - INFO - __main__ - save last model!
03/18/2022 15:52:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:52:57 - INFO - __main__ - Printing 3 examples
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 15:52:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:52:57 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:52:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:52:57 - INFO - __main__ - Printing 3 examples
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:52:57 - INFO - __main__ - ['contradiction']
03/18/2022 15:52:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:52:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:52:57 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:53:03 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 15:53:04 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 15:53:04 - INFO - __main__ - Printing 3 examples
03/18/2022 15:53:04 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 15:53:04 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:04 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 15:53:04 - INFO - __main__ - ['neutral']
03/18/2022 15:53:04 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 15:53:04 - INFO - __main__ - ['entailment']
03/18/2022 15:53:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:53:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:53:04 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 15:53:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0002_8_predictions.txt
03/18/2022 15:53:05 - INFO - __main__ - ACC on test data: 0.5536
03/18/2022 15:53:06 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0002, bsz=8, dev_performance=0.65625, test_performance=0.5535714285714286
03/18/2022 15:53:06 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0001, bsz=8 ...
03/18/2022 15:53:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 15:53:06 - INFO - __main__ - Printing 3 examples
03/18/2022 15:53:06 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 15:53:06 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:06 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/18/2022 15:53:06 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:06 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/18/2022 15:53:06 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 15:53:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:53:07 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 15:53:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:53:07 - INFO - __main__ - Printing 3 examples
03/18/2022 15:53:07 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/18/2022 15:53:07 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:07 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 15:53:07 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:07 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/18/2022 15:53:07 - INFO - __main__ - ['contradiction']
03/18/2022 15:53:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:53:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:53:07 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:53:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:53:07 - INFO - __main__ - Starting training!
03/18/2022 15:53:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 15:53:20 - INFO - __main__ - Starting training!
03/18/2022 15:53:24 - INFO - __main__ - Step 10 Global step 10 Train loss 24.669838 on epoch=3
03/18/2022 15:53:29 - INFO - __main__ - Step 20 Global step 20 Train loss 20.762682 on epoch=6
03/18/2022 15:53:35 - INFO - __main__ - Step 30 Global step 30 Train loss 15.860075 on epoch=9
03/18/2022 15:53:40 - INFO - __main__ - Step 40 Global step 40 Train loss 13.183316 on epoch=13
03/18/2022 15:53:45 - INFO - __main__ - Step 50 Global step 50 Train loss 12.506298 on epoch=16
03/18/2022 15:53:45 - INFO - __main__ - Global step 50 Train loss 17.396442 ACC 0.03125 on epoch=16
03/18/2022 15:53:51 - INFO - __main__ - Step 60 Global step 60 Train loss 11.924128 on epoch=19
03/18/2022 15:53:56 - INFO - __main__ - Step 70 Global step 70 Train loss 11.491919 on epoch=23
03/18/2022 15:54:01 - INFO - __main__ - Step 80 Global step 80 Train loss 10.860042 on epoch=26
03/18/2022 15:54:06 - INFO - __main__ - Step 90 Global step 90 Train loss 11.258966 on epoch=29
03/18/2022 15:54:11 - INFO - __main__ - Step 100 Global step 100 Train loss 10.295591 on epoch=33
03/18/2022 15:54:12 - INFO - __main__ - Global step 100 Train loss 11.166129 ACC 0.0 on epoch=33
03/18/2022 15:54:17 - INFO - __main__ - Step 110 Global step 110 Train loss 9.808466 on epoch=36
03/18/2022 15:54:22 - INFO - __main__ - Step 120 Global step 120 Train loss 9.654621 on epoch=39
03/18/2022 15:54:27 - INFO - __main__ - Step 130 Global step 130 Train loss 9.957245 on epoch=43
03/18/2022 15:54:32 - INFO - __main__ - Step 140 Global step 140 Train loss 9.516579 on epoch=46
03/18/2022 15:54:37 - INFO - __main__ - Step 150 Global step 150 Train loss 9.085653 on epoch=49
03/18/2022 15:54:38 - INFO - __main__ - Global step 150 Train loss 9.604513 ACC 0.0 on epoch=49
03/18/2022 15:54:43 - INFO - __main__ - Step 160 Global step 160 Train loss 8.880049 on epoch=53
03/18/2022 15:54:48 - INFO - __main__ - Step 170 Global step 170 Train loss 8.588170 on epoch=56
03/18/2022 15:54:53 - INFO - __main__ - Step 180 Global step 180 Train loss 8.500669 on epoch=59
03/18/2022 15:54:58 - INFO - __main__ - Step 190 Global step 190 Train loss 8.012528 on epoch=63
03/18/2022 15:55:03 - INFO - __main__ - Step 200 Global step 200 Train loss 7.757347 on epoch=66
03/18/2022 15:55:03 - INFO - __main__ - Global step 200 Train loss 8.347752 ACC 0.0 on epoch=66
03/18/2022 15:55:08 - INFO - __main__ - Step 210 Global step 210 Train loss 7.626959 on epoch=69
03/18/2022 15:55:13 - INFO - __main__ - Step 220 Global step 220 Train loss 7.464116 on epoch=73
03/18/2022 15:55:18 - INFO - __main__ - Step 230 Global step 230 Train loss 7.125655 on epoch=76
03/18/2022 15:55:23 - INFO - __main__ - Step 240 Global step 240 Train loss 6.988399 on epoch=79
03/18/2022 15:55:28 - INFO - __main__ - Step 250 Global step 250 Train loss 6.425581 on epoch=83
03/18/2022 15:55:29 - INFO - __main__ - Global step 250 Train loss 7.126143 ACC 0.0 on epoch=83
03/18/2022 15:55:34 - INFO - __main__ - Step 260 Global step 260 Train loss 6.419995 on epoch=86
03/18/2022 15:55:39 - INFO - __main__ - Step 270 Global step 270 Train loss 6.048869 on epoch=89
03/18/2022 15:55:44 - INFO - __main__ - Step 280 Global step 280 Train loss 5.651490 on epoch=93
03/18/2022 15:55:49 - INFO - __main__ - Step 290 Global step 290 Train loss 5.347934 on epoch=96
03/18/2022 15:55:54 - INFO - __main__ - Step 300 Global step 300 Train loss 5.338181 on epoch=99
03/18/2022 15:55:54 - INFO - __main__ - Global step 300 Train loss 5.761294 ACC 0.0 on epoch=99
03/18/2022 15:55:59 - INFO - __main__ - Step 310 Global step 310 Train loss 3.858703 on epoch=103
03/18/2022 15:56:04 - INFO - __main__ - Step 320 Global step 320 Train loss 4.134172 on epoch=106
03/18/2022 15:56:09 - INFO - __main__ - Step 330 Global step 330 Train loss 4.547904 on epoch=109
03/18/2022 15:56:14 - INFO - __main__ - Step 340 Global step 340 Train loss 3.458125 on epoch=113
03/18/2022 15:56:19 - INFO - __main__ - Step 350 Global step 350 Train loss 3.012692 on epoch=116
03/18/2022 15:56:20 - INFO - __main__ - Global step 350 Train loss 3.802319 ACC 0.0 on epoch=116
03/18/2022 15:56:25 - INFO - __main__ - Step 360 Global step 360 Train loss 3.094025 on epoch=119
03/18/2022 15:56:30 - INFO - __main__ - Step 370 Global step 370 Train loss 2.724903 on epoch=123
03/18/2022 15:56:35 - INFO - __main__ - Step 380 Global step 380 Train loss 2.444515 on epoch=126
03/18/2022 15:56:40 - INFO - __main__ - Step 390 Global step 390 Train loss 3.318517 on epoch=129
03/18/2022 15:56:44 - INFO - __main__ - Step 400 Global step 400 Train loss 2.698724 on epoch=133
03/18/2022 15:56:45 - INFO - __main__ - Global step 400 Train loss 2.856137 ACC 0.0 on epoch=133
03/18/2022 15:56:50 - INFO - __main__ - Step 410 Global step 410 Train loss 2.229374 on epoch=136
03/18/2022 15:56:55 - INFO - __main__ - Step 420 Global step 420 Train loss 2.834183 on epoch=139
03/18/2022 15:57:00 - INFO - __main__ - Step 430 Global step 430 Train loss 2.709996 on epoch=143
03/18/2022 15:57:05 - INFO - __main__ - Step 440 Global step 440 Train loss 2.243349 on epoch=146
03/18/2022 15:57:10 - INFO - __main__ - Step 450 Global step 450 Train loss 2.023187 on epoch=149
03/18/2022 15:57:11 - INFO - __main__ - Global step 450 Train loss 2.408018 ACC 0.0 on epoch=149
03/18/2022 15:57:16 - INFO - __main__ - Step 460 Global step 460 Train loss 2.362459 on epoch=153
03/18/2022 15:57:21 - INFO - __main__ - Step 470 Global step 470 Train loss 2.090097 on epoch=156
03/18/2022 15:57:26 - INFO - __main__ - Step 480 Global step 480 Train loss 2.662092 on epoch=159
03/18/2022 15:57:31 - INFO - __main__ - Step 490 Global step 490 Train loss 1.938451 on epoch=163
03/18/2022 15:57:36 - INFO - __main__ - Step 500 Global step 500 Train loss 2.266718 on epoch=166
03/18/2022 15:57:37 - INFO - __main__ - Global step 500 Train loss 2.263963 ACC 0.21875 on epoch=166
03/18/2022 15:57:43 - INFO - __main__ - Step 510 Global step 510 Train loss 2.064876 on epoch=169
03/18/2022 15:57:48 - INFO - __main__ - Step 520 Global step 520 Train loss 1.744466 on epoch=173
03/18/2022 15:57:53 - INFO - __main__ - Step 530 Global step 530 Train loss 1.946803 on epoch=176
03/18/2022 15:57:58 - INFO - __main__ - Step 540 Global step 540 Train loss 2.008070 on epoch=179
03/18/2022 15:58:03 - INFO - __main__ - Step 550 Global step 550 Train loss 2.220117 on epoch=183
03/18/2022 15:58:04 - INFO - __main__ - Global step 550 Train loss 1.996866 ACC 0.0 on epoch=183
03/18/2022 15:58:09 - INFO - __main__ - Step 560 Global step 560 Train loss 1.624602 on epoch=186
03/18/2022 15:58:14 - INFO - __main__ - Step 570 Global step 570 Train loss 1.664139 on epoch=189
03/18/2022 15:58:19 - INFO - __main__ - Step 580 Global step 580 Train loss 1.686620 on epoch=193
03/18/2022 15:58:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.899538 on epoch=196
03/18/2022 15:58:29 - INFO - __main__ - Step 600 Global step 600 Train loss 1.735476 on epoch=199
03/18/2022 15:58:30 - INFO - __main__ - Global step 600 Train loss 1.722075 ACC 0.28125 on epoch=199
03/18/2022 15:58:36 - INFO - __main__ - Step 610 Global step 610 Train loss 1.784576 on epoch=203
03/18/2022 15:58:41 - INFO - __main__ - Step 620 Global step 620 Train loss 1.670507 on epoch=206
03/18/2022 15:58:46 - INFO - __main__ - Step 630 Global step 630 Train loss 1.623587 on epoch=209
03/18/2022 15:58:51 - INFO - __main__ - Step 640 Global step 640 Train loss 1.627947 on epoch=213
03/18/2022 15:58:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.583439 on epoch=216
03/18/2022 15:58:56 - INFO - __main__ - Global step 650 Train loss 1.658011 ACC 0.0 on epoch=216
03/18/2022 15:59:02 - INFO - __main__ - Step 660 Global step 660 Train loss 1.243443 on epoch=219
03/18/2022 15:59:07 - INFO - __main__ - Step 670 Global step 670 Train loss 1.574858 on epoch=223
03/18/2022 15:59:12 - INFO - __main__ - Step 680 Global step 680 Train loss 1.270853 on epoch=226
03/18/2022 15:59:17 - INFO - __main__ - Step 690 Global step 690 Train loss 1.525687 on epoch=229
03/18/2022 15:59:22 - INFO - __main__ - Step 700 Global step 700 Train loss 1.349858 on epoch=233
03/18/2022 15:59:23 - INFO - __main__ - Global step 700 Train loss 1.392940 ACC 0.15625 on epoch=233
03/18/2022 15:59:28 - INFO - __main__ - Step 710 Global step 710 Train loss 1.540285 on epoch=236
03/18/2022 15:59:33 - INFO - __main__ - Step 720 Global step 720 Train loss 1.276857 on epoch=239
03/18/2022 15:59:38 - INFO - __main__ - Step 730 Global step 730 Train loss 1.222983 on epoch=243
03/18/2022 15:59:43 - INFO - __main__ - Step 740 Global step 740 Train loss 1.283816 on epoch=246
03/18/2022 15:59:48 - INFO - __main__ - Step 750 Global step 750 Train loss 1.280468 on epoch=249
03/18/2022 15:59:49 - INFO - __main__ - Global step 750 Train loss 1.320882 ACC 0.25 on epoch=249
03/18/2022 15:59:54 - INFO - __main__ - Step 760 Global step 760 Train loss 1.403975 on epoch=253
03/18/2022 15:59:59 - INFO - __main__ - Step 770 Global step 770 Train loss 1.089515 on epoch=256
03/18/2022 16:00:04 - INFO - __main__ - Step 780 Global step 780 Train loss 1.690524 on epoch=259
03/18/2022 16:00:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.958387 on epoch=263
03/18/2022 16:00:14 - INFO - __main__ - Step 800 Global step 800 Train loss 1.095692 on epoch=266
03/18/2022 16:00:15 - INFO - __main__ - Global step 800 Train loss 1.247619 ACC 0.375 on epoch=266
03/18/2022 16:00:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.907589 on epoch=269
03/18/2022 16:00:26 - INFO - __main__ - Step 820 Global step 820 Train loss 1.125424 on epoch=273
03/18/2022 16:00:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.774785 on epoch=276
03/18/2022 16:00:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.794875 on epoch=279
03/18/2022 16:00:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.938969 on epoch=283
03/18/2022 16:00:42 - INFO - __main__ - Global step 850 Train loss 0.908328 ACC 0.34375 on epoch=283
03/18/2022 16:00:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.616829 on epoch=286
03/18/2022 16:00:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.224238 on epoch=289
03/18/2022 16:00:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.241199 on epoch=293
03/18/2022 16:01:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.687743 on epoch=296
03/18/2022 16:01:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.490439 on epoch=299
03/18/2022 16:01:08 - INFO - __main__ - Global step 900 Train loss 0.452090 ACC 0.46875 on epoch=299
03/18/2022 16:01:09 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:01:09 - INFO - __main__ - Printing 3 examples
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:01:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:01:09 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:01:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:01:09 - INFO - __main__ - Printing 3 examples
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:01:09 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:09 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:01:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:01:09 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:01:09 - INFO - __main__ - save last model!
03/18/2022 16:01:16 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:01:17 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:01:17 - INFO - __main__ - Printing 3 examples
03/18/2022 16:01:17 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:01:17 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:17 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:01:17 - INFO - __main__ - ['neutral']
03/18/2022 16:01:17 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:01:17 - INFO - __main__ - ['entailment']
03/18/2022 16:01:17 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:01:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:01:17 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:01:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:01:19 - INFO - __main__ - Starting training!
03/18/2022 16:01:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0001_8_predictions.txt
03/18/2022 16:01:19 - INFO - __main__ - ACC on test data: 0.4643
03/18/2022 16:01:20 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0001, bsz=8, dev_performance=0.46875, test_performance=0.4642857142857143
03/18/2022 16:01:20 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0005, bsz=8 ...
03/18/2022 16:01:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:01:21 - INFO - __main__ - Printing 3 examples
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:01:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:01:21 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:01:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:01:21 - INFO - __main__ - Printing 3 examples
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:01:21 - INFO - __main__ - ['contradiction']
03/18/2022 16:01:21 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:01:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:01:21 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:01:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:01:34 - INFO - __main__ - Starting training!
03/18/2022 16:01:38 - INFO - __main__ - Step 10 Global step 10 Train loss 24.070034 on epoch=3
03/18/2022 16:01:43 - INFO - __main__ - Step 20 Global step 20 Train loss 19.302361 on epoch=6
03/18/2022 16:01:48 - INFO - __main__ - Step 30 Global step 30 Train loss 12.028052 on epoch=9
03/18/2022 16:01:53 - INFO - __main__ - Step 40 Global step 40 Train loss 10.446978 on epoch=13
03/18/2022 16:01:58 - INFO - __main__ - Step 50 Global step 50 Train loss 9.427254 on epoch=16
03/18/2022 16:01:59 - INFO - __main__ - Global step 50 Train loss 15.054935 ACC 0.0 on epoch=16
03/18/2022 16:02:04 - INFO - __main__ - Step 60 Global step 60 Train loss 8.037718 on epoch=19
03/18/2022 16:02:10 - INFO - __main__ - Step 70 Global step 70 Train loss 7.259619 on epoch=23
03/18/2022 16:02:15 - INFO - __main__ - Step 80 Global step 80 Train loss 5.292140 on epoch=26
03/18/2022 16:02:20 - INFO - __main__ - Step 90 Global step 90 Train loss 4.459306 on epoch=29
03/18/2022 16:02:25 - INFO - __main__ - Step 100 Global step 100 Train loss 2.935190 on epoch=33
03/18/2022 16:02:25 - INFO - __main__ - Global step 100 Train loss 5.596795 ACC 0.0 on epoch=33
03/18/2022 16:02:30 - INFO - __main__ - Step 110 Global step 110 Train loss 2.638625 on epoch=36
03/18/2022 16:02:35 - INFO - __main__ - Step 120 Global step 120 Train loss 2.385677 on epoch=39
03/18/2022 16:02:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.825917 on epoch=43
03/18/2022 16:02:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.534928 on epoch=46
03/18/2022 16:02:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.881403 on epoch=49
03/18/2022 16:02:51 - INFO - __main__ - Global step 150 Train loss 2.253310 ACC 0.5 on epoch=49
03/18/2022 16:02:56 - INFO - __main__ - Step 160 Global step 160 Train loss 1.479394 on epoch=53
03/18/2022 16:03:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.368503 on epoch=56
03/18/2022 16:03:07 - INFO - __main__ - Step 180 Global step 180 Train loss 1.703398 on epoch=59
03/18/2022 16:03:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.711889 on epoch=63
03/18/2022 16:03:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.179925 on epoch=66
03/18/2022 16:03:17 - INFO - __main__ - Global step 200 Train loss 1.488621 ACC 0.0 on epoch=66
03/18/2022 16:03:22 - INFO - __main__ - Step 210 Global step 210 Train loss 1.222058 on epoch=69
03/18/2022 16:03:27 - INFO - __main__ - Step 220 Global step 220 Train loss 1.425418 on epoch=73
03/18/2022 16:03:32 - INFO - __main__ - Step 230 Global step 230 Train loss 1.042269 on epoch=76
03/18/2022 16:03:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.028944 on epoch=79
03/18/2022 16:03:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.951944 on epoch=83
03/18/2022 16:03:43 - INFO - __main__ - Global step 250 Train loss 1.134126 ACC 0.0 on epoch=83
03/18/2022 16:03:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.843153 on epoch=86
03/18/2022 16:03:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.901066 on epoch=89
03/18/2022 16:03:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.884369 on epoch=93
03/18/2022 16:04:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.671230 on epoch=96
03/18/2022 16:04:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.740456 on epoch=99
03/18/2022 16:04:09 - INFO - __main__ - Global step 300 Train loss 0.808055 ACC 0.34375 on epoch=99
03/18/2022 16:04:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.686984 on epoch=103
03/18/2022 16:04:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.746923 on epoch=106
03/18/2022 16:04:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.499702 on epoch=109
03/18/2022 16:04:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.590028 on epoch=113
03/18/2022 16:04:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.541028 on epoch=116
03/18/2022 16:04:34 - INFO - __main__ - Global step 350 Train loss 0.612933 ACC 0.40625 on epoch=116
03/18/2022 16:04:39 - INFO - __main__ - Step 360 Global step 360 Train loss 1.719655 on epoch=119
03/18/2022 16:04:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.717700 on epoch=123
03/18/2022 16:04:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.967845 on epoch=126
03/18/2022 16:04:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.588438 on epoch=129
03/18/2022 16:04:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.646445 on epoch=133
03/18/2022 16:05:00 - INFO - __main__ - Global step 400 Train loss 0.928016 ACC 0.5 on epoch=133
03/18/2022 16:05:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.554262 on epoch=136
03/18/2022 16:05:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.620960 on epoch=139
03/18/2022 16:05:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.645287 on epoch=143
03/18/2022 16:05:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.516059 on epoch=146
03/18/2022 16:05:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.472386 on epoch=149
03/18/2022 16:05:26 - INFO - __main__ - Global step 450 Train loss 0.561790 ACC 0.5 on epoch=149
03/18/2022 16:05:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.492606 on epoch=153
03/18/2022 16:05:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.566390 on epoch=156
03/18/2022 16:05:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.427270 on epoch=159
03/18/2022 16:05:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.500162 on epoch=163
03/18/2022 16:05:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.453496 on epoch=166
03/18/2022 16:05:51 - INFO - __main__ - Global step 500 Train loss 0.487985 ACC 0.53125 on epoch=166
03/18/2022 16:05:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.489939 on epoch=169
03/18/2022 16:06:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.589045 on epoch=173
03/18/2022 16:06:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.409616 on epoch=176
03/18/2022 16:06:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.466964 on epoch=179
03/18/2022 16:06:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.466582 on epoch=183
03/18/2022 16:06:18 - INFO - __main__ - Global step 550 Train loss 0.484429 ACC 0.4375 on epoch=183
03/18/2022 16:06:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.647995 on epoch=186
03/18/2022 16:06:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.703676 on epoch=189
03/18/2022 16:06:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.603488 on epoch=193
03/18/2022 16:06:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.403064 on epoch=196
03/18/2022 16:06:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.390692 on epoch=199
03/18/2022 16:06:43 - INFO - __main__ - Global step 600 Train loss 0.549783 ACC 0.3125 on epoch=199
03/18/2022 16:06:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.467126 on epoch=203
03/18/2022 16:06:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.404253 on epoch=206
03/18/2022 16:06:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.550250 on epoch=209
03/18/2022 16:07:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.509138 on epoch=213
03/18/2022 16:07:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.421812 on epoch=216
03/18/2022 16:07:09 - INFO - __main__ - Global step 650 Train loss 0.470516 ACC 0.5 on epoch=216
03/18/2022 16:07:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.473032 on epoch=219
03/18/2022 16:07:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.465460 on epoch=223
03/18/2022 16:07:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.419746 on epoch=226
03/18/2022 16:07:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.471463 on epoch=229
03/18/2022 16:07:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.403125 on epoch=233
03/18/2022 16:07:35 - INFO - __main__ - Global step 700 Train loss 0.446565 ACC 0.625 on epoch=233
03/18/2022 16:07:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.390389 on epoch=236
03/18/2022 16:07:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.441208 on epoch=239
03/18/2022 16:07:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.445909 on epoch=243
03/18/2022 16:07:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.422257 on epoch=246
03/18/2022 16:08:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.541525 on epoch=249
03/18/2022 16:08:01 - INFO - __main__ - Global step 750 Train loss 0.448258 ACC 0.5 on epoch=249
03/18/2022 16:08:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.521139 on epoch=253
03/18/2022 16:08:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.416410 on epoch=256
03/18/2022 16:08:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.416229 on epoch=259
03/18/2022 16:08:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.426398 on epoch=263
03/18/2022 16:08:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.451496 on epoch=266
03/18/2022 16:08:27 - INFO - __main__ - Global step 800 Train loss 0.446334 ACC 0.5 on epoch=266
03/18/2022 16:08:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.380467 on epoch=269
03/18/2022 16:08:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.405042 on epoch=273
03/18/2022 16:08:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.441136 on epoch=276
03/18/2022 16:08:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.412567 on epoch=279
03/18/2022 16:08:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.394143 on epoch=283
03/18/2022 16:08:53 - INFO - __main__ - Global step 850 Train loss 0.406671 ACC 0.5 on epoch=283
03/18/2022 16:08:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.416887 on epoch=286
03/18/2022 16:09:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.387967 on epoch=289
03/18/2022 16:09:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.398626 on epoch=293
03/18/2022 16:09:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.439882 on epoch=296
03/18/2022 16:09:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.411367 on epoch=299
03/18/2022 16:09:19 - INFO - __main__ - Global step 900 Train loss 0.410946 ACC 0.5 on epoch=299
03/18/2022 16:09:19 - INFO - __main__ - save last model!
03/18/2022 16:09:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:09:20 - INFO - __main__ - Printing 3 examples
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:09:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:09:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:09:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:09:20 - INFO - __main__ - Printing 3 examples
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:09:20 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:09:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:09:20 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:09:26 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:09:27 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:09:27 - INFO - __main__ - Printing 3 examples
03/18/2022 16:09:27 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:09:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:27 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:09:27 - INFO - __main__ - ['neutral']
03/18/2022 16:09:27 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:09:27 - INFO - __main__ - ['entailment']
03/18/2022 16:09:27 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:09:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:09:27 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:09:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0005_8_predictions.txt
03/18/2022 16:09:28 - INFO - __main__ - ACC on test data: 0.5179
03/18/2022 16:09:29 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0005, bsz=8, dev_performance=0.625, test_performance=0.5178571428571429
03/18/2022 16:09:29 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0003, bsz=8 ...
03/18/2022 16:09:29 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:09:29 - INFO - __main__ - Printing 3 examples
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:09:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:09:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:09:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:09:30 - INFO - __main__ - Printing 3 examples
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:09:30 - INFO - __main__ - ['contradiction']
03/18/2022 16:09:30 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:09:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:09:30 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:09:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:09:31 - INFO - __main__ - Starting training!
03/18/2022 16:09:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:09:40 - INFO - __main__ - Starting training!
03/18/2022 16:09:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.986298 on epoch=3
03/18/2022 16:09:49 - INFO - __main__ - Step 20 Global step 20 Train loss 19.063133 on epoch=6
03/18/2022 16:09:54 - INFO - __main__ - Step 30 Global step 30 Train loss 12.691050 on epoch=9
03/18/2022 16:09:59 - INFO - __main__ - Step 40 Global step 40 Train loss 11.566119 on epoch=13
03/18/2022 16:10:04 - INFO - __main__ - Step 50 Global step 50 Train loss 10.739103 on epoch=16
03/18/2022 16:10:05 - INFO - __main__ - Global step 50 Train loss 15.609140 ACC 0.0 on epoch=16
03/18/2022 16:10:10 - INFO - __main__ - Step 60 Global step 60 Train loss 9.617154 on epoch=19
03/18/2022 16:10:16 - INFO - __main__ - Step 70 Global step 70 Train loss 9.116438 on epoch=23
03/18/2022 16:10:21 - INFO - __main__ - Step 80 Global step 80 Train loss 8.198581 on epoch=26
03/18/2022 16:10:26 - INFO - __main__ - Step 90 Global step 90 Train loss 7.489057 on epoch=29
03/18/2022 16:10:31 - INFO - __main__ - Step 100 Global step 100 Train loss 6.823960 on epoch=33
03/18/2022 16:10:31 - INFO - __main__ - Global step 100 Train loss 8.249038 ACC 0.0 on epoch=33
03/18/2022 16:10:36 - INFO - __main__ - Step 110 Global step 110 Train loss 5.528059 on epoch=36
03/18/2022 16:10:41 - INFO - __main__ - Step 120 Global step 120 Train loss 4.336258 on epoch=39
03/18/2022 16:10:46 - INFO - __main__ - Step 130 Global step 130 Train loss 2.923142 on epoch=43
03/18/2022 16:10:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.824640 on epoch=46
03/18/2022 16:10:56 - INFO - __main__ - Step 150 Global step 150 Train loss 2.269934 on epoch=49
03/18/2022 16:10:57 - INFO - __main__ - Global step 150 Train loss 3.576406 ACC 0.0 on epoch=49
03/18/2022 16:11:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.021698 on epoch=53
03/18/2022 16:11:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.922803 on epoch=56
03/18/2022 16:11:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.203566 on epoch=59
03/18/2022 16:11:17 - INFO - __main__ - Step 190 Global step 190 Train loss 2.548762 on epoch=63
03/18/2022 16:11:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.217753 on epoch=66
03/18/2022 16:11:23 - INFO - __main__ - Global step 200 Train loss 2.382916 ACC 0.5 on epoch=66
03/18/2022 16:11:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.797332 on epoch=69
03/18/2022 16:11:34 - INFO - __main__ - Step 220 Global step 220 Train loss 1.553634 on epoch=73
03/18/2022 16:11:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.671294 on epoch=76
03/18/2022 16:11:44 - INFO - __main__ - Step 240 Global step 240 Train loss 1.810485 on epoch=79
03/18/2022 16:11:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.461524 on epoch=83
03/18/2022 16:11:49 - INFO - __main__ - Global step 250 Train loss 1.658854 ACC 0.03125 on epoch=83
03/18/2022 16:11:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.448224 on epoch=86
03/18/2022 16:11:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.204508 on epoch=89
03/18/2022 16:12:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.315873 on epoch=93
03/18/2022 16:12:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.396940 on epoch=96
03/18/2022 16:12:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.279807 on epoch=99
03/18/2022 16:12:15 - INFO - __main__ - Global step 300 Train loss 1.329071 ACC 0.28125 on epoch=99
03/18/2022 16:12:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.056310 on epoch=103
03/18/2022 16:12:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.283465 on epoch=106
03/18/2022 16:12:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.973746 on epoch=109
03/18/2022 16:12:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.730498 on epoch=113
03/18/2022 16:12:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.870433 on epoch=116
03/18/2022 16:12:41 - INFO - __main__ - Global step 350 Train loss 0.982890 ACC 0.4375 on epoch=116
03/18/2022 16:12:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.694373 on epoch=119
03/18/2022 16:12:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.834624 on epoch=123
03/18/2022 16:12:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.658295 on epoch=126
03/18/2022 16:13:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.609784 on epoch=129
03/18/2022 16:13:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.312720 on epoch=133
03/18/2022 16:13:07 - INFO - __main__ - Global step 400 Train loss 0.621959 ACC 0.375 on epoch=133
03/18/2022 16:13:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.198416 on epoch=136
03/18/2022 16:13:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.105735 on epoch=139
03/18/2022 16:13:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.082826 on epoch=143
03/18/2022 16:13:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.013152 on epoch=146
03/18/2022 16:13:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.037201 on epoch=149
03/18/2022 16:13:32 - INFO - __main__ - Global step 450 Train loss 0.087466 ACC 0.75 on epoch=149
03/18/2022 16:13:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.009948 on epoch=153
03/18/2022 16:13:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019332 on epoch=156
03/18/2022 16:13:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004303 on epoch=159
03/18/2022 16:13:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002207 on epoch=163
03/18/2022 16:13:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002747 on epoch=166
03/18/2022 16:13:59 - INFO - __main__ - Global step 500 Train loss 0.007707 ACC 0.78125 on epoch=166
03/18/2022 16:14:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001714 on epoch=169
03/18/2022 16:14:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002150 on epoch=173
03/18/2022 16:14:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.011240 on epoch=176
03/18/2022 16:14:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002553 on epoch=179
03/18/2022 16:14:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000837 on epoch=183
03/18/2022 16:14:25 - INFO - __main__ - Global step 550 Train loss 0.003699 ACC 0.71875 on epoch=183
03/18/2022 16:14:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004030 on epoch=186
03/18/2022 16:14:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001167 on epoch=189
03/18/2022 16:14:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000299 on epoch=193
03/18/2022 16:14:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000652 on epoch=196
03/18/2022 16:14:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000259 on epoch=199
03/18/2022 16:14:51 - INFO - __main__ - Global step 600 Train loss 0.001281 ACC 0.6875 on epoch=199
03/18/2022 16:14:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000934 on epoch=203
03/18/2022 16:15:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000113 on epoch=206
03/18/2022 16:15:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.008233 on epoch=209
03/18/2022 16:15:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000741 on epoch=213
03/18/2022 16:15:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000435 on epoch=216
03/18/2022 16:15:17 - INFO - __main__ - Global step 650 Train loss 0.002091 ACC 0.8125 on epoch=216
03/18/2022 16:15:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000327 on epoch=219
03/18/2022 16:15:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001378 on epoch=223
03/18/2022 16:15:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000050 on epoch=226
03/18/2022 16:15:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000184 on epoch=229
03/18/2022 16:15:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000097 on epoch=233
03/18/2022 16:15:43 - INFO - __main__ - Global step 700 Train loss 0.000407 ACC 0.78125 on epoch=233
03/18/2022 16:15:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000095 on epoch=236
03/18/2022 16:15:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000196 on epoch=239
03/18/2022 16:15:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000152 on epoch=243
03/18/2022 16:16:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000161 on epoch=246
03/18/2022 16:16:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.012584 on epoch=249
03/18/2022 16:16:09 - INFO - __main__ - Global step 750 Train loss 0.002637 ACC 0.75 on epoch=249
03/18/2022 16:16:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005112 on epoch=253
03/18/2022 16:16:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001882 on epoch=256
03/18/2022 16:16:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.048030 on epoch=259
03/18/2022 16:16:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000328 on epoch=263
03/18/2022 16:16:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000326 on epoch=266
03/18/2022 16:16:35 - INFO - __main__ - Global step 800 Train loss 0.011136 ACC 0.75 on epoch=266
03/18/2022 16:16:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000381 on epoch=269
03/18/2022 16:16:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001593 on epoch=273
03/18/2022 16:16:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003340 on epoch=276
03/18/2022 16:16:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.020219 on epoch=279
03/18/2022 16:17:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000149 on epoch=283
03/18/2022 16:17:01 - INFO - __main__ - Global step 850 Train loss 0.005136 ACC 0.75 on epoch=283
03/18/2022 16:17:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000245 on epoch=286
03/18/2022 16:17:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000295 on epoch=289
03/18/2022 16:17:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000739 on epoch=293
03/18/2022 16:17:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000222 on epoch=296
03/18/2022 16:17:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000390 on epoch=299
03/18/2022 16:17:27 - INFO - __main__ - Global step 900 Train loss 0.000378 ACC 0.65625 on epoch=299
03/18/2022 16:17:27 - INFO - __main__ - save last model!
03/18/2022 16:17:27 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:17:27 - INFO - __main__ - Printing 3 examples
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:17:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:17:27 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:17:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:17:27 - INFO - __main__ - Printing 3 examples
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:17:27 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:27 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:17:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:17:27 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:17:33 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:17:34 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:17:34 - INFO - __main__ - Printing 3 examples
03/18/2022 16:17:34 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:17:34 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:34 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:17:34 - INFO - __main__ - ['neutral']
03/18/2022 16:17:34 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:17:34 - INFO - __main__ - ['entailment']
03/18/2022 16:17:34 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:17:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:17:34 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:17:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0003_8_predictions.txt
03/18/2022 16:17:35 - INFO - __main__ - ACC on test data: 0.9107
03/18/2022 16:17:36 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0003, bsz=8, dev_performance=0.8125, test_performance=0.9107142857142857
03/18/2022 16:17:36 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0002, bsz=8 ...
03/18/2022 16:17:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:17:36 - INFO - __main__ - Printing 3 examples
03/18/2022 16:17:36 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:17:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:36 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:17:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:36 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:17:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:17:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:17:37 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:17:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:17:37 - INFO - __main__ - Printing 3 examples
03/18/2022 16:17:37 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:17:37 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:37 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:17:37 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:37 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:17:37 - INFO - __main__ - ['contradiction']
03/18/2022 16:17:37 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:17:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:17:37 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:17:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:17:39 - INFO - __main__ - Starting training!
03/18/2022 16:17:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:17:49 - INFO - __main__ - Starting training!
03/18/2022 16:17:54 - INFO - __main__ - Step 10 Global step 10 Train loss 23.050785 on epoch=3
03/18/2022 16:17:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.735331 on epoch=6
03/18/2022 16:18:04 - INFO - __main__ - Step 30 Global step 30 Train loss 13.321386 on epoch=9
03/18/2022 16:18:09 - INFO - __main__ - Step 40 Global step 40 Train loss 11.932589 on epoch=13
03/18/2022 16:18:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.414287 on epoch=16
03/18/2022 16:18:16 - INFO - __main__ - Global step 50 Train loss 15.890875 ACC 0.0 on epoch=16
03/18/2022 16:18:22 - INFO - __main__ - Step 60 Global step 60 Train loss 10.805443 on epoch=19
03/18/2022 16:18:27 - INFO - __main__ - Step 70 Global step 70 Train loss 10.581004 on epoch=23
03/18/2022 16:18:32 - INFO - __main__ - Step 80 Global step 80 Train loss 9.500682 on epoch=26
03/18/2022 16:18:37 - INFO - __main__ - Step 90 Global step 90 Train loss 9.273706 on epoch=29
03/18/2022 16:18:42 - INFO - __main__ - Step 100 Global step 100 Train loss 8.388597 on epoch=33
03/18/2022 16:18:42 - INFO - __main__ - Global step 100 Train loss 9.709888 ACC 0.0 on epoch=33
03/18/2022 16:18:47 - INFO - __main__ - Step 110 Global step 110 Train loss 8.212070 on epoch=36
03/18/2022 16:18:52 - INFO - __main__ - Step 120 Global step 120 Train loss 8.168864 on epoch=39
03/18/2022 16:18:57 - INFO - __main__ - Step 130 Global step 130 Train loss 7.919320 on epoch=43
03/18/2022 16:19:02 - INFO - __main__ - Step 140 Global step 140 Train loss 7.245454 on epoch=46
03/18/2022 16:19:08 - INFO - __main__ - Step 150 Global step 150 Train loss 6.403802 on epoch=49
03/18/2022 16:19:08 - INFO - __main__ - Global step 150 Train loss 7.589902 ACC 0.0 on epoch=49
03/18/2022 16:19:13 - INFO - __main__ - Step 160 Global step 160 Train loss 6.015214 on epoch=53
03/18/2022 16:19:18 - INFO - __main__ - Step 170 Global step 170 Train loss 4.354218 on epoch=56
03/18/2022 16:19:23 - INFO - __main__ - Step 180 Global step 180 Train loss 4.031541 on epoch=59
03/18/2022 16:19:28 - INFO - __main__ - Step 190 Global step 190 Train loss 3.665972 on epoch=63
03/18/2022 16:19:33 - INFO - __main__ - Step 200 Global step 200 Train loss 2.816625 on epoch=66
03/18/2022 16:19:34 - INFO - __main__ - Global step 200 Train loss 4.176714 ACC 0.0 on epoch=66
03/18/2022 16:19:39 - INFO - __main__ - Step 210 Global step 210 Train loss 2.533720 on epoch=69
03/18/2022 16:19:44 - INFO - __main__ - Step 220 Global step 220 Train loss 2.604072 on epoch=73
03/18/2022 16:19:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.968699 on epoch=76
03/18/2022 16:19:54 - INFO - __main__ - Step 240 Global step 240 Train loss 2.164301 on epoch=79
03/18/2022 16:19:59 - INFO - __main__ - Step 250 Global step 250 Train loss 2.530057 on epoch=83
03/18/2022 16:20:00 - INFO - __main__ - Global step 250 Train loss 2.360170 ACC 0.0625 on epoch=83
03/18/2022 16:20:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.002414 on epoch=86
03/18/2022 16:20:10 - INFO - __main__ - Step 270 Global step 270 Train loss 2.195405 on epoch=89
03/18/2022 16:20:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.707193 on epoch=93
03/18/2022 16:20:20 - INFO - __main__ - Step 290 Global step 290 Train loss 2.217428 on epoch=96
03/18/2022 16:20:25 - INFO - __main__ - Step 300 Global step 300 Train loss 2.371236 on epoch=99
03/18/2022 16:20:26 - INFO - __main__ - Global step 300 Train loss 2.098735 ACC 0.0 on epoch=99
03/18/2022 16:20:31 - INFO - __main__ - Step 310 Global step 310 Train loss 2.198113 on epoch=103
03/18/2022 16:20:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.578008 on epoch=106
03/18/2022 16:20:41 - INFO - __main__ - Step 330 Global step 330 Train loss 2.069958 on epoch=109
03/18/2022 16:20:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.498752 on epoch=113
03/18/2022 16:20:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.708716 on epoch=116
03/18/2022 16:20:51 - INFO - __main__ - Global step 350 Train loss 1.810709 ACC 0.5 on epoch=116
03/18/2022 16:20:57 - INFO - __main__ - Step 360 Global step 360 Train loss 1.598772 on epoch=119
03/18/2022 16:21:02 - INFO - __main__ - Step 370 Global step 370 Train loss 1.249176 on epoch=123
03/18/2022 16:21:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.859476 on epoch=126
03/18/2022 16:21:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.204042 on epoch=129
03/18/2022 16:21:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.815206 on epoch=133
03/18/2022 16:21:18 - INFO - __main__ - Global step 400 Train loss 1.145334 ACC 0.0 on epoch=133
03/18/2022 16:21:23 - INFO - __main__ - Step 410 Global step 410 Train loss 1.080948 on epoch=136
03/18/2022 16:21:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.633363 on epoch=139
03/18/2022 16:21:33 - INFO - __main__ - Step 430 Global step 430 Train loss 1.547064 on epoch=143
03/18/2022 16:21:38 - INFO - __main__ - Step 440 Global step 440 Train loss 1.887006 on epoch=146
03/18/2022 16:21:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.276377 on epoch=149
03/18/2022 16:21:44 - INFO - __main__ - Global step 450 Train loss 1.284952 ACC 0.0 on epoch=149
03/18/2022 16:21:49 - INFO - __main__ - Step 460 Global step 460 Train loss 1.539521 on epoch=153
03/18/2022 16:21:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.629661 on epoch=156
03/18/2022 16:21:59 - INFO - __main__ - Step 480 Global step 480 Train loss 1.700932 on epoch=159
03/18/2022 16:22:04 - INFO - __main__ - Step 490 Global step 490 Train loss 1.221948 on epoch=163
03/18/2022 16:22:09 - INFO - __main__ - Step 500 Global step 500 Train loss 1.336417 on epoch=166
03/18/2022 16:22:09 - INFO - __main__ - Global step 500 Train loss 1.485696 ACC 0.40625 on epoch=166
03/18/2022 16:22:14 - INFO - __main__ - Step 510 Global step 510 Train loss 1.464740 on epoch=169
03/18/2022 16:22:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.281210 on epoch=173
03/18/2022 16:22:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.920900 on epoch=176
03/18/2022 16:22:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.395953 on epoch=179
03/18/2022 16:22:35 - INFO - __main__ - Step 550 Global step 550 Train loss 1.197601 on epoch=183
03/18/2022 16:22:36 - INFO - __main__ - Global step 550 Train loss 1.252081 ACC 0.5 on epoch=183
03/18/2022 16:22:41 - INFO - __main__ - Step 560 Global step 560 Train loss 1.153211 on epoch=186
03/18/2022 16:22:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.888986 on epoch=189
03/18/2022 16:22:51 - INFO - __main__ - Step 580 Global step 580 Train loss 1.279455 on epoch=193
03/18/2022 16:22:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.991048 on epoch=196
03/18/2022 16:23:01 - INFO - __main__ - Step 600 Global step 600 Train loss 1.086295 on epoch=199
03/18/2022 16:23:01 - INFO - __main__ - Global step 600 Train loss 1.079799 ACC 0.0 on epoch=199
03/18/2022 16:23:06 - INFO - __main__ - Step 610 Global step 610 Train loss 1.058918 on epoch=203
03/18/2022 16:23:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.976617 on epoch=206
03/18/2022 16:23:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.829127 on epoch=209
03/18/2022 16:23:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.963428 on epoch=213
03/18/2022 16:23:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.712489 on epoch=216
03/18/2022 16:23:27 - INFO - __main__ - Global step 650 Train loss 0.908116 ACC 0.03125 on epoch=216
03/18/2022 16:23:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.805122 on epoch=219
03/18/2022 16:23:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.943548 on epoch=223
03/18/2022 16:23:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.625871 on epoch=226
03/18/2022 16:23:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.726627 on epoch=229
03/18/2022 16:23:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.728030 on epoch=233
03/18/2022 16:23:52 - INFO - __main__ - Global step 700 Train loss 0.765840 ACC 0.21875 on epoch=233
03/18/2022 16:23:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.671940 on epoch=236
03/18/2022 16:24:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.658811 on epoch=239
03/18/2022 16:24:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.636009 on epoch=243
03/18/2022 16:24:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.635071 on epoch=246
03/18/2022 16:24:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.683963 on epoch=249
03/18/2022 16:24:18 - INFO - __main__ - Global step 750 Train loss 0.657159 ACC 0.53125 on epoch=249
03/18/2022 16:24:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.714964 on epoch=253
03/18/2022 16:24:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.618869 on epoch=256
03/18/2022 16:24:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.506275 on epoch=259
03/18/2022 16:24:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.638470 on epoch=263
03/18/2022 16:24:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.711364 on epoch=266
03/18/2022 16:24:44 - INFO - __main__ - Global step 800 Train loss 0.637988 ACC 0.375 on epoch=266
03/18/2022 16:24:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.644112 on epoch=269
03/18/2022 16:24:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.560658 on epoch=273
03/18/2022 16:24:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.597744 on epoch=276
03/18/2022 16:25:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.634670 on epoch=279
03/18/2022 16:25:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.566895 on epoch=283
03/18/2022 16:25:10 - INFO - __main__ - Global step 850 Train loss 0.600816 ACC 0.40625 on epoch=283
03/18/2022 16:25:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.586448 on epoch=286
03/18/2022 16:25:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.624977 on epoch=289
03/18/2022 16:25:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.581920 on epoch=293
03/18/2022 16:25:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.622452 on epoch=296
03/18/2022 16:25:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.587503 on epoch=299
03/18/2022 16:25:35 - INFO - __main__ - Global step 900 Train loss 0.600660 ACC 0.3125 on epoch=299
03/18/2022 16:25:35 - INFO - __main__ - save last model!
03/18/2022 16:25:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:25:36 - INFO - __main__ - Printing 3 examples
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:25:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:25:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:25:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:25:36 - INFO - __main__ - Printing 3 examples
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:25:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:36 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:25:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:25:36 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:25:42 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:25:43 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:25:43 - INFO - __main__ - Printing 3 examples
03/18/2022 16:25:43 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:25:43 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:43 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:25:43 - INFO - __main__ - ['neutral']
03/18/2022 16:25:43 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:25:43 - INFO - __main__ - ['entailment']
03/18/2022 16:25:43 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:25:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:25:43 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:25:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0002_8_predictions.txt
03/18/2022 16:25:44 - INFO - __main__ - ACC on test data: 0.4107
03/18/2022 16:25:44 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0002, bsz=8, dev_performance=0.53125, test_performance=0.4107142857142857
03/18/2022 16:25:44 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0001, bsz=8 ...
03/18/2022 16:25:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:25:45 - INFO - __main__ - Printing 3 examples
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:25:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:25:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:25:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:25:45 - INFO - __main__ - Printing 3 examples
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/18/2022 16:25:45 - INFO - __main__ - ['contradiction']
03/18/2022 16:25:45 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:25:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:25:45 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:25:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:25:49 - INFO - __main__ - Starting training!
03/18/2022 16:25:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:25:56 - INFO - __main__ - Starting training!
03/18/2022 16:26:00 - INFO - __main__ - Step 10 Global step 10 Train loss 23.700743 on epoch=3
03/18/2022 16:26:05 - INFO - __main__ - Step 20 Global step 20 Train loss 23.431417 on epoch=6
03/18/2022 16:26:10 - INFO - __main__ - Step 30 Global step 30 Train loss 18.748829 on epoch=9
03/18/2022 16:26:15 - INFO - __main__ - Step 40 Global step 40 Train loss 15.788730 on epoch=13
03/18/2022 16:26:20 - INFO - __main__ - Step 50 Global step 50 Train loss 13.474470 on epoch=16
03/18/2022 16:26:29 - INFO - __main__ - Global step 50 Train loss 19.028837 ACC 0.0 on epoch=16
03/18/2022 16:26:35 - INFO - __main__ - Step 60 Global step 60 Train loss 13.475794 on epoch=19
03/18/2022 16:26:40 - INFO - __main__ - Step 70 Global step 70 Train loss 13.226814 on epoch=23
03/18/2022 16:26:45 - INFO - __main__ - Step 80 Global step 80 Train loss 11.833727 on epoch=26
03/18/2022 16:26:50 - INFO - __main__ - Step 90 Global step 90 Train loss 11.893633 on epoch=29
03/18/2022 16:26:55 - INFO - __main__ - Step 100 Global step 100 Train loss 11.416878 on epoch=33
03/18/2022 16:27:03 - INFO - __main__ - Global step 100 Train loss 12.369370 ACC 0.0 on epoch=33
03/18/2022 16:27:08 - INFO - __main__ - Step 110 Global step 110 Train loss 11.079326 on epoch=36
03/18/2022 16:27:13 - INFO - __main__ - Step 120 Global step 120 Train loss 10.281587 on epoch=39
03/18/2022 16:27:18 - INFO - __main__ - Step 130 Global step 130 Train loss 11.015051 on epoch=43
03/18/2022 16:27:23 - INFO - __main__ - Step 140 Global step 140 Train loss 10.085793 on epoch=46
03/18/2022 16:27:28 - INFO - __main__ - Step 150 Global step 150 Train loss 9.925822 on epoch=49
03/18/2022 16:27:30 - INFO - __main__ - Global step 150 Train loss 10.477516 ACC 0.0 on epoch=49
03/18/2022 16:27:35 - INFO - __main__ - Step 160 Global step 160 Train loss 10.134893 on epoch=53
03/18/2022 16:27:40 - INFO - __main__ - Step 170 Global step 170 Train loss 9.489812 on epoch=56
03/18/2022 16:27:45 - INFO - __main__ - Step 180 Global step 180 Train loss 9.106879 on epoch=59
03/18/2022 16:27:50 - INFO - __main__ - Step 190 Global step 190 Train loss 8.492159 on epoch=63
03/18/2022 16:27:55 - INFO - __main__ - Step 200 Global step 200 Train loss 7.946510 on epoch=66
03/18/2022 16:27:56 - INFO - __main__ - Global step 200 Train loss 9.034050 ACC 0.0 on epoch=66
03/18/2022 16:28:00 - INFO - __main__ - Step 210 Global step 210 Train loss 8.576781 on epoch=69
03/18/2022 16:28:05 - INFO - __main__ - Step 220 Global step 220 Train loss 7.854144 on epoch=73
03/18/2022 16:28:10 - INFO - __main__ - Step 230 Global step 230 Train loss 8.118324 on epoch=76
03/18/2022 16:28:15 - INFO - __main__ - Step 240 Global step 240 Train loss 7.522514 on epoch=79
03/18/2022 16:28:20 - INFO - __main__ - Step 250 Global step 250 Train loss 7.692618 on epoch=83
03/18/2022 16:28:21 - INFO - __main__ - Global step 250 Train loss 7.952876 ACC 0.0 on epoch=83
03/18/2022 16:28:26 - INFO - __main__ - Step 260 Global step 260 Train loss 7.676353 on epoch=86
03/18/2022 16:28:31 - INFO - __main__ - Step 270 Global step 270 Train loss 6.577305 on epoch=89
03/18/2022 16:28:36 - INFO - __main__ - Step 280 Global step 280 Train loss 6.679532 on epoch=93
03/18/2022 16:28:41 - INFO - __main__ - Step 290 Global step 290 Train loss 5.847260 on epoch=96
03/18/2022 16:28:46 - INFO - __main__ - Step 300 Global step 300 Train loss 5.711883 on epoch=99
03/18/2022 16:28:47 - INFO - __main__ - Global step 300 Train loss 6.498466 ACC 0.0 on epoch=99
03/18/2022 16:28:51 - INFO - __main__ - Step 310 Global step 310 Train loss 5.625992 on epoch=103
03/18/2022 16:28:56 - INFO - __main__ - Step 320 Global step 320 Train loss 4.849511 on epoch=106
03/18/2022 16:29:01 - INFO - __main__ - Step 330 Global step 330 Train loss 4.664773 on epoch=109
03/18/2022 16:29:06 - INFO - __main__ - Step 340 Global step 340 Train loss 5.200112 on epoch=113
03/18/2022 16:29:11 - INFO - __main__ - Step 350 Global step 350 Train loss 3.849810 on epoch=116
03/18/2022 16:29:12 - INFO - __main__ - Global step 350 Train loss 4.838039 ACC 0.0 on epoch=116
03/18/2022 16:29:17 - INFO - __main__ - Step 360 Global step 360 Train loss 3.582235 on epoch=119
03/18/2022 16:29:22 - INFO - __main__ - Step 370 Global step 370 Train loss 3.426036 on epoch=123
03/18/2022 16:29:27 - INFO - __main__ - Step 380 Global step 380 Train loss 2.609341 on epoch=126
03/18/2022 16:29:32 - INFO - __main__ - Step 390 Global step 390 Train loss 2.567563 on epoch=129
03/18/2022 16:29:37 - INFO - __main__ - Step 400 Global step 400 Train loss 2.429668 on epoch=133
03/18/2022 16:29:37 - INFO - __main__ - Global step 400 Train loss 2.922968 ACC 0.0 on epoch=133
03/18/2022 16:29:42 - INFO - __main__ - Step 410 Global step 410 Train loss 2.343847 on epoch=136
03/18/2022 16:29:47 - INFO - __main__ - Step 420 Global step 420 Train loss 2.657143 on epoch=139
03/18/2022 16:29:52 - INFO - __main__ - Step 430 Global step 430 Train loss 2.623211 on epoch=143
03/18/2022 16:29:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.947196 on epoch=146
03/18/2022 16:30:02 - INFO - __main__ - Step 450 Global step 450 Train loss 2.729028 on epoch=149
03/18/2022 16:30:03 - INFO - __main__ - Global step 450 Train loss 2.460085 ACC 0.0 on epoch=149
03/18/2022 16:30:08 - INFO - __main__ - Step 460 Global step 460 Train loss 2.080489 on epoch=153
03/18/2022 16:30:13 - INFO - __main__ - Step 470 Global step 470 Train loss 2.180810 on epoch=156
03/18/2022 16:30:18 - INFO - __main__ - Step 480 Global step 480 Train loss 1.918191 on epoch=159
03/18/2022 16:30:23 - INFO - __main__ - Step 490 Global step 490 Train loss 2.027029 on epoch=163
03/18/2022 16:30:28 - INFO - __main__ - Step 500 Global step 500 Train loss 2.580264 on epoch=166
03/18/2022 16:30:29 - INFO - __main__ - Global step 500 Train loss 2.157357 ACC 0.0 on epoch=166
03/18/2022 16:30:33 - INFO - __main__ - Step 510 Global step 510 Train loss 2.431126 on epoch=169
03/18/2022 16:30:39 - INFO - __main__ - Step 520 Global step 520 Train loss 2.682516 on epoch=173
03/18/2022 16:30:44 - INFO - __main__ - Step 530 Global step 530 Train loss 1.980410 on epoch=176
03/18/2022 16:30:49 - INFO - __main__ - Step 540 Global step 540 Train loss 2.529414 on epoch=179
03/18/2022 16:30:54 - INFO - __main__ - Step 550 Global step 550 Train loss 2.160897 on epoch=183
03/18/2022 16:30:54 - INFO - __main__ - Global step 550 Train loss 2.356872 ACC 0.0 on epoch=183
03/18/2022 16:30:59 - INFO - __main__ - Step 560 Global step 560 Train loss 2.171420 on epoch=186
03/18/2022 16:31:04 - INFO - __main__ - Step 570 Global step 570 Train loss 1.646003 on epoch=189
03/18/2022 16:31:09 - INFO - __main__ - Step 580 Global step 580 Train loss 2.293273 on epoch=193
03/18/2022 16:31:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.786087 on epoch=196
03/18/2022 16:31:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.847087 on epoch=199
03/18/2022 16:31:20 - INFO - __main__ - Global step 600 Train loss 1.948774 ACC 0.0 on epoch=199
03/18/2022 16:31:25 - INFO - __main__ - Step 610 Global step 610 Train loss 1.872563 on epoch=203
03/18/2022 16:31:30 - INFO - __main__ - Step 620 Global step 620 Train loss 2.196537 on epoch=206
03/18/2022 16:31:35 - INFO - __main__ - Step 630 Global step 630 Train loss 1.881796 on epoch=209
03/18/2022 16:31:40 - INFO - __main__ - Step 640 Global step 640 Train loss 1.684916 on epoch=213
03/18/2022 16:31:45 - INFO - __main__ - Step 650 Global step 650 Train loss 1.632746 on epoch=216
03/18/2022 16:31:46 - INFO - __main__ - Global step 650 Train loss 1.853711 ACC 0.25 on epoch=216
03/18/2022 16:31:51 - INFO - __main__ - Step 660 Global step 660 Train loss 1.918118 on epoch=219
03/18/2022 16:31:56 - INFO - __main__ - Step 670 Global step 670 Train loss 2.041726 on epoch=223
03/18/2022 16:32:01 - INFO - __main__ - Step 680 Global step 680 Train loss 1.758977 on epoch=226
03/18/2022 16:32:06 - INFO - __main__ - Step 690 Global step 690 Train loss 1.660327 on epoch=229
03/18/2022 16:32:11 - INFO - __main__ - Step 700 Global step 700 Train loss 1.333355 on epoch=233
03/18/2022 16:32:12 - INFO - __main__ - Global step 700 Train loss 1.742500 ACC 0.1875 on epoch=233
03/18/2022 16:32:17 - INFO - __main__ - Step 710 Global step 710 Train loss 1.960058 on epoch=236
03/18/2022 16:32:22 - INFO - __main__ - Step 720 Global step 720 Train loss 1.520696 on epoch=239
03/18/2022 16:32:27 - INFO - __main__ - Step 730 Global step 730 Train loss 1.826497 on epoch=243
03/18/2022 16:32:32 - INFO - __main__ - Step 740 Global step 740 Train loss 1.232068 on epoch=246
03/18/2022 16:32:37 - INFO - __main__ - Step 750 Global step 750 Train loss 1.075813 on epoch=249
03/18/2022 16:32:37 - INFO - __main__ - Global step 750 Train loss 1.523027 ACC 0.25 on epoch=249
03/18/2022 16:32:42 - INFO - __main__ - Step 760 Global step 760 Train loss 1.293339 on epoch=253
03/18/2022 16:32:47 - INFO - __main__ - Step 770 Global step 770 Train loss 1.617675 on epoch=256
03/18/2022 16:32:52 - INFO - __main__ - Step 780 Global step 780 Train loss 1.369504 on epoch=259
03/18/2022 16:32:57 - INFO - __main__ - Step 790 Global step 790 Train loss 1.028173 on epoch=263
03/18/2022 16:33:02 - INFO - __main__ - Step 800 Global step 800 Train loss 1.436078 on epoch=266
03/18/2022 16:33:03 - INFO - __main__ - Global step 800 Train loss 1.348954 ACC 0.25 on epoch=266
03/18/2022 16:33:08 - INFO - __main__ - Step 810 Global step 810 Train loss 1.291303 on epoch=269
03/18/2022 16:33:13 - INFO - __main__ - Step 820 Global step 820 Train loss 1.224814 on epoch=273
03/18/2022 16:33:18 - INFO - __main__ - Step 830 Global step 830 Train loss 1.226617 on epoch=276
03/18/2022 16:33:23 - INFO - __main__ - Step 840 Global step 840 Train loss 1.343206 on epoch=279
03/18/2022 16:33:28 - INFO - __main__ - Step 850 Global step 850 Train loss 1.291606 on epoch=283
03/18/2022 16:33:29 - INFO - __main__ - Global step 850 Train loss 1.275509 ACC 0.25 on epoch=283
03/18/2022 16:33:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.105446 on epoch=286
03/18/2022 16:33:39 - INFO - __main__ - Step 870 Global step 870 Train loss 1.168635 on epoch=289
03/18/2022 16:33:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.991678 on epoch=293
03/18/2022 16:33:49 - INFO - __main__ - Step 890 Global step 890 Train loss 1.136393 on epoch=296
03/18/2022 16:33:54 - INFO - __main__ - Step 900 Global step 900 Train loss 1.317102 on epoch=299
03/18/2022 16:33:54 - INFO - __main__ - Global step 900 Train loss 1.143851 ACC 0.25 on epoch=299
03/18/2022 16:33:54 - INFO - __main__ - save last model!
03/18/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:33:55 - INFO - __main__ - Printing 3 examples
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:33:55 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:33:55 - INFO - __main__ - Printing 3 examples
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:33:55 - INFO - __main__ - ['contradiction']
03/18/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:33:55 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:34:01 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:34:01 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:34:01 - INFO - __main__ - Printing 3 examples
03/18/2022 16:34:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:34:01 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:34:01 - INFO - __main__ - ['neutral']
03/18/2022 16:34:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:34:01 - INFO - __main__ - ['entailment']
03/18/2022 16:34:01 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:34:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:34:01 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:34:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0001_8_predictions.txt
03/18/2022 16:34:02 - INFO - __main__ - ACC on test data: 0.3750
03/18/2022 16:34:03 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0001, bsz=8, dev_performance=0.25, test_performance=0.375
03/18/2022 16:34:03 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0005, bsz=8 ...
03/18/2022 16:34:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:34:04 - INFO - __main__ - Printing 3 examples
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:34:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:34:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:34:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:34:04 - INFO - __main__ - Printing 3 examples
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:34:04 - INFO - __main__ - ['contradiction']
03/18/2022 16:34:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:34:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:34:04 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:34:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:34:08 - INFO - __main__ - Starting training!
03/18/2022 16:34:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:34:14 - INFO - __main__ - Starting training!
03/18/2022 16:34:18 - INFO - __main__ - Step 10 Global step 10 Train loss 24.494402 on epoch=3
03/18/2022 16:34:23 - INFO - __main__ - Step 20 Global step 20 Train loss 16.957052 on epoch=6
03/18/2022 16:34:28 - INFO - __main__ - Step 30 Global step 30 Train loss 11.695494 on epoch=9
03/18/2022 16:34:33 - INFO - __main__ - Step 40 Global step 40 Train loss 9.753374 on epoch=13
03/18/2022 16:34:38 - INFO - __main__ - Step 50 Global step 50 Train loss 8.952628 on epoch=16
03/18/2022 16:34:40 - INFO - __main__ - Global step 50 Train loss 14.370588 ACC 0.09375 on epoch=16
03/18/2022 16:34:46 - INFO - __main__ - Step 60 Global step 60 Train loss 7.674291 on epoch=19
03/18/2022 16:34:51 - INFO - __main__ - Step 70 Global step 70 Train loss 6.985018 on epoch=23
03/18/2022 16:34:56 - INFO - __main__ - Step 80 Global step 80 Train loss 5.317081 on epoch=26
03/18/2022 16:35:01 - INFO - __main__ - Step 90 Global step 90 Train loss 3.334990 on epoch=29
03/18/2022 16:35:06 - INFO - __main__ - Step 100 Global step 100 Train loss 2.647042 on epoch=33
03/18/2022 16:35:07 - INFO - __main__ - Global step 100 Train loss 5.191684 ACC 0.5 on epoch=33
03/18/2022 16:35:13 - INFO - __main__ - Step 110 Global step 110 Train loss 2.045837 on epoch=36
03/18/2022 16:35:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.959483 on epoch=39
03/18/2022 16:35:23 - INFO - __main__ - Step 130 Global step 130 Train loss 2.296538 on epoch=43
03/18/2022 16:35:28 - INFO - __main__ - Step 140 Global step 140 Train loss 2.471667 on epoch=46
03/18/2022 16:35:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.069081 on epoch=49
03/18/2022 16:35:34 - INFO - __main__ - Global step 150 Train loss 2.168521 ACC 0.0 on epoch=49
03/18/2022 16:35:39 - INFO - __main__ - Step 160 Global step 160 Train loss 1.712106 on epoch=53
03/18/2022 16:35:44 - INFO - __main__ - Step 170 Global step 170 Train loss 1.708573 on epoch=56
03/18/2022 16:35:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.734887 on epoch=59
03/18/2022 16:35:54 - INFO - __main__ - Step 190 Global step 190 Train loss 1.439155 on epoch=63
03/18/2022 16:35:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.153978 on epoch=66
03/18/2022 16:36:00 - INFO - __main__ - Global step 200 Train loss 1.549740 ACC 0.5 on epoch=66
03/18/2022 16:36:05 - INFO - __main__ - Step 210 Global step 210 Train loss 1.347662 on epoch=69
03/18/2022 16:36:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.331967 on epoch=73
03/18/2022 16:36:15 - INFO - __main__ - Step 230 Global step 230 Train loss 1.052122 on epoch=76
03/18/2022 16:36:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.920524 on epoch=79
03/18/2022 16:36:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.836634 on epoch=83
03/18/2022 16:36:26 - INFO - __main__ - Global step 250 Train loss 1.097782 ACC 0.40625 on epoch=83
03/18/2022 16:36:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.555876 on epoch=86
03/18/2022 16:36:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.119221 on epoch=89
03/18/2022 16:36:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.923664 on epoch=93
03/18/2022 16:36:46 - INFO - __main__ - Step 290 Global step 290 Train loss 1.013492 on epoch=96
03/18/2022 16:36:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.536386 on epoch=99
03/18/2022 16:36:51 - INFO - __main__ - Global step 300 Train loss 1.029728 ACC 0.09375 on epoch=99
03/18/2022 16:36:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.471613 on epoch=103
03/18/2022 16:37:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.397728 on epoch=106
03/18/2022 16:37:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.460368 on epoch=109
03/18/2022 16:37:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.444117 on epoch=113
03/18/2022 16:37:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.355078 on epoch=116
03/18/2022 16:37:17 - INFO - __main__ - Global step 350 Train loss 0.425781 ACC 0.59375 on epoch=116
03/18/2022 16:37:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.417136 on epoch=119
03/18/2022 16:37:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.397305 on epoch=123
03/18/2022 16:37:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.350059 on epoch=126
03/18/2022 16:37:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.212211 on epoch=129
03/18/2022 16:37:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.262573 on epoch=133
03/18/2022 16:37:45 - INFO - __main__ - Global step 400 Train loss 0.327857 ACC 0.1875 on epoch=133
03/18/2022 16:37:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.267148 on epoch=136
03/18/2022 16:37:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.187278 on epoch=139
03/18/2022 16:38:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.175470 on epoch=143
03/18/2022 16:38:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.087874 on epoch=146
03/18/2022 16:38:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.134350 on epoch=149
03/18/2022 16:38:11 - INFO - __main__ - Global step 450 Train loss 0.170424 ACC 0.5625 on epoch=149
03/18/2022 16:38:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.099202 on epoch=153
03/18/2022 16:38:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.133616 on epoch=156
03/18/2022 16:38:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.087756 on epoch=159
03/18/2022 16:38:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.064732 on epoch=163
03/18/2022 16:38:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.063416 on epoch=166
03/18/2022 16:38:37 - INFO - __main__ - Global step 500 Train loss 0.089745 ACC 0.71875 on epoch=166
03/18/2022 16:38:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.034460 on epoch=169
03/18/2022 16:38:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.057068 on epoch=173
03/18/2022 16:38:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.028307 on epoch=176
03/18/2022 16:38:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.031865 on epoch=179
03/18/2022 16:39:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014180 on epoch=183
03/18/2022 16:39:03 - INFO - __main__ - Global step 550 Train loss 0.033176 ACC 0.65625 on epoch=183
03/18/2022 16:39:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.054089 on epoch=186
03/18/2022 16:39:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.019875 on epoch=189
03/18/2022 16:39:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.060330 on epoch=193
03/18/2022 16:39:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.008611 on epoch=196
03/18/2022 16:39:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.012873 on epoch=199
03/18/2022 16:39:29 - INFO - __main__ - Global step 600 Train loss 0.031156 ACC 0.71875 on epoch=199
03/18/2022 16:39:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.005898 on epoch=203
03/18/2022 16:39:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.014566 on epoch=206
03/18/2022 16:39:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004015 on epoch=209
03/18/2022 16:39:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002269 on epoch=213
03/18/2022 16:39:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002882 on epoch=216
03/18/2022 16:39:55 - INFO - __main__ - Global step 650 Train loss 0.005926 ACC 0.6875 on epoch=216
03/18/2022 16:40:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.507371 on epoch=219
03/18/2022 16:40:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.023710 on epoch=223
03/18/2022 16:40:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.020392 on epoch=226
03/18/2022 16:40:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.117571 on epoch=229
03/18/2022 16:40:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.047239 on epoch=233
03/18/2022 16:40:21 - INFO - __main__ - Global step 700 Train loss 0.143257 ACC 0.5 on epoch=233
03/18/2022 16:40:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.048928 on epoch=236
03/18/2022 16:40:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.011952 on epoch=239
03/18/2022 16:40:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003997 on epoch=243
03/18/2022 16:40:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002399 on epoch=246
03/18/2022 16:40:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.004979 on epoch=249
03/18/2022 16:40:47 - INFO - __main__ - Global step 750 Train loss 0.014451 ACC 0.53125 on epoch=249
03/18/2022 16:40:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.045071 on epoch=253
03/18/2022 16:40:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.085763 on epoch=256
03/18/2022 16:41:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.002558 on epoch=259
03/18/2022 16:41:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.056131 on epoch=263
03/18/2022 16:41:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.028867 on epoch=266
03/18/2022 16:41:12 - INFO - __main__ - Global step 800 Train loss 0.043678 ACC 0.65625 on epoch=266
03/18/2022 16:41:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004666 on epoch=269
03/18/2022 16:41:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.008272 on epoch=273
03/18/2022 16:41:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.009764 on epoch=276
03/18/2022 16:41:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.049254 on epoch=279
03/18/2022 16:41:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001477 on epoch=283
03/18/2022 16:41:38 - INFO - __main__ - Global step 850 Train loss 0.014687 ACC 0.53125 on epoch=283
03/18/2022 16:41:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002018 on epoch=286
03/18/2022 16:41:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003522 on epoch=289
03/18/2022 16:41:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000518 on epoch=293
03/18/2022 16:41:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001281 on epoch=296
03/18/2022 16:42:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.009086 on epoch=299
03/18/2022 16:42:04 - INFO - __main__ - Global step 900 Train loss 0.003285 ACC 0.28125 on epoch=299
03/18/2022 16:42:04 - INFO - __main__ - save last model!
03/18/2022 16:42:05 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:42:05 - INFO - __main__ - Printing 3 examples
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:42:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:42:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:42:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:42:05 - INFO - __main__ - Printing 3 examples
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:42:05 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:05 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:42:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:42:05 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:42:12 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:42:13 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:42:13 - INFO - __main__ - Printing 3 examples
03/18/2022 16:42:13 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:42:13 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:13 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:42:13 - INFO - __main__ - ['neutral']
03/18/2022 16:42:13 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:42:13 - INFO - __main__ - ['entailment']
03/18/2022 16:42:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:42:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:42:13 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:42:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0005_8_predictions.txt
03/18/2022 16:42:14 - INFO - __main__ - ACC on test data: 0.4821
03/18/2022 16:42:14 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0005, bsz=8, dev_performance=0.71875, test_performance=0.48214285714285715
03/18/2022 16:42:14 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0003, bsz=8 ...
03/18/2022 16:42:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:42:15 - INFO - __main__ - Printing 3 examples
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:42:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:42:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:42:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:42:15 - INFO - __main__ - Printing 3 examples
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:42:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:42:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:42:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:42:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:42:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:42:16 - INFO - __main__ - Starting training!
03/18/2022 16:42:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:42:28 - INFO - __main__ - Starting training!
03/18/2022 16:42:32 - INFO - __main__ - Step 10 Global step 10 Train loss 22.169859 on epoch=3
03/18/2022 16:42:37 - INFO - __main__ - Step 20 Global step 20 Train loss 20.030153 on epoch=6
03/18/2022 16:42:42 - INFO - __main__ - Step 30 Global step 30 Train loss 15.604979 on epoch=9
03/18/2022 16:42:47 - INFO - __main__ - Step 40 Global step 40 Train loss 13.394081 on epoch=13
03/18/2022 16:42:52 - INFO - __main__ - Step 50 Global step 50 Train loss 11.265009 on epoch=16
03/18/2022 16:42:52 - INFO - __main__ - Global step 50 Train loss 16.492815 ACC 0.03125 on epoch=16
03/18/2022 16:42:58 - INFO - __main__ - Step 60 Global step 60 Train loss 10.345153 on epoch=19
03/18/2022 16:43:03 - INFO - __main__ - Step 70 Global step 70 Train loss 9.439796 on epoch=23
03/18/2022 16:43:08 - INFO - __main__ - Step 80 Global step 80 Train loss 8.564462 on epoch=26
03/18/2022 16:43:13 - INFO - __main__ - Step 90 Global step 90 Train loss 8.317230 on epoch=29
03/18/2022 16:43:18 - INFO - __main__ - Step 100 Global step 100 Train loss 7.563848 on epoch=33
03/18/2022 16:43:19 - INFO - __main__ - Global step 100 Train loss 8.846097 ACC 0.0 on epoch=33
03/18/2022 16:43:24 - INFO - __main__ - Step 110 Global step 110 Train loss 7.223634 on epoch=36
03/18/2022 16:43:29 - INFO - __main__ - Step 120 Global step 120 Train loss 6.351329 on epoch=39
03/18/2022 16:43:34 - INFO - __main__ - Step 130 Global step 130 Train loss 5.299439 on epoch=43
03/18/2022 16:43:39 - INFO - __main__ - Step 140 Global step 140 Train loss 4.614775 on epoch=46
03/18/2022 16:43:44 - INFO - __main__ - Step 150 Global step 150 Train loss 4.230326 on epoch=49
03/18/2022 16:43:45 - INFO - __main__ - Global step 150 Train loss 5.543900 ACC 0.0 on epoch=49
03/18/2022 16:43:50 - INFO - __main__ - Step 160 Global step 160 Train loss 2.398604 on epoch=53
03/18/2022 16:43:55 - INFO - __main__ - Step 170 Global step 170 Train loss 2.937371 on epoch=56
03/18/2022 16:44:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.156078 on epoch=59
03/18/2022 16:44:05 - INFO - __main__ - Step 190 Global step 190 Train loss 2.319997 on epoch=63
03/18/2022 16:44:10 - INFO - __main__ - Step 200 Global step 200 Train loss 1.790687 on epoch=66
03/18/2022 16:44:11 - INFO - __main__ - Global step 200 Train loss 2.320547 ACC 0.0 on epoch=66
03/18/2022 16:44:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.969435 on epoch=69
03/18/2022 16:44:21 - INFO - __main__ - Step 220 Global step 220 Train loss 2.576814 on epoch=73
03/18/2022 16:44:26 - INFO - __main__ - Step 230 Global step 230 Train loss 1.899980 on epoch=76
03/18/2022 16:44:31 - INFO - __main__ - Step 240 Global step 240 Train loss 2.311790 on epoch=79
03/18/2022 16:44:36 - INFO - __main__ - Step 250 Global step 250 Train loss 2.301903 on epoch=83
03/18/2022 16:44:37 - INFO - __main__ - Global step 250 Train loss 2.211984 ACC 0.5 on epoch=83
03/18/2022 16:44:43 - INFO - __main__ - Step 260 Global step 260 Train loss 1.962391 on epoch=86
03/18/2022 16:44:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.768886 on epoch=89
03/18/2022 16:44:53 - INFO - __main__ - Step 280 Global step 280 Train loss 1.683515 on epoch=93
03/18/2022 16:44:58 - INFO - __main__ - Step 290 Global step 290 Train loss 1.810068 on epoch=96
03/18/2022 16:45:03 - INFO - __main__ - Step 300 Global step 300 Train loss 1.525230 on epoch=99
03/18/2022 16:45:04 - INFO - __main__ - Global step 300 Train loss 1.750018 ACC 0.0 on epoch=99
03/18/2022 16:45:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.372639 on epoch=103
03/18/2022 16:45:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.536400 on epoch=106
03/18/2022 16:45:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.427639 on epoch=109
03/18/2022 16:45:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.395204 on epoch=113
03/18/2022 16:45:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.394559 on epoch=116
03/18/2022 16:45:30 - INFO - __main__ - Global step 350 Train loss 1.425288 ACC 0.0 on epoch=116
03/18/2022 16:45:35 - INFO - __main__ - Step 360 Global step 360 Train loss 1.617623 on epoch=119
03/18/2022 16:45:40 - INFO - __main__ - Step 370 Global step 370 Train loss 1.294363 on epoch=123
03/18/2022 16:45:45 - INFO - __main__ - Step 380 Global step 380 Train loss 1.502596 on epoch=126
03/18/2022 16:45:50 - INFO - __main__ - Step 390 Global step 390 Train loss 1.313416 on epoch=129
03/18/2022 16:45:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.190049 on epoch=133
03/18/2022 16:45:55 - INFO - __main__ - Global step 400 Train loss 1.383610 ACC 0.0 on epoch=133
03/18/2022 16:46:00 - INFO - __main__ - Step 410 Global step 410 Train loss 1.066373 on epoch=136
03/18/2022 16:46:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.960037 on epoch=139
03/18/2022 16:46:10 - INFO - __main__ - Step 430 Global step 430 Train loss 1.040321 on epoch=143
03/18/2022 16:46:15 - INFO - __main__ - Step 440 Global step 440 Train loss 1.028176 on epoch=146
03/18/2022 16:46:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.911273 on epoch=149
03/18/2022 16:46:21 - INFO - __main__ - Global step 450 Train loss 1.001236 ACC 0.0 on epoch=149
03/18/2022 16:46:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.853185 on epoch=153
03/18/2022 16:46:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.827103 on epoch=156
03/18/2022 16:46:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.910309 on epoch=159
03/18/2022 16:46:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.588787 on epoch=163
03/18/2022 16:46:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.627581 on epoch=166
03/18/2022 16:46:47 - INFO - __main__ - Global step 500 Train loss 0.761393 ACC 0.0 on epoch=166
03/18/2022 16:46:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.753005 on epoch=169
03/18/2022 16:46:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.614699 on epoch=173
03/18/2022 16:47:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.574361 on epoch=176
03/18/2022 16:47:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.725316 on epoch=179
03/18/2022 16:47:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.634099 on epoch=183
03/18/2022 16:47:13 - INFO - __main__ - Global step 550 Train loss 0.660296 ACC 0.5 on epoch=183
03/18/2022 16:47:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.592475 on epoch=186
03/18/2022 16:47:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.588197 on epoch=189
03/18/2022 16:47:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.644856 on epoch=193
03/18/2022 16:47:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.610428 on epoch=196
03/18/2022 16:47:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.608027 on epoch=199
03/18/2022 16:47:39 - INFO - __main__ - Global step 600 Train loss 0.608796 ACC 0.5 on epoch=199
03/18/2022 16:47:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.524954 on epoch=203
03/18/2022 16:47:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.682529 on epoch=206
03/18/2022 16:47:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.549733 on epoch=209
03/18/2022 16:47:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.476200 on epoch=213
03/18/2022 16:48:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.543832 on epoch=216
03/18/2022 16:48:04 - INFO - __main__ - Global step 650 Train loss 0.555449 ACC 0.34375 on epoch=216
03/18/2022 16:48:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.530739 on epoch=219
03/18/2022 16:48:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.488902 on epoch=223
03/18/2022 16:48:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.531016 on epoch=226
03/18/2022 16:48:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.463088 on epoch=229
03/18/2022 16:48:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.435123 on epoch=233
03/18/2022 16:48:30 - INFO - __main__ - Global step 700 Train loss 0.489774 ACC 0.375 on epoch=233
03/18/2022 16:48:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.472139 on epoch=236
03/18/2022 16:48:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.478680 on epoch=239
03/18/2022 16:48:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.502254 on epoch=243
03/18/2022 16:48:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.516169 on epoch=246
03/18/2022 16:48:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.492406 on epoch=249
03/18/2022 16:48:56 - INFO - __main__ - Global step 750 Train loss 0.492330 ACC 0.4375 on epoch=249
03/18/2022 16:49:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.470499 on epoch=253
03/18/2022 16:49:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.559239 on epoch=256
03/18/2022 16:49:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.441779 on epoch=259
03/18/2022 16:49:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.450165 on epoch=263
03/18/2022 16:49:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.612197 on epoch=266
03/18/2022 16:49:22 - INFO - __main__ - Global step 800 Train loss 0.506776 ACC 0.5 on epoch=266
03/18/2022 16:49:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.443341 on epoch=269
03/18/2022 16:49:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.413288 on epoch=273
03/18/2022 16:49:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.416853 on epoch=276
03/18/2022 16:49:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.459329 on epoch=279
03/18/2022 16:49:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.440659 on epoch=283
03/18/2022 16:49:48 - INFO - __main__ - Global step 850 Train loss 0.434694 ACC 0.59375 on epoch=283
03/18/2022 16:49:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.439344 on epoch=286
03/18/2022 16:49:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.475267 on epoch=289
03/18/2022 16:50:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.438452 on epoch=293
03/18/2022 16:50:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.441474 on epoch=296
03/18/2022 16:50:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.463624 on epoch=299
03/18/2022 16:50:14 - INFO - __main__ - Global step 900 Train loss 0.451632 ACC 0.875 on epoch=299
03/18/2022 16:50:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:50:15 - INFO - __main__ - Printing 3 examples
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:50:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:50:15 - INFO - __main__ - save last model!
03/18/2022 16:50:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:50:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:50:15 - INFO - __main__ - Printing 3 examples
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:50:15 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:50:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:50:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:50:21 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:50:22 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:50:22 - INFO - __main__ - Printing 3 examples
03/18/2022 16:50:22 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:50:22 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:22 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:50:22 - INFO - __main__ - ['neutral']
03/18/2022 16:50:22 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:50:22 - INFO - __main__ - ['entailment']
03/18/2022 16:50:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:50:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:50:22 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:50:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0003_8_predictions.txt
03/18/2022 16:50:23 - INFO - __main__ - ACC on test data: 0.6786
03/18/2022 16:50:24 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0003, bsz=8, dev_performance=0.875, test_performance=0.6785714285714286
03/18/2022 16:50:24 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0002, bsz=8 ...
03/18/2022 16:50:25 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:50:25 - INFO - __main__ - Printing 3 examples
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:50:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:50:25 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:50:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:50:25 - INFO - __main__ - Printing 3 examples
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:50:25 - INFO - __main__ - ['contradiction']
03/18/2022 16:50:25 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:50:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:50:25 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:50:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:50:28 - INFO - __main__ - Starting training!
03/18/2022 16:50:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:50:35 - INFO - __main__ - Starting training!
03/18/2022 16:50:39 - INFO - __main__ - Step 10 Global step 10 Train loss 23.130341 on epoch=3
03/18/2022 16:50:44 - INFO - __main__ - Step 20 Global step 20 Train loss 21.305696 on epoch=6
03/18/2022 16:50:49 - INFO - __main__ - Step 30 Global step 30 Train loss 14.714220 on epoch=9
03/18/2022 16:50:54 - INFO - __main__ - Step 40 Global step 40 Train loss 11.274431 on epoch=13
03/18/2022 16:50:59 - INFO - __main__ - Step 50 Global step 50 Train loss 10.919560 on epoch=16
03/18/2022 16:51:00 - INFO - __main__ - Global step 50 Train loss 16.268850 ACC 0.0 on epoch=16
03/18/2022 16:51:06 - INFO - __main__ - Step 60 Global step 60 Train loss 10.416716 on epoch=19
03/18/2022 16:51:11 - INFO - __main__ - Step 70 Global step 70 Train loss 9.690409 on epoch=23
03/18/2022 16:51:16 - INFO - __main__ - Step 80 Global step 80 Train loss 8.985697 on epoch=26
03/18/2022 16:51:21 - INFO - __main__ - Step 90 Global step 90 Train loss 8.807966 on epoch=29
03/18/2022 16:51:26 - INFO - __main__ - Step 100 Global step 100 Train loss 8.105517 on epoch=33
03/18/2022 16:51:27 - INFO - __main__ - Global step 100 Train loss 9.201261 ACC 0.03125 on epoch=33
03/18/2022 16:51:33 - INFO - __main__ - Step 110 Global step 110 Train loss 8.004819 on epoch=36
03/18/2022 16:51:38 - INFO - __main__ - Step 120 Global step 120 Train loss 7.452974 on epoch=39
03/18/2022 16:51:43 - INFO - __main__ - Step 130 Global step 130 Train loss 6.391152 on epoch=43
03/18/2022 16:51:48 - INFO - __main__ - Step 140 Global step 140 Train loss 6.072641 on epoch=46
03/18/2022 16:51:53 - INFO - __main__ - Step 150 Global step 150 Train loss 5.776539 on epoch=49
03/18/2022 16:51:54 - INFO - __main__ - Global step 150 Train loss 6.739625 ACC 0.0 on epoch=49
03/18/2022 16:51:59 - INFO - __main__ - Step 160 Global step 160 Train loss 4.638895 on epoch=53
03/18/2022 16:52:04 - INFO - __main__ - Step 170 Global step 170 Train loss 4.265440 on epoch=56
03/18/2022 16:52:09 - INFO - __main__ - Step 180 Global step 180 Train loss 3.218016 on epoch=59
03/18/2022 16:52:14 - INFO - __main__ - Step 190 Global step 190 Train loss 3.062031 on epoch=63
03/18/2022 16:52:19 - INFO - __main__ - Step 200 Global step 200 Train loss 2.779661 on epoch=66
03/18/2022 16:52:20 - INFO - __main__ - Global step 200 Train loss 3.592808 ACC 0.0 on epoch=66
03/18/2022 16:52:25 - INFO - __main__ - Step 210 Global step 210 Train loss 2.361144 on epoch=69
03/18/2022 16:52:30 - INFO - __main__ - Step 220 Global step 220 Train loss 2.490352 on epoch=73
03/18/2022 16:52:35 - INFO - __main__ - Step 230 Global step 230 Train loss 2.614934 on epoch=76
03/18/2022 16:52:40 - INFO - __main__ - Step 240 Global step 240 Train loss 2.475876 on epoch=79
03/18/2022 16:52:45 - INFO - __main__ - Step 250 Global step 250 Train loss 2.576409 on epoch=83
03/18/2022 16:52:46 - INFO - __main__ - Global step 250 Train loss 2.503743 ACC 0.0 on epoch=83
03/18/2022 16:52:51 - INFO - __main__ - Step 260 Global step 260 Train loss 1.949682 on epoch=86
03/18/2022 16:52:56 - INFO - __main__ - Step 270 Global step 270 Train loss 2.496359 on epoch=89
03/18/2022 16:53:01 - INFO - __main__ - Step 280 Global step 280 Train loss 2.319935 on epoch=93
03/18/2022 16:53:06 - INFO - __main__ - Step 290 Global step 290 Train loss 2.191574 on epoch=96
03/18/2022 16:53:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.062706 on epoch=99
03/18/2022 16:53:12 - INFO - __main__ - Global step 300 Train loss 2.204051 ACC 0.0 on epoch=99
03/18/2022 16:53:17 - INFO - __main__ - Step 310 Global step 310 Train loss 1.840051 on epoch=103
03/18/2022 16:53:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.108361 on epoch=106
03/18/2022 16:53:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.998716 on epoch=109
03/18/2022 16:53:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.615316 on epoch=113
03/18/2022 16:53:37 - INFO - __main__ - Step 350 Global step 350 Train loss 1.864964 on epoch=116
03/18/2022 16:53:37 - INFO - __main__ - Global step 350 Train loss 1.885482 ACC 0.5 on epoch=116
03/18/2022 16:53:43 - INFO - __main__ - Step 360 Global step 360 Train loss 1.411857 on epoch=119
03/18/2022 16:53:48 - INFO - __main__ - Step 370 Global step 370 Train loss 1.289490 on epoch=123
03/18/2022 16:53:54 - INFO - __main__ - Step 380 Global step 380 Train loss 1.785929 on epoch=126
03/18/2022 16:53:59 - INFO - __main__ - Step 390 Global step 390 Train loss 1.343392 on epoch=129
03/18/2022 16:54:04 - INFO - __main__ - Step 400 Global step 400 Train loss 1.428179 on epoch=133
03/18/2022 16:54:04 - INFO - __main__ - Global step 400 Train loss 1.451769 ACC 0.4375 on epoch=133
03/18/2022 16:54:09 - INFO - __main__ - Step 410 Global step 410 Train loss 1.224382 on epoch=136
03/18/2022 16:54:14 - INFO - __main__ - Step 420 Global step 420 Train loss 1.031917 on epoch=139
03/18/2022 16:54:19 - INFO - __main__ - Step 430 Global step 430 Train loss 1.001741 on epoch=143
03/18/2022 16:54:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.971120 on epoch=146
03/18/2022 16:54:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.770461 on epoch=149
03/18/2022 16:54:30 - INFO - __main__ - Global step 450 Train loss 0.999924 ACC 0.4375 on epoch=149
03/18/2022 16:54:35 - INFO - __main__ - Step 460 Global step 460 Train loss 1.176324 on epoch=153
03/18/2022 16:54:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.657610 on epoch=156
03/18/2022 16:54:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.392260 on epoch=159
03/18/2022 16:54:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.464929 on epoch=163
03/18/2022 16:54:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.302436 on epoch=166
03/18/2022 16:54:56 - INFO - __main__ - Global step 500 Train loss 0.598712 ACC 0.8125 on epoch=166
03/18/2022 16:55:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.153816 on epoch=169
03/18/2022 16:55:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.071497 on epoch=173
03/18/2022 16:55:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.060609 on epoch=176
03/18/2022 16:55:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.075931 on epoch=179
03/18/2022 16:55:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.065012 on epoch=183
03/18/2022 16:55:23 - INFO - __main__ - Global step 550 Train loss 0.085373 ACC 0.8125 on epoch=183
03/18/2022 16:55:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.014803 on epoch=186
03/18/2022 16:55:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024285 on epoch=189
03/18/2022 16:55:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021921 on epoch=193
03/18/2022 16:55:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003718 on epoch=196
03/18/2022 16:55:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.025713 on epoch=199
03/18/2022 16:55:50 - INFO - __main__ - Global step 600 Train loss 0.018088 ACC 0.75 on epoch=199
03/18/2022 16:55:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003808 on epoch=203
03/18/2022 16:56:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004858 on epoch=206
03/18/2022 16:56:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000597 on epoch=209
03/18/2022 16:56:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003602 on epoch=213
03/18/2022 16:56:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002534 on epoch=216
03/18/2022 16:56:16 - INFO - __main__ - Global step 650 Train loss 0.003080 ACC 0.8125 on epoch=216
03/18/2022 16:56:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.005907 on epoch=219
03/18/2022 16:56:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002523 on epoch=223
03/18/2022 16:56:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.003768 on epoch=226
03/18/2022 16:56:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.008853 on epoch=229
03/18/2022 16:56:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002875 on epoch=233
03/18/2022 16:56:42 - INFO - __main__ - Global step 700 Train loss 0.004785 ACC 0.84375 on epoch=233
03/18/2022 16:56:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.004326 on epoch=236
03/18/2022 16:56:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001975 on epoch=239
03/18/2022 16:56:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002598 on epoch=243
03/18/2022 16:57:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001213 on epoch=246
03/18/2022 16:57:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001421 on epoch=249
03/18/2022 16:57:08 - INFO - __main__ - Global step 750 Train loss 0.002307 ACC 0.84375 on epoch=249
03/18/2022 16:57:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002130 on epoch=253
03/18/2022 16:57:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000560 on epoch=256
03/18/2022 16:57:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001044 on epoch=259
03/18/2022 16:57:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001509 on epoch=263
03/18/2022 16:57:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.006896 on epoch=266
03/18/2022 16:57:35 - INFO - __main__ - Global step 800 Train loss 0.002428 ACC 0.875 on epoch=266
03/18/2022 16:57:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.012860 on epoch=269
03/18/2022 16:57:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001274 on epoch=273
03/18/2022 16:57:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000392 on epoch=276
03/18/2022 16:57:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000319 on epoch=279
03/18/2022 16:58:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000420 on epoch=283
03/18/2022 16:58:01 - INFO - __main__ - Global step 850 Train loss 0.003053 ACC 0.8125 on epoch=283
03/18/2022 16:58:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.056572 on epoch=286
03/18/2022 16:58:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.022763 on epoch=289
03/18/2022 16:58:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008741 on epoch=293
03/18/2022 16:58:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000650 on epoch=296
03/18/2022 16:58:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.013251 on epoch=299
03/18/2022 16:58:27 - INFO - __main__ - Global step 900 Train loss 0.020396 ACC 0.90625 on epoch=299
03/18/2022 16:58:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:58:28 - INFO - __main__ - Printing 3 examples
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 16:58:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:58:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:58:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:58:28 - INFO - __main__ - Printing 3 examples
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:58:28 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:28 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:58:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:58:28 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:58:28 - INFO - __main__ - save last model!
03/18/2022 16:58:36 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 16:58:36 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 16:58:36 - INFO - __main__ - Printing 3 examples
03/18/2022 16:58:36 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 16:58:36 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:36 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 16:58:36 - INFO - __main__ - ['neutral']
03/18/2022 16:58:36 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 16:58:36 - INFO - __main__ - ['entailment']
03/18/2022 16:58:36 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:58:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:58:37 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 16:58:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0002_8_predictions.txt
03/18/2022 16:58:38 - INFO - __main__ - ACC on test data: 0.7679
03/18/2022 16:58:38 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0002, bsz=8, dev_performance=0.90625, test_performance=0.7678571428571429
03/18/2022 16:58:38 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0001, bsz=8 ...
03/18/2022 16:58:39 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 16:58:39 - INFO - __main__ - Printing 3 examples
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 16:58:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:58:39 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 16:58:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 16:58:39 - INFO - __main__ - Printing 3 examples
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/18/2022 16:58:39 - INFO - __main__ - ['contradiction']
03/18/2022 16:58:39 - INFO - __main__ - Tokenizing Input ...
03/18/2022 16:58:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 16:58:39 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 16:58:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:58:41 - INFO - __main__ - Starting training!
03/18/2022 16:58:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 16:58:52 - INFO - __main__ - Starting training!
03/18/2022 16:58:56 - INFO - __main__ - Step 10 Global step 10 Train loss 24.386883 on epoch=3
03/18/2022 16:59:01 - INFO - __main__ - Step 20 Global step 20 Train loss 20.126482 on epoch=6
03/18/2022 16:59:06 - INFO - __main__ - Step 30 Global step 30 Train loss 17.053364 on epoch=9
03/18/2022 16:59:11 - INFO - __main__ - Step 40 Global step 40 Train loss 14.478702 on epoch=13
03/18/2022 16:59:16 - INFO - __main__ - Step 50 Global step 50 Train loss 13.983606 on epoch=16
03/18/2022 16:59:21 - INFO - __main__ - Global step 50 Train loss 18.005808 ACC 0.125 on epoch=16
03/18/2022 16:59:27 - INFO - __main__ - Step 60 Global step 60 Train loss 14.159198 on epoch=19
03/18/2022 16:59:32 - INFO - __main__ - Step 70 Global step 70 Train loss 10.758441 on epoch=23
03/18/2022 16:59:37 - INFO - __main__ - Step 80 Global step 80 Train loss 10.916933 on epoch=26
03/18/2022 16:59:42 - INFO - __main__ - Step 90 Global step 90 Train loss 11.252592 on epoch=29
03/18/2022 16:59:47 - INFO - __main__ - Step 100 Global step 100 Train loss 10.824862 on epoch=33
03/18/2022 16:59:48 - INFO - __main__ - Global step 100 Train loss 11.582405 ACC 0.0 on epoch=33
03/18/2022 16:59:53 - INFO - __main__ - Step 110 Global step 110 Train loss 10.910662 on epoch=36
03/18/2022 16:59:58 - INFO - __main__ - Step 120 Global step 120 Train loss 10.524472 on epoch=39
03/18/2022 17:00:03 - INFO - __main__ - Step 130 Global step 130 Train loss 9.841447 on epoch=43
03/18/2022 17:00:08 - INFO - __main__ - Step 140 Global step 140 Train loss 8.981348 on epoch=46
03/18/2022 17:00:13 - INFO - __main__ - Step 150 Global step 150 Train loss 9.578838 on epoch=49
03/18/2022 17:00:14 - INFO - __main__ - Global step 150 Train loss 9.967354 ACC 0.03125 on epoch=49
03/18/2022 17:00:19 - INFO - __main__ - Step 160 Global step 160 Train loss 9.375766 on epoch=53
03/18/2022 17:00:24 - INFO - __main__ - Step 170 Global step 170 Train loss 8.729680 on epoch=56
03/18/2022 17:00:29 - INFO - __main__ - Step 180 Global step 180 Train loss 8.825286 on epoch=59
03/18/2022 17:00:34 - INFO - __main__ - Step 190 Global step 190 Train loss 7.796586 on epoch=63
03/18/2022 17:00:39 - INFO - __main__ - Step 200 Global step 200 Train loss 8.274309 on epoch=66
03/18/2022 17:00:40 - INFO - __main__ - Global step 200 Train loss 8.600326 ACC 0.03125 on epoch=66
03/18/2022 17:00:45 - INFO - __main__ - Step 210 Global step 210 Train loss 8.165281 on epoch=69
03/18/2022 17:00:50 - INFO - __main__ - Step 220 Global step 220 Train loss 7.622808 on epoch=73
03/18/2022 17:00:55 - INFO - __main__ - Step 230 Global step 230 Train loss 7.526224 on epoch=76
03/18/2022 17:01:00 - INFO - __main__ - Step 240 Global step 240 Train loss 7.391792 on epoch=79
03/18/2022 17:01:05 - INFO - __main__ - Step 250 Global step 250 Train loss 6.754896 on epoch=83
03/18/2022 17:01:06 - INFO - __main__ - Global step 250 Train loss 7.492201 ACC 0.0 on epoch=83
03/18/2022 17:01:11 - INFO - __main__ - Step 260 Global step 260 Train loss 6.340086 on epoch=86
03/18/2022 17:01:16 - INFO - __main__ - Step 270 Global step 270 Train loss 6.612010 on epoch=89
03/18/2022 17:01:21 - INFO - __main__ - Step 280 Global step 280 Train loss 6.087272 on epoch=93
03/18/2022 17:01:26 - INFO - __main__ - Step 290 Global step 290 Train loss 5.942603 on epoch=96
03/18/2022 17:01:31 - INFO - __main__ - Step 300 Global step 300 Train loss 5.676692 on epoch=99
03/18/2022 17:01:32 - INFO - __main__ - Global step 300 Train loss 6.131733 ACC 0.0 on epoch=99
03/18/2022 17:01:37 - INFO - __main__ - Step 310 Global step 310 Train loss 5.260982 on epoch=103
03/18/2022 17:01:42 - INFO - __main__ - Step 320 Global step 320 Train loss 4.346076 on epoch=106
03/18/2022 17:01:47 - INFO - __main__ - Step 330 Global step 330 Train loss 3.823288 on epoch=109
03/18/2022 17:01:52 - INFO - __main__ - Step 340 Global step 340 Train loss 3.282111 on epoch=113
03/18/2022 17:01:57 - INFO - __main__ - Step 350 Global step 350 Train loss 4.125780 on epoch=116
03/18/2022 17:01:57 - INFO - __main__ - Global step 350 Train loss 4.167647 ACC 0.0 on epoch=116
03/18/2022 17:02:02 - INFO - __main__ - Step 360 Global step 360 Train loss 3.277155 on epoch=119
03/18/2022 17:02:07 - INFO - __main__ - Step 370 Global step 370 Train loss 2.610882 on epoch=123
03/18/2022 17:02:12 - INFO - __main__ - Step 380 Global step 380 Train loss 3.301580 on epoch=126
03/18/2022 17:02:17 - INFO - __main__ - Step 390 Global step 390 Train loss 2.312076 on epoch=129
03/18/2022 17:02:22 - INFO - __main__ - Step 400 Global step 400 Train loss 2.391957 on epoch=133
03/18/2022 17:02:23 - INFO - __main__ - Global step 400 Train loss 2.778730 ACC 0.0 on epoch=133
03/18/2022 17:02:28 - INFO - __main__ - Step 410 Global step 410 Train loss 3.114488 on epoch=136
03/18/2022 17:02:33 - INFO - __main__ - Step 420 Global step 420 Train loss 2.311915 on epoch=139
03/18/2022 17:02:38 - INFO - __main__ - Step 430 Global step 430 Train loss 2.198385 on epoch=143
03/18/2022 17:02:43 - INFO - __main__ - Step 440 Global step 440 Train loss 2.675559 on epoch=146
03/18/2022 17:02:48 - INFO - __main__ - Step 450 Global step 450 Train loss 2.268252 on epoch=149
03/18/2022 17:02:48 - INFO - __main__ - Global step 450 Train loss 2.513720 ACC 0.0 on epoch=149
03/18/2022 17:02:53 - INFO - __main__ - Step 460 Global step 460 Train loss 2.345047 on epoch=153
03/18/2022 17:02:58 - INFO - __main__ - Step 470 Global step 470 Train loss 2.487428 on epoch=156
03/18/2022 17:03:03 - INFO - __main__ - Step 480 Global step 480 Train loss 2.064851 on epoch=159
03/18/2022 17:03:08 - INFO - __main__ - Step 490 Global step 490 Train loss 2.470958 on epoch=163
03/18/2022 17:03:14 - INFO - __main__ - Step 500 Global step 500 Train loss 2.134452 on epoch=166
03/18/2022 17:03:14 - INFO - __main__ - Global step 500 Train loss 2.300547 ACC 0.15625 on epoch=166
03/18/2022 17:03:20 - INFO - __main__ - Step 510 Global step 510 Train loss 1.897419 on epoch=169
03/18/2022 17:03:25 - INFO - __main__ - Step 520 Global step 520 Train loss 2.117154 on epoch=173
03/18/2022 17:03:30 - INFO - __main__ - Step 530 Global step 530 Train loss 2.462810 on epoch=176
03/18/2022 17:03:35 - INFO - __main__ - Step 540 Global step 540 Train loss 1.949807 on epoch=179
03/18/2022 17:03:40 - INFO - __main__ - Step 550 Global step 550 Train loss 1.969789 on epoch=183
03/18/2022 17:03:40 - INFO - __main__ - Global step 550 Train loss 2.079396 ACC 0.0 on epoch=183
03/18/2022 17:03:45 - INFO - __main__ - Step 560 Global step 560 Train loss 2.082350 on epoch=186
03/18/2022 17:03:50 - INFO - __main__ - Step 570 Global step 570 Train loss 1.706425 on epoch=189
03/18/2022 17:03:55 - INFO - __main__ - Step 580 Global step 580 Train loss 1.740660 on epoch=193
03/18/2022 17:04:00 - INFO - __main__ - Step 590 Global step 590 Train loss 2.400380 on epoch=196
03/18/2022 17:04:05 - INFO - __main__ - Step 600 Global step 600 Train loss 1.361688 on epoch=199
03/18/2022 17:04:06 - INFO - __main__ - Global step 600 Train loss 1.858301 ACC 0.34375 on epoch=199
03/18/2022 17:04:12 - INFO - __main__ - Step 610 Global step 610 Train loss 1.692353 on epoch=203
03/18/2022 17:04:17 - INFO - __main__ - Step 620 Global step 620 Train loss 1.479125 on epoch=206
03/18/2022 17:04:22 - INFO - __main__ - Step 630 Global step 630 Train loss 1.873463 on epoch=209
03/18/2022 17:04:27 - INFO - __main__ - Step 640 Global step 640 Train loss 1.663669 on epoch=213
03/18/2022 17:04:32 - INFO - __main__ - Step 650 Global step 650 Train loss 1.573686 on epoch=216
03/18/2022 17:04:33 - INFO - __main__ - Global step 650 Train loss 1.656459 ACC 0.3125 on epoch=216
03/18/2022 17:04:38 - INFO - __main__ - Step 660 Global step 660 Train loss 1.474114 on epoch=219
03/18/2022 17:04:43 - INFO - __main__ - Step 670 Global step 670 Train loss 1.366665 on epoch=223
03/18/2022 17:04:48 - INFO - __main__ - Step 680 Global step 680 Train loss 1.303080 on epoch=226
03/18/2022 17:04:53 - INFO - __main__ - Step 690 Global step 690 Train loss 1.431778 on epoch=229
03/18/2022 17:04:58 - INFO - __main__ - Step 700 Global step 700 Train loss 1.381389 on epoch=233
03/18/2022 17:04:58 - INFO - __main__ - Global step 700 Train loss 1.391405 ACC 0.34375 on epoch=233
03/18/2022 17:05:03 - INFO - __main__ - Step 710 Global step 710 Train loss 1.279178 on epoch=236
03/18/2022 17:05:08 - INFO - __main__ - Step 720 Global step 720 Train loss 1.502353 on epoch=239
03/18/2022 17:05:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.300862 on epoch=243
03/18/2022 17:05:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.595421 on epoch=246
03/18/2022 17:05:24 - INFO - __main__ - Step 750 Global step 750 Train loss 1.237308 on epoch=249
03/18/2022 17:05:24 - INFO - __main__ - Global step 750 Train loss 1.383024 ACC 0.40625 on epoch=249
03/18/2022 17:05:30 - INFO - __main__ - Step 760 Global step 760 Train loss 1.006397 on epoch=253
03/18/2022 17:05:35 - INFO - __main__ - Step 770 Global step 770 Train loss 1.290981 on epoch=256
03/18/2022 17:05:40 - INFO - __main__ - Step 780 Global step 780 Train loss 1.034180 on epoch=259
03/18/2022 17:05:45 - INFO - __main__ - Step 790 Global step 790 Train loss 1.200906 on epoch=263
03/18/2022 17:05:50 - INFO - __main__ - Step 800 Global step 800 Train loss 1.310360 on epoch=266
03/18/2022 17:05:51 - INFO - __main__ - Global step 800 Train loss 1.168565 ACC 0.4375 on epoch=266
03/18/2022 17:05:56 - INFO - __main__ - Step 810 Global step 810 Train loss 1.006027 on epoch=269
03/18/2022 17:06:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.889805 on epoch=273
03/18/2022 17:06:06 - INFO - __main__ - Step 830 Global step 830 Train loss 1.056862 on epoch=276
03/18/2022 17:06:11 - INFO - __main__ - Step 840 Global step 840 Train loss 1.040410 on epoch=279
03/18/2022 17:06:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.972877 on epoch=283
03/18/2022 17:06:17 - INFO - __main__ - Global step 850 Train loss 0.993196 ACC 0.03125 on epoch=283
03/18/2022 17:06:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.608104 on epoch=286
03/18/2022 17:06:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.496778 on epoch=289
03/18/2022 17:06:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.345281 on epoch=293
03/18/2022 17:06:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.200269 on epoch=296
03/18/2022 17:06:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.152534 on epoch=299
03/18/2022 17:06:43 - INFO - __main__ - Global step 900 Train loss 0.360593 ACC 0.65625 on epoch=299
03/18/2022 17:06:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:06:43 - INFO - __main__ - Printing 3 examples
03/18/2022 17:06:43 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:06:43 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:43 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:06:43 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:43 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:06:43 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:06:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:06:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:06:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:06:44 - INFO - __main__ - Printing 3 examples
03/18/2022 17:06:44 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:06:44 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:44 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:06:44 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:44 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:06:44 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:44 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:06:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:06:44 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:06:44 - INFO - __main__ - save last model!
03/18/2022 17:06:51 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:06:52 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:06:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:06:52 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:06:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:52 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:06:52 - INFO - __main__ - ['neutral']
03/18/2022 17:06:52 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:06:52 - INFO - __main__ - ['entailment']
03/18/2022 17:06:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:06:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:06:52 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:06:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0001_8_predictions.txt
03/18/2022 17:06:53 - INFO - __main__ - ACC on test data: 0.6786
03/18/2022 17:06:53 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0001, bsz=8, dev_performance=0.65625, test_performance=0.6785714285714286
03/18/2022 17:06:53 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0005, bsz=8 ...
03/18/2022 17:06:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:06:54 - INFO - __main__ - Starting training!
03/18/2022 17:06:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:06:54 - INFO - __main__ - Printing 3 examples
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:06:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:06:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:06:54 - INFO - __main__ - Printing 3 examples
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:06:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:06:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:06:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:06:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:07:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:07:05 - INFO - __main__ - Starting training!
03/18/2022 17:07:09 - INFO - __main__ - Step 10 Global step 10 Train loss 23.639183 on epoch=3
03/18/2022 17:07:13 - INFO - __main__ - Step 20 Global step 20 Train loss 16.101271 on epoch=6
03/18/2022 17:07:18 - INFO - __main__ - Step 30 Global step 30 Train loss 12.022154 on epoch=9
03/18/2022 17:07:23 - INFO - __main__ - Step 40 Global step 40 Train loss 10.166967 on epoch=13
03/18/2022 17:07:28 - INFO - __main__ - Step 50 Global step 50 Train loss 9.602284 on epoch=16
03/18/2022 17:07:29 - INFO - __main__ - Global step 50 Train loss 14.306371 ACC 0.0 on epoch=16
03/18/2022 17:07:35 - INFO - __main__ - Step 60 Global step 60 Train loss 7.979054 on epoch=19
03/18/2022 17:07:40 - INFO - __main__ - Step 70 Global step 70 Train loss 6.557456 on epoch=23
03/18/2022 17:07:45 - INFO - __main__ - Step 80 Global step 80 Train loss 5.580970 on epoch=26
03/18/2022 17:07:50 - INFO - __main__ - Step 90 Global step 90 Train loss 3.963857 on epoch=29
03/18/2022 17:07:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.010280 on epoch=33
03/18/2022 17:07:55 - INFO - __main__ - Global step 100 Train loss 5.018323 ACC 0.0 on epoch=33
03/18/2022 17:08:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.563199 on epoch=36
03/18/2022 17:08:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.451412 on epoch=39
03/18/2022 17:08:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.476192 on epoch=43
03/18/2022 17:08:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.475749 on epoch=46
03/18/2022 17:08:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.465932 on epoch=49
03/18/2022 17:08:21 - INFO - __main__ - Global step 150 Train loss 0.486497 ACC 0.0 on epoch=49
03/18/2022 17:08:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.412497 on epoch=53
03/18/2022 17:08:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.446163 on epoch=56
03/18/2022 17:08:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.446054 on epoch=59
03/18/2022 17:08:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.396029 on epoch=63
03/18/2022 17:08:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.385260 on epoch=66
03/18/2022 17:08:46 - INFO - __main__ - Global step 200 Train loss 0.417201 ACC 0.5 on epoch=66
03/18/2022 17:08:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.416409 on epoch=69
03/18/2022 17:08:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.381714 on epoch=73
03/18/2022 17:09:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.373705 on epoch=76
03/18/2022 17:09:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.486941 on epoch=79
03/18/2022 17:09:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.562384 on epoch=83
03/18/2022 17:09:13 - INFO - __main__ - Global step 250 Train loss 0.644231 ACC 0.5 on epoch=83
03/18/2022 17:09:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.881765 on epoch=86
03/18/2022 17:09:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.327070 on epoch=89
03/18/2022 17:09:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.337422 on epoch=93
03/18/2022 17:09:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.351233 on epoch=96
03/18/2022 17:09:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.303070 on epoch=99
03/18/2022 17:09:39 - INFO - __main__ - Global step 300 Train loss 0.440112 ACC 0.5 on epoch=99
03/18/2022 17:09:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.265185 on epoch=103
03/18/2022 17:09:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.153997 on epoch=106
03/18/2022 17:09:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.093037 on epoch=109
03/18/2022 17:09:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.171543 on epoch=113
03/18/2022 17:10:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.082512 on epoch=116
03/18/2022 17:10:05 - INFO - __main__ - Global step 350 Train loss 0.153255 ACC 0.0625 on epoch=116
03/18/2022 17:10:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.047139 on epoch=119
03/18/2022 17:10:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.052420 on epoch=123
03/18/2022 17:10:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.028491 on epoch=126
03/18/2022 17:10:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.015978 on epoch=129
03/18/2022 17:10:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.032450 on epoch=133
03/18/2022 17:10:30 - INFO - __main__ - Global step 400 Train loss 0.035296 ACC 0.65625 on epoch=133
03/18/2022 17:10:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.027793 on epoch=136
03/18/2022 17:10:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.016402 on epoch=139
03/18/2022 17:10:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.018943 on epoch=143
03/18/2022 17:10:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008209 on epoch=146
03/18/2022 17:10:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.029038 on epoch=149
03/18/2022 17:10:57 - INFO - __main__ - Global step 450 Train loss 0.020077 ACC 0.46875 on epoch=149
03/18/2022 17:11:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.046755 on epoch=153
03/18/2022 17:11:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.006056 on epoch=156
03/18/2022 17:11:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005759 on epoch=159
03/18/2022 17:11:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003503 on epoch=163
03/18/2022 17:11:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002929 on epoch=166
03/18/2022 17:11:23 - INFO - __main__ - Global step 500 Train loss 0.013000 ACC 0.40625 on epoch=166
03/18/2022 17:11:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.008600 on epoch=169
03/18/2022 17:11:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002086 on epoch=173
03/18/2022 17:11:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005664 on epoch=176
03/18/2022 17:11:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004004 on epoch=179
03/18/2022 17:11:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.009105 on epoch=183
03/18/2022 17:11:49 - INFO - __main__ - Global step 550 Train loss 0.005892 ACC 0.09375 on epoch=183
03/18/2022 17:11:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017423 on epoch=186
03/18/2022 17:11:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003843 on epoch=189
03/18/2022 17:12:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001878 on epoch=193
03/18/2022 17:12:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000973 on epoch=196
03/18/2022 17:12:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000747 on epoch=199
03/18/2022 17:12:15 - INFO - __main__ - Global step 600 Train loss 0.004973 ACC 0.3125 on epoch=199
03/18/2022 17:12:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001072 on epoch=203
03/18/2022 17:12:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001218 on epoch=206
03/18/2022 17:12:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000790 on epoch=209
03/18/2022 17:12:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.004194 on epoch=213
03/18/2022 17:12:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007826 on epoch=216
03/18/2022 17:12:41 - INFO - __main__ - Global step 650 Train loss 0.003020 ACC 0.25 on epoch=216
03/18/2022 17:12:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001256 on epoch=219
03/18/2022 17:12:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.003102 on epoch=223
03/18/2022 17:12:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000383 on epoch=226
03/18/2022 17:13:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000255 on epoch=229
03/18/2022 17:13:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000524 on epoch=233
03/18/2022 17:13:07 - INFO - __main__ - Global step 700 Train loss 0.001104 ACC 0.21875 on epoch=233
03/18/2022 17:13:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000268 on epoch=236
03/18/2022 17:13:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000658 on epoch=239
03/18/2022 17:13:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000662 on epoch=243
03/18/2022 17:13:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000497 on epoch=246
03/18/2022 17:13:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000672 on epoch=249
03/18/2022 17:13:33 - INFO - __main__ - Global step 750 Train loss 0.000551 ACC 0.15625 on epoch=249
03/18/2022 17:13:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000259 on epoch=253
03/18/2022 17:13:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000280 on epoch=256
03/18/2022 17:13:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.010961 on epoch=259
03/18/2022 17:13:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000900 on epoch=263
03/18/2022 17:13:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000450 on epoch=266
03/18/2022 17:13:59 - INFO - __main__ - Global step 800 Train loss 0.002570 ACC 0.28125 on epoch=266
03/18/2022 17:14:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000580 on epoch=269
03/18/2022 17:14:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001125 on epoch=273
03/18/2022 17:14:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001221 on epoch=276
03/18/2022 17:14:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004163 on epoch=279
03/18/2022 17:14:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000130 on epoch=283
03/18/2022 17:14:25 - INFO - __main__ - Global step 850 Train loss 0.001444 ACC 0.21875 on epoch=283
03/18/2022 17:14:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000191 on epoch=286
03/18/2022 17:14:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000094 on epoch=289
03/18/2022 17:14:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000068 on epoch=293
03/18/2022 17:14:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000082 on epoch=296
03/18/2022 17:14:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000502 on epoch=299
03/18/2022 17:14:51 - INFO - __main__ - Global step 900 Train loss 0.000187 ACC 0.28125 on epoch=299
03/18/2022 17:14:51 - INFO - __main__ - save last model!
03/18/2022 17:14:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:14:51 - INFO - __main__ - Printing 3 examples
03/18/2022 17:14:51 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:14:51 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:51 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:14:51 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:51 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:14:51 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:14:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:14:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:14:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:14:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:14:52 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:14:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:52 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:14:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:52 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:14:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:14:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:14:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:14:58 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:14:58 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:14:58 - INFO - __main__ - Printing 3 examples
03/18/2022 17:14:58 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:14:58 - INFO - __main__ - ['contradiction']
03/18/2022 17:14:58 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:14:58 - INFO - __main__ - ['neutral']
03/18/2022 17:14:58 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:14:58 - INFO - __main__ - ['entailment']
03/18/2022 17:14:58 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:14:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:14:58 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:15:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0005_8_predictions.txt
03/18/2022 17:15:00 - INFO - __main__ - ACC on test data: 0.6429
03/18/2022 17:15:00 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.6428571428571429
03/18/2022 17:15:00 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0003, bsz=8 ...
03/18/2022 17:15:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:15:01 - INFO - __main__ - Printing 3 examples
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:15:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:15:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:15:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:15:01 - INFO - __main__ - Printing 3 examples
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:15:01 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:15:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:15:01 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:15:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:15:05 - INFO - __main__ - Starting training!
03/18/2022 17:15:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:15:11 - INFO - __main__ - Starting training!
03/18/2022 17:15:15 - INFO - __main__ - Step 10 Global step 10 Train loss 24.181927 on epoch=3
03/18/2022 17:15:20 - INFO - __main__ - Step 20 Global step 20 Train loss 19.097143 on epoch=6
03/18/2022 17:15:25 - INFO - __main__ - Step 30 Global step 30 Train loss 12.464950 on epoch=9
03/18/2022 17:15:30 - INFO - __main__ - Step 40 Global step 40 Train loss 10.472844 on epoch=13
03/18/2022 17:15:35 - INFO - __main__ - Step 50 Global step 50 Train loss 10.892766 on epoch=16
03/18/2022 17:15:36 - INFO - __main__ - Global step 50 Train loss 15.421926 ACC 0.0 on epoch=16
03/18/2022 17:15:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.145343 on epoch=19
03/18/2022 17:15:47 - INFO - __main__ - Step 70 Global step 70 Train loss 9.084954 on epoch=23
03/18/2022 17:15:52 - INFO - __main__ - Step 80 Global step 80 Train loss 8.477336 on epoch=26
03/18/2022 17:15:57 - INFO - __main__ - Step 90 Global step 90 Train loss 7.839208 on epoch=29
03/18/2022 17:16:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.513139 on epoch=33
03/18/2022 17:16:02 - INFO - __main__ - Global step 100 Train loss 8.411996 ACC 0.0 on epoch=33
03/18/2022 17:16:07 - INFO - __main__ - Step 110 Global step 110 Train loss 5.867490 on epoch=36
03/18/2022 17:16:12 - INFO - __main__ - Step 120 Global step 120 Train loss 5.522395 on epoch=39
03/18/2022 17:16:17 - INFO - __main__ - Step 130 Global step 130 Train loss 4.352760 on epoch=43
03/18/2022 17:16:22 - INFO - __main__ - Step 140 Global step 140 Train loss 3.333195 on epoch=46
03/18/2022 17:16:27 - INFO - __main__ - Step 150 Global step 150 Train loss 2.621188 on epoch=49
03/18/2022 17:16:28 - INFO - __main__ - Global step 150 Train loss 4.339406 ACC 0.0 on epoch=49
03/18/2022 17:16:33 - INFO - __main__ - Step 160 Global step 160 Train loss 2.582572 on epoch=53
03/18/2022 17:16:38 - INFO - __main__ - Step 170 Global step 170 Train loss 2.876990 on epoch=56
03/18/2022 17:16:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.615855 on epoch=59
03/18/2022 17:16:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.355668 on epoch=63
03/18/2022 17:16:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.216587 on epoch=66
03/18/2022 17:16:53 - INFO - __main__ - Global step 200 Train loss 1.329535 ACC 0.65625 on epoch=66
03/18/2022 17:16:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.129837 on epoch=69
03/18/2022 17:17:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.057241 on epoch=73
03/18/2022 17:17:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.030289 on epoch=76
03/18/2022 17:17:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.018729 on epoch=79
03/18/2022 17:17:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.032494 on epoch=83
03/18/2022 17:17:20 - INFO - __main__ - Global step 250 Train loss 0.053718 ACC 0.65625 on epoch=83
03/18/2022 17:17:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.010956 on epoch=86
03/18/2022 17:17:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.014298 on epoch=89
03/18/2022 17:17:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.008558 on epoch=93
03/18/2022 17:17:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.014648 on epoch=96
03/18/2022 17:17:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002077 on epoch=99
03/18/2022 17:17:45 - INFO - __main__ - Global step 300 Train loss 0.010107 ACC 0.65625 on epoch=99
03/18/2022 17:17:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.011903 on epoch=103
03/18/2022 17:17:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.005875 on epoch=106
03/18/2022 17:18:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001588 on epoch=109
03/18/2022 17:18:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002487 on epoch=113
03/18/2022 17:18:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.003111 on epoch=116
03/18/2022 17:18:11 - INFO - __main__ - Global step 350 Train loss 0.004993 ACC 0.6875 on epoch=116
03/18/2022 17:18:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.013461 on epoch=119
03/18/2022 17:18:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001481 on epoch=123
03/18/2022 17:18:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002388 on epoch=126
03/18/2022 17:18:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000972 on epoch=129
03/18/2022 17:18:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002451 on epoch=133
03/18/2022 17:18:37 - INFO - __main__ - Global step 400 Train loss 0.004151 ACC 0.6875 on epoch=133
03/18/2022 17:18:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003450 on epoch=136
03/18/2022 17:18:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001182 on epoch=139
03/18/2022 17:18:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000319 on epoch=143
03/18/2022 17:18:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000525 on epoch=146
03/18/2022 17:19:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000303 on epoch=149
03/18/2022 17:19:02 - INFO - __main__ - Global step 450 Train loss 0.001156 ACC 0.75 on epoch=149
03/18/2022 17:19:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000430 on epoch=153
03/18/2022 17:19:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000120 on epoch=156
03/18/2022 17:19:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000203 on epoch=159
03/18/2022 17:19:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000384 on epoch=163
03/18/2022 17:19:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000062 on epoch=166
03/18/2022 17:19:28 - INFO - __main__ - Global step 500 Train loss 0.000240 ACC 0.75 on epoch=166
03/18/2022 17:19:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000401 on epoch=169
03/18/2022 17:19:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000103 on epoch=173
03/18/2022 17:19:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000069 on epoch=176
03/18/2022 17:19:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.026835 on epoch=179
03/18/2022 17:19:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005241 on epoch=183
03/18/2022 17:19:54 - INFO - __main__ - Global step 550 Train loss 0.006530 ACC 0.59375 on epoch=183
03/18/2022 17:19:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001144 on epoch=186
03/18/2022 17:20:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000657 on epoch=189
03/18/2022 17:20:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000180 on epoch=193
03/18/2022 17:20:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000735 on epoch=196
03/18/2022 17:20:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000059 on epoch=199
03/18/2022 17:20:20 - INFO - __main__ - Global step 600 Train loss 0.000555 ACC 0.71875 on epoch=199
03/18/2022 17:20:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000086 on epoch=203
03/18/2022 17:20:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000151 on epoch=206
03/18/2022 17:20:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000107 on epoch=209
03/18/2022 17:20:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000250 on epoch=213
03/18/2022 17:20:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000095 on epoch=216
03/18/2022 17:20:45 - INFO - __main__ - Global step 650 Train loss 0.000138 ACC 0.71875 on epoch=216
03/18/2022 17:20:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.003318 on epoch=219
03/18/2022 17:20:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000192 on epoch=223
03/18/2022 17:20:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000296 on epoch=226
03/18/2022 17:21:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000212 on epoch=229
03/18/2022 17:21:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000131 on epoch=233
03/18/2022 17:21:10 - INFO - __main__ - Global step 700 Train loss 0.000830 ACC 0.71875 on epoch=233
03/18/2022 17:21:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000087 on epoch=236
03/18/2022 17:21:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000063 on epoch=239
03/18/2022 17:21:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000081 on epoch=243
03/18/2022 17:21:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000056 on epoch=246
03/18/2022 17:21:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000073 on epoch=249
03/18/2022 17:21:35 - INFO - __main__ - Global step 750 Train loss 0.000072 ACC 0.71875 on epoch=249
03/18/2022 17:21:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000263 on epoch=253
03/18/2022 17:21:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000025 on epoch=256
03/18/2022 17:21:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000027 on epoch=259
03/18/2022 17:21:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000022 on epoch=263
03/18/2022 17:22:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.267048 on epoch=266
03/18/2022 17:22:01 - INFO - __main__ - Global step 800 Train loss 0.053477 ACC 0.6875 on epoch=266
03/18/2022 17:22:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.079580 on epoch=269
03/18/2022 17:22:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000050 on epoch=273
03/18/2022 17:22:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.015375 on epoch=276
03/18/2022 17:22:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.210052 on epoch=279
03/18/2022 17:22:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.321108 on epoch=283
03/18/2022 17:22:26 - INFO - __main__ - Global step 850 Train loss 0.125233 ACC 0.65625 on epoch=283
03/18/2022 17:22:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.486075 on epoch=286
03/18/2022 17:22:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.262210 on epoch=289
03/18/2022 17:22:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.487778 on epoch=293
03/18/2022 17:22:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.168157 on epoch=296
03/18/2022 17:22:51 - INFO - __main__ - Step 900 Global step 900 Train loss 1.342622 on epoch=299
03/18/2022 17:22:52 - INFO - __main__ - Global step 900 Train loss 0.549369 ACC 0.0 on epoch=299
03/18/2022 17:22:52 - INFO - __main__ - save last model!
03/18/2022 17:22:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:22:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:22:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:22:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:22:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:22:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:22:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:22:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:22:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:22:59 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:22:59 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:22:59 - INFO - __main__ - Printing 3 examples
03/18/2022 17:22:59 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:22:59 - INFO - __main__ - ['contradiction']
03/18/2022 17:22:59 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:22:59 - INFO - __main__ - ['neutral']
03/18/2022 17:22:59 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:22:59 - INFO - __main__ - ['entailment']
03/18/2022 17:22:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:22:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:22:59 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:23:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0003_8_predictions.txt
03/18/2022 17:23:01 - INFO - __main__ - ACC on test data: 0.6607
03/18/2022 17:23:01 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0003, bsz=8, dev_performance=0.75, test_performance=0.6607142857142857
03/18/2022 17:23:01 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0002, bsz=8 ...
03/18/2022 17:23:02 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:23:02 - INFO - __main__ - Printing 3 examples
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:23:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:23:02 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:23:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:23:02 - INFO - __main__ - Printing 3 examples
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:23:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:23:02 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:23:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:23:02 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:23:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:23:03 - INFO - __main__ - Starting training!
03/18/2022 17:23:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:23:12 - INFO - __main__ - Starting training!
03/18/2022 17:23:16 - INFO - __main__ - Step 10 Global step 10 Train loss 24.327145 on epoch=3
03/18/2022 17:23:21 - INFO - __main__ - Step 20 Global step 20 Train loss 20.754482 on epoch=6
03/18/2022 17:23:26 - INFO - __main__ - Step 30 Global step 30 Train loss 16.694517 on epoch=9
03/18/2022 17:23:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.960553 on epoch=13
03/18/2022 17:23:36 - INFO - __main__ - Step 50 Global step 50 Train loss 11.213639 on epoch=16
03/18/2022 17:23:37 - INFO - __main__ - Global step 50 Train loss 17.190067 ACC 0.03125 on epoch=16
03/18/2022 17:23:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.851874 on epoch=19
03/18/2022 17:23:47 - INFO - __main__ - Step 70 Global step 70 Train loss 10.456504 on epoch=23
03/18/2022 17:23:52 - INFO - __main__ - Step 80 Global step 80 Train loss 10.233584 on epoch=26
03/18/2022 17:23:57 - INFO - __main__ - Step 90 Global step 90 Train loss 9.010965 on epoch=29
03/18/2022 17:24:02 - INFO - __main__ - Step 100 Global step 100 Train loss 9.004260 on epoch=33
03/18/2022 17:24:03 - INFO - __main__ - Global step 100 Train loss 9.911437 ACC 0.0 on epoch=33
03/18/2022 17:24:08 - INFO - __main__ - Step 110 Global step 110 Train loss 8.892572 on epoch=36
03/18/2022 17:24:13 - INFO - __main__ - Step 120 Global step 120 Train loss 8.396235 on epoch=39
03/18/2022 17:24:18 - INFO - __main__ - Step 130 Global step 130 Train loss 8.216691 on epoch=43
03/18/2022 17:24:23 - INFO - __main__ - Step 140 Global step 140 Train loss 7.297095 on epoch=46
03/18/2022 17:24:28 - INFO - __main__ - Step 150 Global step 150 Train loss 6.681160 on epoch=49
03/18/2022 17:24:29 - INFO - __main__ - Global step 150 Train loss 7.896750 ACC 0.0 on epoch=49
03/18/2022 17:24:34 - INFO - __main__ - Step 160 Global step 160 Train loss 5.746166 on epoch=53
03/18/2022 17:24:39 - INFO - __main__ - Step 170 Global step 170 Train loss 5.474674 on epoch=56
03/18/2022 17:24:44 - INFO - __main__ - Step 180 Global step 180 Train loss 4.688749 on epoch=59
03/18/2022 17:24:49 - INFO - __main__ - Step 190 Global step 190 Train loss 4.274948 on epoch=63
03/18/2022 17:24:54 - INFO - __main__ - Step 200 Global step 200 Train loss 3.803706 on epoch=66
03/18/2022 17:24:54 - INFO - __main__ - Global step 200 Train loss 4.797648 ACC 0.0 on epoch=66
03/18/2022 17:24:59 - INFO - __main__ - Step 210 Global step 210 Train loss 2.789459 on epoch=69
03/18/2022 17:25:04 - INFO - __main__ - Step 220 Global step 220 Train loss 2.589547 on epoch=73
03/18/2022 17:25:09 - INFO - __main__ - Step 230 Global step 230 Train loss 2.943612 on epoch=76
03/18/2022 17:25:14 - INFO - __main__ - Step 240 Global step 240 Train loss 2.351763 on epoch=79
03/18/2022 17:25:19 - INFO - __main__ - Step 250 Global step 250 Train loss 2.249332 on epoch=83
03/18/2022 17:25:20 - INFO - __main__ - Global step 250 Train loss 2.584743 ACC 0.0 on epoch=83
03/18/2022 17:25:25 - INFO - __main__ - Step 260 Global step 260 Train loss 2.866132 on epoch=86
03/18/2022 17:25:30 - INFO - __main__ - Step 270 Global step 270 Train loss 2.501105 on epoch=89
03/18/2022 17:25:35 - INFO - __main__ - Step 280 Global step 280 Train loss 2.316021 on epoch=93
03/18/2022 17:25:40 - INFO - __main__ - Step 290 Global step 290 Train loss 2.335426 on epoch=96
03/18/2022 17:25:45 - INFO - __main__ - Step 300 Global step 300 Train loss 2.012996 on epoch=99
03/18/2022 17:25:45 - INFO - __main__ - Global step 300 Train loss 2.406336 ACC 0.5 on epoch=99
03/18/2022 17:25:52 - INFO - __main__ - Step 310 Global step 310 Train loss 2.184464 on epoch=103
03/18/2022 17:25:57 - INFO - __main__ - Step 320 Global step 320 Train loss 1.903163 on epoch=106
03/18/2022 17:26:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.921901 on epoch=109
03/18/2022 17:26:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.786010 on epoch=113
03/18/2022 17:26:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.877430 on epoch=116
03/18/2022 17:26:12 - INFO - __main__ - Global step 350 Train loss 1.934594 ACC 0.0 on epoch=116
03/18/2022 17:26:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.835122 on epoch=119
03/18/2022 17:26:22 - INFO - __main__ - Step 370 Global step 370 Train loss 1.625843 on epoch=123
03/18/2022 17:26:27 - INFO - __main__ - Step 380 Global step 380 Train loss 1.386683 on epoch=126
03/18/2022 17:26:32 - INFO - __main__ - Step 390 Global step 390 Train loss 1.116596 on epoch=129
03/18/2022 17:26:37 - INFO - __main__ - Step 400 Global step 400 Train loss 1.191722 on epoch=133
03/18/2022 17:26:37 - INFO - __main__ - Global step 400 Train loss 1.431193 ACC 0.0 on epoch=133
03/18/2022 17:26:42 - INFO - __main__ - Step 410 Global step 410 Train loss 1.131562 on epoch=136
03/18/2022 17:26:47 - INFO - __main__ - Step 420 Global step 420 Train loss 1.495683 on epoch=139
03/18/2022 17:26:52 - INFO - __main__ - Step 430 Global step 430 Train loss 1.543018 on epoch=143
03/18/2022 17:26:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.628235 on epoch=146
03/18/2022 17:27:02 - INFO - __main__ - Step 450 Global step 450 Train loss 1.712761 on epoch=149
03/18/2022 17:27:03 - INFO - __main__ - Global step 450 Train loss 1.502252 ACC 0.1875 on epoch=149
03/18/2022 17:27:08 - INFO - __main__ - Step 460 Global step 460 Train loss 1.151678 on epoch=153
03/18/2022 17:27:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.150041 on epoch=156
03/18/2022 17:27:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.456831 on epoch=159
03/18/2022 17:27:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.952940 on epoch=163
03/18/2022 17:27:27 - INFO - __main__ - Step 500 Global step 500 Train loss 1.151759 on epoch=166
03/18/2022 17:27:28 - INFO - __main__ - Global step 500 Train loss 1.172650 ACC 0.4375 on epoch=166
03/18/2022 17:27:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.978151 on epoch=169
03/18/2022 17:27:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.972270 on epoch=173
03/18/2022 17:27:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.943420 on epoch=176
03/18/2022 17:27:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.903328 on epoch=179
03/18/2022 17:27:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.960626 on epoch=183
03/18/2022 17:27:53 - INFO - __main__ - Global step 550 Train loss 0.951559 ACC 0.375 on epoch=183
03/18/2022 17:27:58 - INFO - __main__ - Step 560 Global step 560 Train loss 1.260393 on epoch=186
03/18/2022 17:28:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.889408 on epoch=189
03/18/2022 17:28:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.857814 on epoch=193
03/18/2022 17:28:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.658047 on epoch=196
03/18/2022 17:28:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.755334 on epoch=199
03/18/2022 17:28:18 - INFO - __main__ - Global step 600 Train loss 0.884199 ACC 0.375 on epoch=199
03/18/2022 17:28:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.658430 on epoch=203
03/18/2022 17:28:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.790698 on epoch=206
03/18/2022 17:28:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.534227 on epoch=209
03/18/2022 17:28:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.608508 on epoch=213
03/18/2022 17:28:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.559029 on epoch=216
03/18/2022 17:28:44 - INFO - __main__ - Global step 650 Train loss 0.630178 ACC 0.59375 on epoch=216
03/18/2022 17:28:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.535523 on epoch=219
03/18/2022 17:28:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.501557 on epoch=223
03/18/2022 17:29:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.449922 on epoch=226
03/18/2022 17:29:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.596069 on epoch=229
03/18/2022 17:29:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.388396 on epoch=233
03/18/2022 17:29:10 - INFO - __main__ - Global step 700 Train loss 0.494293 ACC 0.71875 on epoch=233
03/18/2022 17:29:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.276264 on epoch=236
03/18/2022 17:29:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.197445 on epoch=239
03/18/2022 17:29:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.288163 on epoch=243
03/18/2022 17:29:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.143688 on epoch=246
03/18/2022 17:29:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.280289 on epoch=249
03/18/2022 17:29:37 - INFO - __main__ - Global step 750 Train loss 0.237170 ACC 0.71875 on epoch=249
03/18/2022 17:29:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.094936 on epoch=253
03/18/2022 17:29:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.074032 on epoch=256
03/18/2022 17:29:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.130561 on epoch=259
03/18/2022 17:29:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.217471 on epoch=263
03/18/2022 17:30:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.024663 on epoch=266
03/18/2022 17:30:02 - INFO - __main__ - Global step 800 Train loss 0.108332 ACC 0.5 on epoch=266
03/18/2022 17:30:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.028978 on epoch=269
03/18/2022 17:30:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.094847 on epoch=273
03/18/2022 17:30:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.109675 on epoch=276
03/18/2022 17:30:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.050031 on epoch=279
03/18/2022 17:30:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.037274 on epoch=283
03/18/2022 17:30:27 - INFO - __main__ - Global step 850 Train loss 0.064161 ACC 0.78125 on epoch=283
03/18/2022 17:30:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.029587 on epoch=286
03/18/2022 17:30:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.055176 on epoch=289
03/18/2022 17:30:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.129034 on epoch=293
03/18/2022 17:30:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.091698 on epoch=296
03/18/2022 17:30:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.095159 on epoch=299
03/18/2022 17:30:54 - INFO - __main__ - Global step 900 Train loss 0.080131 ACC 0.75 on epoch=299
03/18/2022 17:30:54 - INFO - __main__ - save last model!
03/18/2022 17:30:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:30:54 - INFO - __main__ - Printing 3 examples
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:30:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:30:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:30:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:30:54 - INFO - __main__ - Printing 3 examples
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:30:54 - INFO - __main__ - ['contradiction']
03/18/2022 17:30:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:30:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:30:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:31:01 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:31:01 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:31:01 - INFO - __main__ - Printing 3 examples
03/18/2022 17:31:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:31:01 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:31:01 - INFO - __main__ - ['neutral']
03/18/2022 17:31:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:31:01 - INFO - __main__ - ['entailment']
03/18/2022 17:31:01 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:31:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:31:01 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:31:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0002_8_predictions.txt
03/18/2022 17:31:03 - INFO - __main__ - ACC on test data: 0.6964
03/18/2022 17:31:03 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0002, bsz=8, dev_performance=0.78125, test_performance=0.6964285714285714
03/18/2022 17:31:03 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0001, bsz=8 ...
03/18/2022 17:31:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:31:04 - INFO - __main__ - Printing 3 examples
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:31:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:31:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:31:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:31:04 - INFO - __main__ - Printing 3 examples
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/18/2022 17:31:04 - INFO - __main__ - ['contradiction']
03/18/2022 17:31:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:31:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:31:04 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:31:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:31:05 - INFO - __main__ - Starting training!
03/18/2022 17:31:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:31:14 - INFO - __main__ - Starting training!
03/18/2022 17:31:19 - INFO - __main__ - Step 10 Global step 10 Train loss 24.024834 on epoch=3
03/18/2022 17:31:23 - INFO - __main__ - Step 20 Global step 20 Train loss 23.553518 on epoch=6
03/18/2022 17:31:28 - INFO - __main__ - Step 30 Global step 30 Train loss 17.643917 on epoch=9
03/18/2022 17:31:33 - INFO - __main__ - Step 40 Global step 40 Train loss 13.694063 on epoch=13
03/18/2022 17:31:38 - INFO - __main__ - Step 50 Global step 50 Train loss 12.091622 on epoch=16
03/18/2022 17:31:40 - INFO - __main__ - Global step 50 Train loss 18.201591 ACC 0.0 on epoch=16
03/18/2022 17:31:46 - INFO - __main__ - Step 60 Global step 60 Train loss 12.346920 on epoch=19
03/18/2022 17:31:50 - INFO - __main__ - Step 70 Global step 70 Train loss 11.390164 on epoch=23
03/18/2022 17:31:55 - INFO - __main__ - Step 80 Global step 80 Train loss 10.762378 on epoch=26
03/18/2022 17:32:00 - INFO - __main__ - Step 90 Global step 90 Train loss 11.598393 on epoch=29
03/18/2022 17:32:05 - INFO - __main__ - Step 100 Global step 100 Train loss 10.316819 on epoch=33
03/18/2022 17:32:06 - INFO - __main__ - Global step 100 Train loss 11.282933 ACC 0.0 on epoch=33
03/18/2022 17:32:11 - INFO - __main__ - Step 110 Global step 110 Train loss 10.461116 on epoch=36
03/18/2022 17:32:16 - INFO - __main__ - Step 120 Global step 120 Train loss 10.081095 on epoch=39
03/18/2022 17:32:21 - INFO - __main__ - Step 130 Global step 130 Train loss 9.131267 on epoch=43
03/18/2022 17:32:26 - INFO - __main__ - Step 140 Global step 140 Train loss 9.452866 on epoch=46
03/18/2022 17:32:31 - INFO - __main__ - Step 150 Global step 150 Train loss 9.575666 on epoch=49
03/18/2022 17:32:31 - INFO - __main__ - Global step 150 Train loss 9.740401 ACC 0.0 on epoch=49
03/18/2022 17:32:36 - INFO - __main__ - Step 160 Global step 160 Train loss 8.839961 on epoch=53
03/18/2022 17:32:41 - INFO - __main__ - Step 170 Global step 170 Train loss 8.615837 on epoch=56
03/18/2022 17:32:46 - INFO - __main__ - Step 180 Global step 180 Train loss 9.326337 on epoch=59
03/18/2022 17:32:51 - INFO - __main__ - Step 190 Global step 190 Train loss 7.799944 on epoch=63
03/18/2022 17:32:56 - INFO - __main__ - Step 200 Global step 200 Train loss 7.714045 on epoch=66
03/18/2022 17:32:57 - INFO - __main__ - Global step 200 Train loss 8.459225 ACC 0.0 on epoch=66
03/18/2022 17:33:02 - INFO - __main__ - Step 210 Global step 210 Train loss 8.048897 on epoch=69
03/18/2022 17:33:07 - INFO - __main__ - Step 220 Global step 220 Train loss 7.423891 on epoch=73
03/18/2022 17:33:11 - INFO - __main__ - Step 230 Global step 230 Train loss 6.690946 on epoch=76
03/18/2022 17:33:16 - INFO - __main__ - Step 240 Global step 240 Train loss 6.833378 on epoch=79
03/18/2022 17:33:21 - INFO - __main__ - Step 250 Global step 250 Train loss 6.650389 on epoch=83
03/18/2022 17:33:22 - INFO - __main__ - Global step 250 Train loss 7.129500 ACC 0.0 on epoch=83
03/18/2022 17:33:27 - INFO - __main__ - Step 260 Global step 260 Train loss 6.724447 on epoch=86
03/18/2022 17:33:32 - INFO - __main__ - Step 270 Global step 270 Train loss 5.997027 on epoch=89
03/18/2022 17:33:37 - INFO - __main__ - Step 280 Global step 280 Train loss 5.424082 on epoch=93
03/18/2022 17:33:42 - INFO - __main__ - Step 290 Global step 290 Train loss 6.021345 on epoch=96
03/18/2022 17:33:47 - INFO - __main__ - Step 300 Global step 300 Train loss 5.340161 on epoch=99
03/18/2022 17:33:47 - INFO - __main__ - Global step 300 Train loss 5.901412 ACC 0.0 on epoch=99
03/18/2022 17:33:52 - INFO - __main__ - Step 310 Global step 310 Train loss 5.002875 on epoch=103
03/18/2022 17:33:57 - INFO - __main__ - Step 320 Global step 320 Train loss 4.041205 on epoch=106
03/18/2022 17:34:02 - INFO - __main__ - Step 330 Global step 330 Train loss 3.833804 on epoch=109
03/18/2022 17:34:07 - INFO - __main__ - Step 340 Global step 340 Train loss 3.564538 on epoch=113
03/18/2022 17:34:12 - INFO - __main__ - Step 350 Global step 350 Train loss 2.799204 on epoch=116
03/18/2022 17:34:13 - INFO - __main__ - Global step 350 Train loss 3.848325 ACC 0.0 on epoch=116
03/18/2022 17:34:18 - INFO - __main__ - Step 360 Global step 360 Train loss 3.144078 on epoch=119
03/18/2022 17:34:22 - INFO - __main__ - Step 370 Global step 370 Train loss 2.273309 on epoch=123
03/18/2022 17:34:27 - INFO - __main__ - Step 380 Global step 380 Train loss 3.355745 on epoch=126
03/18/2022 17:34:32 - INFO - __main__ - Step 390 Global step 390 Train loss 2.295022 on epoch=129
03/18/2022 17:34:37 - INFO - __main__ - Step 400 Global step 400 Train loss 2.304602 on epoch=133
03/18/2022 17:34:38 - INFO - __main__ - Global step 400 Train loss 2.674551 ACC 0.0 on epoch=133
03/18/2022 17:34:43 - INFO - __main__ - Step 410 Global step 410 Train loss 2.558824 on epoch=136
03/18/2022 17:34:48 - INFO - __main__ - Step 420 Global step 420 Train loss 2.907731 on epoch=139
03/18/2022 17:34:53 - INFO - __main__ - Step 430 Global step 430 Train loss 2.445355 on epoch=143
03/18/2022 17:34:58 - INFO - __main__ - Step 440 Global step 440 Train loss 2.867859 on epoch=146
03/18/2022 17:35:02 - INFO - __main__ - Step 450 Global step 450 Train loss 2.776511 on epoch=149
03/18/2022 17:35:03 - INFO - __main__ - Global step 450 Train loss 2.711256 ACC 0.0 on epoch=149
03/18/2022 17:35:08 - INFO - __main__ - Step 460 Global step 460 Train loss 2.264769 on epoch=153
03/18/2022 17:35:13 - INFO - __main__ - Step 470 Global step 470 Train loss 2.136827 on epoch=156
03/18/2022 17:35:18 - INFO - __main__ - Step 480 Global step 480 Train loss 2.634463 on epoch=159
03/18/2022 17:35:23 - INFO - __main__ - Step 490 Global step 490 Train loss 2.060624 on epoch=163
03/18/2022 17:35:28 - INFO - __main__ - Step 500 Global step 500 Train loss 2.172411 on epoch=166
03/18/2022 17:35:28 - INFO - __main__ - Global step 500 Train loss 2.253819 ACC 0.5 on epoch=166
03/18/2022 17:35:34 - INFO - __main__ - Step 510 Global step 510 Train loss 1.993601 on epoch=169
03/18/2022 17:35:39 - INFO - __main__ - Step 520 Global step 520 Train loss 2.145232 on epoch=173
03/18/2022 17:35:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.366989 on epoch=176
03/18/2022 17:35:49 - INFO - __main__ - Step 540 Global step 540 Train loss 1.971356 on epoch=179
03/18/2022 17:35:54 - INFO - __main__ - Step 550 Global step 550 Train loss 1.992892 on epoch=183
03/18/2022 17:35:54 - INFO - __main__ - Global step 550 Train loss 2.094014 ACC 0.0625 on epoch=183
03/18/2022 17:35:59 - INFO - __main__ - Step 560 Global step 560 Train loss 2.139822 on epoch=186
03/18/2022 17:36:04 - INFO - __main__ - Step 570 Global step 570 Train loss 1.975377 on epoch=189
03/18/2022 17:36:09 - INFO - __main__ - Step 580 Global step 580 Train loss 2.117924 on epoch=193
03/18/2022 17:36:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.612480 on epoch=196
03/18/2022 17:36:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.708601 on epoch=199
03/18/2022 17:36:19 - INFO - __main__ - Global step 600 Train loss 1.910841 ACC 0.4375 on epoch=199
03/18/2022 17:36:24 - INFO - __main__ - Step 610 Global step 610 Train loss 1.915873 on epoch=203
03/18/2022 17:36:29 - INFO - __main__ - Step 620 Global step 620 Train loss 1.539142 on epoch=206
03/18/2022 17:36:34 - INFO - __main__ - Step 630 Global step 630 Train loss 1.646138 on epoch=209
03/18/2022 17:36:39 - INFO - __main__ - Step 640 Global step 640 Train loss 1.238386 on epoch=213
03/18/2022 17:36:44 - INFO - __main__ - Step 650 Global step 650 Train loss 1.636510 on epoch=216
03/18/2022 17:36:45 - INFO - __main__ - Global step 650 Train loss 1.595209 ACC 0.1875 on epoch=216
03/18/2022 17:36:49 - INFO - __main__ - Step 660 Global step 660 Train loss 1.741681 on epoch=219
03/18/2022 17:36:54 - INFO - __main__ - Step 670 Global step 670 Train loss 1.729519 on epoch=223
03/18/2022 17:36:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.431385 on epoch=226
03/18/2022 17:37:04 - INFO - __main__ - Step 690 Global step 690 Train loss 1.374653 on epoch=229
03/18/2022 17:37:09 - INFO - __main__ - Step 700 Global step 700 Train loss 2.002478 on epoch=233
03/18/2022 17:37:10 - INFO - __main__ - Global step 700 Train loss 1.655943 ACC 0.1875 on epoch=233
03/18/2022 17:37:15 - INFO - __main__ - Step 710 Global step 710 Train loss 1.315463 on epoch=236
03/18/2022 17:37:20 - INFO - __main__ - Step 720 Global step 720 Train loss 1.160784 on epoch=239
03/18/2022 17:37:25 - INFO - __main__ - Step 730 Global step 730 Train loss 1.215247 on epoch=243
03/18/2022 17:37:30 - INFO - __main__ - Step 740 Global step 740 Train loss 1.189223 on epoch=246
03/18/2022 17:37:35 - INFO - __main__ - Step 750 Global step 750 Train loss 1.031696 on epoch=249
03/18/2022 17:37:35 - INFO - __main__ - Global step 750 Train loss 1.182483 ACC 0.3125 on epoch=249
03/18/2022 17:37:40 - INFO - __main__ - Step 760 Global step 760 Train loss 1.025221 on epoch=253
03/18/2022 17:37:45 - INFO - __main__ - Step 770 Global step 770 Train loss 1.081321 on epoch=256
03/18/2022 17:37:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.843318 on epoch=259
03/18/2022 17:37:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.576799 on epoch=263
03/18/2022 17:38:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.228785 on epoch=266
03/18/2022 17:38:01 - INFO - __main__ - Global step 800 Train loss 0.751089 ACC 0.46875 on epoch=266
03/18/2022 17:38:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.155062 on epoch=269
03/18/2022 17:38:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.145202 on epoch=273
03/18/2022 17:38:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.172883 on epoch=276
03/18/2022 17:38:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.097636 on epoch=279
03/18/2022 17:38:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.106300 on epoch=283
03/18/2022 17:38:26 - INFO - __main__ - Global step 850 Train loss 0.135416 ACC 0.46875 on epoch=283
03/18/2022 17:38:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.121058 on epoch=286
03/18/2022 17:38:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.076190 on epoch=289
03/18/2022 17:38:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.073901 on epoch=293
03/18/2022 17:38:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.059260 on epoch=296
03/18/2022 17:38:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.032083 on epoch=299
03/18/2022 17:38:52 - INFO - __main__ - Global step 900 Train loss 0.072499 ACC 0.5625 on epoch=299
03/18/2022 17:38:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:38:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:38:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:38:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:38:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:38:52 - INFO - __main__ - Printing 3 examples
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:38:52 - INFO - __main__ - ['contradiction']
03/18/2022 17:38:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:38:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:38:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:38:53 - INFO - __main__ - save last model!
03/18/2022 17:38:59 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:39:00 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:39:00 - INFO - __main__ - Printing 3 examples
03/18/2022 17:39:00 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:39:00 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:00 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:39:00 - INFO - __main__ - ['neutral']
03/18/2022 17:39:00 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:39:00 - INFO - __main__ - ['entailment']
03/18/2022 17:39:00 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:39:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:39:00 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:39:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0001_8_predictions.txt
03/18/2022 17:39:01 - INFO - __main__ - ACC on test data: 0.6607
03/18/2022 17:39:02 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.6607142857142857
03/18/2022 17:39:02 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0005, bsz=8 ...
03/18/2022 17:39:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:39:03 - INFO - __main__ - Printing 3 examples
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:39:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:39:03 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:39:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:39:03 - INFO - __main__ - Printing 3 examples
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:39:03 - INFO - __main__ - ['contradiction']
03/18/2022 17:39:03 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:39:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:39:03 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:39:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:39:03 - INFO - __main__ - Starting training!
03/18/2022 17:39:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:39:13 - INFO - __main__ - Starting training!
03/18/2022 17:39:17 - INFO - __main__ - Step 10 Global step 10 Train loss 22.762074 on epoch=3
03/18/2022 17:39:21 - INFO - __main__ - Step 20 Global step 20 Train loss 20.979076 on epoch=6
03/18/2022 17:39:26 - INFO - __main__ - Step 30 Global step 30 Train loss 14.101656 on epoch=9
03/18/2022 17:39:31 - INFO - __main__ - Step 40 Global step 40 Train loss 11.127287 on epoch=13
03/18/2022 17:39:36 - INFO - __main__ - Step 50 Global step 50 Train loss 9.486111 on epoch=16
03/18/2022 17:39:37 - INFO - __main__ - Global step 50 Train loss 15.691242 ACC 0.1875 on epoch=16
03/18/2022 17:39:43 - INFO - __main__ - Step 60 Global step 60 Train loss 8.888457 on epoch=19
03/18/2022 17:39:48 - INFO - __main__ - Step 70 Global step 70 Train loss 7.373435 on epoch=23
03/18/2022 17:39:53 - INFO - __main__ - Step 80 Global step 80 Train loss 6.202013 on epoch=26
03/18/2022 17:39:58 - INFO - __main__ - Step 90 Global step 90 Train loss 5.137794 on epoch=29
03/18/2022 17:40:03 - INFO - __main__ - Step 100 Global step 100 Train loss 3.316676 on epoch=33
03/18/2022 17:40:04 - INFO - __main__ - Global step 100 Train loss 6.183674 ACC 0.40625 on epoch=33
03/18/2022 17:40:10 - INFO - __main__ - Step 110 Global step 110 Train loss 2.405016 on epoch=36
03/18/2022 17:40:15 - INFO - __main__ - Step 120 Global step 120 Train loss 1.974441 on epoch=39
03/18/2022 17:40:20 - INFO - __main__ - Step 130 Global step 130 Train loss 2.241128 on epoch=43
03/18/2022 17:40:25 - INFO - __main__ - Step 140 Global step 140 Train loss 2.488488 on epoch=46
03/18/2022 17:40:30 - INFO - __main__ - Step 150 Global step 150 Train loss 1.751395 on epoch=49
03/18/2022 17:40:30 - INFO - __main__ - Global step 150 Train loss 2.172094 ACC 0.5 on epoch=49
03/18/2022 17:40:36 - INFO - __main__ - Step 160 Global step 160 Train loss 1.743259 on epoch=53
03/18/2022 17:40:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.890145 on epoch=56
03/18/2022 17:40:46 - INFO - __main__ - Step 180 Global step 180 Train loss 1.566582 on epoch=59
03/18/2022 17:40:51 - INFO - __main__ - Step 190 Global step 190 Train loss 1.426845 on epoch=63
03/18/2022 17:40:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.196272 on epoch=66
03/18/2022 17:40:57 - INFO - __main__ - Global step 200 Train loss 1.564620 ACC 0.0 on epoch=66
03/18/2022 17:41:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.211912 on epoch=69
03/18/2022 17:41:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.038578 on epoch=73
03/18/2022 17:41:12 - INFO - __main__ - Step 230 Global step 230 Train loss 1.032724 on epoch=76
03/18/2022 17:41:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.737538 on epoch=79
03/18/2022 17:41:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.526643 on epoch=83
03/18/2022 17:41:23 - INFO - __main__ - Global step 250 Train loss 0.909479 ACC 0.0 on epoch=83
03/18/2022 17:41:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.771911 on epoch=86
03/18/2022 17:41:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.994567 on epoch=89
03/18/2022 17:41:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.848406 on epoch=93
03/18/2022 17:41:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.603981 on epoch=96
03/18/2022 17:41:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.494779 on epoch=99
03/18/2022 17:41:48 - INFO - __main__ - Global step 300 Train loss 0.742729 ACC 0.6875 on epoch=99
03/18/2022 17:41:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.423744 on epoch=103
03/18/2022 17:41:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.411681 on epoch=106
03/18/2022 17:42:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.296381 on epoch=109
03/18/2022 17:42:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.110660 on epoch=113
03/18/2022 17:42:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.165369 on epoch=116
03/18/2022 17:42:15 - INFO - __main__ - Global step 350 Train loss 0.281567 ACC 0.71875 on epoch=116
03/18/2022 17:42:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.081480 on epoch=119
03/18/2022 17:42:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.038056 on epoch=123
03/18/2022 17:42:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.061029 on epoch=126
03/18/2022 17:42:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.013009 on epoch=129
03/18/2022 17:42:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.028727 on epoch=133
03/18/2022 17:42:41 - INFO - __main__ - Global step 400 Train loss 0.044460 ACC 0.78125 on epoch=133
03/18/2022 17:42:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.029458 on epoch=136
03/18/2022 17:42:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.013474 on epoch=139
03/18/2022 17:42:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.002142 on epoch=143
03/18/2022 17:43:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004650 on epoch=146
03/18/2022 17:43:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000639 on epoch=149
03/18/2022 17:43:07 - INFO - __main__ - Global step 450 Train loss 0.010073 ACC 0.8125 on epoch=149
03/18/2022 17:43:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001736 on epoch=153
03/18/2022 17:43:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.007066 on epoch=156
03/18/2022 17:43:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001855 on epoch=159
03/18/2022 17:43:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.016796 on epoch=163
03/18/2022 17:43:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017630 on epoch=166
03/18/2022 17:43:33 - INFO - __main__ - Global step 500 Train loss 0.009016 ACC 0.75 on epoch=166
03/18/2022 17:43:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000705 on epoch=169
03/18/2022 17:43:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.024823 on epoch=173
03/18/2022 17:43:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009278 on epoch=176
03/18/2022 17:43:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001127 on epoch=179
03/18/2022 17:43:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014259 on epoch=183
03/18/2022 17:43:59 - INFO - __main__ - Global step 550 Train loss 0.010039 ACC 0.84375 on epoch=183
03/18/2022 17:44:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002436 on epoch=186
03/18/2022 17:44:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.008811 on epoch=189
03/18/2022 17:44:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.022696 on epoch=193
03/18/2022 17:44:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.052158 on epoch=196
03/18/2022 17:44:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.470093 on epoch=199
03/18/2022 17:44:25 - INFO - __main__ - Global step 600 Train loss 0.111239 ACC 0.75 on epoch=199
03/18/2022 17:44:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.051965 on epoch=203
03/18/2022 17:44:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.007871 on epoch=206
03/18/2022 17:44:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.046909 on epoch=209
03/18/2022 17:44:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.221837 on epoch=213
03/18/2022 17:44:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.132799 on epoch=216
03/18/2022 17:44:51 - INFO - __main__ - Global step 650 Train loss 0.092276 ACC 0.90625 on epoch=216
03/18/2022 17:44:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006272 on epoch=219
03/18/2022 17:45:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000601 on epoch=223
03/18/2022 17:45:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.013425 on epoch=226
03/18/2022 17:45:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001912 on epoch=229
03/18/2022 17:45:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000411 on epoch=233
03/18/2022 17:45:18 - INFO - __main__ - Global step 700 Train loss 0.004524 ACC 0.9375 on epoch=233
03/18/2022 17:45:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000693 on epoch=236
03/18/2022 17:45:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002460 on epoch=239
03/18/2022 17:45:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000051 on epoch=243
03/18/2022 17:45:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000418 on epoch=246
03/18/2022 17:45:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000124 on epoch=249
03/18/2022 17:45:44 - INFO - __main__ - Global step 750 Train loss 0.000749 ACC 0.90625 on epoch=249
03/18/2022 17:45:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000279 on epoch=253
03/18/2022 17:45:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000023 on epoch=256
03/18/2022 17:45:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000021 on epoch=259
03/18/2022 17:46:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000245 on epoch=263
03/18/2022 17:46:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000747 on epoch=266
03/18/2022 17:46:10 - INFO - __main__ - Global step 800 Train loss 0.000263 ACC 0.875 on epoch=266
03/18/2022 17:46:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000128 on epoch=269
03/18/2022 17:46:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000029 on epoch=273
03/18/2022 17:46:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001732 on epoch=276
03/18/2022 17:46:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000247 on epoch=279
03/18/2022 17:46:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000272 on epoch=283
03/18/2022 17:46:36 - INFO - __main__ - Global step 850 Train loss 0.000482 ACC 0.875 on epoch=283
03/18/2022 17:46:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000414 on epoch=286
03/18/2022 17:46:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000070 on epoch=289
03/18/2022 17:46:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000102 on epoch=293
03/18/2022 17:46:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000014 on epoch=296
03/18/2022 17:47:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000016 on epoch=299
03/18/2022 17:47:02 - INFO - __main__ - Global step 900 Train loss 0.000123 ACC 0.875 on epoch=299
03/18/2022 17:47:02 - INFO - __main__ - save last model!
03/18/2022 17:47:02 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:47:02 - INFO - __main__ - Printing 3 examples
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:47:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:47:02 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:47:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:47:02 - INFO - __main__ - Printing 3 examples
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:47:02 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:02 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:47:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:47:02 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:47:09 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:47:10 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:47:10 - INFO - __main__ - Printing 3 examples
03/18/2022 17:47:10 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:47:10 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:10 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:47:10 - INFO - __main__ - ['neutral']
03/18/2022 17:47:10 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:47:10 - INFO - __main__ - ['entailment']
03/18/2022 17:47:10 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:47:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:47:10 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:47:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0005_8_predictions.txt
03/18/2022 17:47:11 - INFO - __main__ - ACC on test data: 0.7321
03/18/2022 17:47:12 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.7321428571428571
03/18/2022 17:47:12 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0003, bsz=8 ...
03/18/2022 17:47:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:47:13 - INFO - __main__ - Printing 3 examples
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:47:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:47:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:47:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:47:13 - INFO - __main__ - Printing 3 examples
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:47:13 - INFO - __main__ - ['contradiction']
03/18/2022 17:47:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:47:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:47:13 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:47:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:47:13 - INFO - __main__ - Starting training!
03/18/2022 17:47:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:47:23 - INFO - __main__ - Starting training!
03/18/2022 17:47:27 - INFO - __main__ - Step 10 Global step 10 Train loss 23.861065 on epoch=3
03/18/2022 17:47:32 - INFO - __main__ - Step 20 Global step 20 Train loss 18.277279 on epoch=6
03/18/2022 17:47:37 - INFO - __main__ - Step 30 Global step 30 Train loss 12.964890 on epoch=9
03/18/2022 17:47:42 - INFO - __main__ - Step 40 Global step 40 Train loss 11.243505 on epoch=13
03/18/2022 17:47:47 - INFO - __main__ - Step 50 Global step 50 Train loss 9.769276 on epoch=16
03/18/2022 17:47:48 - INFO - __main__ - Global step 50 Train loss 15.223203 ACC 0.0625 on epoch=16
03/18/2022 17:47:54 - INFO - __main__ - Step 60 Global step 60 Train loss 9.692982 on epoch=19
03/18/2022 17:47:59 - INFO - __main__ - Step 70 Global step 70 Train loss 8.961264 on epoch=23
03/18/2022 17:48:04 - INFO - __main__ - Step 80 Global step 80 Train loss 8.069929 on epoch=26
03/18/2022 17:48:09 - INFO - __main__ - Step 90 Global step 90 Train loss 8.085009 on epoch=29
03/18/2022 17:48:14 - INFO - __main__ - Step 100 Global step 100 Train loss 6.599860 on epoch=33
03/18/2022 17:48:14 - INFO - __main__ - Global step 100 Train loss 8.281809 ACC 0.0 on epoch=33
03/18/2022 17:48:19 - INFO - __main__ - Step 110 Global step 110 Train loss 6.008464 on epoch=36
03/18/2022 17:48:24 - INFO - __main__ - Step 120 Global step 120 Train loss 4.966251 on epoch=39
03/18/2022 17:48:29 - INFO - __main__ - Step 130 Global step 130 Train loss 3.911288 on epoch=43
03/18/2022 17:48:34 - INFO - __main__ - Step 140 Global step 140 Train loss 3.537575 on epoch=46
03/18/2022 17:48:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.823481 on epoch=49
03/18/2022 17:48:40 - INFO - __main__ - Global step 150 Train loss 4.249412 ACC 0.0 on epoch=49
03/18/2022 17:48:45 - INFO - __main__ - Step 160 Global step 160 Train loss 2.375172 on epoch=53
03/18/2022 17:48:50 - INFO - __main__ - Step 170 Global step 170 Train loss 2.234390 on epoch=56
03/18/2022 17:48:55 - INFO - __main__ - Step 180 Global step 180 Train loss 2.459449 on epoch=59
03/18/2022 17:49:00 - INFO - __main__ - Step 190 Global step 190 Train loss 2.087809 on epoch=63
03/18/2022 17:49:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.894350 on epoch=66
03/18/2022 17:49:05 - INFO - __main__ - Global step 200 Train loss 2.210234 ACC 0.0 on epoch=66
03/18/2022 17:49:10 - INFO - __main__ - Step 210 Global step 210 Train loss 2.367004 on epoch=69
03/18/2022 17:49:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.883371 on epoch=73
03/18/2022 17:49:20 - INFO - __main__ - Step 230 Global step 230 Train loss 1.719326 on epoch=76
03/18/2022 17:49:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.626700 on epoch=79
03/18/2022 17:49:30 - INFO - __main__ - Step 250 Global step 250 Train loss 1.787107 on epoch=83
03/18/2022 17:49:31 - INFO - __main__ - Global step 250 Train loss 1.876702 ACC 0.25 on epoch=83
03/18/2022 17:49:37 - INFO - __main__ - Step 260 Global step 260 Train loss 1.814368 on epoch=86
03/18/2022 17:49:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.470538 on epoch=89
03/18/2022 17:49:47 - INFO - __main__ - Step 280 Global step 280 Train loss 1.313484 on epoch=93
03/18/2022 17:49:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.281748 on epoch=96
03/18/2022 17:49:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.074506 on epoch=99
03/18/2022 17:49:58 - INFO - __main__ - Global step 300 Train loss 1.390929 ACC 0.0 on epoch=99
03/18/2022 17:50:02 - INFO - __main__ - Step 310 Global step 310 Train loss 1.249936 on epoch=103
03/18/2022 17:50:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.931299 on epoch=106
03/18/2022 17:50:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.864738 on epoch=109
03/18/2022 17:50:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.928931 on epoch=113
03/18/2022 17:50:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.404817 on epoch=116
03/18/2022 17:50:23 - INFO - __main__ - Global step 350 Train loss 1.075944 ACC 0.625 on epoch=116
03/18/2022 17:50:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.632381 on epoch=119
03/18/2022 17:50:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.307050 on epoch=123
03/18/2022 17:50:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.143657 on epoch=126
03/18/2022 17:50:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.097848 on epoch=129
03/18/2022 17:50:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.047353 on epoch=133
03/18/2022 17:50:49 - INFO - __main__ - Global step 400 Train loss 0.245658 ACC 0.625 on epoch=133
03/18/2022 17:50:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.022623 on epoch=136
03/18/2022 17:50:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.023760 on epoch=139
03/18/2022 17:51:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.020629 on epoch=143
03/18/2022 17:51:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005256 on epoch=146
03/18/2022 17:51:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003492 on epoch=149
03/18/2022 17:51:15 - INFO - __main__ - Global step 450 Train loss 0.015152 ACC 0.6875 on epoch=149
03/18/2022 17:51:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.004944 on epoch=153
03/18/2022 17:51:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.004592 on epoch=156
03/18/2022 17:51:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004066 on epoch=159
03/18/2022 17:51:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000709 on epoch=163
03/18/2022 17:51:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001057 on epoch=166
03/18/2022 17:51:41 - INFO - __main__ - Global step 500 Train loss 0.003074 ACC 0.6875 on epoch=166
03/18/2022 17:51:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002527 on epoch=169
03/18/2022 17:51:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000637 on epoch=173
03/18/2022 17:51:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001021 on epoch=176
03/18/2022 17:52:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001058 on epoch=179
03/18/2022 17:52:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004795 on epoch=183
03/18/2022 17:52:07 - INFO - __main__ - Global step 550 Train loss 0.002008 ACC 0.875 on epoch=183
03/18/2022 17:52:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000901 on epoch=186
03/18/2022 17:52:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001054 on epoch=189
03/18/2022 17:52:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000292 on epoch=193
03/18/2022 17:52:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000509 on epoch=196
03/18/2022 17:52:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000789 on epoch=199
03/18/2022 17:52:33 - INFO - __main__ - Global step 600 Train loss 0.000709 ACC 0.71875 on epoch=199
03/18/2022 17:52:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.013313 on epoch=203
03/18/2022 17:52:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002319 on epoch=206
03/18/2022 17:52:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000774 on epoch=209
03/18/2022 17:52:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002703 on epoch=213
03/18/2022 17:52:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000193 on epoch=216
03/18/2022 17:52:58 - INFO - __main__ - Global step 650 Train loss 0.003861 ACC 0.59375 on epoch=216
03/18/2022 17:53:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000286 on epoch=219
03/18/2022 17:53:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002165 on epoch=223
03/18/2022 17:53:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000297 on epoch=226
03/18/2022 17:53:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000201 on epoch=229
03/18/2022 17:53:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000278 on epoch=233
03/18/2022 17:53:24 - INFO - __main__ - Global step 700 Train loss 0.000645 ACC 0.78125 on epoch=233
03/18/2022 17:53:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000100 on epoch=236
03/18/2022 17:53:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000341 on epoch=239
03/18/2022 17:53:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000219 on epoch=243
03/18/2022 17:53:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000099 on epoch=246
03/18/2022 17:53:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000168 on epoch=249
03/18/2022 17:53:49 - INFO - __main__ - Global step 750 Train loss 0.000185 ACC 0.71875 on epoch=249
03/18/2022 17:53:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000055 on epoch=253
03/18/2022 17:53:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000068 on epoch=256
03/18/2022 17:54:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004739 on epoch=259
03/18/2022 17:54:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000197 on epoch=263
03/18/2022 17:54:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000094 on epoch=266
03/18/2022 17:54:15 - INFO - __main__ - Global step 800 Train loss 0.001031 ACC 0.65625 on epoch=266
03/18/2022 17:54:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000098 on epoch=269
03/18/2022 17:54:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.009708 on epoch=273
03/18/2022 17:54:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000675 on epoch=276
03/18/2022 17:54:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000162 on epoch=279
03/18/2022 17:54:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000430 on epoch=283
03/18/2022 17:54:40 - INFO - __main__ - Global step 850 Train loss 0.002215 ACC 0.90625 on epoch=283
03/18/2022 17:54:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000374 on epoch=286
03/18/2022 17:54:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000119 on epoch=289
03/18/2022 17:54:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000079 on epoch=293
03/18/2022 17:55:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000088 on epoch=296
03/18/2022 17:55:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000113 on epoch=299
03/18/2022 17:55:07 - INFO - __main__ - Global step 900 Train loss 0.000155 ACC 0.875 on epoch=299
03/18/2022 17:55:07 - INFO - __main__ - save last model!
03/18/2022 17:55:07 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:55:07 - INFO - __main__ - Printing 3 examples
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 17:55:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:55:07 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:55:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:55:07 - INFO - __main__ - Printing 3 examples
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:55:07 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:55:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:55:08 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:55:14 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 17:55:15 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 17:55:15 - INFO - __main__ - Printing 3 examples
03/18/2022 17:55:15 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 17:55:15 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:15 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 17:55:15 - INFO - __main__ - ['neutral']
03/18/2022 17:55:15 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 17:55:15 - INFO - __main__ - ['entailment']
03/18/2022 17:55:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:55:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:55:15 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 17:55:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0003_8_predictions.txt
03/18/2022 17:55:17 - INFO - __main__ - ACC on test data: 0.7857
03/18/2022 17:55:19 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0003, bsz=8, dev_performance=0.90625, test_performance=0.7857142857142857
03/18/2022 17:55:19 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0002, bsz=8 ...
03/18/2022 17:55:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 17:55:20 - INFO - __main__ - Printing 3 examples
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 17:55:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:55:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 17:55:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 17:55:20 - INFO - __main__ - Printing 3 examples
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 17:55:20 - INFO - __main__ - ['contradiction']
03/18/2022 17:55:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 17:55:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 17:55:20 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 17:55:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:55:20 - INFO - __main__ - Starting training!
03/18/2022 17:55:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 17:55:33 - INFO - __main__ - Starting training!
03/18/2022 17:55:37 - INFO - __main__ - Step 10 Global step 10 Train loss 22.984398 on epoch=3
03/18/2022 17:55:41 - INFO - __main__ - Step 20 Global step 20 Train loss 21.290867 on epoch=6
03/18/2022 17:55:46 - INFO - __main__ - Step 30 Global step 30 Train loss 13.875212 on epoch=9
03/18/2022 17:55:51 - INFO - __main__ - Step 40 Global step 40 Train loss 12.724720 on epoch=13
03/18/2022 17:55:56 - INFO - __main__ - Step 50 Global step 50 Train loss 11.192095 on epoch=16
03/18/2022 17:55:58 - INFO - __main__ - Global step 50 Train loss 16.413458 ACC 0.0625 on epoch=16
03/18/2022 17:56:03 - INFO - __main__ - Step 60 Global step 60 Train loss 10.405977 on epoch=19
03/18/2022 17:56:08 - INFO - __main__ - Step 70 Global step 70 Train loss 10.155141 on epoch=23
03/18/2022 17:56:13 - INFO - __main__ - Step 80 Global step 80 Train loss 9.404710 on epoch=26
03/18/2022 17:56:18 - INFO - __main__ - Step 90 Global step 90 Train loss 8.653197 on epoch=29
03/18/2022 17:56:23 - INFO - __main__ - Step 100 Global step 100 Train loss 8.383303 on epoch=33
03/18/2022 17:56:25 - INFO - __main__ - Global step 100 Train loss 9.400466 ACC 0.0 on epoch=33
03/18/2022 17:56:30 - INFO - __main__ - Step 110 Global step 110 Train loss 8.284357 on epoch=36
03/18/2022 17:56:35 - INFO - __main__ - Step 120 Global step 120 Train loss 7.772016 on epoch=39
03/18/2022 17:56:40 - INFO - __main__ - Step 130 Global step 130 Train loss 7.180003 on epoch=43
03/18/2022 17:56:45 - INFO - __main__ - Step 140 Global step 140 Train loss 6.505501 on epoch=46
03/18/2022 17:56:50 - INFO - __main__ - Step 150 Global step 150 Train loss 5.639555 on epoch=49
03/18/2022 17:56:51 - INFO - __main__ - Global step 150 Train loss 7.076287 ACC 0.09375 on epoch=49
03/18/2022 17:56:57 - INFO - __main__ - Step 160 Global step 160 Train loss 5.477738 on epoch=53
03/18/2022 17:57:02 - INFO - __main__ - Step 170 Global step 170 Train loss 4.885947 on epoch=56
03/18/2022 17:57:07 - INFO - __main__ - Step 180 Global step 180 Train loss 4.208758 on epoch=59
03/18/2022 17:57:12 - INFO - __main__ - Step 190 Global step 190 Train loss 2.928733 on epoch=63
03/18/2022 17:57:17 - INFO - __main__ - Step 200 Global step 200 Train loss 2.438099 on epoch=66
03/18/2022 17:57:18 - INFO - __main__ - Global step 200 Train loss 3.987855 ACC 0.0 on epoch=66
03/18/2022 17:57:23 - INFO - __main__ - Step 210 Global step 210 Train loss 3.379292 on epoch=69
03/18/2022 17:57:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.870544 on epoch=73
03/18/2022 17:57:33 - INFO - __main__ - Step 230 Global step 230 Train loss 2.542527 on epoch=76
03/18/2022 17:57:38 - INFO - __main__ - Step 240 Global step 240 Train loss 2.632930 on epoch=79
03/18/2022 17:57:43 - INFO - __main__ - Step 250 Global step 250 Train loss 2.316503 on epoch=83
03/18/2022 17:57:43 - INFO - __main__ - Global step 250 Train loss 2.548359 ACC 0.0 on epoch=83
03/18/2022 17:57:48 - INFO - __main__ - Step 260 Global step 260 Train loss 2.338629 on epoch=86
03/18/2022 17:57:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.988720 on epoch=89
03/18/2022 17:57:58 - INFO - __main__ - Step 280 Global step 280 Train loss 1.982762 on epoch=93
03/18/2022 17:58:03 - INFO - __main__ - Step 290 Global step 290 Train loss 2.313138 on epoch=96
03/18/2022 17:58:08 - INFO - __main__ - Step 300 Global step 300 Train loss 1.876236 on epoch=99
03/18/2022 17:58:09 - INFO - __main__ - Global step 300 Train loss 2.099897 ACC 0.0 on epoch=99
03/18/2022 17:58:14 - INFO - __main__ - Step 310 Global step 310 Train loss 1.969866 on epoch=103
03/18/2022 17:58:19 - INFO - __main__ - Step 320 Global step 320 Train loss 2.194320 on epoch=106
03/18/2022 17:58:24 - INFO - __main__ - Step 330 Global step 330 Train loss 1.607455 on epoch=109
03/18/2022 17:58:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.754799 on epoch=113
03/18/2022 17:58:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.679085 on epoch=116
03/18/2022 17:58:35 - INFO - __main__ - Global step 350 Train loss 1.841105 ACC 0.0 on epoch=116
03/18/2022 17:58:40 - INFO - __main__ - Step 360 Global step 360 Train loss 1.483059 on epoch=119
03/18/2022 17:58:45 - INFO - __main__ - Step 370 Global step 370 Train loss 1.319018 on epoch=123
03/18/2022 17:58:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.585284 on epoch=126
03/18/2022 17:58:55 - INFO - __main__ - Step 390 Global step 390 Train loss 1.446611 on epoch=129
03/18/2022 17:59:00 - INFO - __main__ - Step 400 Global step 400 Train loss 1.352792 on epoch=133
03/18/2022 17:59:00 - INFO - __main__ - Global step 400 Train loss 1.437353 ACC 0.0 on epoch=133
03/18/2022 17:59:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.583193 on epoch=136
03/18/2022 17:59:10 - INFO - __main__ - Step 420 Global step 420 Train loss 1.329501 on epoch=139
03/18/2022 17:59:15 - INFO - __main__ - Step 430 Global step 430 Train loss 1.193280 on epoch=143
03/18/2022 17:59:20 - INFO - __main__ - Step 440 Global step 440 Train loss 1.477520 on epoch=146
03/18/2022 17:59:25 - INFO - __main__ - Step 450 Global step 450 Train loss 1.237456 on epoch=149
03/18/2022 17:59:26 - INFO - __main__ - Global step 450 Train loss 1.364190 ACC 0.09375 on epoch=149
03/18/2022 17:59:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.828869 on epoch=153
03/18/2022 17:59:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.943370 on epoch=156
03/18/2022 17:59:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.551802 on epoch=159
03/18/2022 17:59:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.188043 on epoch=163
03/18/2022 17:59:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.217562 on epoch=166
03/18/2022 17:59:52 - INFO - __main__ - Global step 500 Train loss 0.545929 ACC 0.8125 on epoch=166
03/18/2022 17:59:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.065933 on epoch=169
03/18/2022 18:00:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.028875 on epoch=173
03/18/2022 18:00:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.061678 on epoch=176
03/18/2022 18:00:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024212 on epoch=179
03/18/2022 18:00:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.008615 on epoch=183
03/18/2022 18:00:18 - INFO - __main__ - Global step 550 Train loss 0.037863 ACC 0.65625 on epoch=183
03/18/2022 18:00:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009309 on epoch=186
03/18/2022 18:00:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.016718 on epoch=189
03/18/2022 18:00:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011953 on epoch=193
03/18/2022 18:00:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001589 on epoch=196
03/18/2022 18:00:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007547 on epoch=199
03/18/2022 18:00:44 - INFO - __main__ - Global step 600 Train loss 0.009423 ACC 0.8125 on epoch=199
03/18/2022 18:00:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.013019 on epoch=203
03/18/2022 18:00:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001048 on epoch=206
03/18/2022 18:00:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001691 on epoch=209
03/18/2022 18:01:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001178 on epoch=213
03/18/2022 18:01:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001144 on epoch=216
03/18/2022 18:01:10 - INFO - __main__ - Global step 650 Train loss 0.003616 ACC 0.84375 on epoch=216
03/18/2022 18:01:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001012 on epoch=219
03/18/2022 18:01:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000980 on epoch=223
03/18/2022 18:01:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.002748 on epoch=226
03/18/2022 18:01:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000702 on epoch=229
03/18/2022 18:01:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.473664 on epoch=233
03/18/2022 18:01:36 - INFO - __main__ - Global step 700 Train loss 0.095821 ACC 0.6875 on epoch=233
03/18/2022 18:01:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.957394 on epoch=236
03/18/2022 18:01:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.316956 on epoch=239
03/18/2022 18:01:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.488735 on epoch=243
03/18/2022 18:01:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.712881 on epoch=246
03/18/2022 18:02:01 - INFO - __main__ - Step 750 Global step 750 Train loss 1.085437 on epoch=249
03/18/2022 18:02:02 - INFO - __main__ - Global step 750 Train loss 0.712281 ACC 0.5625 on epoch=249
03/18/2022 18:02:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.703429 on epoch=253
03/18/2022 18:02:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.518724 on epoch=256
03/18/2022 18:02:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.468198 on epoch=259
03/18/2022 18:02:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.406516 on epoch=263
03/18/2022 18:02:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.425282 on epoch=266
03/18/2022 18:02:28 - INFO - __main__ - Global step 800 Train loss 0.504430 ACC 0.5 on epoch=266
03/18/2022 18:02:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.331244 on epoch=269
03/18/2022 18:02:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.434381 on epoch=273
03/18/2022 18:02:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.402972 on epoch=276
03/18/2022 18:02:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.357326 on epoch=279
03/18/2022 18:02:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.349575 on epoch=283
03/18/2022 18:02:54 - INFO - __main__ - Global step 850 Train loss 0.375100 ACC 0.65625 on epoch=283
03/18/2022 18:02:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.303215 on epoch=286
03/18/2022 18:03:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.347619 on epoch=289
03/18/2022 18:03:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.259588 on epoch=293
03/18/2022 18:03:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.277757 on epoch=296
03/18/2022 18:03:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.260601 on epoch=299
03/18/2022 18:03:20 - INFO - __main__ - Global step 900 Train loss 0.289756 ACC 0.8125 on epoch=299
03/18/2022 18:03:20 - INFO - __main__ - save last model!
03/18/2022 18:03:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 18:03:20 - INFO - __main__ - Printing 3 examples
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:03:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:03:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 18:03:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 18:03:20 - INFO - __main__ - Printing 3 examples
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 18:03:20 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:03:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:03:21 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 18:03:27 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:03:28 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 18:03:28 - INFO - __main__ - Printing 3 examples
03/18/2022 18:03:28 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 18:03:28 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:28 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 18:03:28 - INFO - __main__ - ['neutral']
03/18/2022 18:03:28 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 18:03:28 - INFO - __main__ - ['entailment']
03/18/2022 18:03:28 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:03:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:03:28 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 18:03:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0002_8_predictions.txt
03/18/2022 18:03:29 - INFO - __main__ - ACC on test data: 0.6964
03/18/2022 18:03:30 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.6964285714285714
03/18/2022 18:03:30 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0001, bsz=8 ...
03/18/2022 18:03:31 - INFO - __main__ - Start tokenizing ... 48 instances
03/18/2022 18:03:31 - INFO - __main__ - Printing 3 examples
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:03:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:03:31 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/18/2022 18:03:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 18:03:31 - INFO - __main__ - Printing 3 examples
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/18/2022 18:03:31 - INFO - __main__ - ['contradiction']
03/18/2022 18:03:31 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:03:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:03:31 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 18:03:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:03:33 - INFO - __main__ - Starting training!
03/18/2022 18:03:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:03:41 - INFO - __main__ - Starting training!
03/18/2022 18:03:45 - INFO - __main__ - Step 10 Global step 10 Train loss 22.849842 on epoch=3
03/18/2022 18:03:51 - INFO - __main__ - Step 20 Global step 20 Train loss 22.602551 on epoch=6
03/18/2022 18:03:56 - INFO - __main__ - Step 30 Global step 30 Train loss 17.976002 on epoch=9
03/18/2022 18:04:00 - INFO - __main__ - Step 40 Global step 40 Train loss 16.972614 on epoch=13
03/18/2022 18:04:05 - INFO - __main__ - Step 50 Global step 50 Train loss 15.114253 on epoch=16
03/18/2022 18:04:16 - INFO - __main__ - Global step 50 Train loss 19.103050 ACC 0.0 on epoch=16
03/18/2022 18:04:21 - INFO - __main__ - Step 60 Global step 60 Train loss 14.324321 on epoch=19
03/18/2022 18:04:26 - INFO - __main__ - Step 70 Global step 70 Train loss 12.081800 on epoch=23
03/18/2022 18:04:31 - INFO - __main__ - Step 80 Global step 80 Train loss 11.574195 on epoch=26
03/18/2022 18:04:36 - INFO - __main__ - Step 90 Global step 90 Train loss 11.625860 on epoch=29
03/18/2022 18:04:41 - INFO - __main__ - Step 100 Global step 100 Train loss 11.074059 on epoch=33
03/18/2022 18:04:42 - INFO - __main__ - Global step 100 Train loss 12.136047 ACC 0.0 on epoch=33
03/18/2022 18:04:47 - INFO - __main__ - Step 110 Global step 110 Train loss 10.585264 on epoch=36
03/18/2022 18:04:52 - INFO - __main__ - Step 120 Global step 120 Train loss 10.908677 on epoch=39
03/18/2022 18:04:57 - INFO - __main__ - Step 130 Global step 130 Train loss 10.146281 on epoch=43
03/18/2022 18:05:02 - INFO - __main__ - Step 140 Global step 140 Train loss 10.063967 on epoch=46
03/18/2022 18:05:07 - INFO - __main__ - Step 150 Global step 150 Train loss 9.921547 on epoch=49
03/18/2022 18:05:07 - INFO - __main__ - Global step 150 Train loss 10.325148 ACC 0.0 on epoch=49
03/18/2022 18:05:12 - INFO - __main__ - Step 160 Global step 160 Train loss 9.374818 on epoch=53
03/18/2022 18:05:17 - INFO - __main__ - Step 170 Global step 170 Train loss 9.070202 on epoch=56
03/18/2022 18:05:22 - INFO - __main__ - Step 180 Global step 180 Train loss 8.930036 on epoch=59
03/18/2022 18:05:27 - INFO - __main__ - Step 190 Global step 190 Train loss 8.490116 on epoch=63
03/18/2022 18:05:32 - INFO - __main__ - Step 200 Global step 200 Train loss 7.808676 on epoch=66
03/18/2022 18:05:33 - INFO - __main__ - Global step 200 Train loss 8.734770 ACC 0.0 on epoch=66
03/18/2022 18:05:38 - INFO - __main__ - Step 210 Global step 210 Train loss 8.431479 on epoch=69
03/18/2022 18:05:43 - INFO - __main__ - Step 220 Global step 220 Train loss 8.042021 on epoch=73
03/18/2022 18:05:48 - INFO - __main__ - Step 230 Global step 230 Train loss 7.746922 on epoch=76
03/18/2022 18:05:53 - INFO - __main__ - Step 240 Global step 240 Train loss 7.638916 on epoch=79
03/18/2022 18:05:58 - INFO - __main__ - Step 250 Global step 250 Train loss 7.166656 on epoch=83
03/18/2022 18:05:59 - INFO - __main__ - Global step 250 Train loss 7.805199 ACC 0.0 on epoch=83
03/18/2022 18:06:04 - INFO - __main__ - Step 260 Global step 260 Train loss 7.238828 on epoch=86
03/18/2022 18:06:09 - INFO - __main__ - Step 270 Global step 270 Train loss 6.752162 on epoch=89
03/18/2022 18:06:13 - INFO - __main__ - Step 280 Global step 280 Train loss 6.727719 on epoch=93
03/18/2022 18:06:18 - INFO - __main__ - Step 290 Global step 290 Train loss 6.254463 on epoch=96
03/18/2022 18:06:23 - INFO - __main__ - Step 300 Global step 300 Train loss 5.835128 on epoch=99
03/18/2022 18:06:24 - INFO - __main__ - Global step 300 Train loss 6.561660 ACC 0.0 on epoch=99
03/18/2022 18:06:29 - INFO - __main__ - Step 310 Global step 310 Train loss 5.331395 on epoch=103
03/18/2022 18:06:34 - INFO - __main__ - Step 320 Global step 320 Train loss 5.265955 on epoch=106
03/18/2022 18:06:39 - INFO - __main__ - Step 330 Global step 330 Train loss 5.028251 on epoch=109
03/18/2022 18:06:44 - INFO - __main__ - Step 340 Global step 340 Train loss 4.729470 on epoch=113
03/18/2022 18:06:49 - INFO - __main__ - Step 350 Global step 350 Train loss 3.644103 on epoch=116
03/18/2022 18:06:49 - INFO - __main__ - Global step 350 Train loss 4.799835 ACC 0.1875 on epoch=116
03/18/2022 18:06:56 - INFO - __main__ - Step 360 Global step 360 Train loss 4.151715 on epoch=119
03/18/2022 18:07:01 - INFO - __main__ - Step 370 Global step 370 Train loss 3.590949 on epoch=123
03/18/2022 18:07:06 - INFO - __main__ - Step 380 Global step 380 Train loss 3.908294 on epoch=126
03/18/2022 18:07:11 - INFO - __main__ - Step 390 Global step 390 Train loss 2.999217 on epoch=129
03/18/2022 18:07:16 - INFO - __main__ - Step 400 Global step 400 Train loss 3.111932 on epoch=133
03/18/2022 18:07:17 - INFO - __main__ - Global step 400 Train loss 3.552422 ACC 0.15625 on epoch=133
03/18/2022 18:07:22 - INFO - __main__ - Step 410 Global step 410 Train loss 3.486420 on epoch=136
03/18/2022 18:07:27 - INFO - __main__ - Step 420 Global step 420 Train loss 3.242224 on epoch=139
03/18/2022 18:07:31 - INFO - __main__ - Step 430 Global step 430 Train loss 2.783288 on epoch=143
03/18/2022 18:07:36 - INFO - __main__ - Step 440 Global step 440 Train loss 2.174154 on epoch=146
03/18/2022 18:07:41 - INFO - __main__ - Step 450 Global step 450 Train loss 2.873997 on epoch=149
03/18/2022 18:07:42 - INFO - __main__ - Global step 450 Train loss 2.912017 ACC 0.0 on epoch=149
03/18/2022 18:07:47 - INFO - __main__ - Step 460 Global step 460 Train loss 3.113546 on epoch=153
03/18/2022 18:07:52 - INFO - __main__ - Step 470 Global step 470 Train loss 2.083456 on epoch=156
03/18/2022 18:07:57 - INFO - __main__ - Step 480 Global step 480 Train loss 2.752341 on epoch=159
03/18/2022 18:08:02 - INFO - __main__ - Step 490 Global step 490 Train loss 2.261524 on epoch=163
03/18/2022 18:08:07 - INFO - __main__ - Step 500 Global step 500 Train loss 2.063297 on epoch=166
03/18/2022 18:08:08 - INFO - __main__ - Global step 500 Train loss 2.454832 ACC 0.0 on epoch=166
03/18/2022 18:08:13 - INFO - __main__ - Step 510 Global step 510 Train loss 2.457857 on epoch=169
03/18/2022 18:08:17 - INFO - __main__ - Step 520 Global step 520 Train loss 2.655579 on epoch=173
03/18/2022 18:08:22 - INFO - __main__ - Step 530 Global step 530 Train loss 2.283997 on epoch=176
03/18/2022 18:08:27 - INFO - __main__ - Step 540 Global step 540 Train loss 2.210850 on epoch=179
03/18/2022 18:08:32 - INFO - __main__ - Step 550 Global step 550 Train loss 2.767626 on epoch=183
03/18/2022 18:08:33 - INFO - __main__ - Global step 550 Train loss 2.475182 ACC 0.0 on epoch=183
03/18/2022 18:08:38 - INFO - __main__ - Step 560 Global step 560 Train loss 2.381965 on epoch=186
03/18/2022 18:08:43 - INFO - __main__ - Step 570 Global step 570 Train loss 2.773344 on epoch=189
03/18/2022 18:08:48 - INFO - __main__ - Step 580 Global step 580 Train loss 2.383110 on epoch=193
03/18/2022 18:08:53 - INFO - __main__ - Step 590 Global step 590 Train loss 2.043140 on epoch=196
03/18/2022 18:08:58 - INFO - __main__ - Step 600 Global step 600 Train loss 2.158372 on epoch=199
03/18/2022 18:08:59 - INFO - __main__ - Global step 600 Train loss 2.347986 ACC 0.0 on epoch=199
03/18/2022 18:09:04 - INFO - __main__ - Step 610 Global step 610 Train loss 2.087832 on epoch=203
03/18/2022 18:09:09 - INFO - __main__ - Step 620 Global step 620 Train loss 2.298213 on epoch=206
03/18/2022 18:09:14 - INFO - __main__ - Step 630 Global step 630 Train loss 2.074667 on epoch=209
03/18/2022 18:09:19 - INFO - __main__ - Step 640 Global step 640 Train loss 2.307605 on epoch=213
03/18/2022 18:09:24 - INFO - __main__ - Step 650 Global step 650 Train loss 2.428252 on epoch=216
03/18/2022 18:09:24 - INFO - __main__ - Global step 650 Train loss 2.239314 ACC 0.0 on epoch=216
03/18/2022 18:09:29 - INFO - __main__ - Step 660 Global step 660 Train loss 1.968043 on epoch=219
03/18/2022 18:09:35 - INFO - __main__ - Step 670 Global step 670 Train loss 2.000087 on epoch=223
03/18/2022 18:09:40 - INFO - __main__ - Step 680 Global step 680 Train loss 1.420291 on epoch=226
03/18/2022 18:09:45 - INFO - __main__ - Step 690 Global step 690 Train loss 2.493421 on epoch=229
03/18/2022 18:09:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.780602 on epoch=233
03/18/2022 18:09:50 - INFO - __main__ - Global step 700 Train loss 1.932489 ACC 0.0 on epoch=233
03/18/2022 18:09:55 - INFO - __main__ - Step 710 Global step 710 Train loss 2.275777 on epoch=236
03/18/2022 18:10:00 - INFO - __main__ - Step 720 Global step 720 Train loss 1.791532 on epoch=239
03/18/2022 18:10:05 - INFO - __main__ - Step 730 Global step 730 Train loss 2.009146 on epoch=243
03/18/2022 18:10:10 - INFO - __main__ - Step 740 Global step 740 Train loss 1.986465 on epoch=246
03/18/2022 18:10:15 - INFO - __main__ - Step 750 Global step 750 Train loss 1.939303 on epoch=249
03/18/2022 18:10:16 - INFO - __main__ - Global step 750 Train loss 2.000445 ACC 0.0 on epoch=249
03/18/2022 18:10:21 - INFO - __main__ - Step 760 Global step 760 Train loss 2.071703 on epoch=253
03/18/2022 18:10:26 - INFO - __main__ - Step 770 Global step 770 Train loss 1.612314 on epoch=256
03/18/2022 18:10:31 - INFO - __main__ - Step 780 Global step 780 Train loss 1.817325 on epoch=259
03/18/2022 18:10:36 - INFO - __main__ - Step 790 Global step 790 Train loss 1.265281 on epoch=263
03/18/2022 18:10:41 - INFO - __main__ - Step 800 Global step 800 Train loss 1.781494 on epoch=266
03/18/2022 18:10:42 - INFO - __main__ - Global step 800 Train loss 1.709623 ACC 0.0 on epoch=266
03/18/2022 18:10:47 - INFO - __main__ - Step 810 Global step 810 Train loss 1.312717 on epoch=269
03/18/2022 18:10:52 - INFO - __main__ - Step 820 Global step 820 Train loss 1.647924 on epoch=273
03/18/2022 18:10:57 - INFO - __main__ - Step 830 Global step 830 Train loss 1.511182 on epoch=276
03/18/2022 18:11:02 - INFO - __main__ - Step 840 Global step 840 Train loss 1.284966 on epoch=279
03/18/2022 18:11:07 - INFO - __main__ - Step 850 Global step 850 Train loss 1.277887 on epoch=283
03/18/2022 18:11:08 - INFO - __main__ - Global step 850 Train loss 1.406935 ACC 0.0 on epoch=283
03/18/2022 18:11:13 - INFO - __main__ - Step 860 Global step 860 Train loss 1.332614 on epoch=286
03/18/2022 18:11:18 - INFO - __main__ - Step 870 Global step 870 Train loss 1.508151 on epoch=289
03/18/2022 18:11:23 - INFO - __main__ - Step 880 Global step 880 Train loss 1.534449 on epoch=293
03/18/2022 18:11:28 - INFO - __main__ - Step 890 Global step 890 Train loss 1.343009 on epoch=296
03/18/2022 18:11:33 - INFO - __main__ - Step 900 Global step 900 Train loss 1.713760 on epoch=299
03/18/2022 18:11:34 - INFO - __main__ - Global step 900 Train loss 1.486397 ACC 0.0 on epoch=299
03/18/2022 18:11:34 - INFO - __main__ - save last model!
03/18/2022 18:11:40 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:11:41 - INFO - __main__ - Start tokenizing ... 56 instances
03/18/2022 18:11:41 - INFO - __main__ - Printing 3 examples
03/18/2022 18:11:41 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/18/2022 18:11:41 - INFO - __main__ - ['contradiction']
03/18/2022 18:11:41 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/18/2022 18:11:41 - INFO - __main__ - ['neutral']
03/18/2022 18:11:41 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/18/2022 18:11:41 - INFO - __main__ - ['entailment']
03/18/2022 18:11:41 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:11:41 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:11:41 - INFO - __main__ - Loaded 56 examples from test data
03/18/2022 18:11:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0001_8_predictions.txt
03/18/2022 18:11:42 - INFO - __main__ - ACC on test data: 0.1071
03/18/2022 18:11:43 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0001, bsz=8, dev_performance=0.1875, test_performance=0.10714285714285714
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (21987): No such process
Task: dbpedia_14, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/18/2022 18:11:49 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 18:11:49 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-dbpedia_14
03/18/2022 18:11:49 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 18:11:49 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-dbpedia_14
03/18/2022 18:11:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 18:11:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 18:11:50 - INFO - __main__ - args.device: cuda:0
03/18/2022 18:11:50 - INFO - __main__ - Using 2 gpus
03/18/2022 18:11:50 - INFO - __main__ - args.device: cuda:1
03/18/2022 18:11:50 - INFO - __main__ - Using 2 gpus
03/18/2022 18:11:50 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/18/2022 18:11:50 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/18/2022 18:11:55 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.0005, bsz=8 ...
03/18/2022 18:11:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:11:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:11:56 - INFO - __main__ - Printing 3 examples
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:11:56 - INFO - __main__ - Printing 3 examples
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:11:56 - INFO - __main__ - ['Animal']
03/18/2022 18:11:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:11:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:11:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:11:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:11:57 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:11:57 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:11:57 - INFO - __main__ - Printing 3 examples
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:11:57 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:11:57 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:11:57 - INFO - __main__ - Printing 3 examples
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:11:57 - INFO - __main__ - ['Animal']
03/18/2022 18:11:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:11:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:11:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:11:57 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:11:57 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:12:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:12:10 - INFO - __main__ - Starting training!
03/18/2022 18:12:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:12:10 - INFO - __main__ - Starting training!
03/18/2022 18:12:15 - INFO - __main__ - Step 10 Global step 10 Train loss 21.627476 on epoch=0
03/18/2022 18:12:19 - INFO - __main__ - Step 20 Global step 20 Train loss 17.592800 on epoch=1
03/18/2022 18:12:24 - INFO - __main__ - Step 30 Global step 30 Train loss 12.907498 on epoch=2
03/18/2022 18:12:29 - INFO - __main__ - Step 40 Global step 40 Train loss 11.268637 on epoch=2
03/18/2022 18:12:34 - INFO - __main__ - Step 50 Global step 50 Train loss 9.889603 on epoch=3
03/18/2022 18:12:38 - INFO - __main__ - Global step 50 Train loss 14.657204 Classification-F1 0.0 on epoch=3
03/18/2022 18:12:45 - INFO - __main__ - Step 60 Global step 60 Train loss 8.191944 on epoch=4
03/18/2022 18:12:50 - INFO - __main__ - Step 70 Global step 70 Train loss 8.383531 on epoch=4
03/18/2022 18:12:54 - INFO - __main__ - Step 80 Global step 80 Train loss 6.656740 on epoch=5
03/18/2022 18:12:59 - INFO - __main__ - Step 90 Global step 90 Train loss 5.681627 on epoch=6
03/18/2022 18:13:04 - INFO - __main__ - Step 100 Global step 100 Train loss 4.493013 on epoch=7
03/18/2022 18:13:07 - INFO - __main__ - Global step 100 Train loss 6.681372 Classification-F1 0.0 on epoch=7
03/18/2022 18:13:12 - INFO - __main__ - Step 110 Global step 110 Train loss 3.671910 on epoch=7
03/18/2022 18:13:17 - INFO - __main__ - Step 120 Global step 120 Train loss 3.507943 on epoch=8
03/18/2022 18:13:22 - INFO - __main__ - Step 130 Global step 130 Train loss 2.650837 on epoch=9
03/18/2022 18:13:27 - INFO - __main__ - Step 140 Global step 140 Train loss 3.341502 on epoch=9
03/18/2022 18:13:32 - INFO - __main__ - Step 150 Global step 150 Train loss 2.559005 on epoch=10
03/18/2022 18:13:35 - INFO - __main__ - Global step 150 Train loss 3.146240 Classification-F1 0.01185064935064935 on epoch=10
03/18/2022 18:13:42 - INFO - __main__ - Step 160 Global step 160 Train loss 2.425599 on epoch=11
03/18/2022 18:13:47 - INFO - __main__ - Step 170 Global step 170 Train loss 2.397511 on epoch=12
03/18/2022 18:13:52 - INFO - __main__ - Step 180 Global step 180 Train loss 2.366848 on epoch=12
03/18/2022 18:13:57 - INFO - __main__ - Step 190 Global step 190 Train loss 2.521674 on epoch=13
03/18/2022 18:14:01 - INFO - __main__ - Step 200 Global step 200 Train loss 1.989295 on epoch=14
03/18/2022 18:14:05 - INFO - __main__ - Global step 200 Train loss 2.340186 Classification-F1 0.009644364074743823 on epoch=14
03/18/2022 18:14:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.705014 on epoch=14
03/18/2022 18:14:14 - INFO - __main__ - Step 220 Global step 220 Train loss 2.340796 on epoch=15
03/18/2022 18:14:19 - INFO - __main__ - Step 230 Global step 230 Train loss 1.997422 on epoch=16
03/18/2022 18:14:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.929914 on epoch=17
03/18/2022 18:14:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.672676 on epoch=17
03/18/2022 18:14:31 - INFO - __main__ - Global step 250 Train loss 1.929164 Classification-F1 0.009003601440576232 on epoch=17
03/18/2022 18:14:36 - INFO - __main__ - Step 260 Global step 260 Train loss 1.639586 on epoch=18
03/18/2022 18:14:41 - INFO - __main__ - Step 270 Global step 270 Train loss 1.531948 on epoch=19
03/18/2022 18:14:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.361700 on epoch=19
03/18/2022 18:14:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.491451 on epoch=20
03/18/2022 18:14:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.408027 on epoch=21
03/18/2022 18:14:58 - INFO - __main__ - Global step 300 Train loss 1.486542 Classification-F1 0.026390604812819955 on epoch=21
03/18/2022 18:15:04 - INFO - __main__ - Step 310 Global step 310 Train loss 1.448964 on epoch=22
03/18/2022 18:15:09 - INFO - __main__ - Step 320 Global step 320 Train loss 1.182626 on epoch=22
03/18/2022 18:15:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.328971 on epoch=23
03/18/2022 18:15:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.190047 on epoch=24
03/18/2022 18:15:23 - INFO - __main__ - Step 350 Global step 350 Train loss 1.350934 on epoch=24
03/18/2022 18:15:27 - INFO - __main__ - Global step 350 Train loss 1.300309 Classification-F1 0.020452607339383283 on epoch=24
03/18/2022 18:15:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.233311 on epoch=25
03/18/2022 18:15:36 - INFO - __main__ - Step 370 Global step 370 Train loss 1.195183 on epoch=26
03/18/2022 18:15:41 - INFO - __main__ - Step 380 Global step 380 Train loss 1.085388 on epoch=27
03/18/2022 18:15:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.060529 on epoch=27
03/18/2022 18:15:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.046736 on epoch=28
03/18/2022 18:15:53 - INFO - __main__ - Global step 400 Train loss 1.124229 Classification-F1 0.009563658099222952 on epoch=28
03/18/2022 18:15:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.975468 on epoch=29
03/18/2022 18:16:03 - INFO - __main__ - Step 420 Global step 420 Train loss 1.225118 on epoch=29
03/18/2022 18:16:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.007632 on epoch=30
03/18/2022 18:16:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.034392 on epoch=31
03/18/2022 18:16:17 - INFO - __main__ - Step 450 Global step 450 Train loss 1.034285 on epoch=32
03/18/2022 18:16:20 - INFO - __main__ - Global step 450 Train loss 1.055379 Classification-F1 0.0395124716553288 on epoch=32
03/18/2022 18:16:26 - INFO - __main__ - Step 460 Global step 460 Train loss 1.006694 on epoch=32
03/18/2022 18:16:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.975417 on epoch=33
03/18/2022 18:16:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.867372 on epoch=34
03/18/2022 18:16:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.973184 on epoch=34
03/18/2022 18:16:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.945531 on epoch=35
03/18/2022 18:16:48 - INFO - __main__ - Global step 500 Train loss 0.953640 Classification-F1 0.017788724685276407 on epoch=35
03/18/2022 18:16:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.931963 on epoch=36
03/18/2022 18:16:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.946368 on epoch=37
03/18/2022 18:17:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.895152 on epoch=37
03/18/2022 18:17:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.897772 on epoch=38
03/18/2022 18:17:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.865954 on epoch=39
03/18/2022 18:17:17 - INFO - __main__ - Global step 550 Train loss 0.907442 Classification-F1 0.017662951705504897 on epoch=39
03/18/2022 18:17:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.897352 on epoch=39
03/18/2022 18:17:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.881088 on epoch=40
03/18/2022 18:17:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.865940 on epoch=41
03/18/2022 18:17:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.916943 on epoch=42
03/18/2022 18:17:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.845842 on epoch=42
03/18/2022 18:17:45 - INFO - __main__ - Global step 600 Train loss 0.881433 Classification-F1 0.028149638233671843 on epoch=42
03/18/2022 18:17:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.817326 on epoch=43
03/18/2022 18:17:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.768894 on epoch=44
03/18/2022 18:18:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.792170 on epoch=44
03/18/2022 18:18:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.764964 on epoch=45
03/18/2022 18:18:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.719120 on epoch=46
03/18/2022 18:18:14 - INFO - __main__ - Global step 650 Train loss 0.772495 Classification-F1 0.12467181229858713 on epoch=46
03/18/2022 18:18:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.708074 on epoch=47
03/18/2022 18:18:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.596557 on epoch=47
03/18/2022 18:18:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.668187 on epoch=48
03/18/2022 18:18:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.524826 on epoch=49
03/18/2022 18:18:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.484019 on epoch=49
03/18/2022 18:18:43 - INFO - __main__ - Global step 700 Train loss 0.596332 Classification-F1 0.580066049408269 on epoch=49
03/18/2022 18:18:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.366709 on epoch=50
03/18/2022 18:18:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.432244 on epoch=51
03/18/2022 18:18:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.302876 on epoch=52
03/18/2022 18:19:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.243346 on epoch=52
03/18/2022 18:19:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.243295 on epoch=53
03/18/2022 18:19:12 - INFO - __main__ - Global step 750 Train loss 0.317694 Classification-F1 0.8101179136796197 on epoch=53
03/18/2022 18:19:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.182700 on epoch=54
03/18/2022 18:19:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.172451 on epoch=54
03/18/2022 18:19:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.114738 on epoch=55
03/18/2022 18:19:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.150556 on epoch=56
03/18/2022 18:19:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.157566 on epoch=57
03/18/2022 18:19:42 - INFO - __main__ - Global step 800 Train loss 0.155602 Classification-F1 0.7717983534933769 on epoch=57
03/18/2022 18:19:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.100897 on epoch=57
03/18/2022 18:19:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.074376 on epoch=58
03/18/2022 18:19:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.037928 on epoch=59
03/18/2022 18:20:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.083117 on epoch=59
03/18/2022 18:20:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.080188 on epoch=60
03/18/2022 18:20:10 - INFO - __main__ - Global step 850 Train loss 0.075301 Classification-F1 0.718107030677155 on epoch=60
03/18/2022 18:20:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.035586 on epoch=61
03/18/2022 18:20:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.073464 on epoch=62
03/18/2022 18:20:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.040304 on epoch=62
03/18/2022 18:20:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.034530 on epoch=63
03/18/2022 18:20:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.011202 on epoch=64
03/18/2022 18:20:39 - INFO - __main__ - Global step 900 Train loss 0.039017 Classification-F1 0.6924058613993317 on epoch=64
03/18/2022 18:20:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.032042 on epoch=64
03/18/2022 18:20:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.104131 on epoch=65
03/18/2022 18:20:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003728 on epoch=66
03/18/2022 18:20:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.024902 on epoch=67
03/18/2022 18:21:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.008308 on epoch=67
03/18/2022 18:21:07 - INFO - __main__ - Global step 950 Train loss 0.034622 Classification-F1 0.6182171464571844 on epoch=67
03/18/2022 18:21:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.038359 on epoch=68
03/18/2022 18:21:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.014610 on epoch=69
03/18/2022 18:21:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.019213 on epoch=69
03/18/2022 18:21:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.019652 on epoch=70
03/18/2022 18:21:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001242 on epoch=71
03/18/2022 18:21:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:21:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:21:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:21:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:21:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:21:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:21:33 - INFO - __main__ - ['Animal']
03/18/2022 18:21:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:21:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:21:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:21:36 - INFO - __main__ - Global step 1000 Train loss 0.018615 Classification-F1 0.6710698594467167 on epoch=71
03/18/2022 18:21:36 - INFO - __main__ - save last model!
03/18/2022 18:21:44 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:21:45 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 18:21:45 - INFO - __main__ - Printing 3 examples
03/18/2022 18:21:45 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 18:21:45 - INFO - __main__ - ['Animal']
03/18/2022 18:21:45 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 18:21:45 - INFO - __main__ - ['Animal']
03/18/2022 18:21:45 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 18:21:45 - INFO - __main__ - ['Village']
03/18/2022 18:21:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:21:47 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:21:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:21:47 - INFO - __main__ - Starting training!
03/18/2022 18:21:50 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 18:23:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_100_0.0005_8_predictions.txt
03/18/2022 18:23:02 - INFO - __main__ - Classification-F1 on test data: 0.5170
03/18/2022 18:23:02 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.0005, bsz=8, dev_performance=0.8101179136796197, test_performance=0.517035997859685
03/18/2022 18:23:02 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.0003, bsz=8 ...
03/18/2022 18:23:03 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:23:03 - INFO - __main__ - Printing 3 examples
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:23:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:23:03 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:23:03 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:23:03 - INFO - __main__ - Printing 3 examples
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:23:03 - INFO - __main__ - ['Animal']
03/18/2022 18:23:03 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:23:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:23:04 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:23:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:23:16 - INFO - __main__ - Starting training!
03/18/2022 18:23:23 - INFO - __main__ - Step 10 Global step 10 Train loss 21.539223 on epoch=0
03/18/2022 18:23:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.194490 on epoch=1
03/18/2022 18:23:32 - INFO - __main__ - Step 30 Global step 30 Train loss 13.356493 on epoch=2
03/18/2022 18:23:37 - INFO - __main__ - Step 40 Global step 40 Train loss 12.022323 on epoch=2
03/18/2022 18:23:43 - INFO - __main__ - Step 50 Global step 50 Train loss 11.277196 on epoch=3
03/18/2022 18:23:47 - INFO - __main__ - Global step 50 Train loss 15.477945 Classification-F1 0.0 on epoch=3
03/18/2022 18:23:52 - INFO - __main__ - Step 60 Global step 60 Train loss 9.319657 on epoch=4
03/18/2022 18:23:57 - INFO - __main__ - Step 70 Global step 70 Train loss 10.085843 on epoch=4
03/18/2022 18:24:03 - INFO - __main__ - Step 80 Global step 80 Train loss 9.136515 on epoch=5
03/18/2022 18:24:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.871536 on epoch=6
03/18/2022 18:24:13 - INFO - __main__ - Step 100 Global step 100 Train loss 7.808675 on epoch=7
03/18/2022 18:24:16 - INFO - __main__ - Global step 100 Train loss 9.044445 Classification-F1 0.0 on epoch=7
03/18/2022 18:24:21 - INFO - __main__ - Step 110 Global step 110 Train loss 7.342142 on epoch=7
03/18/2022 18:24:26 - INFO - __main__ - Step 120 Global step 120 Train loss 7.119976 on epoch=8
03/18/2022 18:24:31 - INFO - __main__ - Step 130 Global step 130 Train loss 6.179040 on epoch=9
03/18/2022 18:24:37 - INFO - __main__ - Step 140 Global step 140 Train loss 5.055860 on epoch=9
03/18/2022 18:24:42 - INFO - __main__ - Step 150 Global step 150 Train loss 3.911396 on epoch=10
03/18/2022 18:24:45 - INFO - __main__ - Global step 150 Train loss 5.921683 Classification-F1 0.02141779788838612 on epoch=10
03/18/2022 18:24:51 - INFO - __main__ - Step 160 Global step 160 Train loss 3.335262 on epoch=11
03/18/2022 18:24:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.589535 on epoch=12
03/18/2022 18:25:01 - INFO - __main__ - Step 180 Global step 180 Train loss 3.343747 on epoch=12
03/18/2022 18:25:06 - INFO - __main__ - Step 190 Global step 190 Train loss 3.132832 on epoch=13
03/18/2022 18:25:11 - INFO - __main__ - Step 200 Global step 200 Train loss 3.080470 on epoch=14
03/18/2022 18:25:14 - INFO - __main__ - Global step 200 Train loss 3.296369 Classification-F1 0.35516159147954207 on epoch=14
03/18/2022 18:25:20 - INFO - __main__ - Step 210 Global step 210 Train loss 3.114062 on epoch=14
03/18/2022 18:25:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.320027 on epoch=15
03/18/2022 18:25:30 - INFO - __main__ - Step 230 Global step 230 Train loss 2.599642 on epoch=16
03/18/2022 18:25:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.190611 on epoch=17
03/18/2022 18:25:40 - INFO - __main__ - Step 250 Global step 250 Train loss 2.028798 on epoch=17
03/18/2022 18:25:43 - INFO - __main__ - Global step 250 Train loss 2.450628 Classification-F1 0.5835871750846618 on epoch=17
03/18/2022 18:25:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.968549 on epoch=18
03/18/2022 18:25:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.608781 on epoch=19
03/18/2022 18:25:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.416847 on epoch=19
03/18/2022 18:26:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.631556 on epoch=20
03/18/2022 18:26:09 - INFO - __main__ - Step 300 Global step 300 Train loss 1.167565 on epoch=21
03/18/2022 18:26:12 - INFO - __main__ - Global step 300 Train loss 1.558660 Classification-F1 0.6750530062764037 on epoch=21
03/18/2022 18:26:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.860488 on epoch=22
03/18/2022 18:26:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.358689 on epoch=22
03/18/2022 18:26:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.363516 on epoch=23
03/18/2022 18:26:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.061505 on epoch=24
03/18/2022 18:26:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.108474 on epoch=24
03/18/2022 18:26:42 - INFO - __main__ - Global step 350 Train loss 0.350534 Classification-F1 0.8232158767663393 on epoch=24
03/18/2022 18:26:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.043591 on epoch=25
03/18/2022 18:26:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.023217 on epoch=26
03/18/2022 18:26:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.014206 on epoch=27
03/18/2022 18:27:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.008114 on epoch=27
03/18/2022 18:27:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015298 on epoch=28
03/18/2022 18:27:12 - INFO - __main__ - Global step 400 Train loss 0.020885 Classification-F1 0.7497497015021293 on epoch=28
03/18/2022 18:27:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.017159 on epoch=29
03/18/2022 18:27:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004971 on epoch=29
03/18/2022 18:27:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.020670 on epoch=30
03/18/2022 18:27:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.015857 on epoch=31
03/18/2022 18:27:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011710 on epoch=32
03/18/2022 18:27:41 - INFO - __main__ - Global step 450 Train loss 0.014073 Classification-F1 0.7709753541892097 on epoch=32
03/18/2022 18:27:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002227 on epoch=32
03/18/2022 18:27:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002436 on epoch=33
03/18/2022 18:27:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.007457 on epoch=34
03/18/2022 18:28:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001914 on epoch=34
03/18/2022 18:28:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000967 on epoch=35
03/18/2022 18:28:11 - INFO - __main__ - Global step 500 Train loss 0.003000 Classification-F1 0.777119005097295 on epoch=35
03/18/2022 18:28:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.023108 on epoch=36
03/18/2022 18:28:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001887 on epoch=37
03/18/2022 18:28:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001180 on epoch=37
03/18/2022 18:28:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002652 on epoch=38
03/18/2022 18:28:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001377 on epoch=39
03/18/2022 18:28:40 - INFO - __main__ - Global step 550 Train loss 0.006041 Classification-F1 0.6637933457068581 on epoch=39
03/18/2022 18:28:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005682 on epoch=39
03/18/2022 18:28:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001079 on epoch=40
03/18/2022 18:28:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001181 on epoch=41
03/18/2022 18:29:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000938 on epoch=42
03/18/2022 18:29:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000265 on epoch=42
03/18/2022 18:29:09 - INFO - __main__ - Global step 600 Train loss 0.001829 Classification-F1 0.6154159860598151 on epoch=42
03/18/2022 18:29:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000541 on epoch=43
03/18/2022 18:29:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001685 on epoch=44
03/18/2022 18:29:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000389 on epoch=44
03/18/2022 18:29:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000720 on epoch=45
03/18/2022 18:29:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000443 on epoch=46
03/18/2022 18:29:39 - INFO - __main__ - Global step 650 Train loss 0.000755 Classification-F1 0.6308217985708907 on epoch=46
03/18/2022 18:29:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000500 on epoch=47
03/18/2022 18:29:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.013481 on epoch=47
03/18/2022 18:29:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000856 on epoch=48
03/18/2022 18:29:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.003977 on epoch=49
03/18/2022 18:30:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000775 on epoch=49
03/18/2022 18:30:08 - INFO - __main__ - Global step 700 Train loss 0.003918 Classification-F1 0.7705914457669676 on epoch=49
03/18/2022 18:30:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001727 on epoch=50
03/18/2022 18:30:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000362 on epoch=51
03/18/2022 18:30:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002509 on epoch=52
03/18/2022 18:30:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000206 on epoch=52
03/18/2022 18:30:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001028 on epoch=53
03/18/2022 18:30:37 - INFO - __main__ - Global step 750 Train loss 0.001167 Classification-F1 0.7312960023349473 on epoch=53
03/18/2022 18:30:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000379 on epoch=54
03/18/2022 18:30:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.006064 on epoch=54
03/18/2022 18:30:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000478 on epoch=55
03/18/2022 18:30:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005314 on epoch=56
03/18/2022 18:31:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001151 on epoch=57
03/18/2022 18:31:06 - INFO - __main__ - Global step 800 Train loss 0.002677 Classification-F1 0.7051686039536557 on epoch=57
03/18/2022 18:31:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000512 on epoch=57
03/18/2022 18:31:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000453 on epoch=58
03/18/2022 18:31:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000169 on epoch=59
03/18/2022 18:31:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000098 on epoch=59
03/18/2022 18:31:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000069 on epoch=60
03/18/2022 18:31:36 - INFO - __main__ - Global step 850 Train loss 0.000260 Classification-F1 0.7531911524405587 on epoch=60
03/18/2022 18:31:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000059 on epoch=61
03/18/2022 18:31:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003354 on epoch=62
03/18/2022 18:31:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000092 on epoch=62
03/18/2022 18:31:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000210 on epoch=63
03/18/2022 18:32:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003007 on epoch=64
03/18/2022 18:32:05 - INFO - __main__ - Global step 900 Train loss 0.001344 Classification-F1 0.8320576069315988 on epoch=64
03/18/2022 18:32:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000183 on epoch=64
03/18/2022 18:32:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000185 on epoch=65
03/18/2022 18:32:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000644 on epoch=66
03/18/2022 18:32:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000147 on epoch=67
03/18/2022 18:32:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000389 on epoch=67
03/18/2022 18:32:35 - INFO - __main__ - Global step 950 Train loss 0.000310 Classification-F1 0.6428268586594422 on epoch=67
03/18/2022 18:32:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000017 on epoch=68
03/18/2022 18:32:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000097 on epoch=69
03/18/2022 18:32:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000209 on epoch=69
03/18/2022 18:32:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000261 on epoch=70
03/18/2022 18:33:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000025 on epoch=71
03/18/2022 18:33:01 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:33:01 - INFO - __main__ - Printing 3 examples
03/18/2022 18:33:01 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:33:01 - INFO - __main__ - ['Animal']
03/18/2022 18:33:01 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:33:01 - INFO - __main__ - ['Animal']
03/18/2022 18:33:01 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:33:01 - INFO - __main__ - ['Animal']
03/18/2022 18:33:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:33:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:33:02 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:33:02 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:33:02 - INFO - __main__ - Printing 3 examples
03/18/2022 18:33:02 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:33:02 - INFO - __main__ - ['Animal']
03/18/2022 18:33:02 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:33:02 - INFO - __main__ - ['Animal']
03/18/2022 18:33:02 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:33:02 - INFO - __main__ - ['Animal']
03/18/2022 18:33:02 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:33:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:33:02 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:33:04 - INFO - __main__ - Global step 1000 Train loss 0.000122 Classification-F1 0.644018470645657 on epoch=71
03/18/2022 18:33:04 - INFO - __main__ - save last model!
03/18/2022 18:33:11 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:33:12 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 18:33:12 - INFO - __main__ - Printing 3 examples
03/18/2022 18:33:12 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 18:33:12 - INFO - __main__ - ['Animal']
03/18/2022 18:33:12 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 18:33:12 - INFO - __main__ - ['Animal']
03/18/2022 18:33:12 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 18:33:12 - INFO - __main__ - ['Village']
03/18/2022 18:33:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:33:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:33:13 - INFO - __main__ - Starting training!
03/18/2022 18:33:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:33:17 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 18:34:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_100_0.0003_8_predictions.txt
03/18/2022 18:34:31 - INFO - __main__ - Classification-F1 on test data: 0.4035
03/18/2022 18:34:32 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.0003, bsz=8, dev_performance=0.8320576069315988, test_performance=0.40349914717455715
03/18/2022 18:34:32 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.0002, bsz=8 ...
03/18/2022 18:34:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:34:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:34:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:34:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:34:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:34:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:34:33 - INFO - __main__ - ['Animal']
03/18/2022 18:34:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:34:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:34:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:34:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:34:46 - INFO - __main__ - Starting training!
03/18/2022 18:34:52 - INFO - __main__ - Step 10 Global step 10 Train loss 21.375927 on epoch=0
03/18/2022 18:34:57 - INFO - __main__ - Step 20 Global step 20 Train loss 18.832851 on epoch=1
03/18/2022 18:35:02 - INFO - __main__ - Step 30 Global step 30 Train loss 17.419567 on epoch=2
03/18/2022 18:35:07 - INFO - __main__ - Step 40 Global step 40 Train loss 14.698720 on epoch=2
03/18/2022 18:35:12 - INFO - __main__ - Step 50 Global step 50 Train loss 13.815346 on epoch=3
03/18/2022 18:35:20 - INFO - __main__ - Global step 50 Train loss 17.228483 Classification-F1 0.0 on epoch=3
03/18/2022 18:35:25 - INFO - __main__ - Step 60 Global step 60 Train loss 11.933564 on epoch=4
03/18/2022 18:35:30 - INFO - __main__ - Step 70 Global step 70 Train loss 11.986891 on epoch=4
03/18/2022 18:35:35 - INFO - __main__ - Step 80 Global step 80 Train loss 10.799269 on epoch=5
03/18/2022 18:35:40 - INFO - __main__ - Step 90 Global step 90 Train loss 10.523233 on epoch=6
03/18/2022 18:35:45 - INFO - __main__ - Step 100 Global step 100 Train loss 10.060938 on epoch=7
03/18/2022 18:35:49 - INFO - __main__ - Global step 100 Train loss 11.060779 Classification-F1 0.0 on epoch=7
03/18/2022 18:35:54 - INFO - __main__ - Step 110 Global step 110 Train loss 9.568652 on epoch=7
03/18/2022 18:35:59 - INFO - __main__ - Step 120 Global step 120 Train loss 9.298299 on epoch=8
03/18/2022 18:36:04 - INFO - __main__ - Step 130 Global step 130 Train loss 8.365786 on epoch=9
03/18/2022 18:36:09 - INFO - __main__ - Step 140 Global step 140 Train loss 8.392832 on epoch=9
03/18/2022 18:36:14 - INFO - __main__ - Step 150 Global step 150 Train loss 8.201012 on epoch=10
03/18/2022 18:36:18 - INFO - __main__ - Global step 150 Train loss 8.765316 Classification-F1 0.0 on epoch=10
03/18/2022 18:36:23 - INFO - __main__ - Step 160 Global step 160 Train loss 8.084681 on epoch=11
03/18/2022 18:36:28 - INFO - __main__ - Step 170 Global step 170 Train loss 7.644240 on epoch=12
03/18/2022 18:36:33 - INFO - __main__ - Step 180 Global step 180 Train loss 6.864742 on epoch=12
03/18/2022 18:36:38 - INFO - __main__ - Step 190 Global step 190 Train loss 6.758916 on epoch=13
03/18/2022 18:36:43 - INFO - __main__ - Step 200 Global step 200 Train loss 5.379442 on epoch=14
03/18/2022 18:36:47 - INFO - __main__ - Global step 200 Train loss 6.946405 Classification-F1 0.0 on epoch=14
03/18/2022 18:36:52 - INFO - __main__ - Step 210 Global step 210 Train loss 6.101847 on epoch=14
03/18/2022 18:36:57 - INFO - __main__ - Step 220 Global step 220 Train loss 4.873981 on epoch=15
03/18/2022 18:37:02 - INFO - __main__ - Step 230 Global step 230 Train loss 4.481059 on epoch=16
03/18/2022 18:37:07 - INFO - __main__ - Step 240 Global step 240 Train loss 4.281279 on epoch=17
03/18/2022 18:37:12 - INFO - __main__ - Step 250 Global step 250 Train loss 3.525210 on epoch=17
03/18/2022 18:37:16 - INFO - __main__ - Global step 250 Train loss 4.652675 Classification-F1 0.019813243589135898 on epoch=17
03/18/2022 18:37:22 - INFO - __main__ - Step 260 Global step 260 Train loss 3.581676 on epoch=18
03/18/2022 18:37:27 - INFO - __main__ - Step 270 Global step 270 Train loss 3.003324 on epoch=19
03/18/2022 18:37:32 - INFO - __main__ - Step 280 Global step 280 Train loss 3.124654 on epoch=19
03/18/2022 18:37:37 - INFO - __main__ - Step 290 Global step 290 Train loss 3.036085 on epoch=20
03/18/2022 18:37:42 - INFO - __main__ - Step 300 Global step 300 Train loss 2.941715 on epoch=21
03/18/2022 18:37:45 - INFO - __main__ - Global step 300 Train loss 3.137491 Classification-F1 0.1958805332598436 on epoch=21
03/18/2022 18:37:51 - INFO - __main__ - Step 310 Global step 310 Train loss 2.846117 on epoch=22
03/18/2022 18:37:56 - INFO - __main__ - Step 320 Global step 320 Train loss 2.772325 on epoch=22
03/18/2022 18:38:01 - INFO - __main__ - Step 330 Global step 330 Train loss 2.446346 on epoch=23
03/18/2022 18:38:06 - INFO - __main__ - Step 340 Global step 340 Train loss 2.521241 on epoch=24
03/18/2022 18:38:11 - INFO - __main__ - Step 350 Global step 350 Train loss 2.462244 on epoch=24
03/18/2022 18:38:15 - INFO - __main__ - Global step 350 Train loss 2.609655 Classification-F1 0.3074723080037148 on epoch=24
03/18/2022 18:38:20 - INFO - __main__ - Step 360 Global step 360 Train loss 2.612289 on epoch=25
03/18/2022 18:38:25 - INFO - __main__ - Step 370 Global step 370 Train loss 1.892786 on epoch=26
03/18/2022 18:38:30 - INFO - __main__ - Step 380 Global step 380 Train loss 2.416950 on epoch=27
03/18/2022 18:38:36 - INFO - __main__ - Step 390 Global step 390 Train loss 2.298527 on epoch=27
03/18/2022 18:38:41 - INFO - __main__ - Step 400 Global step 400 Train loss 2.181005 on epoch=28
03/18/2022 18:38:43 - INFO - __main__ - Global step 400 Train loss 2.280311 Classification-F1 0.3557883887160881 on epoch=28
03/18/2022 18:38:49 - INFO - __main__ - Step 410 Global step 410 Train loss 1.782577 on epoch=29
03/18/2022 18:38:54 - INFO - __main__ - Step 420 Global step 420 Train loss 1.994177 on epoch=29
03/18/2022 18:38:59 - INFO - __main__ - Step 430 Global step 430 Train loss 2.031780 on epoch=30
03/18/2022 18:39:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.717314 on epoch=31
03/18/2022 18:39:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.460229 on epoch=32
03/18/2022 18:39:13 - INFO - __main__ - Global step 450 Train loss 1.797215 Classification-F1 0.5989349809130307 on epoch=32
03/18/2022 18:39:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.616057 on epoch=32
03/18/2022 18:39:23 - INFO - __main__ - Step 470 Global step 470 Train loss 1.588161 on epoch=33
03/18/2022 18:39:28 - INFO - __main__ - Step 480 Global step 480 Train loss 1.496136 on epoch=34
03/18/2022 18:39:34 - INFO - __main__ - Step 490 Global step 490 Train loss 1.682002 on epoch=34
03/18/2022 18:39:39 - INFO - __main__ - Step 500 Global step 500 Train loss 1.285481 on epoch=35
03/18/2022 18:39:42 - INFO - __main__ - Global step 500 Train loss 1.533568 Classification-F1 0.5259502770918878 on epoch=35
03/18/2022 18:39:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.571295 on epoch=36
03/18/2022 18:39:52 - INFO - __main__ - Step 520 Global step 520 Train loss 1.093245 on epoch=37
03/18/2022 18:39:57 - INFO - __main__ - Step 530 Global step 530 Train loss 1.186745 on epoch=37
03/18/2022 18:40:02 - INFO - __main__ - Step 540 Global step 540 Train loss 1.127110 on epoch=38
03/18/2022 18:40:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.903753 on epoch=39
03/18/2022 18:40:11 - INFO - __main__ - Global step 550 Train loss 1.176430 Classification-F1 0.8026543929189609 on epoch=39
03/18/2022 18:40:16 - INFO - __main__ - Step 560 Global step 560 Train loss 1.026532 on epoch=39
03/18/2022 18:40:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.815798 on epoch=40
03/18/2022 18:40:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.863284 on epoch=41
03/18/2022 18:40:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.639420 on epoch=42
03/18/2022 18:40:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.691158 on epoch=42
03/18/2022 18:40:41 - INFO - __main__ - Global step 600 Train loss 0.807238 Classification-F1 0.7661674587169576 on epoch=42
03/18/2022 18:40:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.527379 on epoch=43
03/18/2022 18:40:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.452441 on epoch=44
03/18/2022 18:40:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.313029 on epoch=44
03/18/2022 18:41:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.195549 on epoch=45
03/18/2022 18:41:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.307997 on epoch=46
03/18/2022 18:41:10 - INFO - __main__ - Global step 650 Train loss 0.359279 Classification-F1 0.863798891990814 on epoch=46
03/18/2022 18:41:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.644994 on epoch=47
03/18/2022 18:41:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.448925 on epoch=47
03/18/2022 18:41:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.655282 on epoch=48
03/18/2022 18:41:31 - INFO - __main__ - Step 690 Global step 690 Train loss 1.085784 on epoch=49
03/18/2022 18:41:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.493287 on epoch=49
03/18/2022 18:41:40 - INFO - __main__ - Global step 700 Train loss 0.665654 Classification-F1 0.4971592506126842 on epoch=49
03/18/2022 18:41:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.707230 on epoch=50
03/18/2022 18:41:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.995061 on epoch=51
03/18/2022 18:41:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.867419 on epoch=52
03/18/2022 18:42:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.619133 on epoch=52
03/18/2022 18:42:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.448733 on epoch=53
03/18/2022 18:42:09 - INFO - __main__ - Global step 750 Train loss 0.727515 Classification-F1 0.6311343570443793 on epoch=53
03/18/2022 18:42:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.438923 on epoch=54
03/18/2022 18:42:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.458674 on epoch=54
03/18/2022 18:42:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.478734 on epoch=55
03/18/2022 18:42:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.502641 on epoch=56
03/18/2022 18:42:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.581180 on epoch=57
03/18/2022 18:42:38 - INFO - __main__ - Global step 800 Train loss 0.492031 Classification-F1 0.703842624754336 on epoch=57
03/18/2022 18:42:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.480334 on epoch=57
03/18/2022 18:42:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.411761 on epoch=58
03/18/2022 18:42:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.460078 on epoch=59
03/18/2022 18:42:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.508790 on epoch=59
03/18/2022 18:43:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.407137 on epoch=60
03/18/2022 18:43:07 - INFO - __main__ - Global step 850 Train loss 0.453620 Classification-F1 0.6817406019506353 on epoch=60
03/18/2022 18:43:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.393783 on epoch=61
03/18/2022 18:43:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.348860 on epoch=62
03/18/2022 18:43:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.379512 on epoch=62
03/18/2022 18:43:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.228257 on epoch=63
03/18/2022 18:43:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.246299 on epoch=64
03/18/2022 18:43:37 - INFO - __main__ - Global step 900 Train loss 0.319342 Classification-F1 0.7003338200397955 on epoch=64
03/18/2022 18:43:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.240857 on epoch=64
03/18/2022 18:43:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.294086 on epoch=65
03/18/2022 18:43:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.242105 on epoch=66
03/18/2022 18:43:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.197200 on epoch=67
03/18/2022 18:44:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.256826 on epoch=67
03/18/2022 18:44:06 - INFO - __main__ - Global step 950 Train loss 0.246215 Classification-F1 0.7485434199140567 on epoch=67
03/18/2022 18:44:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.204305 on epoch=68
03/18/2022 18:44:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.228156 on epoch=69
03/18/2022 18:44:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.188983 on epoch=69
03/18/2022 18:44:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.153709 on epoch=70
03/18/2022 18:44:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.236115 on epoch=71
03/18/2022 18:44:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:44:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:44:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:44:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:44:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:44:33 - INFO - __main__ - Printing 3 examples
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:44:33 - INFO - __main__ - ['Animal']
03/18/2022 18:44:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:44:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:44:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:44:35 - INFO - __main__ - Global step 1000 Train loss 0.202253 Classification-F1 0.7457492578287231 on epoch=71
03/18/2022 18:44:35 - INFO - __main__ - save last model!
03/18/2022 18:44:42 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:44:43 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 18:44:43 - INFO - __main__ - Printing 3 examples
03/18/2022 18:44:43 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 18:44:43 - INFO - __main__ - ['Animal']
03/18/2022 18:44:43 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 18:44:43 - INFO - __main__ - ['Animal']
03/18/2022 18:44:43 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 18:44:43 - INFO - __main__ - ['Village']
03/18/2022 18:44:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:44:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:44:44 - INFO - __main__ - Starting training!
03/18/2022 18:44:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:44:48 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 18:45:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_100_0.0002_8_predictions.txt
03/18/2022 18:45:55 - INFO - __main__ - Classification-F1 on test data: 0.5270
03/18/2022 18:45:55 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.0002, bsz=8, dev_performance=0.863798891990814, test_performance=0.5269943081847976
03/18/2022 18:45:55 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.0001, bsz=8 ...
03/18/2022 18:45:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:45:56 - INFO - __main__ - Printing 3 examples
03/18/2022 18:45:56 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/18/2022 18:45:56 - INFO - __main__ - ['Animal']
03/18/2022 18:45:56 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/18/2022 18:45:56 - INFO - __main__ - ['Animal']
03/18/2022 18:45:56 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/18/2022 18:45:56 - INFO - __main__ - ['Animal']
03/18/2022 18:45:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:45:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:45:57 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:45:57 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:45:57 - INFO - __main__ - Printing 3 examples
03/18/2022 18:45:57 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/18/2022 18:45:57 - INFO - __main__ - ['Animal']
03/18/2022 18:45:57 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/18/2022 18:45:57 - INFO - __main__ - ['Animal']
03/18/2022 18:45:57 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/18/2022 18:45:57 - INFO - __main__ - ['Animal']
03/18/2022 18:45:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:45:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:45:57 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:46:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:46:07 - INFO - __main__ - Starting training!
03/18/2022 18:46:11 - INFO - __main__ - Step 10 Global step 10 Train loss 22.820013 on epoch=0
03/18/2022 18:46:16 - INFO - __main__ - Step 20 Global step 20 Train loss 21.391171 on epoch=1
03/18/2022 18:46:21 - INFO - __main__ - Step 30 Global step 30 Train loss 18.317434 on epoch=2
03/18/2022 18:46:26 - INFO - __main__ - Step 40 Global step 40 Train loss 15.960073 on epoch=2
03/18/2022 18:46:31 - INFO - __main__ - Step 50 Global step 50 Train loss 14.874403 on epoch=3
03/18/2022 18:47:23 - INFO - __main__ - Global step 50 Train loss 18.672617 Classification-F1 0.0 on epoch=3
03/18/2022 18:47:29 - INFO - __main__ - Step 60 Global step 60 Train loss 13.455400 on epoch=4
03/18/2022 18:47:34 - INFO - __main__ - Step 70 Global step 70 Train loss 14.032954 on epoch=4
03/18/2022 18:47:39 - INFO - __main__ - Step 80 Global step 80 Train loss 12.418798 on epoch=5
03/18/2022 18:47:44 - INFO - __main__ - Step 90 Global step 90 Train loss 12.015481 on epoch=6
03/18/2022 18:47:48 - INFO - __main__ - Step 100 Global step 100 Train loss 11.332301 on epoch=7
03/18/2022 18:47:57 - INFO - __main__ - Global step 100 Train loss 12.650988 Classification-F1 0.0 on epoch=7
03/18/2022 18:48:02 - INFO - __main__ - Step 110 Global step 110 Train loss 11.283659 on epoch=7
03/18/2022 18:48:07 - INFO - __main__ - Step 120 Global step 120 Train loss 10.970621 on epoch=8
03/18/2022 18:48:12 - INFO - __main__ - Step 130 Global step 130 Train loss 10.069982 on epoch=9
03/18/2022 18:48:17 - INFO - __main__ - Step 140 Global step 140 Train loss 10.824726 on epoch=9
03/18/2022 18:48:22 - INFO - __main__ - Step 150 Global step 150 Train loss 10.405472 on epoch=10
03/18/2022 18:48:30 - INFO - __main__ - Global step 150 Train loss 10.710891 Classification-F1 0.0 on epoch=10
03/18/2022 18:48:35 - INFO - __main__ - Step 160 Global step 160 Train loss 10.099282 on epoch=11
03/18/2022 18:48:40 - INFO - __main__ - Step 170 Global step 170 Train loss 9.818647 on epoch=12
03/18/2022 18:48:45 - INFO - __main__ - Step 180 Global step 180 Train loss 9.553027 on epoch=12
03/18/2022 18:48:50 - INFO - __main__ - Step 190 Global step 190 Train loss 9.636122 on epoch=13
03/18/2022 18:48:55 - INFO - __main__ - Step 200 Global step 200 Train loss 9.540972 on epoch=14
03/18/2022 18:49:03 - INFO - __main__ - Global step 200 Train loss 9.729610 Classification-F1 0.0 on epoch=14
03/18/2022 18:49:08 - INFO - __main__ - Step 210 Global step 210 Train loss 9.734945 on epoch=14
03/18/2022 18:49:13 - INFO - __main__ - Step 220 Global step 220 Train loss 9.310676 on epoch=15
03/18/2022 18:49:18 - INFO - __main__ - Step 230 Global step 230 Train loss 8.912642 on epoch=16
03/18/2022 18:49:23 - INFO - __main__ - Step 240 Global step 240 Train loss 8.531734 on epoch=17
03/18/2022 18:49:28 - INFO - __main__ - Step 250 Global step 250 Train loss 8.645346 on epoch=17
03/18/2022 18:49:35 - INFO - __main__ - Global step 250 Train loss 9.027068 Classification-F1 0.0 on epoch=17
03/18/2022 18:49:40 - INFO - __main__ - Step 260 Global step 260 Train loss 8.745422 on epoch=18
03/18/2022 18:49:45 - INFO - __main__ - Step 270 Global step 270 Train loss 8.249506 on epoch=19
03/18/2022 18:49:50 - INFO - __main__ - Step 280 Global step 280 Train loss 8.566740 on epoch=19
03/18/2022 18:49:55 - INFO - __main__ - Step 290 Global step 290 Train loss 7.524636 on epoch=20
03/18/2022 18:50:00 - INFO - __main__ - Step 300 Global step 300 Train loss 7.483594 on epoch=21
03/18/2022 18:50:07 - INFO - __main__ - Global step 300 Train loss 8.113980 Classification-F1 0.0 on epoch=21
03/18/2022 18:50:12 - INFO - __main__ - Step 310 Global step 310 Train loss 7.091534 on epoch=22
03/18/2022 18:50:17 - INFO - __main__ - Step 320 Global step 320 Train loss 7.585413 on epoch=22
03/18/2022 18:50:22 - INFO - __main__ - Step 330 Global step 330 Train loss 6.758424 on epoch=23
03/18/2022 18:50:27 - INFO - __main__ - Step 340 Global step 340 Train loss 6.708970 on epoch=24
03/18/2022 18:50:32 - INFO - __main__ - Step 350 Global step 350 Train loss 6.737291 on epoch=24
03/18/2022 18:50:38 - INFO - __main__ - Global step 350 Train loss 6.976326 Classification-F1 0.0 on epoch=24
03/18/2022 18:50:43 - INFO - __main__ - Step 360 Global step 360 Train loss 6.374274 on epoch=25
03/18/2022 18:50:48 - INFO - __main__ - Step 370 Global step 370 Train loss 5.599408 on epoch=26
03/18/2022 18:50:53 - INFO - __main__ - Step 380 Global step 380 Train loss 5.384621 on epoch=27
03/18/2022 18:50:57 - INFO - __main__ - Step 390 Global step 390 Train loss 5.353869 on epoch=27
03/18/2022 18:51:02 - INFO - __main__ - Step 400 Global step 400 Train loss 4.853025 on epoch=28
03/18/2022 18:51:08 - INFO - __main__ - Global step 400 Train loss 5.513040 Classification-F1 0.0 on epoch=28
03/18/2022 18:51:13 - INFO - __main__ - Step 410 Global step 410 Train loss 4.332096 on epoch=29
03/18/2022 18:51:18 - INFO - __main__ - Step 420 Global step 420 Train loss 4.179768 on epoch=29
03/18/2022 18:51:23 - INFO - __main__ - Step 430 Global step 430 Train loss 3.826521 on epoch=30
03/18/2022 18:51:28 - INFO - __main__ - Step 440 Global step 440 Train loss 3.790164 on epoch=31
03/18/2022 18:51:33 - INFO - __main__ - Step 450 Global step 450 Train loss 3.510431 on epoch=32
03/18/2022 18:51:36 - INFO - __main__ - Global step 450 Train loss 3.927796 Classification-F1 0.10925116713352008 on epoch=32
03/18/2022 18:51:42 - INFO - __main__ - Step 460 Global step 460 Train loss 3.533551 on epoch=32
03/18/2022 18:51:47 - INFO - __main__ - Step 470 Global step 470 Train loss 3.687066 on epoch=33
03/18/2022 18:51:52 - INFO - __main__ - Step 480 Global step 480 Train loss 3.582031 on epoch=34
03/18/2022 18:51:57 - INFO - __main__ - Step 490 Global step 490 Train loss 3.190330 on epoch=34
03/18/2022 18:52:02 - INFO - __main__ - Step 500 Global step 500 Train loss 3.082073 on epoch=35
03/18/2022 18:52:05 - INFO - __main__ - Global step 500 Train loss 3.415010 Classification-F1 0.2548770911839276 on epoch=35
03/18/2022 18:52:11 - INFO - __main__ - Step 510 Global step 510 Train loss 2.827536 on epoch=36
03/18/2022 18:52:16 - INFO - __main__ - Step 520 Global step 520 Train loss 2.457074 on epoch=37
03/18/2022 18:52:21 - INFO - __main__ - Step 530 Global step 530 Train loss 3.354713 on epoch=37
03/18/2022 18:52:26 - INFO - __main__ - Step 540 Global step 540 Train loss 3.426803 on epoch=38
03/18/2022 18:52:31 - INFO - __main__ - Step 550 Global step 550 Train loss 2.998767 on epoch=39
03/18/2022 18:52:34 - INFO - __main__ - Global step 550 Train loss 3.012979 Classification-F1 0.40149453178550776 on epoch=39
03/18/2022 18:52:40 - INFO - __main__ - Step 560 Global step 560 Train loss 3.142235 on epoch=39
03/18/2022 18:52:45 - INFO - __main__ - Step 570 Global step 570 Train loss 2.996457 on epoch=40
03/18/2022 18:52:50 - INFO - __main__ - Step 580 Global step 580 Train loss 2.799083 on epoch=41
03/18/2022 18:52:55 - INFO - __main__ - Step 590 Global step 590 Train loss 2.748955 on epoch=42
03/18/2022 18:53:00 - INFO - __main__ - Step 600 Global step 600 Train loss 2.725843 on epoch=42
03/18/2022 18:53:03 - INFO - __main__ - Global step 600 Train loss 2.882515 Classification-F1 0.49892456285310277 on epoch=42
03/18/2022 18:53:09 - INFO - __main__ - Step 610 Global step 610 Train loss 2.725088 on epoch=43
03/18/2022 18:53:14 - INFO - __main__ - Step 620 Global step 620 Train loss 2.367149 on epoch=44
03/18/2022 18:53:19 - INFO - __main__ - Step 630 Global step 630 Train loss 3.058365 on epoch=44
03/18/2022 18:53:24 - INFO - __main__ - Step 640 Global step 640 Train loss 2.569865 on epoch=45
03/18/2022 18:53:29 - INFO - __main__ - Step 650 Global step 650 Train loss 2.702746 on epoch=46
03/18/2022 18:53:32 - INFO - __main__ - Global step 650 Train loss 2.684643 Classification-F1 0.4704196404677627 on epoch=46
03/18/2022 18:53:37 - INFO - __main__ - Step 660 Global step 660 Train loss 2.224635 on epoch=47
03/18/2022 18:53:42 - INFO - __main__ - Step 670 Global step 670 Train loss 2.074642 on epoch=47
03/18/2022 18:53:47 - INFO - __main__ - Step 680 Global step 680 Train loss 2.344478 on epoch=48
03/18/2022 18:53:52 - INFO - __main__ - Step 690 Global step 690 Train loss 2.404298 on epoch=49
03/18/2022 18:53:57 - INFO - __main__ - Step 700 Global step 700 Train loss 2.184512 on epoch=49
03/18/2022 18:54:01 - INFO - __main__ - Global step 700 Train loss 2.246513 Classification-F1 0.6154794002238104 on epoch=49
03/18/2022 18:54:07 - INFO - __main__ - Step 710 Global step 710 Train loss 2.231473 on epoch=50
03/18/2022 18:54:12 - INFO - __main__ - Step 720 Global step 720 Train loss 2.212796 on epoch=51
03/18/2022 18:54:17 - INFO - __main__ - Step 730 Global step 730 Train loss 2.208604 on epoch=52
03/18/2022 18:54:22 - INFO - __main__ - Step 740 Global step 740 Train loss 1.940344 on epoch=52
03/18/2022 18:54:27 - INFO - __main__ - Step 750 Global step 750 Train loss 1.879113 on epoch=53
03/18/2022 18:54:30 - INFO - __main__ - Global step 750 Train loss 2.094466 Classification-F1 0.6357791553845197 on epoch=53
03/18/2022 18:54:36 - INFO - __main__ - Step 760 Global step 760 Train loss 1.891761 on epoch=54
03/18/2022 18:54:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.909034 on epoch=54
03/18/2022 18:54:46 - INFO - __main__ - Step 780 Global step 780 Train loss 2.133639 on epoch=55
03/18/2022 18:54:51 - INFO - __main__ - Step 790 Global step 790 Train loss 1.817161 on epoch=56
03/18/2022 18:54:57 - INFO - __main__ - Step 800 Global step 800 Train loss 1.777938 on epoch=57
03/18/2022 18:55:00 - INFO - __main__ - Global step 800 Train loss 1.905906 Classification-F1 0.6114434507310412 on epoch=57
03/18/2022 18:55:05 - INFO - __main__ - Step 810 Global step 810 Train loss 1.621856 on epoch=57
03/18/2022 18:55:10 - INFO - __main__ - Step 820 Global step 820 Train loss 1.723320 on epoch=58
03/18/2022 18:55:15 - INFO - __main__ - Step 830 Global step 830 Train loss 1.346460 on epoch=59
03/18/2022 18:55:20 - INFO - __main__ - Step 840 Global step 840 Train loss 1.750751 on epoch=59
03/18/2022 18:55:25 - INFO - __main__ - Step 850 Global step 850 Train loss 2.036373 on epoch=60
03/18/2022 18:55:28 - INFO - __main__ - Global step 850 Train loss 1.695752 Classification-F1 0.604298856218002 on epoch=60
03/18/2022 18:55:33 - INFO - __main__ - Step 860 Global step 860 Train loss 1.680482 on epoch=61
03/18/2022 18:55:38 - INFO - __main__ - Step 870 Global step 870 Train loss 1.389218 on epoch=62
03/18/2022 18:55:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.953530 on epoch=62
03/18/2022 18:55:49 - INFO - __main__ - Step 890 Global step 890 Train loss 1.119047 on epoch=63
03/18/2022 18:55:54 - INFO - __main__ - Step 900 Global step 900 Train loss 1.227199 on epoch=64
03/18/2022 18:55:57 - INFO - __main__ - Global step 900 Train loss 1.273895 Classification-F1 0.5225808464586896 on epoch=64
03/18/2022 18:56:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.818294 on epoch=64
03/18/2022 18:56:07 - INFO - __main__ - Step 920 Global step 920 Train loss 1.015509 on epoch=65
03/18/2022 18:56:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.629057 on epoch=66
03/18/2022 18:56:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.689058 on epoch=67
03/18/2022 18:56:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.190898 on epoch=67
03/18/2022 18:56:26 - INFO - __main__ - Global step 950 Train loss 0.668563 Classification-F1 0.650411362462001 on epoch=67
03/18/2022 18:56:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.292313 on epoch=68
03/18/2022 18:56:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.272560 on epoch=69
03/18/2022 18:56:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.098581 on epoch=69
03/18/2022 18:56:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.548080 on epoch=70
03/18/2022 18:56:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.387549 on epoch=71
03/18/2022 18:56:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:56:54 - INFO - __main__ - Printing 3 examples
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 18:56:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:56:54 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:56:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:56:54 - INFO - __main__ - Printing 3 examples
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 18:56:54 - INFO - __main__ - ['Animal']
03/18/2022 18:56:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:56:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:56:54 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:56:56 - INFO - __main__ - Global step 1000 Train loss 0.319817 Classification-F1 0.7232951019165157 on epoch=71
03/18/2022 18:56:57 - INFO - __main__ - save last model!
03/18/2022 18:57:04 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 18:57:05 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 18:57:05 - INFO - __main__ - Printing 3 examples
03/18/2022 18:57:05 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 18:57:05 - INFO - __main__ - ['Animal']
03/18/2022 18:57:05 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 18:57:05 - INFO - __main__ - ['Animal']
03/18/2022 18:57:05 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 18:57:05 - INFO - __main__ - ['Village']
03/18/2022 18:57:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:57:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:57:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:57:07 - INFO - __main__ - Starting training!
03/18/2022 18:57:10 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 18:58:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_100_0.0001_8_predictions.txt
03/18/2022 18:58:20 - INFO - __main__ - Classification-F1 on test data: 0.2488
03/18/2022 18:58:21 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.0001, bsz=8, dev_performance=0.7232951019165157, test_performance=0.2487759013636919
03/18/2022 18:58:21 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.0005, bsz=8 ...
03/18/2022 18:58:22 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:58:22 - INFO - __main__ - Printing 3 examples
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 18:58:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:58:22 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 18:58:22 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 18:58:22 - INFO - __main__ - Printing 3 examples
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 18:58:22 - INFO - __main__ - ['Animal']
03/18/2022 18:58:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 18:58:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 18:58:22 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 18:58:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 18:58:33 - INFO - __main__ - Starting training!
03/18/2022 18:58:37 - INFO - __main__ - Step 10 Global step 10 Train loss 21.970549 on epoch=0
03/18/2022 18:58:42 - INFO - __main__ - Step 20 Global step 20 Train loss 16.881428 on epoch=1
03/18/2022 18:58:47 - INFO - __main__ - Step 30 Global step 30 Train loss 12.003261 on epoch=2
03/18/2022 18:58:52 - INFO - __main__ - Step 40 Global step 40 Train loss 11.274877 on epoch=2
03/18/2022 18:58:57 - INFO - __main__ - Step 50 Global step 50 Train loss 9.057416 on epoch=3
03/18/2022 18:59:01 - INFO - __main__ - Global step 50 Train loss 14.237507 Classification-F1 0.0 on epoch=3
03/18/2022 18:59:07 - INFO - __main__ - Step 60 Global step 60 Train loss 9.090981 on epoch=4
03/18/2022 18:59:12 - INFO - __main__ - Step 70 Global step 70 Train loss 8.144142 on epoch=4
03/18/2022 18:59:17 - INFO - __main__ - Step 80 Global step 80 Train loss 6.165516 on epoch=5
03/18/2022 18:59:22 - INFO - __main__ - Step 90 Global step 90 Train loss 5.332602 on epoch=6
03/18/2022 18:59:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.688576 on epoch=7
03/18/2022 18:59:31 - INFO - __main__ - Global step 100 Train loss 6.084364 Classification-F1 0.4776293205163227 on epoch=7
03/18/2022 18:59:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.037400 on epoch=7
03/18/2022 18:59:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.752835 on epoch=8
03/18/2022 18:59:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.589461 on epoch=9
03/18/2022 18:59:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.476718 on epoch=9
03/18/2022 18:59:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.419409 on epoch=10
03/18/2022 19:00:01 - INFO - __main__ - Global step 150 Train loss 0.655164 Classification-F1 0.5766655414875229 on epoch=10
03/18/2022 19:00:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.306360 on epoch=11
03/18/2022 19:00:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.183091 on epoch=12
03/18/2022 19:00:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.246350 on epoch=12
03/18/2022 19:00:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.143187 on epoch=13
03/18/2022 19:00:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.196578 on epoch=14
03/18/2022 19:00:31 - INFO - __main__ - Global step 200 Train loss 0.215113 Classification-F1 0.5057375709862919 on epoch=14
03/18/2022 19:00:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.202253 on epoch=14
03/18/2022 19:00:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.153307 on epoch=15
03/18/2022 19:00:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.181676 on epoch=16
03/18/2022 19:00:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.156136 on epoch=17
03/18/2022 19:00:56 - INFO - __main__ - Step 250 Global step 250 Train loss 3.277377 on epoch=17
03/18/2022 19:01:00 - INFO - __main__ - Global step 250 Train loss 0.794150 Classification-F1 0.5992049181394085 on epoch=17
03/18/2022 19:01:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.104735 on epoch=18
03/18/2022 19:01:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.067645 on epoch=19
03/18/2022 19:01:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.039633 on epoch=19
03/18/2022 19:01:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.031343 on epoch=20
03/18/2022 19:01:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.018249 on epoch=21
03/18/2022 19:01:30 - INFO - __main__ - Global step 300 Train loss 0.052321 Classification-F1 0.6729388490006092 on epoch=21
03/18/2022 19:01:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.059030 on epoch=22
03/18/2022 19:01:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.050133 on epoch=22
03/18/2022 19:01:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.040221 on epoch=23
03/18/2022 19:01:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012984 on epoch=24
03/18/2022 19:01:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009030 on epoch=24
03/18/2022 19:02:00 - INFO - __main__ - Global step 350 Train loss 0.034279 Classification-F1 0.6562636736023832 on epoch=24
03/18/2022 19:02:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.241955 on epoch=25
03/18/2022 19:02:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.063587 on epoch=26
03/18/2022 19:02:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.021535 on epoch=27
03/18/2022 19:02:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.021314 on epoch=27
03/18/2022 19:02:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015415 on epoch=28
03/18/2022 19:02:29 - INFO - __main__ - Global step 400 Train loss 0.272761 Classification-F1 0.8211596911749214 on epoch=28
03/18/2022 19:02:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001437 on epoch=29
03/18/2022 19:02:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.047216 on epoch=29
03/18/2022 19:02:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.014820 on epoch=30
03/18/2022 19:02:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004848 on epoch=31
03/18/2022 19:02:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001313 on epoch=32
03/18/2022 19:02:59 - INFO - __main__ - Global step 450 Train loss 0.013927 Classification-F1 0.6837509477017699 on epoch=32
03/18/2022 19:03:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001918 on epoch=32
03/18/2022 19:03:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001474 on epoch=33
03/18/2022 19:03:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003932 on epoch=34
03/18/2022 19:03:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000283 on epoch=34
03/18/2022 19:03:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005112 on epoch=35
03/18/2022 19:03:28 - INFO - __main__ - Global step 500 Train loss 0.002544 Classification-F1 0.7903328358469488 on epoch=35
03/18/2022 19:03:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007810 on epoch=36
03/18/2022 19:03:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001377 on epoch=37
03/18/2022 19:03:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002928 on epoch=37
03/18/2022 19:03:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001724 on epoch=38
03/18/2022 19:03:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.008228 on epoch=39
03/18/2022 19:03:57 - INFO - __main__ - Global step 550 Train loss 0.004413 Classification-F1 0.648402168026068 on epoch=39
03/18/2022 19:04:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004035 on epoch=39
03/18/2022 19:04:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003371 on epoch=40
03/18/2022 19:04:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.047880 on epoch=41
03/18/2022 19:04:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.008704 on epoch=42
03/18/2022 19:04:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001082 on epoch=42
03/18/2022 19:04:36 - INFO - __main__ - Global step 600 Train loss 0.013014 Classification-F1 0.621901793332059 on epoch=42
03/18/2022 19:04:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002508 on epoch=43
03/18/2022 19:04:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.052515 on epoch=44
03/18/2022 19:04:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004764 on epoch=44
03/18/2022 19:04:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001661 on epoch=45
03/18/2022 19:05:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000964 on epoch=46
03/18/2022 19:05:15 - INFO - __main__ - Global step 650 Train loss 0.012483 Classification-F1 0.637608880551243 on epoch=46
03/18/2022 19:05:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000455 on epoch=47
03/18/2022 19:05:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001498 on epoch=47
03/18/2022 19:05:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000141 on epoch=48
03/18/2022 19:05:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000449 on epoch=49
03/18/2022 19:05:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000188 on epoch=49
03/18/2022 19:05:54 - INFO - __main__ - Global step 700 Train loss 0.000546 Classification-F1 0.6876906717395332 on epoch=49
03/18/2022 19:05:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000117 on epoch=50
03/18/2022 19:06:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000307 on epoch=51
03/18/2022 19:06:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000145 on epoch=52
03/18/2022 19:06:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000062 on epoch=52
03/18/2022 19:06:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000375 on epoch=53
03/18/2022 19:06:32 - INFO - __main__ - Global step 750 Train loss 0.000201 Classification-F1 0.5836088149824935 on epoch=53
03/18/2022 19:06:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.111024 on epoch=54
03/18/2022 19:06:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001071 on epoch=54
03/18/2022 19:06:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000323 on epoch=55
03/18/2022 19:06:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000285 on epoch=56
03/18/2022 19:06:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000986 on epoch=57
03/18/2022 19:07:11 - INFO - __main__ - Global step 800 Train loss 0.022738 Classification-F1 0.6071594032735798 on epoch=57
03/18/2022 19:07:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000604 on epoch=57
03/18/2022 19:07:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000571 on epoch=58
03/18/2022 19:07:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000329 on epoch=59
03/18/2022 19:07:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000211 on epoch=59
03/18/2022 19:07:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000542 on epoch=60
03/18/2022 19:07:50 - INFO - __main__ - Global step 850 Train loss 0.000451 Classification-F1 0.5333235130670616 on epoch=60
03/18/2022 19:07:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001014 on epoch=61
03/18/2022 19:08:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000400 on epoch=62
03/18/2022 19:08:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000187 on epoch=62
03/18/2022 19:08:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000039 on epoch=63
03/18/2022 19:08:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000106 on epoch=64
03/18/2022 19:08:30 - INFO - __main__ - Global step 900 Train loss 0.000349 Classification-F1 0.5641945199558234 on epoch=64
03/18/2022 19:08:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000784 on epoch=64
03/18/2022 19:08:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000144 on epoch=65
03/18/2022 19:08:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000061 on epoch=66
03/18/2022 19:08:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000075 on epoch=67
03/18/2022 19:08:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000042 on epoch=67
03/18/2022 19:09:09 - INFO - __main__ - Global step 950 Train loss 0.000221 Classification-F1 0.5574020223925348 on epoch=67
03/18/2022 19:09:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000057 on epoch=68
03/18/2022 19:09:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000066 on epoch=69
03/18/2022 19:09:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000044 on epoch=69
03/18/2022 19:09:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000052 on epoch=70
03/18/2022 19:09:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000124 on epoch=71
03/18/2022 19:09:36 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:09:36 - INFO - __main__ - Printing 3 examples
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 19:09:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:09:36 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:09:36 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:09:36 - INFO - __main__ - Printing 3 examples
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:09:36 - INFO - __main__ - ['Animal']
03/18/2022 19:09:36 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:09:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:09:36 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:09:39 - INFO - __main__ - Global step 1000 Train loss 0.000068 Classification-F1 0.5976908149128421 on epoch=71
03/18/2022 19:09:39 - INFO - __main__ - save last model!
03/18/2022 19:09:46 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 19:09:47 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 19:09:47 - INFO - __main__ - Printing 3 examples
03/18/2022 19:09:47 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 19:09:47 - INFO - __main__ - ['Animal']
03/18/2022 19:09:47 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 19:09:47 - INFO - __main__ - ['Animal']
03/18/2022 19:09:47 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 19:09:47 - INFO - __main__ - ['Village']
03/18/2022 19:09:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:09:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:09:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:09:49 - INFO - __main__ - Starting training!
03/18/2022 19:09:52 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 19:11:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_13_0.0005_8_predictions.txt
03/18/2022 19:11:05 - INFO - __main__ - Classification-F1 on test data: 0.3533
03/18/2022 19:11:06 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.0005, bsz=8, dev_performance=0.8211596911749214, test_performance=0.3533118525843519
03/18/2022 19:11:06 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.0003, bsz=8 ...
03/18/2022 19:11:07 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:11:07 - INFO - __main__ - Printing 3 examples
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:11:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:11:07 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:11:07 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:11:07 - INFO - __main__ - Printing 3 examples
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:11:07 - INFO - __main__ - ['Animal']
03/18/2022 19:11:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:11:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:11:07 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:11:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:11:18 - INFO - __main__ - Starting training!
03/18/2022 19:11:23 - INFO - __main__ - Step 10 Global step 10 Train loss 21.102980 on epoch=0
03/18/2022 19:11:28 - INFO - __main__ - Step 20 Global step 20 Train loss 19.112865 on epoch=1
03/18/2022 19:11:33 - INFO - __main__ - Step 30 Global step 30 Train loss 15.199519 on epoch=2
03/18/2022 19:11:38 - INFO - __main__ - Step 40 Global step 40 Train loss 12.882207 on epoch=2
03/18/2022 19:11:43 - INFO - __main__ - Step 50 Global step 50 Train loss 11.604928 on epoch=3
03/18/2022 19:11:48 - INFO - __main__ - Global step 50 Train loss 15.980499 Classification-F1 0.0 on epoch=3
03/18/2022 19:11:53 - INFO - __main__ - Step 60 Global step 60 Train loss 11.253529 on epoch=4
03/18/2022 19:11:58 - INFO - __main__ - Step 70 Global step 70 Train loss 10.488556 on epoch=4
03/18/2022 19:12:03 - INFO - __main__ - Step 80 Global step 80 Train loss 9.168336 on epoch=5
03/18/2022 19:12:08 - INFO - __main__ - Step 90 Global step 90 Train loss 9.360455 on epoch=6
03/18/2022 19:12:13 - INFO - __main__ - Step 100 Global step 100 Train loss 8.457563 on epoch=7
03/18/2022 19:12:19 - INFO - __main__ - Global step 100 Train loss 9.745687 Classification-F1 0.0 on epoch=7
03/18/2022 19:12:23 - INFO - __main__ - Step 110 Global step 110 Train loss 7.965778 on epoch=7
03/18/2022 19:12:28 - INFO - __main__ - Step 120 Global step 120 Train loss 7.293800 on epoch=8
03/18/2022 19:12:33 - INFO - __main__ - Step 130 Global step 130 Train loss 7.040181 on epoch=9
03/18/2022 19:12:38 - INFO - __main__ - Step 140 Global step 140 Train loss 6.184018 on epoch=9
03/18/2022 19:12:43 - INFO - __main__ - Step 150 Global step 150 Train loss 5.841172 on epoch=10
03/18/2022 19:12:48 - INFO - __main__ - Global step 150 Train loss 6.864990 Classification-F1 0.0 on epoch=10
03/18/2022 19:12:53 - INFO - __main__ - Step 160 Global step 160 Train loss 4.807264 on epoch=11
03/18/2022 19:12:58 - INFO - __main__ - Step 170 Global step 170 Train loss 3.505845 on epoch=12
03/18/2022 19:13:03 - INFO - __main__ - Step 180 Global step 180 Train loss 3.140637 on epoch=12
03/18/2022 19:13:08 - INFO - __main__ - Step 190 Global step 190 Train loss 2.912046 on epoch=13
03/18/2022 19:13:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.934976 on epoch=14
03/18/2022 19:13:16 - INFO - __main__ - Global step 200 Train loss 3.460154 Classification-F1 0.04406263126883576 on epoch=14
03/18/2022 19:13:23 - INFO - __main__ - Step 210 Global step 210 Train loss 3.554254 on epoch=14
03/18/2022 19:13:28 - INFO - __main__ - Step 220 Global step 220 Train loss 2.370583 on epoch=15
03/18/2022 19:13:33 - INFO - __main__ - Step 230 Global step 230 Train loss 2.695547 on epoch=16
03/18/2022 19:13:38 - INFO - __main__ - Step 240 Global step 240 Train loss 2.330970 on epoch=17
03/18/2022 19:13:43 - INFO - __main__ - Step 250 Global step 250 Train loss 2.776845 on epoch=17
03/18/2022 19:13:45 - INFO - __main__ - Global step 250 Train loss 2.745640 Classification-F1 0.1654874197347161 on epoch=17
03/18/2022 19:13:51 - INFO - __main__ - Step 260 Global step 260 Train loss 2.525229 on epoch=18
03/18/2022 19:13:56 - INFO - __main__ - Step 270 Global step 270 Train loss 1.929761 on epoch=19
03/18/2022 19:14:01 - INFO - __main__ - Step 280 Global step 280 Train loss 2.287091 on epoch=19
03/18/2022 19:14:06 - INFO - __main__ - Step 290 Global step 290 Train loss 1.966386 on epoch=20
03/18/2022 19:14:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.028603 on epoch=21
03/18/2022 19:14:13 - INFO - __main__ - Global step 300 Train loss 2.147414 Classification-F1 0.33821417017384053 on epoch=21
03/18/2022 19:14:19 - INFO - __main__ - Step 310 Global step 310 Train loss 2.045343 on epoch=22
03/18/2022 19:14:24 - INFO - __main__ - Step 320 Global step 320 Train loss 2.016140 on epoch=22
03/18/2022 19:14:29 - INFO - __main__ - Step 330 Global step 330 Train loss 1.785656 on epoch=23
03/18/2022 19:14:34 - INFO - __main__ - Step 340 Global step 340 Train loss 1.847153 on epoch=24
03/18/2022 19:14:39 - INFO - __main__ - Step 350 Global step 350 Train loss 1.603190 on epoch=24
03/18/2022 19:14:42 - INFO - __main__ - Global step 350 Train loss 1.859496 Classification-F1 0.5930907232738792 on epoch=24
03/18/2022 19:14:48 - INFO - __main__ - Step 360 Global step 360 Train loss 1.824401 on epoch=25
03/18/2022 19:14:53 - INFO - __main__ - Step 370 Global step 370 Train loss 1.313892 on epoch=26
03/18/2022 19:14:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.257010 on epoch=27
03/18/2022 19:15:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.556801 on epoch=27
03/18/2022 19:15:08 - INFO - __main__ - Step 400 Global step 400 Train loss 1.100433 on epoch=28
03/18/2022 19:15:11 - INFO - __main__ - Global step 400 Train loss 1.410507 Classification-F1 0.6461587523308763 on epoch=28
03/18/2022 19:15:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.302022 on epoch=29
03/18/2022 19:15:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.043847 on epoch=29
03/18/2022 19:15:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.909332 on epoch=30
03/18/2022 19:15:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.706093 on epoch=31
03/18/2022 19:15:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.232149 on epoch=32
03/18/2022 19:15:41 - INFO - __main__ - Global step 450 Train loss 0.838688 Classification-F1 0.9062002608034146 on epoch=32
03/18/2022 19:15:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.032234 on epoch=32
03/18/2022 19:15:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.058808 on epoch=33
03/18/2022 19:15:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.018601 on epoch=34
03/18/2022 19:16:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.021620 on epoch=34
03/18/2022 19:16:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011257 on epoch=35
03/18/2022 19:16:11 - INFO - __main__ - Global step 500 Train loss 0.028504 Classification-F1 0.7356922842063964 on epoch=35
03/18/2022 19:16:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.043864 on epoch=36
03/18/2022 19:16:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.017185 on epoch=37
03/18/2022 19:16:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006403 on epoch=37
03/18/2022 19:16:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.015561 on epoch=38
03/18/2022 19:16:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.018648 on epoch=39
03/18/2022 19:16:39 - INFO - __main__ - Global step 550 Train loss 0.020332 Classification-F1 0.7649720298676655 on epoch=39
03/18/2022 19:16:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000925 on epoch=39
03/18/2022 19:16:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003309 on epoch=40
03/18/2022 19:16:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000745 on epoch=41
03/18/2022 19:16:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002503 on epoch=42
03/18/2022 19:17:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001033 on epoch=42
03/18/2022 19:17:08 - INFO - __main__ - Global step 600 Train loss 0.001703 Classification-F1 0.8315053926905259 on epoch=42
03/18/2022 19:17:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.017069 on epoch=43
03/18/2022 19:17:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001383 on epoch=44
03/18/2022 19:17:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004482 on epoch=44
03/18/2022 19:17:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001129 on epoch=45
03/18/2022 19:17:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001720 on epoch=46
03/18/2022 19:17:37 - INFO - __main__ - Global step 650 Train loss 0.005157 Classification-F1 0.7927362782746119 on epoch=46
03/18/2022 19:17:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.003651 on epoch=47
03/18/2022 19:17:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000356 on epoch=47
03/18/2022 19:17:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000191 on epoch=48
03/18/2022 19:17:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000312 on epoch=49
03/18/2022 19:18:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000196 on epoch=49
03/18/2022 19:18:06 - INFO - __main__ - Global step 700 Train loss 0.000941 Classification-F1 0.746533386796715 on epoch=49
03/18/2022 19:18:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000766 on epoch=50
03/18/2022 19:18:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000252 on epoch=51
03/18/2022 19:18:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000137 on epoch=52
03/18/2022 19:18:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000359 on epoch=52
03/18/2022 19:18:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.005352 on epoch=53
03/18/2022 19:18:35 - INFO - __main__ - Global step 750 Train loss 0.001373 Classification-F1 0.8291558852279914 on epoch=53
03/18/2022 19:18:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.022380 on epoch=54
03/18/2022 19:18:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000456 on epoch=54
03/18/2022 19:18:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000412 on epoch=55
03/18/2022 19:18:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001195 on epoch=56
03/18/2022 19:19:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000140 on epoch=57
03/18/2022 19:19:04 - INFO - __main__ - Global step 800 Train loss 0.004917 Classification-F1 0.8914847627377339 on epoch=57
03/18/2022 19:19:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000381 on epoch=57
03/18/2022 19:19:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000153 on epoch=58
03/18/2022 19:19:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000093 on epoch=59
03/18/2022 19:19:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001049 on epoch=59
03/18/2022 19:19:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000324 on epoch=60
03/18/2022 19:19:33 - INFO - __main__ - Global step 850 Train loss 0.000400 Classification-F1 0.8087223587223588 on epoch=60
03/18/2022 19:19:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000654 on epoch=61
03/18/2022 19:19:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000105 on epoch=62
03/18/2022 19:19:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000101 on epoch=62
03/18/2022 19:19:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000118 on epoch=63
03/18/2022 19:19:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001660 on epoch=64
03/18/2022 19:20:03 - INFO - __main__ - Global step 900 Train loss 0.000528 Classification-F1 0.813191894471659 on epoch=64
03/18/2022 19:20:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.006037 on epoch=64
03/18/2022 19:20:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000170 on epoch=65
03/18/2022 19:20:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003292 on epoch=66
03/18/2022 19:20:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.002356 on epoch=67
03/18/2022 19:20:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.011804 on epoch=67
03/18/2022 19:20:32 - INFO - __main__ - Global step 950 Train loss 0.004732 Classification-F1 0.7098820626115155 on epoch=67
03/18/2022 19:20:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001361 on epoch=68
03/18/2022 19:20:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000089 on epoch=69
03/18/2022 19:20:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000071 on epoch=69
03/18/2022 19:20:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000350 on epoch=70
03/18/2022 19:20:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000098 on epoch=71
03/18/2022 19:20:58 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:20:58 - INFO - __main__ - Printing 3 examples
03/18/2022 19:20:58 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:20:58 - INFO - __main__ - ['Animal']
03/18/2022 19:20:58 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:20:58 - INFO - __main__ - ['Animal']
03/18/2022 19:20:58 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:20:58 - INFO - __main__ - ['Animal']
03/18/2022 19:20:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 19:20:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:20:59 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:20:59 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:20:59 - INFO - __main__ - Printing 3 examples
03/18/2022 19:20:59 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:20:59 - INFO - __main__ - ['Animal']
03/18/2022 19:20:59 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:20:59 - INFO - __main__ - ['Animal']
03/18/2022 19:20:59 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:20:59 - INFO - __main__ - ['Animal']
03/18/2022 19:20:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:20:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:20:59 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:21:01 - INFO - __main__ - Global step 1000 Train loss 0.000394 Classification-F1 0.8092651901013531 on epoch=71
03/18/2022 19:21:01 - INFO - __main__ - save last model!
03/18/2022 19:21:08 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 19:21:09 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 19:21:09 - INFO - __main__ - Printing 3 examples
03/18/2022 19:21:09 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 19:21:09 - INFO - __main__ - ['Animal']
03/18/2022 19:21:09 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 19:21:09 - INFO - __main__ - ['Animal']
03/18/2022 19:21:09 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 19:21:09 - INFO - __main__ - ['Village']
03/18/2022 19:21:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:21:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:21:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:21:12 - INFO - __main__ - Starting training!
03/18/2022 19:21:14 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 19:22:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_13_0.0003_8_predictions.txt
03/18/2022 19:22:30 - INFO - __main__ - Classification-F1 on test data: 0.3952
03/18/2022 19:22:31 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.0003, bsz=8, dev_performance=0.9062002608034146, test_performance=0.39515480268513603
03/18/2022 19:22:31 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.0002, bsz=8 ...
03/18/2022 19:22:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:22:32 - INFO - __main__ - Printing 3 examples
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:22:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:22:32 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:22:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:22:32 - INFO - __main__ - Printing 3 examples
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:22:32 - INFO - __main__ - ['Animal']
03/18/2022 19:22:32 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:22:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:22:32 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:22:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:22:45 - INFO - __main__ - Starting training!
03/18/2022 19:22:50 - INFO - __main__ - Step 10 Global step 10 Train loss 22.041140 on epoch=0
03/18/2022 19:22:54 - INFO - __main__ - Step 20 Global step 20 Train loss 18.649347 on epoch=1
03/18/2022 19:22:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.138323 on epoch=2
03/18/2022 19:23:04 - INFO - __main__ - Step 40 Global step 40 Train loss 14.604747 on epoch=2
03/18/2022 19:23:09 - INFO - __main__ - Step 50 Global step 50 Train loss 12.174756 on epoch=3
03/18/2022 19:23:49 - INFO - __main__ - Global step 50 Train loss 16.921661 Classification-F1 0.0 on epoch=3
03/18/2022 19:23:54 - INFO - __main__ - Step 60 Global step 60 Train loss 11.896108 on epoch=4
03/18/2022 19:23:59 - INFO - __main__ - Step 70 Global step 70 Train loss 11.794416 on epoch=4
03/18/2022 19:24:04 - INFO - __main__ - Step 80 Global step 80 Train loss 10.423922 on epoch=5
03/18/2022 19:24:09 - INFO - __main__ - Step 90 Global step 90 Train loss 10.400890 on epoch=6
03/18/2022 19:24:14 - INFO - __main__ - Step 100 Global step 100 Train loss 9.997606 on epoch=7
03/18/2022 19:24:19 - INFO - __main__ - Global step 100 Train loss 10.902588 Classification-F1 0.0 on epoch=7
03/18/2022 19:24:24 - INFO - __main__ - Step 110 Global step 110 Train loss 10.327933 on epoch=7
03/18/2022 19:24:29 - INFO - __main__ - Step 120 Global step 120 Train loss 8.619787 on epoch=8
03/18/2022 19:24:34 - INFO - __main__ - Step 130 Global step 130 Train loss 9.267843 on epoch=9
03/18/2022 19:24:39 - INFO - __main__ - Step 140 Global step 140 Train loss 9.166416 on epoch=9
03/18/2022 19:24:44 - INFO - __main__ - Step 150 Global step 150 Train loss 8.461493 on epoch=10
03/18/2022 19:24:48 - INFO - __main__ - Global step 150 Train loss 9.168694 Classification-F1 0.0 on epoch=10
03/18/2022 19:24:53 - INFO - __main__ - Step 160 Global step 160 Train loss 7.780091 on epoch=11
03/18/2022 19:24:58 - INFO - __main__ - Step 170 Global step 170 Train loss 7.321939 on epoch=12
03/18/2022 19:25:03 - INFO - __main__ - Step 180 Global step 180 Train loss 6.712518 on epoch=12
03/18/2022 19:25:08 - INFO - __main__ - Step 190 Global step 190 Train loss 6.285270 on epoch=13
03/18/2022 19:25:13 - INFO - __main__ - Step 200 Global step 200 Train loss 6.300572 on epoch=14
03/18/2022 19:25:17 - INFO - __main__ - Global step 200 Train loss 6.880078 Classification-F1 0.0 on epoch=14
03/18/2022 19:25:22 - INFO - __main__ - Step 210 Global step 210 Train loss 5.836945 on epoch=14
03/18/2022 19:25:27 - INFO - __main__ - Step 220 Global step 220 Train loss 4.354169 on epoch=15
03/18/2022 19:25:32 - INFO - __main__ - Step 230 Global step 230 Train loss 4.904359 on epoch=16
03/18/2022 19:25:37 - INFO - __main__ - Step 240 Global step 240 Train loss 4.281119 on epoch=17
03/18/2022 19:25:42 - INFO - __main__ - Step 250 Global step 250 Train loss 3.760347 on epoch=17
03/18/2022 19:25:46 - INFO - __main__ - Global step 250 Train loss 4.627388 Classification-F1 0.011851851851851853 on epoch=17
03/18/2022 19:25:51 - INFO - __main__ - Step 260 Global step 260 Train loss 3.308276 on epoch=18
03/18/2022 19:25:56 - INFO - __main__ - Step 270 Global step 270 Train loss 3.142266 on epoch=19
03/18/2022 19:26:01 - INFO - __main__ - Step 280 Global step 280 Train loss 3.373496 on epoch=19
03/18/2022 19:26:07 - INFO - __main__ - Step 290 Global step 290 Train loss 3.197509 on epoch=20
03/18/2022 19:26:12 - INFO - __main__ - Step 300 Global step 300 Train loss 2.963864 on epoch=21
03/18/2022 19:26:15 - INFO - __main__ - Global step 300 Train loss 3.197082 Classification-F1 0.0099009900990099 on epoch=21
03/18/2022 19:26:20 - INFO - __main__ - Step 310 Global step 310 Train loss 2.919276 on epoch=22
03/18/2022 19:26:25 - INFO - __main__ - Step 320 Global step 320 Train loss 2.778734 on epoch=22
03/18/2022 19:26:30 - INFO - __main__ - Step 330 Global step 330 Train loss 2.880404 on epoch=23
03/18/2022 19:26:35 - INFO - __main__ - Step 340 Global step 340 Train loss 3.294878 on epoch=24
03/18/2022 19:26:41 - INFO - __main__ - Step 350 Global step 350 Train loss 2.959640 on epoch=24
03/18/2022 19:26:44 - INFO - __main__ - Global step 350 Train loss 2.966587 Classification-F1 0.009750053867700925 on epoch=24
03/18/2022 19:26:49 - INFO - __main__ - Step 360 Global step 360 Train loss 2.707225 on epoch=25
03/18/2022 19:26:54 - INFO - __main__ - Step 370 Global step 370 Train loss 2.397763 on epoch=26
03/18/2022 19:26:59 - INFO - __main__ - Step 380 Global step 380 Train loss 2.651291 on epoch=27
03/18/2022 19:27:05 - INFO - __main__ - Step 390 Global step 390 Train loss 2.906553 on epoch=27
03/18/2022 19:27:10 - INFO - __main__ - Step 400 Global step 400 Train loss 2.384749 on epoch=28
03/18/2022 19:27:13 - INFO - __main__ - Global step 400 Train loss 2.609516 Classification-F1 0.018398268398268396 on epoch=28
03/18/2022 19:27:19 - INFO - __main__ - Step 410 Global step 410 Train loss 2.461286 on epoch=29
03/18/2022 19:27:24 - INFO - __main__ - Step 420 Global step 420 Train loss 2.716933 on epoch=29
03/18/2022 19:27:29 - INFO - __main__ - Step 430 Global step 430 Train loss 2.574833 on epoch=30
03/18/2022 19:27:35 - INFO - __main__ - Step 440 Global step 440 Train loss 2.332392 on epoch=31
03/18/2022 19:27:40 - INFO - __main__ - Step 450 Global step 450 Train loss 2.134354 on epoch=32
03/18/2022 19:27:43 - INFO - __main__ - Global step 450 Train loss 2.443959 Classification-F1 0.05362965838509317 on epoch=32
03/18/2022 19:27:49 - INFO - __main__ - Step 460 Global step 460 Train loss 2.536780 on epoch=32
03/18/2022 19:27:54 - INFO - __main__ - Step 470 Global step 470 Train loss 2.150483 on epoch=33
03/18/2022 19:27:59 - INFO - __main__ - Step 480 Global step 480 Train loss 2.244041 on epoch=34
03/18/2022 19:28:04 - INFO - __main__ - Step 490 Global step 490 Train loss 2.372311 on epoch=34
03/18/2022 19:28:10 - INFO - __main__ - Step 500 Global step 500 Train loss 1.843244 on epoch=35
03/18/2022 19:28:13 - INFO - __main__ - Global step 500 Train loss 2.229372 Classification-F1 0.04551001687224598 on epoch=35
03/18/2022 19:28:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.971898 on epoch=36
03/18/2022 19:28:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.908461 on epoch=37
03/18/2022 19:28:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.925440 on epoch=37
03/18/2022 19:28:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.591853 on epoch=38
03/18/2022 19:28:39 - INFO - __main__ - Step 550 Global step 550 Train loss 2.069272 on epoch=39
03/18/2022 19:28:42 - INFO - __main__ - Global step 550 Train loss 1.893384 Classification-F1 0.0293463105963106 on epoch=39
03/18/2022 19:28:47 - INFO - __main__ - Step 560 Global step 560 Train loss 2.014955 on epoch=39
03/18/2022 19:28:52 - INFO - __main__ - Step 570 Global step 570 Train loss 2.000371 on epoch=40
03/18/2022 19:28:57 - INFO - __main__ - Step 580 Global step 580 Train loss 1.907541 on epoch=41
03/18/2022 19:29:03 - INFO - __main__ - Step 590 Global step 590 Train loss 1.869377 on epoch=42
03/18/2022 19:29:08 - INFO - __main__ - Step 600 Global step 600 Train loss 1.727189 on epoch=42
03/18/2022 19:29:10 - INFO - __main__ - Global step 600 Train loss 1.903886 Classification-F1 0.06073204302974764 on epoch=42
03/18/2022 19:29:16 - INFO - __main__ - Step 610 Global step 610 Train loss 1.514304 on epoch=43
03/18/2022 19:29:21 - INFO - __main__ - Step 620 Global step 620 Train loss 1.553648 on epoch=44
03/18/2022 19:29:26 - INFO - __main__ - Step 630 Global step 630 Train loss 1.622012 on epoch=44
03/18/2022 19:29:32 - INFO - __main__ - Step 640 Global step 640 Train loss 1.601787 on epoch=45
03/18/2022 19:29:37 - INFO - __main__ - Step 650 Global step 650 Train loss 1.686187 on epoch=46
03/18/2022 19:29:39 - INFO - __main__ - Global step 650 Train loss 1.595588 Classification-F1 0.03304131889356419 on epoch=46
03/18/2022 19:29:44 - INFO - __main__ - Step 660 Global step 660 Train loss 1.466504 on epoch=47
03/18/2022 19:29:50 - INFO - __main__ - Step 670 Global step 670 Train loss 1.598652 on epoch=47
03/18/2022 19:29:55 - INFO - __main__ - Step 680 Global step 680 Train loss 1.322636 on epoch=48
03/18/2022 19:30:00 - INFO - __main__ - Step 690 Global step 690 Train loss 1.612673 on epoch=49
03/18/2022 19:30:05 - INFO - __main__ - Step 700 Global step 700 Train loss 1.493675 on epoch=49
03/18/2022 19:30:07 - INFO - __main__ - Global step 700 Train loss 1.498828 Classification-F1 0.10831088210861904 on epoch=49
03/18/2022 19:30:13 - INFO - __main__ - Step 710 Global step 710 Train loss 1.323011 on epoch=50
03/18/2022 19:30:18 - INFO - __main__ - Step 720 Global step 720 Train loss 1.396872 on epoch=51
03/18/2022 19:30:23 - INFO - __main__ - Step 730 Global step 730 Train loss 1.417610 on epoch=52
03/18/2022 19:30:28 - INFO - __main__ - Step 740 Global step 740 Train loss 1.299875 on epoch=52
03/18/2022 19:30:34 - INFO - __main__ - Step 750 Global step 750 Train loss 1.405400 on epoch=53
03/18/2022 19:30:37 - INFO - __main__ - Global step 750 Train loss 1.368553 Classification-F1 0.13948460827630021 on epoch=53
03/18/2022 19:30:43 - INFO - __main__ - Step 760 Global step 760 Train loss 1.578216 on epoch=54
03/18/2022 19:30:48 - INFO - __main__ - Step 770 Global step 770 Train loss 1.345812 on epoch=54
03/18/2022 19:30:53 - INFO - __main__ - Step 780 Global step 780 Train loss 1.213188 on epoch=55
03/18/2022 19:30:58 - INFO - __main__ - Step 790 Global step 790 Train loss 1.285774 on epoch=56
03/18/2022 19:31:03 - INFO - __main__ - Step 800 Global step 800 Train loss 1.174813 on epoch=57
03/18/2022 19:31:06 - INFO - __main__ - Global step 800 Train loss 1.319561 Classification-F1 0.25308349994214435 on epoch=57
03/18/2022 19:31:12 - INFO - __main__ - Step 810 Global step 810 Train loss 1.210771 on epoch=57
03/18/2022 19:31:17 - INFO - __main__ - Step 820 Global step 820 Train loss 1.050022 on epoch=58
03/18/2022 19:31:22 - INFO - __main__ - Step 830 Global step 830 Train loss 1.026333 on epoch=59
03/18/2022 19:31:27 - INFO - __main__ - Step 840 Global step 840 Train loss 1.175125 on epoch=59
03/18/2022 19:31:33 - INFO - __main__ - Step 850 Global step 850 Train loss 1.043728 on epoch=60
03/18/2022 19:31:35 - INFO - __main__ - Global step 850 Train loss 1.101196 Classification-F1 0.3153167668895394 on epoch=60
03/18/2022 19:31:41 - INFO - __main__ - Step 860 Global step 860 Train loss 1.127887 on epoch=61
03/18/2022 19:31:47 - INFO - __main__ - Step 870 Global step 870 Train loss 1.030223 on epoch=62
03/18/2022 19:31:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.993384 on epoch=62
03/18/2022 19:31:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.949222 on epoch=63
03/18/2022 19:32:02 - INFO - __main__ - Step 900 Global step 900 Train loss 1.109941 on epoch=64
03/18/2022 19:32:05 - INFO - __main__ - Global step 900 Train loss 1.042132 Classification-F1 0.42591351673232064 on epoch=64
03/18/2022 19:32:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.917203 on epoch=64
03/18/2022 19:32:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.968250 on epoch=65
03/18/2022 19:32:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.815324 on epoch=66
03/18/2022 19:32:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.577388 on epoch=67
03/18/2022 19:32:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.356296 on epoch=67
03/18/2022 19:32:36 - INFO - __main__ - Global step 950 Train loss 0.726892 Classification-F1 0.7957057626899119 on epoch=67
03/18/2022 19:32:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.313651 on epoch=68
03/18/2022 19:32:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.293010 on epoch=69
03/18/2022 19:32:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.245902 on epoch=69
03/18/2022 19:32:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.257658 on epoch=70
03/18/2022 19:33:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.213909 on epoch=71
03/18/2022 19:33:04 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:33:04 - INFO - __main__ - Printing 3 examples
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 19:33:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:33:04 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:33:04 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:33:04 - INFO - __main__ - Printing 3 examples
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:33:04 - INFO - __main__ - ['Animal']
03/18/2022 19:33:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:33:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:33:05 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:33:06 - INFO - __main__ - Global step 1000 Train loss 0.264826 Classification-F1 0.6207744421585071 on epoch=71
03/18/2022 19:33:06 - INFO - __main__ - save last model!
03/18/2022 19:33:14 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 19:33:14 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 19:33:14 - INFO - __main__ - Printing 3 examples
03/18/2022 19:33:14 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 19:33:14 - INFO - __main__ - ['Animal']
03/18/2022 19:33:14 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 19:33:14 - INFO - __main__ - ['Animal']
03/18/2022 19:33:14 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 19:33:14 - INFO - __main__ - ['Village']
03/18/2022 19:33:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:33:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:33:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:33:17 - INFO - __main__ - Starting training!
03/18/2022 19:33:20 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 19:34:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_13_0.0002_8_predictions.txt
03/18/2022 19:34:31 - INFO - __main__ - Classification-F1 on test data: 0.5454
03/18/2022 19:34:31 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.0002, bsz=8, dev_performance=0.7957057626899119, test_performance=0.5454008204542222
03/18/2022 19:34:31 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.0001, bsz=8 ...
03/18/2022 19:34:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:34:32 - INFO - __main__ - Printing 3 examples
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 6080 mm.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:34:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:34:32 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:34:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:34:32 - INFO - __main__ - Printing 3 examples
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/18/2022 19:34:32 - INFO - __main__ - ['Animal']
03/18/2022 19:34:32 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:34:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:34:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:34:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:34:46 - INFO - __main__ - Starting training!
03/18/2022 19:34:51 - INFO - __main__ - Step 10 Global step 10 Train loss 21.520412 on epoch=0
03/18/2022 19:34:56 - INFO - __main__ - Step 20 Global step 20 Train loss 21.768761 on epoch=1
03/18/2022 19:35:01 - INFO - __main__ - Step 30 Global step 30 Train loss 18.922195 on epoch=2
03/18/2022 19:35:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.282118 on epoch=2
03/18/2022 19:35:11 - INFO - __main__ - Step 50 Global step 50 Train loss 16.257015 on epoch=3
03/18/2022 19:35:49 - INFO - __main__ - Global step 50 Train loss 19.150101 Classification-F1 0.0 on epoch=3
03/18/2022 19:35:55 - INFO - __main__ - Step 60 Global step 60 Train loss 14.515246 on epoch=4
03/18/2022 19:36:00 - INFO - __main__ - Step 70 Global step 70 Train loss 13.414660 on epoch=4
03/18/2022 19:36:05 - INFO - __main__ - Step 80 Global step 80 Train loss 12.129008 on epoch=5
03/18/2022 19:36:10 - INFO - __main__ - Step 90 Global step 90 Train loss 12.176827 on epoch=6
03/18/2022 19:36:15 - INFO - __main__ - Step 100 Global step 100 Train loss 11.456322 on epoch=7
03/18/2022 19:36:20 - INFO - __main__ - Global step 100 Train loss 12.738413 Classification-F1 0.0 on epoch=7
03/18/2022 19:36:25 - INFO - __main__ - Step 110 Global step 110 Train loss 11.846145 on epoch=7
03/18/2022 19:36:30 - INFO - __main__ - Step 120 Global step 120 Train loss 10.785608 on epoch=8
03/18/2022 19:36:36 - INFO - __main__ - Step 130 Global step 130 Train loss 10.637308 on epoch=9
03/18/2022 19:36:41 - INFO - __main__ - Step 140 Global step 140 Train loss 11.205399 on epoch=9
03/18/2022 19:36:46 - INFO - __main__ - Step 150 Global step 150 Train loss 10.062483 on epoch=10
03/18/2022 19:36:50 - INFO - __main__ - Global step 150 Train loss 10.907388 Classification-F1 0.0 on epoch=10
03/18/2022 19:36:55 - INFO - __main__ - Step 160 Global step 160 Train loss 10.396417 on epoch=11
03/18/2022 19:37:00 - INFO - __main__ - Step 170 Global step 170 Train loss 9.555222 on epoch=12
03/18/2022 19:37:05 - INFO - __main__ - Step 180 Global step 180 Train loss 9.949598 on epoch=12
03/18/2022 19:37:10 - INFO - __main__ - Step 190 Global step 190 Train loss 9.365098 on epoch=13
03/18/2022 19:37:15 - INFO - __main__ - Step 200 Global step 200 Train loss 10.218744 on epoch=14
03/18/2022 19:37:19 - INFO - __main__ - Global step 200 Train loss 9.897016 Classification-F1 0.0 on epoch=14
03/18/2022 19:37:24 - INFO - __main__ - Step 210 Global step 210 Train loss 9.628219 on epoch=14
03/18/2022 19:37:29 - INFO - __main__ - Step 220 Global step 220 Train loss 8.574846 on epoch=15
03/18/2022 19:37:34 - INFO - __main__ - Step 230 Global step 230 Train loss 8.714071 on epoch=16
03/18/2022 19:37:39 - INFO - __main__ - Step 240 Global step 240 Train loss 8.577414 on epoch=17
03/18/2022 19:37:44 - INFO - __main__ - Step 250 Global step 250 Train loss 8.574575 on epoch=17
03/18/2022 19:37:49 - INFO - __main__ - Global step 250 Train loss 8.813825 Classification-F1 0.0 on epoch=17
03/18/2022 19:37:54 - INFO - __main__ - Step 260 Global step 260 Train loss 8.369124 on epoch=18
03/18/2022 19:37:59 - INFO - __main__ - Step 270 Global step 270 Train loss 8.271435 on epoch=19
03/18/2022 19:38:04 - INFO - __main__ - Step 280 Global step 280 Train loss 8.686850 on epoch=19
03/18/2022 19:38:09 - INFO - __main__ - Step 290 Global step 290 Train loss 8.151566 on epoch=20
03/18/2022 19:38:14 - INFO - __main__ - Step 300 Global step 300 Train loss 7.460390 on epoch=21
03/18/2022 19:38:18 - INFO - __main__ - Global step 300 Train loss 8.187874 Classification-F1 0.0 on epoch=21
03/18/2022 19:38:23 - INFO - __main__ - Step 310 Global step 310 Train loss 7.211986 on epoch=22
03/18/2022 19:38:28 - INFO - __main__ - Step 320 Global step 320 Train loss 6.889043 on epoch=22
03/18/2022 19:38:33 - INFO - __main__ - Step 330 Global step 330 Train loss 6.740361 on epoch=23
03/18/2022 19:38:38 - INFO - __main__ - Step 340 Global step 340 Train loss 6.749833 on epoch=24
03/18/2022 19:38:43 - INFO - __main__ - Step 350 Global step 350 Train loss 5.977789 on epoch=24
03/18/2022 19:38:47 - INFO - __main__ - Global step 350 Train loss 6.713803 Classification-F1 0.0 on epoch=24
03/18/2022 19:38:52 - INFO - __main__ - Step 360 Global step 360 Train loss 5.447007 on epoch=25
03/18/2022 19:38:57 - INFO - __main__ - Step 370 Global step 370 Train loss 5.424234 on epoch=26
03/18/2022 19:39:02 - INFO - __main__ - Step 380 Global step 380 Train loss 5.805684 on epoch=27
03/18/2022 19:39:07 - INFO - __main__ - Step 390 Global step 390 Train loss 4.884050 on epoch=27
03/18/2022 19:39:12 - INFO - __main__ - Step 400 Global step 400 Train loss 4.449574 on epoch=28
03/18/2022 19:39:16 - INFO - __main__ - Global step 400 Train loss 5.202110 Classification-F1 0.0 on epoch=28
03/18/2022 19:39:21 - INFO - __main__ - Step 410 Global step 410 Train loss 3.653984 on epoch=29
03/18/2022 19:39:26 - INFO - __main__ - Step 420 Global step 420 Train loss 4.054518 on epoch=29
03/18/2022 19:39:31 - INFO - __main__ - Step 430 Global step 430 Train loss 3.811877 on epoch=30
03/18/2022 19:39:36 - INFO - __main__ - Step 440 Global step 440 Train loss 3.736741 on epoch=31
03/18/2022 19:39:41 - INFO - __main__ - Step 450 Global step 450 Train loss 3.322072 on epoch=32
03/18/2022 19:39:45 - INFO - __main__ - Global step 450 Train loss 3.715839 Classification-F1 0.03861476155054136 on epoch=32
03/18/2022 19:39:51 - INFO - __main__ - Step 460 Global step 460 Train loss 4.005586 on epoch=32
03/18/2022 19:39:56 - INFO - __main__ - Step 470 Global step 470 Train loss 3.570325 on epoch=33
03/18/2022 19:40:01 - INFO - __main__ - Step 480 Global step 480 Train loss 3.343541 on epoch=34
03/18/2022 19:40:06 - INFO - __main__ - Step 490 Global step 490 Train loss 3.652761 on epoch=34
03/18/2022 19:40:11 - INFO - __main__ - Step 500 Global step 500 Train loss 3.554025 on epoch=35
03/18/2022 19:40:14 - INFO - __main__ - Global step 500 Train loss 3.625247 Classification-F1 0.03224162100165668 on epoch=35
03/18/2022 19:40:19 - INFO - __main__ - Step 510 Global step 510 Train loss 3.182205 on epoch=36
03/18/2022 19:40:25 - INFO - __main__ - Step 520 Global step 520 Train loss 2.624208 on epoch=37
03/18/2022 19:40:30 - INFO - __main__ - Step 530 Global step 530 Train loss 3.482604 on epoch=37
03/18/2022 19:40:35 - INFO - __main__ - Step 540 Global step 540 Train loss 2.738483 on epoch=38
03/18/2022 19:40:40 - INFO - __main__ - Step 550 Global step 550 Train loss 3.318108 on epoch=39
03/18/2022 19:40:43 - INFO - __main__ - Global step 550 Train loss 3.069122 Classification-F1 0.052226899343428355 on epoch=39
03/18/2022 19:40:49 - INFO - __main__ - Step 560 Global step 560 Train loss 3.246746 on epoch=39
03/18/2022 19:40:54 - INFO - __main__ - Step 570 Global step 570 Train loss 2.824469 on epoch=40
03/18/2022 19:40:59 - INFO - __main__ - Step 580 Global step 580 Train loss 3.131852 on epoch=41
03/18/2022 19:41:04 - INFO - __main__ - Step 590 Global step 590 Train loss 3.156106 on epoch=42
03/18/2022 19:41:09 - INFO - __main__ - Step 600 Global step 600 Train loss 3.087251 on epoch=42
03/18/2022 19:41:12 - INFO - __main__ - Global step 600 Train loss 3.089284 Classification-F1 0.22718715537864473 on epoch=42
03/18/2022 19:41:18 - INFO - __main__ - Step 610 Global step 610 Train loss 2.937937 on epoch=43
03/18/2022 19:41:23 - INFO - __main__ - Step 620 Global step 620 Train loss 2.964453 on epoch=44
03/18/2022 19:41:28 - INFO - __main__ - Step 630 Global step 630 Train loss 3.041505 on epoch=44
03/18/2022 19:41:33 - INFO - __main__ - Step 640 Global step 640 Train loss 2.389621 on epoch=45
03/18/2022 19:41:38 - INFO - __main__ - Step 650 Global step 650 Train loss 2.799892 on epoch=46
03/18/2022 19:41:42 - INFO - __main__ - Global step 650 Train loss 2.826682 Classification-F1 0.1741120166015124 on epoch=46
03/18/2022 19:41:47 - INFO - __main__ - Step 660 Global step 660 Train loss 2.664611 on epoch=47
03/18/2022 19:41:52 - INFO - __main__ - Step 670 Global step 670 Train loss 2.527905 on epoch=47
03/18/2022 19:41:57 - INFO - __main__ - Step 680 Global step 680 Train loss 2.818367 on epoch=48
03/18/2022 19:42:02 - INFO - __main__ - Step 690 Global step 690 Train loss 3.218695 on epoch=49
03/18/2022 19:42:07 - INFO - __main__ - Step 700 Global step 700 Train loss 2.715519 on epoch=49
03/18/2022 19:42:10 - INFO - __main__ - Global step 700 Train loss 2.789020 Classification-F1 0.19025106786090054 on epoch=49
03/18/2022 19:42:15 - INFO - __main__ - Step 710 Global step 710 Train loss 2.351995 on epoch=50
03/18/2022 19:42:20 - INFO - __main__ - Step 720 Global step 720 Train loss 2.276392 on epoch=51
03/18/2022 19:42:25 - INFO - __main__ - Step 730 Global step 730 Train loss 2.160840 on epoch=52
03/18/2022 19:42:30 - INFO - __main__ - Step 740 Global step 740 Train loss 2.701479 on epoch=52
03/18/2022 19:42:35 - INFO - __main__ - Step 750 Global step 750 Train loss 2.384848 on epoch=53
03/18/2022 19:42:38 - INFO - __main__ - Global step 750 Train loss 2.375110 Classification-F1 0.18174236361211155 on epoch=53
03/18/2022 19:42:43 - INFO - __main__ - Step 760 Global step 760 Train loss 2.356871 on epoch=54
03/18/2022 19:42:49 - INFO - __main__ - Step 770 Global step 770 Train loss 2.663977 on epoch=54
03/18/2022 19:42:54 - INFO - __main__ - Step 780 Global step 780 Train loss 2.399769 on epoch=55
03/18/2022 19:42:59 - INFO - __main__ - Step 790 Global step 790 Train loss 2.492841 on epoch=56
03/18/2022 19:43:04 - INFO - __main__ - Step 800 Global step 800 Train loss 2.165465 on epoch=57
03/18/2022 19:43:07 - INFO - __main__ - Global step 800 Train loss 2.415785 Classification-F1 0.240183917523819 on epoch=57
03/18/2022 19:43:12 - INFO - __main__ - Step 810 Global step 810 Train loss 2.908841 on epoch=57
03/18/2022 19:43:17 - INFO - __main__ - Step 820 Global step 820 Train loss 2.220813 on epoch=58
03/18/2022 19:43:23 - INFO - __main__ - Step 830 Global step 830 Train loss 2.151260 on epoch=59
03/18/2022 19:43:28 - INFO - __main__ - Step 840 Global step 840 Train loss 2.657484 on epoch=59
03/18/2022 19:43:33 - INFO - __main__ - Step 850 Global step 850 Train loss 2.198115 on epoch=60
03/18/2022 19:43:36 - INFO - __main__ - Global step 850 Train loss 2.427303 Classification-F1 0.41246669069438757 on epoch=60
03/18/2022 19:43:41 - INFO - __main__ - Step 860 Global step 860 Train loss 2.109640 on epoch=61
03/18/2022 19:43:46 - INFO - __main__ - Step 870 Global step 870 Train loss 2.274466 on epoch=62
03/18/2022 19:43:51 - INFO - __main__ - Step 880 Global step 880 Train loss 2.505017 on epoch=62
03/18/2022 19:43:56 - INFO - __main__ - Step 890 Global step 890 Train loss 2.237544 on epoch=63
03/18/2022 19:44:01 - INFO - __main__ - Step 900 Global step 900 Train loss 1.955239 on epoch=64
03/18/2022 19:44:05 - INFO - __main__ - Global step 900 Train loss 2.216381 Classification-F1 0.42796996177286584 on epoch=64
03/18/2022 19:44:10 - INFO - __main__ - Step 910 Global step 910 Train loss 2.045428 on epoch=64
03/18/2022 19:44:15 - INFO - __main__ - Step 920 Global step 920 Train loss 1.991435 on epoch=65
03/18/2022 19:44:20 - INFO - __main__ - Step 930 Global step 930 Train loss 2.442478 on epoch=66
03/18/2022 19:44:25 - INFO - __main__ - Step 940 Global step 940 Train loss 1.747056 on epoch=67
03/18/2022 19:44:30 - INFO - __main__ - Step 950 Global step 950 Train loss 1.818605 on epoch=67
03/18/2022 19:44:34 - INFO - __main__ - Global step 950 Train loss 2.009000 Classification-F1 0.30020520592626854 on epoch=67
03/18/2022 19:44:38 - INFO - __main__ - Step 960 Global step 960 Train loss 1.950069 on epoch=68
03/18/2022 19:44:43 - INFO - __main__ - Step 970 Global step 970 Train loss 1.968298 on epoch=69
03/18/2022 19:44:48 - INFO - __main__ - Step 980 Global step 980 Train loss 2.161680 on epoch=69
03/18/2022 19:44:53 - INFO - __main__ - Step 990 Global step 990 Train loss 1.901709 on epoch=70
03/18/2022 19:44:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.784769 on epoch=71
03/18/2022 19:45:00 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:45:00 - INFO - __main__ - Printing 3 examples
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 19:45:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:45:00 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:45:00 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:45:00 - INFO - __main__ - Printing 3 examples
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 19:45:00 - INFO - __main__ - ['Plant']
03/18/2022 19:45:00 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:45:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:45:00 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:45:02 - INFO - __main__ - Global step 1000 Train loss 1.953305 Classification-F1 0.343388297052619 on epoch=71
03/18/2022 19:45:02 - INFO - __main__ - save last model!
03/18/2022 19:45:09 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 19:45:09 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 19:45:09 - INFO - __main__ - Printing 3 examples
03/18/2022 19:45:09 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 19:45:09 - INFO - __main__ - ['Animal']
03/18/2022 19:45:09 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 19:45:09 - INFO - __main__ - ['Animal']
03/18/2022 19:45:09 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 19:45:09 - INFO - __main__ - ['Village']
03/18/2022 19:45:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:45:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:45:11 - INFO - __main__ - Starting training!
03/18/2022 19:45:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:45:15 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 19:46:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_13_0.0001_8_predictions.txt
03/18/2022 19:46:06 - INFO - __main__ - Classification-F1 on test data: 0.3288
03/18/2022 19:46:07 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.0001, bsz=8, dev_performance=0.42796996177286584, test_performance=0.3288153177451872
03/18/2022 19:46:07 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.0005, bsz=8 ...
03/18/2022 19:46:08 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:46:08 - INFO - __main__ - Printing 3 examples
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:46:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:46:08 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:46:08 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:46:08 - INFO - __main__ - Printing 3 examples
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 19:46:08 - INFO - __main__ - ['Plant']
03/18/2022 19:46:08 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:46:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:46:08 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:46:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:46:19 - INFO - __main__ - Starting training!
03/18/2022 19:46:24 - INFO - __main__ - Step 10 Global step 10 Train loss 22.129902 on epoch=0
03/18/2022 19:46:29 - INFO - __main__ - Step 20 Global step 20 Train loss 18.944965 on epoch=1
03/18/2022 19:46:34 - INFO - __main__ - Step 30 Global step 30 Train loss 13.983640 on epoch=2
03/18/2022 19:46:39 - INFO - __main__ - Step 40 Global step 40 Train loss 13.325330 on epoch=2
03/18/2022 19:46:44 - INFO - __main__ - Step 50 Global step 50 Train loss 10.888446 on epoch=3
03/18/2022 19:46:47 - INFO - __main__ - Global step 50 Train loss 15.854457 Classification-F1 0.0 on epoch=3
03/18/2022 19:46:53 - INFO - __main__ - Step 60 Global step 60 Train loss 10.387234 on epoch=4
03/18/2022 19:46:58 - INFO - __main__ - Step 70 Global step 70 Train loss 9.179196 on epoch=4
03/18/2022 19:47:03 - INFO - __main__ - Step 80 Global step 80 Train loss 8.585238 on epoch=5
03/18/2022 19:47:09 - INFO - __main__ - Step 90 Global step 90 Train loss 7.471373 on epoch=6
03/18/2022 19:47:14 - INFO - __main__ - Step 100 Global step 100 Train loss 5.917456 on epoch=7
03/18/2022 19:47:17 - INFO - __main__ - Global step 100 Train loss 8.308100 Classification-F1 0.0 on epoch=7
03/18/2022 19:47:22 - INFO - __main__ - Step 110 Global step 110 Train loss 4.065595 on epoch=7
03/18/2022 19:47:28 - INFO - __main__ - Step 120 Global step 120 Train loss 3.899466 on epoch=8
03/18/2022 19:47:33 - INFO - __main__ - Step 130 Global step 130 Train loss 2.928112 on epoch=9
03/18/2022 19:47:38 - INFO - __main__ - Step 140 Global step 140 Train loss 2.550183 on epoch=9
03/18/2022 19:47:43 - INFO - __main__ - Step 150 Global step 150 Train loss 2.586002 on epoch=10
03/18/2022 19:47:45 - INFO - __main__ - Global step 150 Train loss 3.205872 Classification-F1 0.11601731601731602 on epoch=10
03/18/2022 19:47:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.614276 on epoch=11
03/18/2022 19:47:56 - INFO - __main__ - Step 170 Global step 170 Train loss 2.381138 on epoch=12
03/18/2022 19:48:02 - INFO - __main__ - Step 180 Global step 180 Train loss 2.376701 on epoch=12
03/18/2022 19:48:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.862484 on epoch=13
03/18/2022 19:48:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.309027 on epoch=14
03/18/2022 19:48:14 - INFO - __main__ - Global step 200 Train loss 2.308725 Classification-F1 0.4346518083215624 on epoch=14
03/18/2022 19:48:20 - INFO - __main__ - Step 210 Global step 210 Train loss 2.351384 on epoch=14
03/18/2022 19:48:26 - INFO - __main__ - Step 220 Global step 220 Train loss 1.775262 on epoch=15
03/18/2022 19:48:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.281883 on epoch=16
03/18/2022 19:48:36 - INFO - __main__ - Step 240 Global step 240 Train loss 1.381556 on epoch=17
03/18/2022 19:48:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.009933 on epoch=17
03/18/2022 19:48:44 - INFO - __main__ - Global step 250 Train loss 1.560004 Classification-F1 0.24404356941502636 on epoch=17
03/18/2022 19:48:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.514008 on epoch=18
03/18/2022 19:48:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.141925 on epoch=19
03/18/2022 19:48:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.170481 on epoch=19
03/18/2022 19:49:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.640354 on epoch=20
03/18/2022 19:49:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.266613 on epoch=21
03/18/2022 19:49:13 - INFO - __main__ - Global step 300 Train loss 0.346676 Classification-F1 0.7731047738697681 on epoch=21
03/18/2022 19:49:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.584796 on epoch=22
03/18/2022 19:49:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.709850 on epoch=22
03/18/2022 19:49:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.562249 on epoch=23
03/18/2022 19:49:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.605267 on epoch=24
03/18/2022 19:49:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.461056 on epoch=24
03/18/2022 19:49:44 - INFO - __main__ - Global step 350 Train loss 0.584644 Classification-F1 0.7131958330369713 on epoch=24
03/18/2022 19:49:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.418199 on epoch=25
03/18/2022 19:49:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.335385 on epoch=26
03/18/2022 19:49:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.209766 on epoch=27
03/18/2022 19:50:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.254987 on epoch=27
03/18/2022 19:50:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.144072 on epoch=28
03/18/2022 19:50:13 - INFO - __main__ - Global step 400 Train loss 0.272482 Classification-F1 0.6878563693691526 on epoch=28
03/18/2022 19:50:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.200226 on epoch=29
03/18/2022 19:50:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.140214 on epoch=29
03/18/2022 19:50:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.152114 on epoch=30
03/18/2022 19:50:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.085608 on epoch=31
03/18/2022 19:50:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.095503 on epoch=32
03/18/2022 19:50:43 - INFO - __main__ - Global step 450 Train loss 0.134733 Classification-F1 0.8293354850602751 on epoch=32
03/18/2022 19:50:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.084001 on epoch=32
03/18/2022 19:50:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.076043 on epoch=33
03/18/2022 19:50:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.067252 on epoch=34
03/18/2022 19:51:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.057357 on epoch=34
03/18/2022 19:51:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.045451 on epoch=35
03/18/2022 19:51:13 - INFO - __main__ - Global step 500 Train loss 0.066021 Classification-F1 0.6838208266717503 on epoch=35
03/18/2022 19:51:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.023078 on epoch=36
03/18/2022 19:51:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.053965 on epoch=37
03/18/2022 19:51:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.047944 on epoch=37
03/18/2022 19:51:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.041335 on epoch=38
03/18/2022 19:51:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.046753 on epoch=39
03/18/2022 19:51:43 - INFO - __main__ - Global step 550 Train loss 0.042615 Classification-F1 0.8252889536716373 on epoch=39
03/18/2022 19:51:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.014455 on epoch=39
03/18/2022 19:51:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.037972 on epoch=40
03/18/2022 19:51:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.009322 on epoch=41
03/18/2022 19:52:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014827 on epoch=42
03/18/2022 19:52:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.015238 on epoch=42
03/18/2022 19:52:13 - INFO - __main__ - Global step 600 Train loss 0.018363 Classification-F1 0.8008454206270217 on epoch=42
03/18/2022 19:52:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.014707 on epoch=43
03/18/2022 19:52:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.014059 on epoch=44
03/18/2022 19:52:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.003000 on epoch=44
03/18/2022 19:52:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.022163 on epoch=45
03/18/2022 19:52:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002427 on epoch=46
03/18/2022 19:52:42 - INFO - __main__ - Global step 650 Train loss 0.011271 Classification-F1 0.7481573322596774 on epoch=46
03/18/2022 19:52:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.019092 on epoch=47
03/18/2022 19:52:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002113 on epoch=47
03/18/2022 19:52:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.003168 on epoch=48
03/18/2022 19:53:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004162 on epoch=49
03/18/2022 19:53:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005035 on epoch=49
03/18/2022 19:53:10 - INFO - __main__ - Global step 700 Train loss 0.006714 Classification-F1 0.6882903185725262 on epoch=49
03/18/2022 19:53:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.017077 on epoch=50
03/18/2022 19:53:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.005567 on epoch=51
03/18/2022 19:53:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.004431 on epoch=52
03/18/2022 19:53:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002341 on epoch=52
03/18/2022 19:53:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.006140 on epoch=53
03/18/2022 19:53:39 - INFO - __main__ - Global step 750 Train loss 0.007111 Classification-F1 0.6748411585082782 on epoch=53
03/18/2022 19:53:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006711 on epoch=54
03/18/2022 19:53:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003516 on epoch=54
03/18/2022 19:53:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001078 on epoch=55
03/18/2022 19:53:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002246 on epoch=56
03/18/2022 19:54:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000883 on epoch=57
03/18/2022 19:54:08 - INFO - __main__ - Global step 800 Train loss 0.002887 Classification-F1 0.7238019882717374 on epoch=57
03/18/2022 19:54:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001012 on epoch=57
03/18/2022 19:54:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000662 on epoch=58
03/18/2022 19:54:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.036814 on epoch=59
03/18/2022 19:54:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.048389 on epoch=59
03/18/2022 19:54:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.032348 on epoch=60
03/18/2022 19:54:37 - INFO - __main__ - Global step 850 Train loss 0.023845 Classification-F1 0.5556204174481427 on epoch=60
03/18/2022 19:54:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.032983 on epoch=61
03/18/2022 19:54:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.026528 on epoch=62
03/18/2022 19:54:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.058877 on epoch=62
03/18/2022 19:54:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.005209 on epoch=63
03/18/2022 19:55:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007196 on epoch=64
03/18/2022 19:55:06 - INFO - __main__ - Global step 900 Train loss 0.026159 Classification-F1 0.6762301638464256 on epoch=64
03/18/2022 19:55:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.034266 on epoch=64
03/18/2022 19:55:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.036979 on epoch=65
03/18/2022 19:55:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.015137 on epoch=66
03/18/2022 19:55:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.022139 on epoch=67
03/18/2022 19:55:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002710 on epoch=67
03/18/2022 19:55:36 - INFO - __main__ - Global step 950 Train loss 0.022246 Classification-F1 0.8966612448903113 on epoch=67
03/18/2022 19:55:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001090 on epoch=68
03/18/2022 19:55:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.054711 on epoch=69
03/18/2022 19:55:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.021195 on epoch=69
03/18/2022 19:55:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.002240 on epoch=70
03/18/2022 19:56:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.014138 on epoch=71
03/18/2022 19:56:03 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:56:03 - INFO - __main__ - Printing 3 examples
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 19:56:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:56:03 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:56:03 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:56:03 - INFO - __main__ - Printing 3 examples
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 19:56:03 - INFO - __main__ - ['Plant']
03/18/2022 19:56:03 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:56:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:56:03 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:56:05 - INFO - __main__ - Global step 1000 Train loss 0.018675 Classification-F1 0.800946232168669 on epoch=71
03/18/2022 19:56:05 - INFO - __main__ - save last model!
03/18/2022 19:56:12 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 19:56:13 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 19:56:13 - INFO - __main__ - Printing 3 examples
03/18/2022 19:56:13 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 19:56:13 - INFO - __main__ - ['Animal']
03/18/2022 19:56:13 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 19:56:13 - INFO - __main__ - ['Animal']
03/18/2022 19:56:13 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 19:56:13 - INFO - __main__ - ['Village']
03/18/2022 19:56:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:56:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:56:14 - INFO - __main__ - Starting training!
03/18/2022 19:56:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:56:18 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 19:57:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_21_0.0005_8_predictions.txt
03/18/2022 19:57:32 - INFO - __main__ - Classification-F1 on test data: 0.3786
03/18/2022 19:57:32 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.0005, bsz=8, dev_performance=0.8966612448903113, test_performance=0.37863725733063497
03/18/2022 19:57:32 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.0003, bsz=8 ...
03/18/2022 19:57:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:57:33 - INFO - __main__ - Printing 3 examples
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 19:57:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:57:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 19:57:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 19:57:33 - INFO - __main__ - Printing 3 examples
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 19:57:33 - INFO - __main__ - ['Plant']
03/18/2022 19:57:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 19:57:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 19:57:34 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 19:57:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 19:57:47 - INFO - __main__ - Starting training!
03/18/2022 19:57:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.024654 on epoch=0
03/18/2022 19:57:58 - INFO - __main__ - Step 20 Global step 20 Train loss 19.311100 on epoch=1
03/18/2022 19:58:03 - INFO - __main__ - Step 30 Global step 30 Train loss 14.941861 on epoch=2
03/18/2022 19:58:08 - INFO - __main__ - Step 40 Global step 40 Train loss 13.509738 on epoch=2
03/18/2022 19:58:13 - INFO - __main__ - Step 50 Global step 50 Train loss 11.500751 on epoch=3
03/18/2022 19:58:17 - INFO - __main__ - Global step 50 Train loss 16.257620 Classification-F1 0.0 on epoch=3
03/18/2022 19:58:23 - INFO - __main__ - Step 60 Global step 60 Train loss 11.068556 on epoch=4
03/18/2022 19:58:28 - INFO - __main__ - Step 70 Global step 70 Train loss 10.186668 on epoch=4
03/18/2022 19:58:33 - INFO - __main__ - Step 80 Global step 80 Train loss 9.854844 on epoch=5
03/18/2022 19:58:38 - INFO - __main__ - Step 90 Global step 90 Train loss 8.987858 on epoch=6
03/18/2022 19:58:44 - INFO - __main__ - Step 100 Global step 100 Train loss 8.602451 on epoch=7
03/18/2022 19:58:48 - INFO - __main__ - Global step 100 Train loss 9.740075 Classification-F1 0.0 on epoch=7
03/18/2022 19:58:53 - INFO - __main__ - Step 110 Global step 110 Train loss 8.235892 on epoch=7
03/18/2022 19:58:58 - INFO - __main__ - Step 120 Global step 120 Train loss 7.149685 on epoch=8
03/18/2022 19:59:03 - INFO - __main__ - Step 130 Global step 130 Train loss 6.871947 on epoch=9
03/18/2022 19:59:08 - INFO - __main__ - Step 140 Global step 140 Train loss 5.982557 on epoch=9
03/18/2022 19:59:13 - INFO - __main__ - Step 150 Global step 150 Train loss 5.378006 on epoch=10
03/18/2022 19:59:17 - INFO - __main__ - Global step 150 Train loss 6.723618 Classification-F1 0.0 on epoch=10
03/18/2022 19:59:22 - INFO - __main__ - Step 160 Global step 160 Train loss 4.189298 on epoch=11
03/18/2022 19:59:27 - INFO - __main__ - Step 170 Global step 170 Train loss 3.956387 on epoch=12
03/18/2022 19:59:32 - INFO - __main__ - Step 180 Global step 180 Train loss 3.548518 on epoch=12
03/18/2022 19:59:37 - INFO - __main__ - Step 190 Global step 190 Train loss 3.214887 on epoch=13
03/18/2022 19:59:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.240893 on epoch=14
03/18/2022 19:59:45 - INFO - __main__ - Global step 200 Train loss 3.629997 Classification-F1 0.07165622101683482 on epoch=14
03/18/2022 19:59:51 - INFO - __main__ - Step 210 Global step 210 Train loss 2.872010 on epoch=14
03/18/2022 19:59:57 - INFO - __main__ - Step 220 Global step 220 Train loss 2.891931 on epoch=15
03/18/2022 20:00:02 - INFO - __main__ - Step 230 Global step 230 Train loss 2.634097 on epoch=16
03/18/2022 20:00:07 - INFO - __main__ - Step 240 Global step 240 Train loss 2.612359 on epoch=17
03/18/2022 20:00:12 - INFO - __main__ - Step 250 Global step 250 Train loss 3.107516 on epoch=17
03/18/2022 20:00:15 - INFO - __main__ - Global step 250 Train loss 2.823583 Classification-F1 0.4101723651006865 on epoch=17
03/18/2022 20:00:21 - INFO - __main__ - Step 260 Global step 260 Train loss 2.498122 on epoch=18
03/18/2022 20:00:26 - INFO - __main__ - Step 270 Global step 270 Train loss 2.343041 on epoch=19
03/18/2022 20:00:31 - INFO - __main__ - Step 280 Global step 280 Train loss 2.356583 on epoch=19
03/18/2022 20:00:36 - INFO - __main__ - Step 290 Global step 290 Train loss 2.325480 on epoch=20
03/18/2022 20:00:41 - INFO - __main__ - Step 300 Global step 300 Train loss 2.189955 on epoch=21
03/18/2022 20:00:44 - INFO - __main__ - Global step 300 Train loss 2.342636 Classification-F1 0.6363309077296609 on epoch=21
03/18/2022 20:00:50 - INFO - __main__ - Step 310 Global step 310 Train loss 1.896808 on epoch=22
03/18/2022 20:00:55 - INFO - __main__ - Step 320 Global step 320 Train loss 2.160527 on epoch=22
03/18/2022 20:01:00 - INFO - __main__ - Step 330 Global step 330 Train loss 1.767655 on epoch=23
03/18/2022 20:01:05 - INFO - __main__ - Step 340 Global step 340 Train loss 2.098423 on epoch=24
03/18/2022 20:01:10 - INFO - __main__ - Step 350 Global step 350 Train loss 1.728265 on epoch=24
03/18/2022 20:01:14 - INFO - __main__ - Global step 350 Train loss 1.930336 Classification-F1 0.5406108972942515 on epoch=24
03/18/2022 20:01:19 - INFO - __main__ - Step 360 Global step 360 Train loss 1.706575 on epoch=25
03/18/2022 20:01:24 - INFO - __main__ - Step 370 Global step 370 Train loss 1.377444 on epoch=26
03/18/2022 20:01:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.265564 on epoch=27
03/18/2022 20:01:34 - INFO - __main__ - Step 390 Global step 390 Train loss 1.523124 on epoch=27
03/18/2022 20:01:39 - INFO - __main__ - Step 400 Global step 400 Train loss 1.176537 on epoch=28
03/18/2022 20:01:42 - INFO - __main__ - Global step 400 Train loss 1.409848 Classification-F1 0.6885446796982001 on epoch=28
03/18/2022 20:01:48 - INFO - __main__ - Step 410 Global step 410 Train loss 1.221853 on epoch=29
03/18/2022 20:01:53 - INFO - __main__ - Step 420 Global step 420 Train loss 1.001857 on epoch=29
03/18/2022 20:01:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.942924 on epoch=30
03/18/2022 20:02:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.909204 on epoch=31
03/18/2022 20:02:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.571468 on epoch=32
03/18/2022 20:02:12 - INFO - __main__ - Global step 450 Train loss 0.929461 Classification-F1 0.5279224952426421 on epoch=32
03/18/2022 20:02:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.237762 on epoch=32
03/18/2022 20:02:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186598 on epoch=33
03/18/2022 20:02:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.066048 on epoch=34
03/18/2022 20:02:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.030826 on epoch=34
03/18/2022 20:02:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.045931 on epoch=35
03/18/2022 20:02:41 - INFO - __main__ - Global step 500 Train loss 0.113433 Classification-F1 0.7933424487830323 on epoch=35
03/18/2022 20:02:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.082330 on epoch=36
03/18/2022 20:02:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.019665 on epoch=37
03/18/2022 20:02:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.053439 on epoch=37
03/18/2022 20:03:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.016215 on epoch=38
03/18/2022 20:03:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.028048 on epoch=39
03/18/2022 20:03:10 - INFO - __main__ - Global step 550 Train loss 0.039939 Classification-F1 0.681585182949035 on epoch=39
03/18/2022 20:03:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009867 on epoch=39
03/18/2022 20:03:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001982 on epoch=40
03/18/2022 20:03:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001791 on epoch=41
03/18/2022 20:03:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.066037 on epoch=42
03/18/2022 20:03:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.020025 on epoch=42
03/18/2022 20:03:39 - INFO - __main__ - Global step 600 Train loss 0.019941 Classification-F1 0.7029214504309381 on epoch=42
03/18/2022 20:03:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.034233 on epoch=43
03/18/2022 20:03:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.019689 on epoch=44
03/18/2022 20:03:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.022514 on epoch=44
03/18/2022 20:03:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002753 on epoch=45
03/18/2022 20:04:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.009928 on epoch=46
03/18/2022 20:04:07 - INFO - __main__ - Global step 650 Train loss 0.017824 Classification-F1 0.8213594226180208 on epoch=46
03/18/2022 20:04:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002764 on epoch=47
03/18/2022 20:04:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.021515 on epoch=47
03/18/2022 20:04:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.012009 on epoch=48
03/18/2022 20:04:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.016924 on epoch=49
03/18/2022 20:04:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.010279 on epoch=49
03/18/2022 20:04:36 - INFO - __main__ - Global step 700 Train loss 0.012698 Classification-F1 0.7493448512477452 on epoch=49
03/18/2022 20:04:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001586 on epoch=50
03/18/2022 20:04:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001091 on epoch=51
03/18/2022 20:04:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001750 on epoch=52
03/18/2022 20:04:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004587 on epoch=52
03/18/2022 20:05:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000833 on epoch=53
03/18/2022 20:05:05 - INFO - __main__ - Global step 750 Train loss 0.001969 Classification-F1 0.8074187745618523 on epoch=53
03/18/2022 20:05:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.027429 on epoch=54
03/18/2022 20:05:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001022 on epoch=54
03/18/2022 20:05:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000477 on epoch=55
03/18/2022 20:05:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000284 on epoch=56
03/18/2022 20:05:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000158 on epoch=57
03/18/2022 20:05:34 - INFO - __main__ - Global step 800 Train loss 0.005874 Classification-F1 0.8076689464275446 on epoch=57
03/18/2022 20:05:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000938 on epoch=57
03/18/2022 20:05:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000081 on epoch=58
03/18/2022 20:05:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000154 on epoch=59
03/18/2022 20:05:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001695 on epoch=59
03/18/2022 20:05:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000150 on epoch=60
03/18/2022 20:06:03 - INFO - __main__ - Global step 850 Train loss 0.000604 Classification-F1 0.8155408149330113 on epoch=60
03/18/2022 20:06:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000643 on epoch=61
03/18/2022 20:06:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000198 on epoch=62
03/18/2022 20:06:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.042922 on epoch=62
03/18/2022 20:06:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.075342 on epoch=63
03/18/2022 20:06:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000669 on epoch=64
03/18/2022 20:06:32 - INFO - __main__ - Global step 900 Train loss 0.023955 Classification-F1 0.79551204004329 on epoch=64
03/18/2022 20:06:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002189 on epoch=64
03/18/2022 20:06:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004013 on epoch=65
03/18/2022 20:06:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.169411 on epoch=66
03/18/2022 20:06:52 - INFO - __main__ - Step 940 Global step 940 Train loss 1.864439 on epoch=67
03/18/2022 20:06:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.888173 on epoch=67
03/18/2022 20:07:01 - INFO - __main__ - Global step 950 Train loss 0.585645 Classification-F1 0.5210516223351622 on epoch=67
03/18/2022 20:07:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.080354 on epoch=68
03/18/2022 20:07:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.003298 on epoch=69
03/18/2022 20:07:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001367 on epoch=69
03/18/2022 20:07:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000231 on epoch=70
03/18/2022 20:07:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000381 on epoch=71
03/18/2022 20:07:27 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:07:27 - INFO - __main__ - Printing 3 examples
03/18/2022 20:07:27 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 20:07:27 - INFO - __main__ - ['Plant']
03/18/2022 20:07:27 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 20:07:27 - INFO - __main__ - ['Plant']
03/18/2022 20:07:27 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 20:07:27 - INFO - __main__ - ['Plant']
03/18/2022 20:07:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 20:07:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:07:28 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:07:28 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:07:28 - INFO - __main__ - Printing 3 examples
03/18/2022 20:07:28 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 20:07:28 - INFO - __main__ - ['Plant']
03/18/2022 20:07:28 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 20:07:28 - INFO - __main__ - ['Plant']
03/18/2022 20:07:28 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 20:07:28 - INFO - __main__ - ['Plant']
03/18/2022 20:07:28 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:07:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:07:28 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:07:30 - INFO - __main__ - Global step 1000 Train loss 0.017126 Classification-F1 0.8078797043010753 on epoch=71
03/18/2022 20:07:30 - INFO - __main__ - save last model!
03/18/2022 20:07:37 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 20:07:37 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 20:07:37 - INFO - __main__ - Printing 3 examples
03/18/2022 20:07:37 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 20:07:37 - INFO - __main__ - ['Animal']
03/18/2022 20:07:37 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 20:07:37 - INFO - __main__ - ['Animal']
03/18/2022 20:07:37 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 20:07:37 - INFO - __main__ - ['Village']
03/18/2022 20:07:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:07:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:07:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:07:39 - INFO - __main__ - Starting training!
03/18/2022 20:07:43 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 20:08:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_21_0.0003_8_predictions.txt
03/18/2022 20:08:54 - INFO - __main__ - Classification-F1 on test data: 0.5001
03/18/2022 20:08:54 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.0003, bsz=8, dev_performance=0.8213594226180208, test_performance=0.5001140788813317
03/18/2022 20:08:54 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.0002, bsz=8 ...
03/18/2022 20:08:55 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:08:55 - INFO - __main__ - Printing 3 examples
03/18/2022 20:08:55 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 20:08:55 - INFO - __main__ - ['Plant']
03/18/2022 20:08:55 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 20:08:55 - INFO - __main__ - ['Plant']
03/18/2022 20:08:55 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 20:08:55 - INFO - __main__ - ['Plant']
03/18/2022 20:08:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:08:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:08:56 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:08:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:08:56 - INFO - __main__ - Printing 3 examples
03/18/2022 20:08:56 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 20:08:56 - INFO - __main__ - ['Plant']
03/18/2022 20:08:56 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 20:08:56 - INFO - __main__ - ['Plant']
03/18/2022 20:08:56 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 20:08:56 - INFO - __main__ - ['Plant']
03/18/2022 20:08:56 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:08:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:08:56 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:09:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:09:09 - INFO - __main__ - Starting training!
03/18/2022 20:09:13 - INFO - __main__ - Step 10 Global step 10 Train loss 21.270830 on epoch=0
03/18/2022 20:09:18 - INFO - __main__ - Step 20 Global step 20 Train loss 19.947830 on epoch=1
03/18/2022 20:09:23 - INFO - __main__ - Step 30 Global step 30 Train loss 17.862139 on epoch=2
03/18/2022 20:09:28 - INFO - __main__ - Step 40 Global step 40 Train loss 14.722613 on epoch=2
03/18/2022 20:09:33 - INFO - __main__ - Step 50 Global step 50 Train loss 13.444511 on epoch=3
03/18/2022 20:09:39 - INFO - __main__ - Global step 50 Train loss 17.449583 Classification-F1 0.0 on epoch=3
03/18/2022 20:09:44 - INFO - __main__ - Step 60 Global step 60 Train loss 12.991959 on epoch=4
03/18/2022 20:09:49 - INFO - __main__ - Step 70 Global step 70 Train loss 11.928797 on epoch=4
03/18/2022 20:09:54 - INFO - __main__ - Step 80 Global step 80 Train loss 11.758286 on epoch=5
03/18/2022 20:09:59 - INFO - __main__ - Step 90 Global step 90 Train loss 10.112189 on epoch=6
03/18/2022 20:10:04 - INFO - __main__ - Step 100 Global step 100 Train loss 10.499308 on epoch=7
03/18/2022 20:10:09 - INFO - __main__ - Global step 100 Train loss 11.458108 Classification-F1 0.0 on epoch=7
03/18/2022 20:10:14 - INFO - __main__ - Step 110 Global step 110 Train loss 10.762158 on epoch=7
03/18/2022 20:10:19 - INFO - __main__ - Step 120 Global step 120 Train loss 9.218406 on epoch=8
03/18/2022 20:10:24 - INFO - __main__ - Step 130 Global step 130 Train loss 9.086576 on epoch=9
03/18/2022 20:10:29 - INFO - __main__ - Step 140 Global step 140 Train loss 8.263101 on epoch=9
03/18/2022 20:10:34 - INFO - __main__ - Step 150 Global step 150 Train loss 8.665464 on epoch=10
03/18/2022 20:10:38 - INFO - __main__ - Global step 150 Train loss 9.199142 Classification-F1 0.0 on epoch=10
03/18/2022 20:10:43 - INFO - __main__ - Step 160 Global step 160 Train loss 7.465688 on epoch=11
03/18/2022 20:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 7.621258 on epoch=12
03/18/2022 20:10:53 - INFO - __main__ - Step 180 Global step 180 Train loss 7.525180 on epoch=12
03/18/2022 20:10:58 - INFO - __main__ - Step 190 Global step 190 Train loss 6.315302 on epoch=13
03/18/2022 20:11:03 - INFO - __main__ - Step 200 Global step 200 Train loss 6.361961 on epoch=14
03/18/2022 20:11:07 - INFO - __main__ - Global step 200 Train loss 7.057878 Classification-F1 0.0 on epoch=14
03/18/2022 20:11:12 - INFO - __main__ - Step 210 Global step 210 Train loss 5.259866 on epoch=14
03/18/2022 20:11:17 - INFO - __main__ - Step 220 Global step 220 Train loss 5.017543 on epoch=15
03/18/2022 20:11:22 - INFO - __main__ - Step 230 Global step 230 Train loss 4.370385 on epoch=16
03/18/2022 20:11:27 - INFO - __main__ - Step 240 Global step 240 Train loss 4.641072 on epoch=17
03/18/2022 20:11:33 - INFO - __main__ - Step 250 Global step 250 Train loss 3.765550 on epoch=17
03/18/2022 20:11:36 - INFO - __main__ - Global step 250 Train loss 4.610883 Classification-F1 0.0 on epoch=17
03/18/2022 20:11:41 - INFO - __main__ - Step 260 Global step 260 Train loss 3.626911 on epoch=18
03/18/2022 20:11:46 - INFO - __main__ - Step 270 Global step 270 Train loss 3.512094 on epoch=19
03/18/2022 20:11:51 - INFO - __main__ - Step 280 Global step 280 Train loss 2.817799 on epoch=19
03/18/2022 20:11:56 - INFO - __main__ - Step 290 Global step 290 Train loss 3.084664 on epoch=20
03/18/2022 20:12:01 - INFO - __main__ - Step 300 Global step 300 Train loss 2.991935 on epoch=21
03/18/2022 20:12:04 - INFO - __main__ - Global step 300 Train loss 3.206681 Classification-F1 0.22217321661516332 on epoch=21
03/18/2022 20:12:10 - INFO - __main__ - Step 310 Global step 310 Train loss 3.740342 on epoch=22
03/18/2022 20:12:15 - INFO - __main__ - Step 320 Global step 320 Train loss 3.032720 on epoch=22
03/18/2022 20:12:20 - INFO - __main__ - Step 330 Global step 330 Train loss 2.422876 on epoch=23
03/18/2022 20:12:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.710674 on epoch=24
03/18/2022 20:12:30 - INFO - __main__ - Step 350 Global step 350 Train loss 2.640695 on epoch=24
03/18/2022 20:12:33 - INFO - __main__ - Global step 350 Train loss 2.909461 Classification-F1 0.2887822160972066 on epoch=24
03/18/2022 20:12:39 - INFO - __main__ - Step 360 Global step 360 Train loss 2.531845 on epoch=25
03/18/2022 20:12:43 - INFO - __main__ - Step 370 Global step 370 Train loss 2.655355 on epoch=26
03/18/2022 20:12:48 - INFO - __main__ - Step 380 Global step 380 Train loss 2.416156 on epoch=27
03/18/2022 20:12:53 - INFO - __main__ - Step 390 Global step 390 Train loss 2.580740 on epoch=27
03/18/2022 20:12:58 - INFO - __main__ - Step 400 Global step 400 Train loss 2.146998 on epoch=28
03/18/2022 20:13:01 - INFO - __main__ - Global step 400 Train loss 2.466219 Classification-F1 0.3326171332139052 on epoch=28
03/18/2022 20:13:07 - INFO - __main__ - Step 410 Global step 410 Train loss 2.493475 on epoch=29
03/18/2022 20:13:12 - INFO - __main__ - Step 420 Global step 420 Train loss 2.229227 on epoch=29
03/18/2022 20:13:17 - INFO - __main__ - Step 430 Global step 430 Train loss 2.132704 on epoch=30
03/18/2022 20:13:22 - INFO - __main__ - Step 440 Global step 440 Train loss 2.270384 on epoch=31
03/18/2022 20:13:27 - INFO - __main__ - Step 450 Global step 450 Train loss 2.047641 on epoch=32
03/18/2022 20:13:30 - INFO - __main__ - Global step 450 Train loss 2.234686 Classification-F1 0.5101601722608751 on epoch=32
03/18/2022 20:13:36 - INFO - __main__ - Step 460 Global step 460 Train loss 1.768934 on epoch=32
03/18/2022 20:13:41 - INFO - __main__ - Step 470 Global step 470 Train loss 2.053118 on epoch=33
03/18/2022 20:13:45 - INFO - __main__ - Step 480 Global step 480 Train loss 1.880411 on epoch=34
03/18/2022 20:13:50 - INFO - __main__ - Step 490 Global step 490 Train loss 1.598544 on epoch=34
03/18/2022 20:13:55 - INFO - __main__ - Step 500 Global step 500 Train loss 1.790269 on epoch=35
03/18/2022 20:13:58 - INFO - __main__ - Global step 500 Train loss 1.818255 Classification-F1 0.63762754296707 on epoch=35
03/18/2022 20:14:04 - INFO - __main__ - Step 510 Global step 510 Train loss 1.532808 on epoch=36
03/18/2022 20:14:09 - INFO - __main__ - Step 520 Global step 520 Train loss 1.500100 on epoch=37
03/18/2022 20:14:14 - INFO - __main__ - Step 530 Global step 530 Train loss 1.538185 on epoch=37
03/18/2022 20:14:19 - INFO - __main__ - Step 540 Global step 540 Train loss 1.595811 on epoch=38
03/18/2022 20:14:23 - INFO - __main__ - Step 550 Global step 550 Train loss 1.427979 on epoch=39
03/18/2022 20:14:26 - INFO - __main__ - Global step 550 Train loss 1.518977 Classification-F1 0.4315826793772199 on epoch=39
03/18/2022 20:14:31 - INFO - __main__ - Step 560 Global step 560 Train loss 1.125251 on epoch=39
03/18/2022 20:14:36 - INFO - __main__ - Step 570 Global step 570 Train loss 1.175578 on epoch=40
03/18/2022 20:14:41 - INFO - __main__ - Step 580 Global step 580 Train loss 1.129648 on epoch=41
03/18/2022 20:14:46 - INFO - __main__ - Step 590 Global step 590 Train loss 1.083515 on epoch=42
03/18/2022 20:14:51 - INFO - __main__ - Step 600 Global step 600 Train loss 1.171177 on epoch=42
03/18/2022 20:14:54 - INFO - __main__ - Global step 600 Train loss 1.137034 Classification-F1 0.6332283140222106 on epoch=42
03/18/2022 20:14:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.964205 on epoch=43
03/18/2022 20:15:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.953474 on epoch=44
03/18/2022 20:15:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.967290 on epoch=44
03/18/2022 20:15:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.771556 on epoch=45
03/18/2022 20:15:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.665352 on epoch=46
03/18/2022 20:15:22 - INFO - __main__ - Global step 650 Train loss 0.864375 Classification-F1 0.5561948655536092 on epoch=46
03/18/2022 20:15:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.458393 on epoch=47
03/18/2022 20:15:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.056395 on epoch=47
03/18/2022 20:15:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.038851 on epoch=48
03/18/2022 20:15:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.033828 on epoch=49
03/18/2022 20:15:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.016555 on epoch=49
03/18/2022 20:15:50 - INFO - __main__ - Global step 700 Train loss 0.120804 Classification-F1 0.7365964930839975 on epoch=49
03/18/2022 20:15:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.067217 on epoch=50
03/18/2022 20:16:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.036740 on epoch=51
03/18/2022 20:16:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.058251 on epoch=52
03/18/2022 20:16:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.031238 on epoch=52
03/18/2022 20:16:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.014303 on epoch=53
03/18/2022 20:16:19 - INFO - __main__ - Global step 750 Train loss 0.041550 Classification-F1 0.8435181534327645 on epoch=53
03/18/2022 20:16:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.039325 on epoch=54
03/18/2022 20:16:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.067559 on epoch=54
03/18/2022 20:16:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.022336 on epoch=55
03/18/2022 20:16:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.004546 on epoch=56
03/18/2022 20:16:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.030932 on epoch=57
03/18/2022 20:16:48 - INFO - __main__ - Global step 800 Train loss 0.032940 Classification-F1 0.7263980317460539 on epoch=57
03/18/2022 20:16:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.022612 on epoch=57
03/18/2022 20:16:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004383 on epoch=58
03/18/2022 20:17:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.044397 on epoch=59
03/18/2022 20:17:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001926 on epoch=59
03/18/2022 20:17:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.006105 on epoch=60
03/18/2022 20:17:16 - INFO - __main__ - Global step 850 Train loss 0.015885 Classification-F1 0.7439247979409299 on epoch=60
03/18/2022 20:17:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.019178 on epoch=61
03/18/2022 20:17:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003833 on epoch=62
03/18/2022 20:17:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000601 on epoch=62
03/18/2022 20:17:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.067704 on epoch=63
03/18/2022 20:17:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001805 on epoch=64
03/18/2022 20:17:44 - INFO - __main__ - Global step 900 Train loss 0.018624 Classification-F1 0.7378886908067254 on epoch=64
03/18/2022 20:17:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.003734 on epoch=64
03/18/2022 20:17:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002826 on epoch=65
03/18/2022 20:17:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000841 on epoch=66
03/18/2022 20:18:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.023703 on epoch=67
03/18/2022 20:18:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001343 on epoch=67
03/18/2022 20:18:12 - INFO - __main__ - Global step 950 Train loss 0.006489 Classification-F1 0.8040128282390416 on epoch=67
03/18/2022 20:18:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001267 on epoch=68
03/18/2022 20:18:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.020476 on epoch=69
03/18/2022 20:18:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000302 on epoch=69
03/18/2022 20:18:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000107 on epoch=70
03/18/2022 20:18:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.005299 on epoch=71
03/18/2022 20:18:37 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:18:37 - INFO - __main__ - Printing 3 examples
03/18/2022 20:18:37 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 20:18:37 - INFO - __main__ - ['Plant']
03/18/2022 20:18:37 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 20:18:37 - INFO - __main__ - ['Plant']
03/18/2022 20:18:37 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 20:18:37 - INFO - __main__ - ['Plant']
03/18/2022 20:18:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 20:18:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:18:38 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:18:38 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:18:38 - INFO - __main__ - Printing 3 examples
03/18/2022 20:18:38 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 20:18:38 - INFO - __main__ - ['Plant']
03/18/2022 20:18:38 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 20:18:38 - INFO - __main__ - ['Plant']
03/18/2022 20:18:38 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 20:18:38 - INFO - __main__ - ['Plant']
03/18/2022 20:18:38 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:18:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:18:38 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:18:40 - INFO - __main__ - Global step 1000 Train loss 0.005490 Classification-F1 0.6960485953818688 on epoch=71
03/18/2022 20:18:40 - INFO - __main__ - save last model!
03/18/2022 20:18:47 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 20:18:47 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 20:18:47 - INFO - __main__ - Printing 3 examples
03/18/2022 20:18:47 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 20:18:47 - INFO - __main__ - ['Animal']
03/18/2022 20:18:47 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 20:18:47 - INFO - __main__ - ['Animal']
03/18/2022 20:18:47 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 20:18:47 - INFO - __main__ - ['Village']
03/18/2022 20:18:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:18:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:18:49 - INFO - __main__ - Starting training!
03/18/2022 20:18:49 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:18:53 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 20:20:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_21_0.0002_8_predictions.txt
03/18/2022 20:20:02 - INFO - __main__ - Classification-F1 on test data: 0.4790
03/18/2022 20:20:03 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.0002, bsz=8, dev_performance=0.8435181534327645, test_performance=0.479004323063926
03/18/2022 20:20:03 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.0001, bsz=8 ...
03/18/2022 20:20:04 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:20:04 - INFO - __main__ - Printing 3 examples
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:20:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:20:04 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:20:04 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:20:04 - INFO - __main__ - Printing 3 examples
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceaesunflower family. The plant is native to Europe and Asia.
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 20:20:04 - INFO - __main__ - ['Plant']
03/18/2022 20:20:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:20:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:20:04 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:20:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:20:15 - INFO - __main__ - Starting training!
03/18/2022 20:20:19 - INFO - __main__ - Step 10 Global step 10 Train loss 22.957096 on epoch=0
03/18/2022 20:20:25 - INFO - __main__ - Step 20 Global step 20 Train loss 20.693413 on epoch=1
03/18/2022 20:20:30 - INFO - __main__ - Step 30 Global step 30 Train loss 19.787933 on epoch=2
03/18/2022 20:20:35 - INFO - __main__ - Step 40 Global step 40 Train loss 16.972866 on epoch=2
03/18/2022 20:20:40 - INFO - __main__ - Step 50 Global step 50 Train loss 15.290484 on epoch=3
03/18/2022 20:20:55 - INFO - __main__ - Global step 50 Train loss 19.140358 Classification-F1 0.0 on epoch=3
03/18/2022 20:21:01 - INFO - __main__ - Step 60 Global step 60 Train loss 14.733538 on epoch=4
03/18/2022 20:21:06 - INFO - __main__ - Step 70 Global step 70 Train loss 13.992989 on epoch=4
03/18/2022 20:21:11 - INFO - __main__ - Step 80 Global step 80 Train loss 13.696373 on epoch=5
03/18/2022 20:21:16 - INFO - __main__ - Step 90 Global step 90 Train loss 12.291315 on epoch=6
03/18/2022 20:21:21 - INFO - __main__ - Step 100 Global step 100 Train loss 12.533847 on epoch=7
03/18/2022 20:21:26 - INFO - __main__ - Global step 100 Train loss 13.449612 Classification-F1 0.0 on epoch=7
03/18/2022 20:21:31 - INFO - __main__ - Step 110 Global step 110 Train loss 12.253805 on epoch=7
03/18/2022 20:21:36 - INFO - __main__ - Step 120 Global step 120 Train loss 11.619060 on epoch=8
03/18/2022 20:21:41 - INFO - __main__ - Step 130 Global step 130 Train loss 11.489184 on epoch=9
03/18/2022 20:21:46 - INFO - __main__ - Step 140 Global step 140 Train loss 10.811695 on epoch=9
03/18/2022 20:21:51 - INFO - __main__ - Step 150 Global step 150 Train loss 11.244284 on epoch=10
03/18/2022 20:21:55 - INFO - __main__ - Global step 150 Train loss 11.483606 Classification-F1 0.0 on epoch=10
03/18/2022 20:22:00 - INFO - __main__ - Step 160 Global step 160 Train loss 10.071309 on epoch=11
03/18/2022 20:22:05 - INFO - __main__ - Step 170 Global step 170 Train loss 10.657496 on epoch=12
03/18/2022 20:22:10 - INFO - __main__ - Step 180 Global step 180 Train loss 10.551504 on epoch=12
03/18/2022 20:22:15 - INFO - __main__ - Step 190 Global step 190 Train loss 10.146013 on epoch=13
03/18/2022 20:22:20 - INFO - __main__ - Step 200 Global step 200 Train loss 9.569804 on epoch=14
03/18/2022 20:22:24 - INFO - __main__ - Global step 200 Train loss 10.199226 Classification-F1 0.0 on epoch=14
03/18/2022 20:22:29 - INFO - __main__ - Step 210 Global step 210 Train loss 9.857656 on epoch=14
03/18/2022 20:22:34 - INFO - __main__ - Step 220 Global step 220 Train loss 9.646511 on epoch=15
03/18/2022 20:22:39 - INFO - __main__ - Step 230 Global step 230 Train loss 8.778345 on epoch=16
03/18/2022 20:22:44 - INFO - __main__ - Step 240 Global step 240 Train loss 9.261050 on epoch=17
03/18/2022 20:22:49 - INFO - __main__ - Step 250 Global step 250 Train loss 9.608325 on epoch=17
03/18/2022 20:22:53 - INFO - __main__ - Global step 250 Train loss 9.430377 Classification-F1 0.0 on epoch=17
03/18/2022 20:22:58 - INFO - __main__ - Step 260 Global step 260 Train loss 7.920125 on epoch=18
03/18/2022 20:23:03 - INFO - __main__ - Step 270 Global step 270 Train loss 8.974747 on epoch=19
03/18/2022 20:23:08 - INFO - __main__ - Step 280 Global step 280 Train loss 7.942759 on epoch=19
03/18/2022 20:23:13 - INFO - __main__ - Step 290 Global step 290 Train loss 8.514828 on epoch=20
03/18/2022 20:23:18 - INFO - __main__ - Step 300 Global step 300 Train loss 7.437688 on epoch=21
03/18/2022 20:23:21 - INFO - __main__ - Global step 300 Train loss 8.158030 Classification-F1 0.0 on epoch=21
03/18/2022 20:23:26 - INFO - __main__ - Step 310 Global step 310 Train loss 7.679086 on epoch=22
03/18/2022 20:23:32 - INFO - __main__ - Step 320 Global step 320 Train loss 7.859348 on epoch=22
03/18/2022 20:23:37 - INFO - __main__ - Step 330 Global step 330 Train loss 6.899527 on epoch=23
03/18/2022 20:23:42 - INFO - __main__ - Step 340 Global step 340 Train loss 6.834577 on epoch=24
03/18/2022 20:23:47 - INFO - __main__ - Step 350 Global step 350 Train loss 6.160389 on epoch=24
03/18/2022 20:23:50 - INFO - __main__ - Global step 350 Train loss 7.086586 Classification-F1 0.0 on epoch=24
03/18/2022 20:23:55 - INFO - __main__ - Step 360 Global step 360 Train loss 6.210290 on epoch=25
03/18/2022 20:24:00 - INFO - __main__ - Step 370 Global step 370 Train loss 5.832969 on epoch=26
03/18/2022 20:24:05 - INFO - __main__ - Step 380 Global step 380 Train loss 5.128337 on epoch=27
03/18/2022 20:24:10 - INFO - __main__ - Step 390 Global step 390 Train loss 5.572373 on epoch=27
03/18/2022 20:24:15 - INFO - __main__ - Step 400 Global step 400 Train loss 4.905814 on epoch=28
03/18/2022 20:24:19 - INFO - __main__ - Global step 400 Train loss 5.529957 Classification-F1 0.0 on epoch=28
03/18/2022 20:24:24 - INFO - __main__ - Step 410 Global step 410 Train loss 4.171169 on epoch=29
03/18/2022 20:24:29 - INFO - __main__ - Step 420 Global step 420 Train loss 4.678124 on epoch=29
03/18/2022 20:24:34 - INFO - __main__ - Step 430 Global step 430 Train loss 4.494897 on epoch=30
03/18/2022 20:24:39 - INFO - __main__ - Step 440 Global step 440 Train loss 4.152281 on epoch=31
03/18/2022 20:24:44 - INFO - __main__ - Step 450 Global step 450 Train loss 4.027502 on epoch=32
03/18/2022 20:24:48 - INFO - __main__ - Global step 450 Train loss 4.304795 Classification-F1 0.05715931533903884 on epoch=32
03/18/2022 20:24:54 - INFO - __main__ - Step 460 Global step 460 Train loss 4.160369 on epoch=32
03/18/2022 20:24:59 - INFO - __main__ - Step 470 Global step 470 Train loss 3.636525 on epoch=33
03/18/2022 20:25:04 - INFO - __main__ - Step 480 Global step 480 Train loss 3.504988 on epoch=34
03/18/2022 20:25:09 - INFO - __main__ - Step 490 Global step 490 Train loss 3.418189 on epoch=34
03/18/2022 20:25:14 - INFO - __main__ - Step 500 Global step 500 Train loss 3.208091 on epoch=35
03/18/2022 20:25:17 - INFO - __main__ - Global step 500 Train loss 3.585632 Classification-F1 0.16772292043029213 on epoch=35
03/18/2022 20:25:23 - INFO - __main__ - Step 510 Global step 510 Train loss 3.126794 on epoch=36
03/18/2022 20:25:28 - INFO - __main__ - Step 520 Global step 520 Train loss 3.551408 on epoch=37
03/18/2022 20:25:33 - INFO - __main__ - Step 530 Global step 530 Train loss 3.031528 on epoch=37
03/18/2022 20:25:38 - INFO - __main__ - Step 540 Global step 540 Train loss 3.149148 on epoch=38
03/18/2022 20:25:43 - INFO - __main__ - Step 550 Global step 550 Train loss 3.028970 on epoch=39
03/18/2022 20:25:46 - INFO - __main__ - Global step 550 Train loss 3.177570 Classification-F1 0.2941569104130681 on epoch=39
03/18/2022 20:25:52 - INFO - __main__ - Step 560 Global step 560 Train loss 2.788268 on epoch=39
03/18/2022 20:25:57 - INFO - __main__ - Step 570 Global step 570 Train loss 3.169722 on epoch=40
03/18/2022 20:26:02 - INFO - __main__ - Step 580 Global step 580 Train loss 3.106973 on epoch=41
03/18/2022 20:26:07 - INFO - __main__ - Step 590 Global step 590 Train loss 2.911990 on epoch=42
03/18/2022 20:26:12 - INFO - __main__ - Step 600 Global step 600 Train loss 3.013081 on epoch=42
03/18/2022 20:26:15 - INFO - __main__ - Global step 600 Train loss 2.998007 Classification-F1 0.3715238019310841 on epoch=42
03/18/2022 20:26:21 - INFO - __main__ - Step 610 Global step 610 Train loss 2.548906 on epoch=43
03/18/2022 20:26:26 - INFO - __main__ - Step 620 Global step 620 Train loss 2.972302 on epoch=44
03/18/2022 20:26:31 - INFO - __main__ - Step 630 Global step 630 Train loss 2.674074 on epoch=44
03/18/2022 20:26:36 - INFO - __main__ - Step 640 Global step 640 Train loss 2.472833 on epoch=45
03/18/2022 20:26:42 - INFO - __main__ - Step 650 Global step 650 Train loss 2.670352 on epoch=46
03/18/2022 20:26:45 - INFO - __main__ - Global step 650 Train loss 2.667693 Classification-F1 0.5472999004411878 on epoch=46
03/18/2022 20:26:51 - INFO - __main__ - Step 660 Global step 660 Train loss 2.723919 on epoch=47
03/18/2022 20:26:56 - INFO - __main__ - Step 670 Global step 670 Train loss 2.542641 on epoch=47
03/18/2022 20:27:01 - INFO - __main__ - Step 680 Global step 680 Train loss 2.316991 on epoch=48
03/18/2022 20:27:06 - INFO - __main__ - Step 690 Global step 690 Train loss 2.217442 on epoch=49
03/18/2022 20:27:11 - INFO - __main__ - Step 700 Global step 700 Train loss 2.446429 on epoch=49
03/18/2022 20:27:14 - INFO - __main__ - Global step 700 Train loss 2.449484 Classification-F1 0.6144080236517212 on epoch=49
03/18/2022 20:27:20 - INFO - __main__ - Step 710 Global step 710 Train loss 2.422662 on epoch=50
03/18/2022 20:27:25 - INFO - __main__ - Step 720 Global step 720 Train loss 2.205276 on epoch=51
03/18/2022 20:27:30 - INFO - __main__ - Step 730 Global step 730 Train loss 1.912299 on epoch=52
03/18/2022 20:27:35 - INFO - __main__ - Step 740 Global step 740 Train loss 2.076410 on epoch=52
03/18/2022 20:27:40 - INFO - __main__ - Step 750 Global step 750 Train loss 2.014916 on epoch=53
03/18/2022 20:27:43 - INFO - __main__ - Global step 750 Train loss 2.126313 Classification-F1 0.6040996993024714 on epoch=53
03/18/2022 20:27:48 - INFO - __main__ - Step 760 Global step 760 Train loss 1.807773 on epoch=54
03/18/2022 20:27:53 - INFO - __main__ - Step 770 Global step 770 Train loss 2.026301 on epoch=54
03/18/2022 20:27:58 - INFO - __main__ - Step 780 Global step 780 Train loss 2.152927 on epoch=55
03/18/2022 20:28:03 - INFO - __main__ - Step 790 Global step 790 Train loss 2.133119 on epoch=56
03/18/2022 20:28:08 - INFO - __main__ - Step 800 Global step 800 Train loss 1.770235 on epoch=57
03/18/2022 20:28:11 - INFO - __main__ - Global step 800 Train loss 1.978071 Classification-F1 0.6335771770579883 on epoch=57
03/18/2022 20:28:17 - INFO - __main__ - Step 810 Global step 810 Train loss 2.088063 on epoch=57
03/18/2022 20:28:22 - INFO - __main__ - Step 820 Global step 820 Train loss 1.878256 on epoch=58
03/18/2022 20:28:27 - INFO - __main__ - Step 830 Global step 830 Train loss 1.676432 on epoch=59
03/18/2022 20:28:32 - INFO - __main__ - Step 840 Global step 840 Train loss 1.643948 on epoch=59
03/18/2022 20:28:37 - INFO - __main__ - Step 850 Global step 850 Train loss 1.785189 on epoch=60
03/18/2022 20:28:40 - INFO - __main__ - Global step 850 Train loss 1.814378 Classification-F1 0.63455408652453 on epoch=60
03/18/2022 20:28:46 - INFO - __main__ - Step 860 Global step 860 Train loss 1.922235 on epoch=61
03/18/2022 20:28:51 - INFO - __main__ - Step 870 Global step 870 Train loss 1.715256 on epoch=62
03/18/2022 20:28:56 - INFO - __main__ - Step 880 Global step 880 Train loss 1.465736 on epoch=62
03/18/2022 20:29:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.300547 on epoch=63
03/18/2022 20:29:06 - INFO - __main__ - Step 900 Global step 900 Train loss 1.489514 on epoch=64
03/18/2022 20:29:09 - INFO - __main__ - Global step 900 Train loss 1.578658 Classification-F1 0.6565272326546837 on epoch=64
03/18/2022 20:29:15 - INFO - __main__ - Step 910 Global step 910 Train loss 1.412182 on epoch=64
03/18/2022 20:29:20 - INFO - __main__ - Step 920 Global step 920 Train loss 1.168300 on epoch=65
03/18/2022 20:29:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.367524 on epoch=66
03/18/2022 20:29:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.039195 on epoch=67
03/18/2022 20:29:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.066537 on epoch=67
03/18/2022 20:29:39 - INFO - __main__ - Global step 950 Train loss 0.610748 Classification-F1 0.5230316232915028 on epoch=67
03/18/2022 20:29:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.082033 on epoch=68
03/18/2022 20:29:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.108719 on epoch=69
03/18/2022 20:29:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.092781 on epoch=69
03/18/2022 20:29:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.100450 on epoch=70
03/18/2022 20:30:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.031179 on epoch=71
03/18/2022 20:30:05 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:30:05 - INFO - __main__ - Printing 3 examples
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 20:30:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:30:05 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:30:05 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:30:05 - INFO - __main__ - Printing 3 examples
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:30:05 - INFO - __main__ - ['Company']
03/18/2022 20:30:05 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:30:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:30:06 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:30:07 - INFO - __main__ - Global step 1000 Train loss 0.083032 Classification-F1 0.6297826840259177 on epoch=71
03/18/2022 20:30:07 - INFO - __main__ - save last model!
03/18/2022 20:30:14 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 20:30:15 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 20:30:15 - INFO - __main__ - Printing 3 examples
03/18/2022 20:30:15 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 20:30:15 - INFO - __main__ - ['Animal']
03/18/2022 20:30:15 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 20:30:15 - INFO - __main__ - ['Animal']
03/18/2022 20:30:15 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 20:30:15 - INFO - __main__ - ['Village']
03/18/2022 20:30:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:30:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:30:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:30:18 - INFO - __main__ - Starting training!
03/18/2022 20:30:20 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 20:31:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_21_0.0001_8_predictions.txt
03/18/2022 20:31:17 - INFO - __main__ - Classification-F1 on test data: 0.5870
03/18/2022 20:31:18 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.0001, bsz=8, dev_performance=0.6565272326546837, test_performance=0.5869834270968989
03/18/2022 20:31:18 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.0005, bsz=8 ...
03/18/2022 20:31:19 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:31:19 - INFO - __main__ - Printing 3 examples
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:31:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:31:19 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:31:19 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:31:19 - INFO - __main__ - Printing 3 examples
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:31:19 - INFO - __main__ - ['Company']
03/18/2022 20:31:19 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:31:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:31:19 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:31:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:31:30 - INFO - __main__ - Starting training!
03/18/2022 20:31:34 - INFO - __main__ - Step 10 Global step 10 Train loss 21.713099 on epoch=0
03/18/2022 20:31:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.996859 on epoch=1
03/18/2022 20:31:44 - INFO - __main__ - Step 30 Global step 30 Train loss 14.160646 on epoch=2
03/18/2022 20:31:49 - INFO - __main__ - Step 40 Global step 40 Train loss 10.802165 on epoch=2
03/18/2022 20:31:54 - INFO - __main__ - Step 50 Global step 50 Train loss 10.023813 on epoch=3
03/18/2022 20:31:58 - INFO - __main__ - Global step 50 Train loss 15.139316 Classification-F1 0.0 on epoch=3
03/18/2022 20:32:04 - INFO - __main__ - Step 60 Global step 60 Train loss 9.342122 on epoch=4
03/18/2022 20:32:09 - INFO - __main__ - Step 70 Global step 70 Train loss 8.249459 on epoch=4
03/18/2022 20:32:14 - INFO - __main__ - Step 80 Global step 80 Train loss 6.940488 on epoch=5
03/18/2022 20:32:19 - INFO - __main__ - Step 90 Global step 90 Train loss 6.352944 on epoch=6
03/18/2022 20:32:24 - INFO - __main__ - Step 100 Global step 100 Train loss 4.658708 on epoch=7
03/18/2022 20:32:27 - INFO - __main__ - Global step 100 Train loss 7.108745 Classification-F1 0.0 on epoch=7
03/18/2022 20:32:32 - INFO - __main__ - Step 110 Global step 110 Train loss 3.497679 on epoch=7
03/18/2022 20:32:37 - INFO - __main__ - Step 120 Global step 120 Train loss 3.322189 on epoch=8
03/18/2022 20:32:43 - INFO - __main__ - Step 130 Global step 130 Train loss 2.662703 on epoch=9
03/18/2022 20:32:48 - INFO - __main__ - Step 140 Global step 140 Train loss 2.476806 on epoch=9
03/18/2022 20:32:53 - INFO - __main__ - Step 150 Global step 150 Train loss 2.647798 on epoch=10
03/18/2022 20:32:56 - INFO - __main__ - Global step 150 Train loss 2.921435 Classification-F1 0.09853695101228897 on epoch=10
03/18/2022 20:33:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.614971 on epoch=11
03/18/2022 20:33:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.131552 on epoch=12
03/18/2022 20:33:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.974775 on epoch=12
03/18/2022 20:33:17 - INFO - __main__ - Step 190 Global step 190 Train loss 2.053344 on epoch=13
03/18/2022 20:33:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.006394 on epoch=14
03/18/2022 20:33:25 - INFO - __main__ - Global step 200 Train loss 2.156208 Classification-F1 0.4464556577443787 on epoch=14
03/18/2022 20:33:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.753967 on epoch=14
03/18/2022 20:33:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.670144 on epoch=15
03/18/2022 20:33:41 - INFO - __main__ - Step 230 Global step 230 Train loss 1.498695 on epoch=16
03/18/2022 20:33:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.747908 on epoch=17
03/18/2022 20:33:51 - INFO - __main__ - Step 250 Global step 250 Train loss 1.267927 on epoch=17
03/18/2022 20:33:54 - INFO - __main__ - Global step 250 Train loss 1.587728 Classification-F1 0.6461455053518461 on epoch=17
03/18/2022 20:34:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.219729 on epoch=18
03/18/2022 20:34:05 - INFO - __main__ - Step 270 Global step 270 Train loss 1.162715 on epoch=19
03/18/2022 20:34:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.062060 on epoch=19
03/18/2022 20:34:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.797189 on epoch=20
03/18/2022 20:34:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.684039 on epoch=21
03/18/2022 20:34:24 - INFO - __main__ - Global step 300 Train loss 0.985146 Classification-F1 0.857600515461546 on epoch=21
03/18/2022 20:34:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.735057 on epoch=22
03/18/2022 20:34:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.327021 on epoch=22
03/18/2022 20:34:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.152203 on epoch=23
03/18/2022 20:34:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.242351 on epoch=24
03/18/2022 20:34:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.085316 on epoch=24
03/18/2022 20:34:54 - INFO - __main__ - Global step 350 Train loss 0.308390 Classification-F1 0.8231383801371697 on epoch=24
03/18/2022 20:34:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.039265 on epoch=25
03/18/2022 20:35:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.059763 on epoch=26
03/18/2022 20:35:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025095 on epoch=27
03/18/2022 20:35:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006969 on epoch=27
03/18/2022 20:35:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.033041 on epoch=28
03/18/2022 20:35:23 - INFO - __main__ - Global step 400 Train loss 0.032827 Classification-F1 0.7765568718641529 on epoch=28
03/18/2022 20:35:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015561 on epoch=29
03/18/2022 20:35:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.007676 on epoch=29
03/18/2022 20:35:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.013785 on epoch=30
03/18/2022 20:35:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.013521 on epoch=31
03/18/2022 20:35:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003055 on epoch=32
03/18/2022 20:35:52 - INFO - __main__ - Global step 450 Train loss 0.010720 Classification-F1 0.9383507664128025 on epoch=32
03/18/2022 20:35:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.015517 on epoch=32
03/18/2022 20:36:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.145558 on epoch=33
03/18/2022 20:36:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.420964 on epoch=34
03/18/2022 20:36:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.560314 on epoch=34
03/18/2022 20:36:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.052712 on epoch=35
03/18/2022 20:36:21 - INFO - __main__ - Global step 500 Train loss 0.239013 Classification-F1 0.7169399657092822 on epoch=35
03/18/2022 20:36:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.095851 on epoch=36
03/18/2022 20:36:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.037263 on epoch=37
03/18/2022 20:36:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007453 on epoch=37
03/18/2022 20:36:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.028897 on epoch=38
03/18/2022 20:36:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016720 on epoch=39
03/18/2022 20:36:50 - INFO - __main__ - Global step 550 Train loss 0.037237 Classification-F1 0.6557133670926774 on epoch=39
03/18/2022 20:36:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.022353 on epoch=39
03/18/2022 20:37:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.034315 on epoch=40
03/18/2022 20:37:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.023479 on epoch=41
03/18/2022 20:37:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.233227 on epoch=42
03/18/2022 20:37:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.094638 on epoch=42
03/18/2022 20:37:19 - INFO - __main__ - Global step 600 Train loss 0.081602 Classification-F1 0.7424227443137342 on epoch=42
03/18/2022 20:37:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.183784 on epoch=43
03/18/2022 20:37:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.030529 on epoch=44
03/18/2022 20:37:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.031944 on epoch=44
03/18/2022 20:37:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.010799 on epoch=45
03/18/2022 20:37:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000902 on epoch=46
03/18/2022 20:37:48 - INFO - __main__ - Global step 650 Train loss 0.051592 Classification-F1 0.838805896768124 on epoch=46
03/18/2022 20:37:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001628 on epoch=47
03/18/2022 20:37:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.005255 on epoch=47
03/18/2022 20:38:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.071552 on epoch=48
03/18/2022 20:38:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.029012 on epoch=49
03/18/2022 20:38:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005758 on epoch=49
03/18/2022 20:38:17 - INFO - __main__ - Global step 700 Train loss 0.022641 Classification-F1 0.8701597253321391 on epoch=49
03/18/2022 20:38:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002264 on epoch=50
03/18/2022 20:38:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002101 on epoch=51
03/18/2022 20:38:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.018167 on epoch=52
03/18/2022 20:38:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.031737 on epoch=52
03/18/2022 20:38:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003070 on epoch=53
03/18/2022 20:38:47 - INFO - __main__ - Global step 750 Train loss 0.011468 Classification-F1 0.8760868888957009 on epoch=53
03/18/2022 20:38:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002183 on epoch=54
03/18/2022 20:38:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.011365 on epoch=54
03/18/2022 20:39:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000582 on epoch=55
03/18/2022 20:39:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000235 on epoch=56
03/18/2022 20:39:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000270 on epoch=57
03/18/2022 20:39:16 - INFO - __main__ - Global step 800 Train loss 0.002927 Classification-F1 0.8319513860073633 on epoch=57
03/18/2022 20:39:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002054 on epoch=57
03/18/2022 20:39:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.016633 on epoch=58
03/18/2022 20:39:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000706 on epoch=59
03/18/2022 20:39:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004673 on epoch=59
03/18/2022 20:39:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.030352 on epoch=60
03/18/2022 20:39:46 - INFO - __main__ - Global step 850 Train loss 0.010884 Classification-F1 0.8067798867798869 on epoch=60
03/18/2022 20:39:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003830 on epoch=61
03/18/2022 20:39:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003735 on epoch=62
03/18/2022 20:40:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.010359 on epoch=62
03/18/2022 20:40:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003193 on epoch=63
03/18/2022 20:40:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.008412 on epoch=64
03/18/2022 20:40:15 - INFO - __main__ - Global step 900 Train loss 0.005906 Classification-F1 0.8979469702398774 on epoch=64
03/18/2022 20:40:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000674 on epoch=64
03/18/2022 20:40:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000109 on epoch=65
03/18/2022 20:40:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002377 on epoch=66
03/18/2022 20:40:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.016562 on epoch=67
03/18/2022 20:40:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.004127 on epoch=67
03/18/2022 20:40:44 - INFO - __main__ - Global step 950 Train loss 0.004770 Classification-F1 0.859241355083089 on epoch=67
03/18/2022 20:40:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.007346 on epoch=68
03/18/2022 20:40:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.005491 on epoch=69
03/18/2022 20:41:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.037541 on epoch=69
03/18/2022 20:41:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.026886 on epoch=70
03/18/2022 20:41:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006647 on epoch=71
03/18/2022 20:41:11 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:41:11 - INFO - __main__ - Printing 3 examples
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 20:41:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:41:11 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:41:11 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:41:11 - INFO - __main__ - Printing 3 examples
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:41:11 - INFO - __main__ - ['Company']
03/18/2022 20:41:11 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:41:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:41:12 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:41:14 - INFO - __main__ - Global step 1000 Train loss 0.016782 Classification-F1 0.7301351433372306 on epoch=71
03/18/2022 20:41:14 - INFO - __main__ - save last model!
03/18/2022 20:41:20 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 20:41:21 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 20:41:21 - INFO - __main__ - Printing 3 examples
03/18/2022 20:41:21 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 20:41:21 - INFO - __main__ - ['Animal']
03/18/2022 20:41:21 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 20:41:21 - INFO - __main__ - ['Animal']
03/18/2022 20:41:21 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 20:41:21 - INFO - __main__ - ['Village']
03/18/2022 20:41:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:41:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:41:23 - INFO - __main__ - Starting training!
03/18/2022 20:41:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:41:26 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 20:42:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_42_0.0005_8_predictions.txt
03/18/2022 20:42:38 - INFO - __main__ - Classification-F1 on test data: 0.5410
03/18/2022 20:42:38 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.0005, bsz=8, dev_performance=0.9383507664128025, test_performance=0.5409859366099173
03/18/2022 20:42:38 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.0003, bsz=8 ...
03/18/2022 20:42:39 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:42:39 - INFO - __main__ - Printing 3 examples
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:42:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:42:39 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:42:39 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:42:39 - INFO - __main__ - Printing 3 examples
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:42:39 - INFO - __main__ - ['Company']
03/18/2022 20:42:39 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:42:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:42:39 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:42:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:42:50 - INFO - __main__ - Starting training!
03/18/2022 20:42:54 - INFO - __main__ - Step 10 Global step 10 Train loss 21.432755 on epoch=0
03/18/2022 20:42:59 - INFO - __main__ - Step 20 Global step 20 Train loss 20.207117 on epoch=1
03/18/2022 20:43:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.082867 on epoch=2
03/18/2022 20:43:09 - INFO - __main__ - Step 40 Global step 40 Train loss 12.305840 on epoch=2
03/18/2022 20:43:14 - INFO - __main__ - Step 50 Global step 50 Train loss 12.453300 on epoch=3
03/18/2022 20:43:18 - INFO - __main__ - Global step 50 Train loss 16.496376 Classification-F1 0.0 on epoch=3
03/18/2022 20:43:24 - INFO - __main__ - Step 60 Global step 60 Train loss 11.221573 on epoch=4
03/18/2022 20:43:29 - INFO - __main__ - Step 70 Global step 70 Train loss 10.408216 on epoch=4
03/18/2022 20:43:34 - INFO - __main__ - Step 80 Global step 80 Train loss 8.952236 on epoch=5
03/18/2022 20:43:39 - INFO - __main__ - Step 90 Global step 90 Train loss 9.254520 on epoch=6
03/18/2022 20:43:44 - INFO - __main__ - Step 100 Global step 100 Train loss 8.779819 on epoch=7
03/18/2022 20:43:48 - INFO - __main__ - Global step 100 Train loss 9.723274 Classification-F1 0.0 on epoch=7
03/18/2022 20:43:53 - INFO - __main__ - Step 110 Global step 110 Train loss 7.127203 on epoch=7
03/18/2022 20:43:58 - INFO - __main__ - Step 120 Global step 120 Train loss 7.651306 on epoch=8
03/18/2022 20:44:03 - INFO - __main__ - Step 130 Global step 130 Train loss 6.698513 on epoch=9
03/18/2022 20:44:08 - INFO - __main__ - Step 140 Global step 140 Train loss 6.359554 on epoch=9
03/18/2022 20:44:13 - INFO - __main__ - Step 150 Global step 150 Train loss 5.100979 on epoch=10
03/18/2022 20:44:18 - INFO - __main__ - Global step 150 Train loss 6.587511 Classification-F1 0.0 on epoch=10
03/18/2022 20:44:23 - INFO - __main__ - Step 160 Global step 160 Train loss 4.387213 on epoch=11
03/18/2022 20:44:28 - INFO - __main__ - Step 170 Global step 170 Train loss 4.533603 on epoch=12
03/18/2022 20:44:33 - INFO - __main__ - Step 180 Global step 180 Train loss 3.341206 on epoch=12
03/18/2022 20:44:38 - INFO - __main__ - Step 190 Global step 190 Train loss 3.153209 on epoch=13
03/18/2022 20:44:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.308700 on epoch=14
03/18/2022 20:44:45 - INFO - __main__ - Global step 200 Train loss 3.744786 Classification-F1 0.09958554364821545 on epoch=14
03/18/2022 20:44:51 - INFO - __main__ - Step 210 Global step 210 Train loss 2.804192 on epoch=14
03/18/2022 20:44:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.817369 on epoch=15
03/18/2022 20:45:01 - INFO - __main__ - Step 230 Global step 230 Train loss 3.199672 on epoch=16
03/18/2022 20:45:06 - INFO - __main__ - Step 240 Global step 240 Train loss 3.265921 on epoch=17
03/18/2022 20:45:11 - INFO - __main__ - Step 250 Global step 250 Train loss 2.632645 on epoch=17
03/18/2022 20:45:14 - INFO - __main__ - Global step 250 Train loss 2.943960 Classification-F1 0.009603841536614645 on epoch=17
03/18/2022 20:45:19 - INFO - __main__ - Step 260 Global step 260 Train loss 2.853557 on epoch=18
03/18/2022 20:45:24 - INFO - __main__ - Step 270 Global step 270 Train loss 2.533215 on epoch=19
03/18/2022 20:45:29 - INFO - __main__ - Step 280 Global step 280 Train loss 2.616955 on epoch=19
03/18/2022 20:45:34 - INFO - __main__ - Step 290 Global step 290 Train loss 2.183123 on epoch=20
03/18/2022 20:45:39 - INFO - __main__ - Step 300 Global step 300 Train loss 2.248346 on epoch=21
03/18/2022 20:45:42 - INFO - __main__ - Global step 300 Train loss 2.487039 Classification-F1 0.05145626670290347 on epoch=21
03/18/2022 20:45:47 - INFO - __main__ - Step 310 Global step 310 Train loss 2.355433 on epoch=22
03/18/2022 20:45:52 - INFO - __main__ - Step 320 Global step 320 Train loss 2.037660 on epoch=22
03/18/2022 20:45:57 - INFO - __main__ - Step 330 Global step 330 Train loss 2.066226 on epoch=23
03/18/2022 20:46:02 - INFO - __main__ - Step 340 Global step 340 Train loss 2.059649 on epoch=24
03/18/2022 20:46:07 - INFO - __main__ - Step 350 Global step 350 Train loss 2.079573 on epoch=24
03/18/2022 20:46:11 - INFO - __main__ - Global step 350 Train loss 2.119708 Classification-F1 0.047789368789818425 on epoch=24
03/18/2022 20:46:16 - INFO - __main__ - Step 360 Global step 360 Train loss 1.771453 on epoch=25
03/18/2022 20:46:21 - INFO - __main__ - Step 370 Global step 370 Train loss 2.117831 on epoch=26
03/18/2022 20:46:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.912006 on epoch=27
03/18/2022 20:46:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.537119 on epoch=27
03/18/2022 20:46:36 - INFO - __main__ - Step 400 Global step 400 Train loss 1.711387 on epoch=28
03/18/2022 20:46:38 - INFO - __main__ - Global step 400 Train loss 1.809959 Classification-F1 0.05521532083263514 on epoch=28
03/18/2022 20:46:43 - INFO - __main__ - Step 410 Global step 410 Train loss 1.699427 on epoch=29
03/18/2022 20:46:48 - INFO - __main__ - Step 420 Global step 420 Train loss 1.743432 on epoch=29
03/18/2022 20:46:53 - INFO - __main__ - Step 430 Global step 430 Train loss 1.711931 on epoch=30
03/18/2022 20:46:58 - INFO - __main__ - Step 440 Global step 440 Train loss 1.385716 on epoch=31
03/18/2022 20:47:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.639533 on epoch=32
03/18/2022 20:47:06 - INFO - __main__ - Global step 450 Train loss 1.636008 Classification-F1 0.1945956896960592 on epoch=32
03/18/2022 20:47:12 - INFO - __main__ - Step 460 Global step 460 Train loss 1.554537 on epoch=32
03/18/2022 20:47:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.544179 on epoch=33
03/18/2022 20:47:22 - INFO - __main__ - Step 480 Global step 480 Train loss 1.501571 on epoch=34
03/18/2022 20:47:27 - INFO - __main__ - Step 490 Global step 490 Train loss 1.415290 on epoch=34
03/18/2022 20:47:32 - INFO - __main__ - Step 500 Global step 500 Train loss 1.339304 on epoch=35
03/18/2022 20:47:34 - INFO - __main__ - Global step 500 Train loss 1.470976 Classification-F1 0.21703878623485076 on epoch=35
03/18/2022 20:47:40 - INFO - __main__ - Step 510 Global step 510 Train loss 1.289246 on epoch=36
03/18/2022 20:47:45 - INFO - __main__ - Step 520 Global step 520 Train loss 1.274264 on epoch=37
03/18/2022 20:47:50 - INFO - __main__ - Step 530 Global step 530 Train loss 1.188245 on epoch=37
03/18/2022 20:47:55 - INFO - __main__ - Step 540 Global step 540 Train loss 1.236076 on epoch=38
03/18/2022 20:48:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.237215 on epoch=39
03/18/2022 20:48:02 - INFO - __main__ - Global step 550 Train loss 1.245009 Classification-F1 0.10089488092785022 on epoch=39
03/18/2022 20:48:07 - INFO - __main__ - Step 560 Global step 560 Train loss 1.211354 on epoch=39
03/18/2022 20:48:12 - INFO - __main__ - Step 570 Global step 570 Train loss 1.079463 on epoch=40
03/18/2022 20:48:17 - INFO - __main__ - Step 580 Global step 580 Train loss 1.128259 on epoch=41
03/18/2022 20:48:22 - INFO - __main__ - Step 590 Global step 590 Train loss 1.127212 on epoch=42
03/18/2022 20:48:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.993986 on epoch=42
03/18/2022 20:48:29 - INFO - __main__ - Global step 600 Train loss 1.108055 Classification-F1 0.26860172935126136 on epoch=42
03/18/2022 20:48:34 - INFO - __main__ - Step 610 Global step 610 Train loss 1.074084 on epoch=43
03/18/2022 20:48:39 - INFO - __main__ - Step 620 Global step 620 Train loss 1.103444 on epoch=44
03/18/2022 20:48:44 - INFO - __main__ - Step 630 Global step 630 Train loss 1.058931 on epoch=44
03/18/2022 20:48:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.954871 on epoch=45
03/18/2022 20:48:54 - INFO - __main__ - Step 650 Global step 650 Train loss 1.070478 on epoch=46
03/18/2022 20:48:58 - INFO - __main__ - Global step 650 Train loss 1.052362 Classification-F1 0.011492751730442733 on epoch=46
03/18/2022 20:49:03 - INFO - __main__ - Step 660 Global step 660 Train loss 1.742966 on epoch=47
03/18/2022 20:49:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.689020 on epoch=47
03/18/2022 20:49:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.774771 on epoch=48
03/18/2022 20:49:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.694729 on epoch=49
03/18/2022 20:49:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.629456 on epoch=49
03/18/2022 20:49:27 - INFO - __main__ - Global step 700 Train loss 0.906188 Classification-F1 0.6293364857984065 on epoch=49
03/18/2022 20:49:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.519670 on epoch=50
03/18/2022 20:49:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.529728 on epoch=51
03/18/2022 20:49:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.505804 on epoch=52
03/18/2022 20:49:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.433866 on epoch=52
03/18/2022 20:49:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.340413 on epoch=53
03/18/2022 20:49:57 - INFO - __main__ - Global step 750 Train loss 0.465896 Classification-F1 0.7279710159437051 on epoch=53
03/18/2022 20:50:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.353295 on epoch=54
03/18/2022 20:50:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.355772 on epoch=54
03/18/2022 20:50:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.250498 on epoch=55
03/18/2022 20:50:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.438741 on epoch=56
03/18/2022 20:50:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.364430 on epoch=57
03/18/2022 20:50:27 - INFO - __main__ - Global step 800 Train loss 0.352547 Classification-F1 0.7399546459245983 on epoch=57
03/18/2022 20:50:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.346648 on epoch=57
03/18/2022 20:50:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.225151 on epoch=58
03/18/2022 20:50:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.323234 on epoch=59
03/18/2022 20:50:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.212691 on epoch=59
03/18/2022 20:50:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.209786 on epoch=60
03/18/2022 20:50:57 - INFO - __main__ - Global step 850 Train loss 0.263502 Classification-F1 0.8236522747872046 on epoch=60
03/18/2022 20:51:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.231940 on epoch=61
03/18/2022 20:51:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.223481 on epoch=62
03/18/2022 20:51:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.128795 on epoch=62
03/18/2022 20:51:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.135805 on epoch=63
03/18/2022 20:51:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.186023 on epoch=64
03/18/2022 20:51:27 - INFO - __main__ - Global step 900 Train loss 0.181209 Classification-F1 0.6822255906132642 on epoch=64
03/18/2022 20:51:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.198967 on epoch=64
03/18/2022 20:51:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.097288 on epoch=65
03/18/2022 20:51:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.136052 on epoch=66
03/18/2022 20:51:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.156446 on epoch=67
03/18/2022 20:51:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.188327 on epoch=67
03/18/2022 20:51:57 - INFO - __main__ - Global step 950 Train loss 0.155416 Classification-F1 0.6857227958928837 on epoch=67
03/18/2022 20:52:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.136909 on epoch=68
03/18/2022 20:52:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.146401 on epoch=69
03/18/2022 20:52:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.123807 on epoch=69
03/18/2022 20:52:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.059251 on epoch=70
03/18/2022 20:52:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.091756 on epoch=71
03/18/2022 20:52:23 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:52:23 - INFO - __main__ - Printing 3 examples
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 20:52:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:52:23 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:52:23 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:52:23 - INFO - __main__ - Printing 3 examples
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:52:23 - INFO - __main__ - ['Company']
03/18/2022 20:52:23 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:52:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:52:24 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:52:26 - INFO - __main__ - Global step 1000 Train loss 0.111625 Classification-F1 0.7112106680627164 on epoch=71
03/18/2022 20:52:26 - INFO - __main__ - save last model!
03/18/2022 20:52:33 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 20:52:33 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 20:52:33 - INFO - __main__ - Printing 3 examples
03/18/2022 20:52:33 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 20:52:33 - INFO - __main__ - ['Animal']
03/18/2022 20:52:33 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 20:52:33 - INFO - __main__ - ['Animal']
03/18/2022 20:52:33 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 20:52:33 - INFO - __main__ - ['Village']
03/18/2022 20:52:33 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:52:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:52:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:52:36 - INFO - __main__ - Starting training!
03/18/2022 20:52:39 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 20:53:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_42_0.0003_8_predictions.txt
03/18/2022 20:53:52 - INFO - __main__ - Classification-F1 on test data: 0.6314
03/18/2022 20:53:53 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.0003, bsz=8, dev_performance=0.8236522747872046, test_performance=0.631443852849032
03/18/2022 20:53:53 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.0002, bsz=8 ...
03/18/2022 20:53:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:53:54 - INFO - __main__ - Printing 3 examples
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 20:53:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:53:54 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 20:53:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 20:53:54 - INFO - __main__ - Printing 3 examples
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 20:53:54 - INFO - __main__ - ['Company']
03/18/2022 20:53:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 20:53:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 20:53:54 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 20:54:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 20:54:07 - INFO - __main__ - Starting training!
03/18/2022 20:54:11 - INFO - __main__ - Step 10 Global step 10 Train loss 21.396221 on epoch=0
03/18/2022 20:54:16 - INFO - __main__ - Step 20 Global step 20 Train loss 18.437351 on epoch=1
03/18/2022 20:54:21 - INFO - __main__ - Step 30 Global step 30 Train loss 14.135033 on epoch=2
03/18/2022 20:54:27 - INFO - __main__ - Step 40 Global step 40 Train loss 11.184213 on epoch=2
03/18/2022 20:54:32 - INFO - __main__ - Step 50 Global step 50 Train loss 12.388203 on epoch=3
03/18/2022 20:54:36 - INFO - __main__ - Global step 50 Train loss 15.508204 Classification-F1 0.0 on epoch=3
03/18/2022 20:54:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.817965 on epoch=4
03/18/2022 20:54:47 - INFO - __main__ - Step 70 Global step 70 Train loss 11.049153 on epoch=4
03/18/2022 20:54:52 - INFO - __main__ - Step 80 Global step 80 Train loss 9.808140 on epoch=5
03/18/2022 20:54:58 - INFO - __main__ - Step 90 Global step 90 Train loss 10.371180 on epoch=6
03/18/2022 20:55:03 - INFO - __main__ - Step 100 Global step 100 Train loss 10.121649 on epoch=7
03/18/2022 20:55:07 - INFO - __main__ - Global step 100 Train loss 10.433617 Classification-F1 0.0 on epoch=7
03/18/2022 20:55:12 - INFO - __main__ - Step 110 Global step 110 Train loss 8.318435 on epoch=7
03/18/2022 20:55:17 - INFO - __main__ - Step 120 Global step 120 Train loss 9.031431 on epoch=8
03/18/2022 20:55:22 - INFO - __main__ - Step 130 Global step 130 Train loss 8.673331 on epoch=9
03/18/2022 20:55:27 - INFO - __main__ - Step 140 Global step 140 Train loss 8.388973 on epoch=9
03/18/2022 20:55:33 - INFO - __main__ - Step 150 Global step 150 Train loss 7.388802 on epoch=10
03/18/2022 20:55:36 - INFO - __main__ - Global step 150 Train loss 8.360194 Classification-F1 0.0 on epoch=10
03/18/2022 20:55:41 - INFO - __main__ - Step 160 Global step 160 Train loss 7.402392 on epoch=11
03/18/2022 20:55:47 - INFO - __main__ - Step 170 Global step 170 Train loss 6.805783 on epoch=12
03/18/2022 20:55:52 - INFO - __main__ - Step 180 Global step 180 Train loss 5.660779 on epoch=12
03/18/2022 20:55:57 - INFO - __main__ - Step 190 Global step 190 Train loss 3.603741 on epoch=13
03/18/2022 20:56:01 - INFO - __main__ - Step 200 Global step 200 Train loss 2.043711 on epoch=14
03/18/2022 20:56:05 - INFO - __main__ - Global step 200 Train loss 5.103281 Classification-F1 0.3215583968754883 on epoch=14
03/18/2022 20:56:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.219769 on epoch=14
03/18/2022 20:56:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.314870 on epoch=15
03/18/2022 20:56:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.130846 on epoch=16
03/18/2022 20:56:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.592130 on epoch=17
03/18/2022 20:56:31 - INFO - __main__ - Step 250 Global step 250 Train loss 2.299033 on epoch=17
03/18/2022 20:56:35 - INFO - __main__ - Global step 250 Train loss 1.511330 Classification-F1 0.5114991248905101 on epoch=17
03/18/2022 20:56:41 - INFO - __main__ - Step 260 Global step 260 Train loss 2.726753 on epoch=18
03/18/2022 20:56:46 - INFO - __main__ - Step 270 Global step 270 Train loss 3.261292 on epoch=19
03/18/2022 20:56:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.027745 on epoch=19
03/18/2022 20:56:56 - INFO - __main__ - Step 290 Global step 290 Train loss 2.268368 on epoch=20
03/18/2022 20:57:01 - INFO - __main__ - Step 300 Global step 300 Train loss 2.292136 on epoch=21
03/18/2022 20:57:05 - INFO - __main__ - Global step 300 Train loss 2.315259 Classification-F1 0.6827002049529822 on epoch=21
03/18/2022 20:57:11 - INFO - __main__ - Step 310 Global step 310 Train loss 1.289662 on epoch=22
03/18/2022 20:57:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.608821 on epoch=22
03/18/2022 20:57:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.405898 on epoch=23
03/18/2022 20:57:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.324042 on epoch=24
03/18/2022 20:57:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.436534 on epoch=24
03/18/2022 20:57:35 - INFO - __main__ - Global step 350 Train loss 0.612991 Classification-F1 0.7356489225046712 on epoch=24
03/18/2022 20:57:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.355852 on epoch=25
03/18/2022 20:57:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.617087 on epoch=26
03/18/2022 20:57:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.670326 on epoch=27
03/18/2022 20:57:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.437767 on epoch=27
03/18/2022 20:58:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.251602 on epoch=28
03/18/2022 20:58:05 - INFO - __main__ - Global step 400 Train loss 0.466527 Classification-F1 0.8474907083102143 on epoch=28
03/18/2022 20:58:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.322676 on epoch=29
03/18/2022 20:58:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.360121 on epoch=29
03/18/2022 20:58:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.207576 on epoch=30
03/18/2022 20:58:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.249430 on epoch=31
03/18/2022 20:58:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.210530 on epoch=32
03/18/2022 20:58:36 - INFO - __main__ - Global step 450 Train loss 0.270067 Classification-F1 0.8482516300318323 on epoch=32
03/18/2022 20:58:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.226667 on epoch=32
03/18/2022 20:58:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.078678 on epoch=33
03/18/2022 20:58:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.072585 on epoch=34
03/18/2022 20:58:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.058466 on epoch=34
03/18/2022 20:59:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027647 on epoch=35
03/18/2022 20:59:06 - INFO - __main__ - Global step 500 Train loss 0.092809 Classification-F1 0.7922525993535243 on epoch=35
03/18/2022 20:59:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.077028 on epoch=36
03/18/2022 20:59:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.138909 on epoch=37
03/18/2022 20:59:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.098807 on epoch=37
03/18/2022 20:59:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.076491 on epoch=38
03/18/2022 20:59:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.102902 on epoch=39
03/18/2022 20:59:36 - INFO - __main__ - Global step 550 Train loss 0.098827 Classification-F1 0.9593177300245611 on epoch=39
03/18/2022 20:59:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.119225 on epoch=39
03/18/2022 20:59:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.041271 on epoch=40
03/18/2022 20:59:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.041465 on epoch=41
03/18/2022 20:59:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.028718 on epoch=42
03/18/2022 21:00:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.034386 on epoch=42
03/18/2022 21:00:06 - INFO - __main__ - Global step 600 Train loss 0.053013 Classification-F1 0.9017261322906482 on epoch=42
03/18/2022 21:00:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.024705 on epoch=43
03/18/2022 21:00:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.061252 on epoch=44
03/18/2022 21:00:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.014969 on epoch=44
03/18/2022 21:00:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.077102 on epoch=45
03/18/2022 21:00:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.044715 on epoch=46
03/18/2022 21:00:35 - INFO - __main__ - Global step 650 Train loss 0.044549 Classification-F1 0.8306256448897578 on epoch=46
03/18/2022 21:00:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.022928 on epoch=47
03/18/2022 21:00:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.006636 on epoch=47
03/18/2022 21:00:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.005446 on epoch=48
03/18/2022 21:00:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.029163 on epoch=49
03/18/2022 21:01:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.035001 on epoch=49
03/18/2022 21:01:04 - INFO - __main__ - Global step 700 Train loss 0.019835 Classification-F1 0.7299074146613079 on epoch=49
03/18/2022 21:01:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.023779 on epoch=50
03/18/2022 21:01:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.006944 on epoch=51
03/18/2022 21:01:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.016068 on epoch=52
03/18/2022 21:01:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004484 on epoch=52
03/18/2022 21:01:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.032039 on epoch=53
03/18/2022 21:01:33 - INFO - __main__ - Global step 750 Train loss 0.016663 Classification-F1 0.7509522598997015 on epoch=53
03/18/2022 21:01:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.011906 on epoch=54
03/18/2022 21:01:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.011374 on epoch=54
03/18/2022 21:01:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.020345 on epoch=55
03/18/2022 21:01:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.006288 on epoch=56
03/18/2022 21:01:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.037844 on epoch=57
03/18/2022 21:02:02 - INFO - __main__ - Global step 800 Train loss 0.017551 Classification-F1 0.8030985009149425 on epoch=57
03/18/2022 21:02:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.019341 on epoch=57
03/18/2022 21:02:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.033279 on epoch=58
03/18/2022 21:02:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003334 on epoch=59
03/18/2022 21:02:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003632 on epoch=59
03/18/2022 21:02:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001352 on epoch=60
03/18/2022 21:02:31 - INFO - __main__ - Global step 850 Train loss 0.012188 Classification-F1 0.8937110191095011 on epoch=60
03/18/2022 21:02:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002598 on epoch=61
03/18/2022 21:02:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000602 on epoch=62
03/18/2022 21:02:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000825 on epoch=62
03/18/2022 21:02:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.022737 on epoch=63
03/18/2022 21:02:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002456 on epoch=64
03/18/2022 21:03:00 - INFO - __main__ - Global step 900 Train loss 0.005844 Classification-F1 0.9731828655215752 on epoch=64
03/18/2022 21:03:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.045914 on epoch=64
03/18/2022 21:03:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001951 on epoch=65
03/18/2022 21:03:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.011674 on epoch=66
03/18/2022 21:03:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003811 on epoch=67
03/18/2022 21:03:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.022705 on epoch=67
03/18/2022 21:03:30 - INFO - __main__ - Global step 950 Train loss 0.017211 Classification-F1 0.7494900379506642 on epoch=67
03/18/2022 21:03:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002297 on epoch=68
03/18/2022 21:03:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.006319 on epoch=69
03/18/2022 21:03:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.026543 on epoch=69
03/18/2022 21:03:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.015845 on epoch=70
03/18/2022 21:03:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002446 on epoch=71
03/18/2022 21:03:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:03:56 - INFO - __main__ - Printing 3 examples
03/18/2022 21:03:56 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 21:03:56 - INFO - __main__ - ['Company']
03/18/2022 21:03:56 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 21:03:56 - INFO - __main__ - ['Company']
03/18/2022 21:03:56 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 21:03:56 - INFO - __main__ - ['Company']
03/18/2022 21:03:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 21:03:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:03:57 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:03:57 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:03:57 - INFO - __main__ - Printing 3 examples
03/18/2022 21:03:57 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 21:03:57 - INFO - __main__ - ['Company']
03/18/2022 21:03:57 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 21:03:57 - INFO - __main__ - ['Company']
03/18/2022 21:03:57 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 21:03:57 - INFO - __main__ - ['Company']
03/18/2022 21:03:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:03:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:03:57 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:03:59 - INFO - __main__ - Global step 1000 Train loss 0.010690 Classification-F1 0.91001856720711 on epoch=71
03/18/2022 21:03:59 - INFO - __main__ - save last model!
03/18/2022 21:04:06 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 21:04:07 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 21:04:07 - INFO - __main__ - Printing 3 examples
03/18/2022 21:04:07 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 21:04:07 - INFO - __main__ - ['Animal']
03/18/2022 21:04:07 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 21:04:07 - INFO - __main__ - ['Animal']
03/18/2022 21:04:07 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 21:04:07 - INFO - __main__ - ['Village']
03/18/2022 21:04:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:04:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:04:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:04:10 - INFO - __main__ - Starting training!
03/18/2022 21:04:12 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 21:05:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_42_0.0002_8_predictions.txt
03/18/2022 21:05:27 - INFO - __main__ - Classification-F1 on test data: 0.4363
03/18/2022 21:05:28 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.0002, bsz=8, dev_performance=0.9731828655215752, test_performance=0.43629633892294445
03/18/2022 21:05:28 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.0001, bsz=8 ...
03/18/2022 21:05:29 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:05:29 - INFO - __main__ - Printing 3 examples
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:05:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:05:29 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:05:29 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:05:29 - INFO - __main__ - Printing 3 examples
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Scz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 21:05:29 - INFO - __main__ - ['Company']
03/18/2022 21:05:29 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:05:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:05:29 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:05:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:05:42 - INFO - __main__ - Starting training!
03/18/2022 21:05:46 - INFO - __main__ - Step 10 Global step 10 Train loss 21.794369 on epoch=0
03/18/2022 21:05:51 - INFO - __main__ - Step 20 Global step 20 Train loss 20.982182 on epoch=1
03/18/2022 21:05:56 - INFO - __main__ - Step 30 Global step 30 Train loss 18.368406 on epoch=2
03/18/2022 21:06:01 - INFO - __main__ - Step 40 Global step 40 Train loss 13.591415 on epoch=2
03/18/2022 21:06:06 - INFO - __main__ - Step 50 Global step 50 Train loss 13.631482 on epoch=3
03/18/2022 21:06:21 - INFO - __main__ - Global step 50 Train loss 17.673573 Classification-F1 0.0 on epoch=3
03/18/2022 21:06:27 - INFO - __main__ - Step 60 Global step 60 Train loss 12.369912 on epoch=4
03/18/2022 21:06:32 - INFO - __main__ - Step 70 Global step 70 Train loss 12.864184 on epoch=4
03/18/2022 21:06:37 - INFO - __main__ - Step 80 Global step 80 Train loss 11.171728 on epoch=5
03/18/2022 21:06:42 - INFO - __main__ - Step 90 Global step 90 Train loss 12.282736 on epoch=6
03/18/2022 21:06:47 - INFO - __main__ - Step 100 Global step 100 Train loss 11.684184 on epoch=7
03/18/2022 21:06:52 - INFO - __main__ - Global step 100 Train loss 12.074549 Classification-F1 0.0 on epoch=7
03/18/2022 21:06:57 - INFO - __main__ - Step 110 Global step 110 Train loss 10.022876 on epoch=7
03/18/2022 21:07:03 - INFO - __main__ - Step 120 Global step 120 Train loss 11.370276 on epoch=8
03/18/2022 21:07:08 - INFO - __main__ - Step 130 Global step 130 Train loss 11.040201 on epoch=9
03/18/2022 21:07:13 - INFO - __main__ - Step 140 Global step 140 Train loss 10.198236 on epoch=9
03/18/2022 21:07:18 - INFO - __main__ - Step 150 Global step 150 Train loss 9.231788 on epoch=10
03/18/2022 21:07:22 - INFO - __main__ - Global step 150 Train loss 10.372676 Classification-F1 0.0 on epoch=10
03/18/2022 21:07:27 - INFO - __main__ - Step 160 Global step 160 Train loss 10.239298 on epoch=11
03/18/2022 21:07:32 - INFO - __main__ - Step 170 Global step 170 Train loss 10.385228 on epoch=12
03/18/2022 21:07:37 - INFO - __main__ - Step 180 Global step 180 Train loss 9.310328 on epoch=12
03/18/2022 21:07:42 - INFO - __main__ - Step 190 Global step 190 Train loss 10.259421 on epoch=13
03/18/2022 21:07:47 - INFO - __main__ - Step 200 Global step 200 Train loss 9.148039 on epoch=14
03/18/2022 21:07:51 - INFO - __main__ - Global step 200 Train loss 9.868464 Classification-F1 0.0 on epoch=14
03/18/2022 21:07:56 - INFO - __main__ - Step 210 Global step 210 Train loss 9.520287 on epoch=14
03/18/2022 21:08:01 - INFO - __main__ - Step 220 Global step 220 Train loss 8.442137 on epoch=15
03/18/2022 21:08:06 - INFO - __main__ - Step 230 Global step 230 Train loss 9.008034 on epoch=16
03/18/2022 21:08:11 - INFO - __main__ - Step 240 Global step 240 Train loss 9.161721 on epoch=17
03/18/2022 21:08:16 - INFO - __main__ - Step 250 Global step 250 Train loss 7.908742 on epoch=17
03/18/2022 21:08:21 - INFO - __main__ - Global step 250 Train loss 8.808185 Classification-F1 0.0 on epoch=17
03/18/2022 21:08:26 - INFO - __main__ - Step 260 Global step 260 Train loss 9.252661 on epoch=18
03/18/2022 21:08:31 - INFO - __main__ - Step 270 Global step 270 Train loss 8.259356 on epoch=19
03/18/2022 21:08:36 - INFO - __main__ - Step 280 Global step 280 Train loss 8.354301 on epoch=19
03/18/2022 21:08:41 - INFO - __main__ - Step 290 Global step 290 Train loss 7.753384 on epoch=20
03/18/2022 21:08:46 - INFO - __main__ - Step 300 Global step 300 Train loss 8.187334 on epoch=21
03/18/2022 21:08:49 - INFO - __main__ - Global step 300 Train loss 8.361407 Classification-F1 0.0 on epoch=21
03/18/2022 21:08:54 - INFO - __main__ - Step 310 Global step 310 Train loss 7.568733 on epoch=22
03/18/2022 21:08:59 - INFO - __main__ - Step 320 Global step 320 Train loss 6.344836 on epoch=22
03/18/2022 21:09:04 - INFO - __main__ - Step 330 Global step 330 Train loss 7.029832 on epoch=23
03/18/2022 21:09:10 - INFO - __main__ - Step 340 Global step 340 Train loss 6.645959 on epoch=24
03/18/2022 21:09:15 - INFO - __main__ - Step 350 Global step 350 Train loss 6.375184 on epoch=24
03/18/2022 21:09:18 - INFO - __main__ - Global step 350 Train loss 6.792908 Classification-F1 0.0 on epoch=24
03/18/2022 21:09:23 - INFO - __main__ - Step 360 Global step 360 Train loss 5.116898 on epoch=25
03/18/2022 21:09:28 - INFO - __main__ - Step 370 Global step 370 Train loss 6.061400 on epoch=26
03/18/2022 21:09:33 - INFO - __main__ - Step 380 Global step 380 Train loss 5.578662 on epoch=27
03/18/2022 21:09:38 - INFO - __main__ - Step 390 Global step 390 Train loss 4.459931 on epoch=27
03/18/2022 21:09:44 - INFO - __main__ - Step 400 Global step 400 Train loss 4.959607 on epoch=28
03/18/2022 21:09:47 - INFO - __main__ - Global step 400 Train loss 5.235300 Classification-F1 0.009001406469760902 on epoch=28
03/18/2022 21:09:53 - INFO - __main__ - Step 410 Global step 410 Train loss 4.880679 on epoch=29
03/18/2022 21:09:58 - INFO - __main__ - Step 420 Global step 420 Train loss 4.266973 on epoch=29
03/18/2022 21:10:03 - INFO - __main__ - Step 430 Global step 430 Train loss 4.031190 on epoch=30
03/18/2022 21:10:08 - INFO - __main__ - Step 440 Global step 440 Train loss 4.256011 on epoch=31
03/18/2022 21:10:13 - INFO - __main__ - Step 450 Global step 450 Train loss 3.924786 on epoch=32
03/18/2022 21:10:16 - INFO - __main__ - Global step 450 Train loss 4.271928 Classification-F1 0.00847457627118644 on epoch=32
03/18/2022 21:10:21 - INFO - __main__ - Step 460 Global step 460 Train loss 3.794830 on epoch=32
03/18/2022 21:10:27 - INFO - __main__ - Step 470 Global step 470 Train loss 3.906132 on epoch=33
03/18/2022 21:10:32 - INFO - __main__ - Step 480 Global step 480 Train loss 3.819634 on epoch=34
03/18/2022 21:10:37 - INFO - __main__ - Step 490 Global step 490 Train loss 3.856062 on epoch=34
03/18/2022 21:10:42 - INFO - __main__ - Step 500 Global step 500 Train loss 3.149659 on epoch=35
03/18/2022 21:10:45 - INFO - __main__ - Global step 500 Train loss 3.705263 Classification-F1 0.04893906378569469 on epoch=35
03/18/2022 21:10:51 - INFO - __main__ - Step 510 Global step 510 Train loss 3.480366 on epoch=36
03/18/2022 21:10:56 - INFO - __main__ - Step 520 Global step 520 Train loss 3.284646 on epoch=37
03/18/2022 21:11:01 - INFO - __main__ - Step 530 Global step 530 Train loss 2.930518 on epoch=37
03/18/2022 21:11:06 - INFO - __main__ - Step 540 Global step 540 Train loss 3.442984 on epoch=38
03/18/2022 21:11:11 - INFO - __main__ - Step 550 Global step 550 Train loss 2.772200 on epoch=39
03/18/2022 21:11:14 - INFO - __main__ - Global step 550 Train loss 3.182143 Classification-F1 0.42194626258214724 on epoch=39
03/18/2022 21:11:20 - INFO - __main__ - Step 560 Global step 560 Train loss 3.383675 on epoch=39
03/18/2022 21:11:25 - INFO - __main__ - Step 570 Global step 570 Train loss 2.757990 on epoch=40
03/18/2022 21:11:30 - INFO - __main__ - Step 580 Global step 580 Train loss 3.039854 on epoch=41
03/18/2022 21:11:35 - INFO - __main__ - Step 590 Global step 590 Train loss 2.555140 on epoch=42
03/18/2022 21:11:40 - INFO - __main__ - Step 600 Global step 600 Train loss 2.787380 on epoch=42
03/18/2022 21:11:44 - INFO - __main__ - Global step 600 Train loss 2.904808 Classification-F1 0.4500735632492504 on epoch=42
03/18/2022 21:11:49 - INFO - __main__ - Step 610 Global step 610 Train loss 2.770034 on epoch=43
03/18/2022 21:11:54 - INFO - __main__ - Step 620 Global step 620 Train loss 2.502221 on epoch=44
03/18/2022 21:11:59 - INFO - __main__ - Step 630 Global step 630 Train loss 2.995503 on epoch=44
03/18/2022 21:12:04 - INFO - __main__ - Step 640 Global step 640 Train loss 2.677069 on epoch=45
03/18/2022 21:12:09 - INFO - __main__ - Step 650 Global step 650 Train loss 2.711998 on epoch=46
03/18/2022 21:12:13 - INFO - __main__ - Global step 650 Train loss 2.731365 Classification-F1 0.5238702670858368 on epoch=46
03/18/2022 21:12:18 - INFO - __main__ - Step 660 Global step 660 Train loss 2.841058 on epoch=47
03/18/2022 21:12:23 - INFO - __main__ - Step 670 Global step 670 Train loss 2.441441 on epoch=47
03/18/2022 21:12:29 - INFO - __main__ - Step 680 Global step 680 Train loss 2.375101 on epoch=48
03/18/2022 21:12:34 - INFO - __main__ - Step 690 Global step 690 Train loss 2.789037 on epoch=49
03/18/2022 21:12:39 - INFO - __main__ - Step 700 Global step 700 Train loss 2.307637 on epoch=49
03/18/2022 21:12:42 - INFO - __main__ - Global step 700 Train loss 2.550855 Classification-F1 0.6033769723686625 on epoch=49
03/18/2022 21:12:48 - INFO - __main__ - Step 710 Global step 710 Train loss 2.430609 on epoch=50
03/18/2022 21:12:53 - INFO - __main__ - Step 720 Global step 720 Train loss 2.422900 on epoch=51
03/18/2022 21:12:58 - INFO - __main__ - Step 730 Global step 730 Train loss 2.210760 on epoch=52
03/18/2022 21:13:03 - INFO - __main__ - Step 740 Global step 740 Train loss 2.064285 on epoch=52
03/18/2022 21:13:08 - INFO - __main__ - Step 750 Global step 750 Train loss 2.448750 on epoch=53
03/18/2022 21:13:11 - INFO - __main__ - Global step 750 Train loss 2.315461 Classification-F1 0.6157060591913189 on epoch=53
03/18/2022 21:13:17 - INFO - __main__ - Step 760 Global step 760 Train loss 2.084434 on epoch=54
03/18/2022 21:13:22 - INFO - __main__ - Step 770 Global step 770 Train loss 2.022234 on epoch=54
03/18/2022 21:13:27 - INFO - __main__ - Step 780 Global step 780 Train loss 1.862977 on epoch=55
03/18/2022 21:13:32 - INFO - __main__ - Step 790 Global step 790 Train loss 2.052984 on epoch=56
03/18/2022 21:13:37 - INFO - __main__ - Step 800 Global step 800 Train loss 1.828872 on epoch=57
03/18/2022 21:13:41 - INFO - __main__ - Global step 800 Train loss 1.970300 Classification-F1 0.6455710558213019 on epoch=57
03/18/2022 21:13:46 - INFO - __main__ - Step 810 Global step 810 Train loss 1.781680 on epoch=57
03/18/2022 21:13:51 - INFO - __main__ - Step 820 Global step 820 Train loss 1.325120 on epoch=58
03/18/2022 21:13:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.805661 on epoch=59
03/18/2022 21:14:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.600159 on epoch=59
03/18/2022 21:14:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.315652 on epoch=60
03/18/2022 21:14:10 - INFO - __main__ - Global step 850 Train loss 0.965655 Classification-F1 0.6427163346886349 on epoch=60
03/18/2022 21:14:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.200028 on epoch=61
03/18/2022 21:14:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.175408 on epoch=62
03/18/2022 21:14:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.069101 on epoch=62
03/18/2022 21:14:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.151915 on epoch=63
03/18/2022 21:14:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.123142 on epoch=64
03/18/2022 21:14:40 - INFO - __main__ - Global step 900 Train loss 0.143919 Classification-F1 0.6840747994685806 on epoch=64
03/18/2022 21:14:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.080565 on epoch=64
03/18/2022 21:14:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.041112 on epoch=65
03/18/2022 21:14:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.066695 on epoch=66
03/18/2022 21:15:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.079041 on epoch=67
03/18/2022 21:15:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.055672 on epoch=67
03/18/2022 21:15:10 - INFO - __main__ - Global step 950 Train loss 0.064617 Classification-F1 0.790872365170605 on epoch=67
03/18/2022 21:15:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.035951 on epoch=68
03/18/2022 21:15:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.075498 on epoch=69
03/18/2022 21:15:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.062099 on epoch=69
03/18/2022 21:15:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.057165 on epoch=70
03/18/2022 21:15:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.051226 on epoch=71
03/18/2022 21:15:37 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:15:37 - INFO - __main__ - Printing 3 examples
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 21:15:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:15:37 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:15:37 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:15:37 - INFO - __main__ - Printing 3 examples
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:15:37 - INFO - __main__ - ['Film']
03/18/2022 21:15:37 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:15:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:15:38 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:15:40 - INFO - __main__ - Global step 1000 Train loss 0.056388 Classification-F1 0.7325103778648653 on epoch=71
03/18/2022 21:15:40 - INFO - __main__ - save last model!
03/18/2022 21:15:47 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 21:15:48 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 21:15:48 - INFO - __main__ - Printing 3 examples
03/18/2022 21:15:48 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 21:15:48 - INFO - __main__ - ['Animal']
03/18/2022 21:15:48 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 21:15:48 - INFO - __main__ - ['Animal']
03/18/2022 21:15:48 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 21:15:48 - INFO - __main__ - ['Village']
03/18/2022 21:15:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:15:50 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:15:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:15:50 - INFO - __main__ - Starting training!
03/18/2022 21:15:53 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 21:17:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_42_0.0001_8_predictions.txt
03/18/2022 21:17:12 - INFO - __main__ - Classification-F1 on test data: 0.4002
03/18/2022 21:17:12 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.0001, bsz=8, dev_performance=0.790872365170605, test_performance=0.4002345414974737
03/18/2022 21:17:12 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.0005, bsz=8 ...
03/18/2022 21:17:13 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:17:13 - INFO - __main__ - Printing 3 examples
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:17:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:17:13 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:17:13 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:17:13 - INFO - __main__ - Printing 3 examples
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:17:13 - INFO - __main__ - ['Film']
03/18/2022 21:17:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:17:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:17:14 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:17:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:17:25 - INFO - __main__ - Starting training!
03/18/2022 21:17:29 - INFO - __main__ - Step 10 Global step 10 Train loss 21.766287 on epoch=0
03/18/2022 21:17:34 - INFO - __main__ - Step 20 Global step 20 Train loss 18.762178 on epoch=1
03/18/2022 21:17:39 - INFO - __main__ - Step 30 Global step 30 Train loss 14.267415 on epoch=2
03/18/2022 21:17:44 - INFO - __main__ - Step 40 Global step 40 Train loss 11.590354 on epoch=2
03/18/2022 21:17:49 - INFO - __main__ - Step 50 Global step 50 Train loss 9.676745 on epoch=3
03/18/2022 21:17:54 - INFO - __main__ - Global step 50 Train loss 15.212597 Classification-F1 0.0 on epoch=3
03/18/2022 21:18:00 - INFO - __main__ - Step 60 Global step 60 Train loss 9.192239 on epoch=4
03/18/2022 21:18:05 - INFO - __main__ - Step 70 Global step 70 Train loss 8.511212 on epoch=4
03/18/2022 21:18:10 - INFO - __main__ - Step 80 Global step 80 Train loss 7.022901 on epoch=5
03/18/2022 21:18:15 - INFO - __main__ - Step 90 Global step 90 Train loss 6.491372 on epoch=6
03/18/2022 21:18:20 - INFO - __main__ - Step 100 Global step 100 Train loss 4.746650 on epoch=7
03/18/2022 21:18:25 - INFO - __main__ - Global step 100 Train loss 7.192874 Classification-F1 0.0 on epoch=7
03/18/2022 21:18:30 - INFO - __main__ - Step 110 Global step 110 Train loss 4.049765 on epoch=7
03/18/2022 21:18:35 - INFO - __main__ - Step 120 Global step 120 Train loss 3.046454 on epoch=8
03/18/2022 21:18:40 - INFO - __main__ - Step 130 Global step 130 Train loss 2.997140 on epoch=9
03/18/2022 21:18:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.901815 on epoch=9
03/18/2022 21:18:51 - INFO - __main__ - Step 150 Global step 150 Train loss 2.556711 on epoch=10
03/18/2022 21:18:54 - INFO - __main__ - Global step 150 Train loss 3.110377 Classification-F1 0.09422871971061499 on epoch=10
03/18/2022 21:19:00 - INFO - __main__ - Step 160 Global step 160 Train loss 2.315436 on epoch=11
03/18/2022 21:19:05 - INFO - __main__ - Step 170 Global step 170 Train loss 2.275332 on epoch=12
03/18/2022 21:19:10 - INFO - __main__ - Step 180 Global step 180 Train loss 2.056507 on epoch=12
03/18/2022 21:19:16 - INFO - __main__ - Step 190 Global step 190 Train loss 2.161611 on epoch=13
03/18/2022 21:19:21 - INFO - __main__ - Step 200 Global step 200 Train loss 1.993843 on epoch=14
03/18/2022 21:19:24 - INFO - __main__ - Global step 200 Train loss 2.160546 Classification-F1 0.4777162103771029 on epoch=14
03/18/2022 21:19:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.645715 on epoch=14
03/18/2022 21:19:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.041714 on epoch=15
03/18/2022 21:19:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.782216 on epoch=16
03/18/2022 21:19:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.255483 on epoch=17
03/18/2022 21:19:50 - INFO - __main__ - Step 250 Global step 250 Train loss 1.546892 on epoch=17
03/18/2022 21:19:54 - INFO - __main__ - Global step 250 Train loss 1.654404 Classification-F1 0.6341644033715644 on epoch=17
03/18/2022 21:20:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.114243 on epoch=18
03/18/2022 21:20:05 - INFO - __main__ - Step 270 Global step 270 Train loss 1.066558 on epoch=19
03/18/2022 21:20:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.955160 on epoch=19
03/18/2022 21:20:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.591428 on epoch=20
03/18/2022 21:20:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.632557 on epoch=21
03/18/2022 21:20:24 - INFO - __main__ - Global step 300 Train loss 0.871989 Classification-F1 0.9124776509750827 on epoch=21
03/18/2022 21:20:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.562247 on epoch=22
03/18/2022 21:20:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.350328 on epoch=22
03/18/2022 21:20:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.116463 on epoch=23
03/18/2022 21:20:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.201448 on epoch=24
03/18/2022 21:20:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.117529 on epoch=24
03/18/2022 21:20:54 - INFO - __main__ - Global step 350 Train loss 0.269603 Classification-F1 0.7728435545532878 on epoch=24
03/18/2022 21:21:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.051341 on epoch=25
03/18/2022 21:21:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.023576 on epoch=26
03/18/2022 21:21:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.032114 on epoch=27
03/18/2022 21:21:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006420 on epoch=27
03/18/2022 21:21:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.117605 on epoch=28
03/18/2022 21:21:24 - INFO - __main__ - Global step 400 Train loss 0.046211 Classification-F1 0.3197382260297574 on epoch=28
03/18/2022 21:21:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.286678 on epoch=29
03/18/2022 21:21:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.107597 on epoch=29
03/18/2022 21:21:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.196471 on epoch=30
03/18/2022 21:21:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.085963 on epoch=31
03/18/2022 21:21:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.095287 on epoch=32
03/18/2022 21:21:54 - INFO - __main__ - Global step 450 Train loss 0.154399 Classification-F1 0.681851737382256 on epoch=32
03/18/2022 21:21:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.421780 on epoch=32
03/18/2022 21:22:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.163834 on epoch=33
03/18/2022 21:22:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.064879 on epoch=34
03/18/2022 21:22:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.046850 on epoch=34
03/18/2022 21:22:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003225 on epoch=35
03/18/2022 21:22:23 - INFO - __main__ - Global step 500 Train loss 0.140113 Classification-F1 0.8570401565987303 on epoch=35
03/18/2022 21:22:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000577 on epoch=36
03/18/2022 21:22:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005861 on epoch=37
03/18/2022 21:22:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000413 on epoch=37
03/18/2022 21:22:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.050233 on epoch=38
03/18/2022 21:22:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000217 on epoch=39
03/18/2022 21:22:53 - INFO - __main__ - Global step 550 Train loss 0.011460 Classification-F1 0.7853911274539458 on epoch=39
03/18/2022 21:22:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003223 on epoch=39
03/18/2022 21:23:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006199 on epoch=40
03/18/2022 21:23:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000881 on epoch=41
03/18/2022 21:23:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000772 on epoch=42
03/18/2022 21:23:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001012 on epoch=42
03/18/2022 21:23:22 - INFO - __main__ - Global step 600 Train loss 0.002417 Classification-F1 0.7337285040419241 on epoch=42
03/18/2022 21:23:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000178 on epoch=43
03/18/2022 21:23:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001175 on epoch=44
03/18/2022 21:23:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000474 on epoch=44
03/18/2022 21:23:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.033553 on epoch=45
03/18/2022 21:23:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000186 on epoch=46
03/18/2022 21:23:51 - INFO - __main__ - Global step 650 Train loss 0.007113 Classification-F1 0.746319935598873 on epoch=46
03/18/2022 21:23:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000393 on epoch=47
03/18/2022 21:24:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000366 on epoch=47
03/18/2022 21:24:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000300 on epoch=48
03/18/2022 21:24:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000216 on epoch=49
03/18/2022 21:24:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000305 on epoch=49
03/18/2022 21:24:21 - INFO - __main__ - Global step 700 Train loss 0.000316 Classification-F1 0.7358597312038326 on epoch=49
03/18/2022 21:24:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000099 on epoch=50
03/18/2022 21:24:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000123 on epoch=51
03/18/2022 21:24:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000446 on epoch=52
03/18/2022 21:24:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000060 on epoch=52
03/18/2022 21:24:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000083 on epoch=53
03/18/2022 21:24:50 - INFO - __main__ - Global step 750 Train loss 0.000162 Classification-F1 0.7395397956049596 on epoch=53
03/18/2022 21:24:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000124 on epoch=54
03/18/2022 21:25:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000093 on epoch=54
03/18/2022 21:25:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000272 on epoch=55
03/18/2022 21:25:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000096 on epoch=56
03/18/2022 21:25:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000144 on epoch=57
03/18/2022 21:25:20 - INFO - __main__ - Global step 800 Train loss 0.000146 Classification-F1 0.733902314525152 on epoch=57
03/18/2022 21:25:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000061 on epoch=57
03/18/2022 21:25:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.013271 on epoch=58
03/18/2022 21:25:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000105 on epoch=59
03/18/2022 21:25:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000080 on epoch=59
03/18/2022 21:25:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001617 on epoch=60
03/18/2022 21:25:49 - INFO - __main__ - Global step 850 Train loss 0.003027 Classification-F1 0.7888098133180745 on epoch=60
03/18/2022 21:25:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000097 on epoch=61
03/18/2022 21:25:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000076 on epoch=62
03/18/2022 21:26:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000055 on epoch=62
03/18/2022 21:26:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000045 on epoch=63
03/18/2022 21:26:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000253 on epoch=64
03/18/2022 21:26:19 - INFO - __main__ - Global step 900 Train loss 0.000105 Classification-F1 0.7928542903464225 on epoch=64
03/18/2022 21:26:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000148 on epoch=64
03/18/2022 21:26:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000043 on epoch=65
03/18/2022 21:26:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000041 on epoch=66
03/18/2022 21:26:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000054 on epoch=67
03/18/2022 21:26:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000031 on epoch=67
03/18/2022 21:26:48 - INFO - __main__ - Global step 950 Train loss 0.000063 Classification-F1 0.7148905076599075 on epoch=67
03/18/2022 21:26:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000039 on epoch=68
03/18/2022 21:26:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000054 on epoch=69
03/18/2022 21:27:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000042 on epoch=69
03/18/2022 21:27:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000029 on epoch=70
03/18/2022 21:27:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000048 on epoch=71
03/18/2022 21:27:15 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:27:15 - INFO - __main__ - Printing 3 examples
03/18/2022 21:27:15 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:27:15 - INFO - __main__ - ['Film']
03/18/2022 21:27:15 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:27:15 - INFO - __main__ - ['Film']
03/18/2022 21:27:15 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:27:15 - INFO - __main__ - ['Film']
03/18/2022 21:27:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 21:27:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:27:16 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:27:16 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:27:16 - INFO - __main__ - Printing 3 examples
03/18/2022 21:27:16 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:27:16 - INFO - __main__ - ['Film']
03/18/2022 21:27:16 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:27:16 - INFO - __main__ - ['Film']
03/18/2022 21:27:16 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:27:16 - INFO - __main__ - ['Film']
03/18/2022 21:27:16 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:27:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:27:16 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:27:18 - INFO - __main__ - Global step 1000 Train loss 0.000042 Classification-F1 0.699201367569271 on epoch=71
03/18/2022 21:27:18 - INFO - __main__ - save last model!
03/18/2022 21:27:25 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 21:27:26 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 21:27:26 - INFO - __main__ - Printing 3 examples
03/18/2022 21:27:26 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 21:27:26 - INFO - __main__ - ['Animal']
03/18/2022 21:27:26 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 21:27:26 - INFO - __main__ - ['Animal']
03/18/2022 21:27:26 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 21:27:26 - INFO - __main__ - ['Village']
03/18/2022 21:27:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:27:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:27:27 - INFO - __main__ - Starting training!
03/18/2022 21:27:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:27:32 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 21:28:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_87_0.0005_8_predictions.txt
03/18/2022 21:28:44 - INFO - __main__ - Classification-F1 on test data: 0.5769
03/18/2022 21:28:45 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.0005, bsz=8, dev_performance=0.9124776509750827, test_performance=0.5768787411033274
03/18/2022 21:28:45 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.0003, bsz=8 ...
03/18/2022 21:28:46 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:28:46 - INFO - __main__ - Printing 3 examples
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:28:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:28:46 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:28:46 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:28:46 - INFO - __main__ - Printing 3 examples
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:28:46 - INFO - __main__ - ['Film']
03/18/2022 21:28:46 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:28:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:28:46 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:28:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:28:57 - INFO - __main__ - Starting training!
03/18/2022 21:29:01 - INFO - __main__ - Step 10 Global step 10 Train loss 21.530092 on epoch=0
03/18/2022 21:29:06 - INFO - __main__ - Step 20 Global step 20 Train loss 19.794470 on epoch=1
03/18/2022 21:29:11 - INFO - __main__ - Step 30 Global step 30 Train loss 15.179298 on epoch=2
03/18/2022 21:29:16 - INFO - __main__ - Step 40 Global step 40 Train loss 13.175756 on epoch=2
03/18/2022 21:29:21 - INFO - __main__ - Step 50 Global step 50 Train loss 10.725250 on epoch=3
03/18/2022 21:29:26 - INFO - __main__ - Global step 50 Train loss 16.080973 Classification-F1 0.0 on epoch=3
03/18/2022 21:29:32 - INFO - __main__ - Step 60 Global step 60 Train loss 11.044618 on epoch=4
03/18/2022 21:29:37 - INFO - __main__ - Step 70 Global step 70 Train loss 10.421407 on epoch=4
03/18/2022 21:29:42 - INFO - __main__ - Step 80 Global step 80 Train loss 8.742533 on epoch=5
03/18/2022 21:29:47 - INFO - __main__ - Step 90 Global step 90 Train loss 9.484579 on epoch=6
03/18/2022 21:29:52 - INFO - __main__ - Step 100 Global step 100 Train loss 8.494881 on epoch=7
03/18/2022 21:29:56 - INFO - __main__ - Global step 100 Train loss 9.637604 Classification-F1 0.0 on epoch=7
03/18/2022 21:30:01 - INFO - __main__ - Step 110 Global step 110 Train loss 7.943656 on epoch=7
03/18/2022 21:30:06 - INFO - __main__ - Step 120 Global step 120 Train loss 6.416966 on epoch=8
03/18/2022 21:30:11 - INFO - __main__ - Step 130 Global step 130 Train loss 6.279704 on epoch=9
03/18/2022 21:30:16 - INFO - __main__ - Step 140 Global step 140 Train loss 6.251411 on epoch=9
03/18/2022 21:30:22 - INFO - __main__ - Step 150 Global step 150 Train loss 4.803168 on epoch=10
03/18/2022 21:30:25 - INFO - __main__ - Global step 150 Train loss 6.338981 Classification-F1 0.0 on epoch=10
03/18/2022 21:30:30 - INFO - __main__ - Step 160 Global step 160 Train loss 4.794475 on epoch=11
03/18/2022 21:30:35 - INFO - __main__ - Step 170 Global step 170 Train loss 3.494185 on epoch=12
03/18/2022 21:30:40 - INFO - __main__ - Step 180 Global step 180 Train loss 3.927711 on epoch=12
03/18/2022 21:30:46 - INFO - __main__ - Step 190 Global step 190 Train loss 2.590534 on epoch=13
03/18/2022 21:30:51 - INFO - __main__ - Step 200 Global step 200 Train loss 3.104715 on epoch=14
03/18/2022 21:30:54 - INFO - __main__ - Global step 200 Train loss 3.582324 Classification-F1 0.14456799504345694 on epoch=14
03/18/2022 21:31:00 - INFO - __main__ - Step 210 Global step 210 Train loss 3.378076 on epoch=14
03/18/2022 21:31:05 - INFO - __main__ - Step 220 Global step 220 Train loss 2.581168 on epoch=15
03/18/2022 21:31:10 - INFO - __main__ - Step 230 Global step 230 Train loss 2.710129 on epoch=16
03/18/2022 21:31:15 - INFO - __main__ - Step 240 Global step 240 Train loss 2.309730 on epoch=17
03/18/2022 21:31:20 - INFO - __main__ - Step 250 Global step 250 Train loss 2.703963 on epoch=17
03/18/2022 21:31:24 - INFO - __main__ - Global step 250 Train loss 2.736613 Classification-F1 0.5078507128624927 on epoch=17
03/18/2022 21:31:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.948676 on epoch=18
03/18/2022 21:31:35 - INFO - __main__ - Step 270 Global step 270 Train loss 2.257836 on epoch=19
03/18/2022 21:31:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.819957 on epoch=19
03/18/2022 21:31:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.893762 on epoch=20
03/18/2022 21:31:50 - INFO - __main__ - Step 300 Global step 300 Train loss 2.175336 on epoch=21
03/18/2022 21:31:53 - INFO - __main__ - Global step 300 Train loss 2.019114 Classification-F1 0.3312951861668099 on epoch=21
03/18/2022 21:31:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.811495 on epoch=22
03/18/2022 21:32:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.840246 on epoch=22
03/18/2022 21:32:08 - INFO - __main__ - Step 330 Global step 330 Train loss 1.520829 on epoch=23
03/18/2022 21:32:13 - INFO - __main__ - Step 340 Global step 340 Train loss 1.934160 on epoch=24
03/18/2022 21:32:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.828062 on epoch=24
03/18/2022 21:32:21 - INFO - __main__ - Global step 350 Train loss 1.786958 Classification-F1 0.34184064652835683 on epoch=24
03/18/2022 21:32:26 - INFO - __main__ - Step 360 Global step 360 Train loss 1.487510 on epoch=25
03/18/2022 21:32:31 - INFO - __main__ - Step 370 Global step 370 Train loss 1.730747 on epoch=26
03/18/2022 21:32:36 - INFO - __main__ - Step 380 Global step 380 Train loss 1.262446 on epoch=27
03/18/2022 21:32:41 - INFO - __main__ - Step 390 Global step 390 Train loss 1.345279 on epoch=27
03/18/2022 21:32:46 - INFO - __main__ - Step 400 Global step 400 Train loss 1.194100 on epoch=28
03/18/2022 21:32:49 - INFO - __main__ - Global step 400 Train loss 1.404016 Classification-F1 0.5617042149523214 on epoch=28
03/18/2022 21:32:55 - INFO - __main__ - Step 410 Global step 410 Train loss 1.395729 on epoch=29
03/18/2022 21:33:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.948882 on epoch=29
03/18/2022 21:33:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.930809 on epoch=30
03/18/2022 21:33:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.974542 on epoch=31
03/18/2022 21:33:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.786684 on epoch=32
03/18/2022 21:33:19 - INFO - __main__ - Global step 450 Train loss 1.007329 Classification-F1 0.6282330133557501 on epoch=32
03/18/2022 21:33:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.718134 on epoch=32
03/18/2022 21:33:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.530859 on epoch=33
03/18/2022 21:33:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.328033 on epoch=34
03/18/2022 21:33:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.245189 on epoch=34
03/18/2022 21:33:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.175811 on epoch=35
03/18/2022 21:33:49 - INFO - __main__ - Global step 500 Train loss 0.399605 Classification-F1 0.7259380661826362 on epoch=35
03/18/2022 21:33:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.242943 on epoch=36
03/18/2022 21:34:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.293204 on epoch=37
03/18/2022 21:34:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.462032 on epoch=37
03/18/2022 21:34:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.040004 on epoch=38
03/18/2022 21:34:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.042572 on epoch=39
03/18/2022 21:34:19 - INFO - __main__ - Global step 550 Train loss 0.216151 Classification-F1 0.836276794554156 on epoch=39
03/18/2022 21:34:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023140 on epoch=39
03/18/2022 21:34:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.012168 on epoch=40
03/18/2022 21:34:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020679 on epoch=41
03/18/2022 21:34:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.007081 on epoch=42
03/18/2022 21:34:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003651 on epoch=42
03/18/2022 21:34:50 - INFO - __main__ - Global step 600 Train loss 0.013344 Classification-F1 0.7134526456172084 on epoch=42
03/18/2022 21:34:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.024205 on epoch=43
03/18/2022 21:35:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.029500 on epoch=44
03/18/2022 21:35:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.003535 on epoch=44
03/18/2022 21:35:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003978 on epoch=45
03/18/2022 21:35:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.004974 on epoch=46
03/18/2022 21:35:20 - INFO - __main__ - Global step 650 Train loss 0.013238 Classification-F1 0.8312079154286259 on epoch=46
03/18/2022 21:35:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.021452 on epoch=47
03/18/2022 21:35:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001150 on epoch=47
03/18/2022 21:35:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.005160 on epoch=48
03/18/2022 21:35:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001330 on epoch=49
03/18/2022 21:35:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002853 on epoch=49
03/18/2022 21:35:59 - INFO - __main__ - Global step 700 Train loss 0.006389 Classification-F1 0.739576928486901 on epoch=49
03/18/2022 21:36:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.023285 on epoch=50
03/18/2022 21:36:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002984 on epoch=51
03/18/2022 21:36:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.006482 on epoch=52
03/18/2022 21:36:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.013968 on epoch=52
03/18/2022 21:36:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001949 on epoch=53
03/18/2022 21:36:29 - INFO - __main__ - Global step 750 Train loss 0.009734 Classification-F1 0.9029558770851235 on epoch=53
03/18/2022 21:36:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000668 on epoch=54
03/18/2022 21:36:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.010751 on epoch=54
03/18/2022 21:36:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.008992 on epoch=55
03/18/2022 21:36:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002051 on epoch=56
03/18/2022 21:36:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004565 on epoch=57
03/18/2022 21:37:00 - INFO - __main__ - Global step 800 Train loss 0.005406 Classification-F1 0.7769455046205637 on epoch=57
03/18/2022 21:37:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.008055 on epoch=57
03/18/2022 21:37:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001019 on epoch=58
03/18/2022 21:37:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000780 on epoch=59
03/18/2022 21:37:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.006004 on epoch=59
03/18/2022 21:37:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000376 on epoch=60
03/18/2022 21:37:29 - INFO - __main__ - Global step 850 Train loss 0.003247 Classification-F1 0.8755910358060895 on epoch=60
03/18/2022 21:37:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000977 on epoch=61
03/18/2022 21:37:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.004635 on epoch=62
03/18/2022 21:37:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002307 on epoch=62
03/18/2022 21:37:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000752 on epoch=63
03/18/2022 21:37:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000662 on epoch=64
03/18/2022 21:37:58 - INFO - __main__ - Global step 900 Train loss 0.001867 Classification-F1 0.8265758242039077 on epoch=64
03/18/2022 21:38:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002085 on epoch=64
03/18/2022 21:38:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.007909 on epoch=65
03/18/2022 21:38:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000277 on epoch=66
03/18/2022 21:38:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001797 on epoch=67
03/18/2022 21:38:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000789 on epoch=67
03/18/2022 21:38:27 - INFO - __main__ - Global step 950 Train loss 0.002571 Classification-F1 0.8715730326052906 on epoch=67
03/18/2022 21:38:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000180 on epoch=68
03/18/2022 21:38:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000105 on epoch=69
03/18/2022 21:38:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000952 on epoch=69
03/18/2022 21:38:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000252 on epoch=70
03/18/2022 21:38:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.024370 on epoch=71
03/18/2022 21:38:53 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:38:53 - INFO - __main__ - Printing 3 examples
03/18/2022 21:38:53 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:38:53 - INFO - __main__ - ['Film']
03/18/2022 21:38:53 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:38:53 - INFO - __main__ - ['Film']
03/18/2022 21:38:53 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:38:53 - INFO - __main__ - ['Film']
03/18/2022 21:38:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 21:38:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:38:54 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:38:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:38:54 - INFO - __main__ - Printing 3 examples
03/18/2022 21:38:54 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:38:54 - INFO - __main__ - ['Film']
03/18/2022 21:38:54 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:38:54 - INFO - __main__ - ['Film']
03/18/2022 21:38:54 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:38:54 - INFO - __main__ - ['Film']
03/18/2022 21:38:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:38:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:38:54 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:38:56 - INFO - __main__ - Global step 1000 Train loss 0.005172 Classification-F1 0.8144943111633154 on epoch=71
03/18/2022 21:38:56 - INFO - __main__ - save last model!
03/18/2022 21:39:03 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 21:39:04 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 21:39:04 - INFO - __main__ - Printing 3 examples
03/18/2022 21:39:04 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 21:39:04 - INFO - __main__ - ['Animal']
03/18/2022 21:39:04 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 21:39:04 - INFO - __main__ - ['Animal']
03/18/2022 21:39:04 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 21:39:04 - INFO - __main__ - ['Village']
03/18/2022 21:39:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:39:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:39:05 - INFO - __main__ - Starting training!
03/18/2022 21:39:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:39:09 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 21:40:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_87_0.0003_8_predictions.txt
03/18/2022 21:40:23 - INFO - __main__ - Classification-F1 on test data: 0.5477
03/18/2022 21:40:23 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.0003, bsz=8, dev_performance=0.9029558770851235, test_performance=0.5477245464229633
03/18/2022 21:40:23 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.0002, bsz=8 ...
03/18/2022 21:40:24 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:40:24 - INFO - __main__ - Printing 3 examples
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:40:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:40:24 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:40:24 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:40:24 - INFO - __main__ - Printing 3 examples
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:40:24 - INFO - __main__ - ['Film']
03/18/2022 21:40:24 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:40:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:40:25 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:40:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:40:36 - INFO - __main__ - Starting training!
03/18/2022 21:40:40 - INFO - __main__ - Step 10 Global step 10 Train loss 21.684725 on epoch=0
03/18/2022 21:40:45 - INFO - __main__ - Step 20 Global step 20 Train loss 18.878021 on epoch=1
03/18/2022 21:40:50 - INFO - __main__ - Step 30 Global step 30 Train loss 13.485672 on epoch=2
03/18/2022 21:40:56 - INFO - __main__ - Step 40 Global step 40 Train loss 12.934611 on epoch=2
03/18/2022 21:41:01 - INFO - __main__ - Step 50 Global step 50 Train loss 10.510648 on epoch=3
03/18/2022 21:41:08 - INFO - __main__ - Global step 50 Train loss 15.498735 Classification-F1 0.0 on epoch=3
03/18/2022 21:41:14 - INFO - __main__ - Step 60 Global step 60 Train loss 11.180617 on epoch=4
03/18/2022 21:41:19 - INFO - __main__ - Step 70 Global step 70 Train loss 11.010981 on epoch=4
03/18/2022 21:41:24 - INFO - __main__ - Step 80 Global step 80 Train loss 9.438072 on epoch=5
03/18/2022 21:41:30 - INFO - __main__ - Step 90 Global step 90 Train loss 10.150191 on epoch=6
03/18/2022 21:41:35 - INFO - __main__ - Step 100 Global step 100 Train loss 9.304764 on epoch=7
03/18/2022 21:41:39 - INFO - __main__ - Global step 100 Train loss 10.216925 Classification-F1 0.0 on epoch=7
03/18/2022 21:41:44 - INFO - __main__ - Step 110 Global step 110 Train loss 9.226053 on epoch=7
03/18/2022 21:41:50 - INFO - __main__ - Step 120 Global step 120 Train loss 8.363039 on epoch=8
03/18/2022 21:41:55 - INFO - __main__ - Step 130 Global step 130 Train loss 8.034192 on epoch=9
03/18/2022 21:42:00 - INFO - __main__ - Step 140 Global step 140 Train loss 8.323243 on epoch=9
03/18/2022 21:42:05 - INFO - __main__ - Step 150 Global step 150 Train loss 7.204687 on epoch=10
03/18/2022 21:42:10 - INFO - __main__ - Global step 150 Train loss 8.230243 Classification-F1 0.0 on epoch=10
03/18/2022 21:42:15 - INFO - __main__ - Step 160 Global step 160 Train loss 7.353421 on epoch=11
03/18/2022 21:42:20 - INFO - __main__ - Step 170 Global step 170 Train loss 6.506694 on epoch=12
03/18/2022 21:42:25 - INFO - __main__ - Step 180 Global step 180 Train loss 5.888156 on epoch=12
03/18/2022 21:42:30 - INFO - __main__ - Step 190 Global step 190 Train loss 5.894235 on epoch=13
03/18/2022 21:42:35 - INFO - __main__ - Step 200 Global step 200 Train loss 4.413037 on epoch=14
03/18/2022 21:42:39 - INFO - __main__ - Global step 200 Train loss 6.011108 Classification-F1 0.06183150183150184 on epoch=14
03/18/2022 21:42:45 - INFO - __main__ - Step 210 Global step 210 Train loss 4.625532 on epoch=14
03/18/2022 21:42:50 - INFO - __main__ - Step 220 Global step 220 Train loss 3.769278 on epoch=15
03/18/2022 21:42:55 - INFO - __main__ - Step 230 Global step 230 Train loss 3.814007 on epoch=16
03/18/2022 21:43:01 - INFO - __main__ - Step 240 Global step 240 Train loss 3.287425 on epoch=17
03/18/2022 21:43:06 - INFO - __main__ - Step 250 Global step 250 Train loss 3.174429 on epoch=17
03/18/2022 21:43:09 - INFO - __main__ - Global step 250 Train loss 3.734134 Classification-F1 0.3857356704724811 on epoch=17
03/18/2022 21:43:15 - INFO - __main__ - Step 260 Global step 260 Train loss 2.814363 on epoch=18
03/18/2022 21:43:20 - INFO - __main__ - Step 270 Global step 270 Train loss 2.755911 on epoch=19
03/18/2022 21:43:26 - INFO - __main__ - Step 280 Global step 280 Train loss 3.119071 on epoch=19
03/18/2022 21:43:31 - INFO - __main__ - Step 290 Global step 290 Train loss 2.795312 on epoch=20
03/18/2022 21:43:36 - INFO - __main__ - Step 300 Global step 300 Train loss 3.084499 on epoch=21
03/18/2022 21:43:39 - INFO - __main__ - Global step 300 Train loss 2.913831 Classification-F1 0.5294592441719771 on epoch=21
03/18/2022 21:43:45 - INFO - __main__ - Step 310 Global step 310 Train loss 2.447827 on epoch=22
03/18/2022 21:43:51 - INFO - __main__ - Step 320 Global step 320 Train loss 2.263283 on epoch=22
03/18/2022 21:43:56 - INFO - __main__ - Step 330 Global step 330 Train loss 2.211825 on epoch=23
03/18/2022 21:44:01 - INFO - __main__ - Step 340 Global step 340 Train loss 2.360755 on epoch=24
03/18/2022 21:44:06 - INFO - __main__ - Step 350 Global step 350 Train loss 2.419827 on epoch=24
03/18/2022 21:44:10 - INFO - __main__ - Global step 350 Train loss 2.340703 Classification-F1 0.5776232278098569 on epoch=24
03/18/2022 21:44:15 - INFO - __main__ - Step 360 Global step 360 Train loss 1.821772 on epoch=25
03/18/2022 21:44:21 - INFO - __main__ - Step 370 Global step 370 Train loss 1.947422 on epoch=26
03/18/2022 21:44:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.893987 on epoch=27
03/18/2022 21:44:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.737795 on epoch=27
03/18/2022 21:44:36 - INFO - __main__ - Step 400 Global step 400 Train loss 1.150294 on epoch=28
03/18/2022 21:44:40 - INFO - __main__ - Global step 400 Train loss 1.710254 Classification-F1 0.4828325663979863 on epoch=28
03/18/2022 21:44:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.181828 on epoch=29
03/18/2022 21:44:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.097135 on epoch=29
03/18/2022 21:44:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.064453 on epoch=30
03/18/2022 21:45:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.053637 on epoch=31
03/18/2022 21:45:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.037264 on epoch=32
03/18/2022 21:45:10 - INFO - __main__ - Global step 450 Train loss 0.086863 Classification-F1 0.8063098788194866 on epoch=32
03/18/2022 21:45:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.034176 on epoch=32
03/18/2022 21:45:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.013159 on epoch=33
03/18/2022 21:45:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.015688 on epoch=34
03/18/2022 21:45:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.016539 on epoch=34
03/18/2022 21:45:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.024051 on epoch=35
03/18/2022 21:45:40 - INFO - __main__ - Global step 500 Train loss 0.020723 Classification-F1 0.8285312001594896 on epoch=35
03/18/2022 21:45:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.046666 on epoch=36
03/18/2022 21:45:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.077590 on epoch=37
03/18/2022 21:45:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003418 on epoch=37
03/18/2022 21:46:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.009210 on epoch=38
03/18/2022 21:46:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001523 on epoch=39
03/18/2022 21:46:10 - INFO - __main__ - Global step 550 Train loss 0.027681 Classification-F1 0.6983641762341952 on epoch=39
03/18/2022 21:46:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000878 on epoch=39
03/18/2022 21:46:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001459 on epoch=40
03/18/2022 21:46:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006951 on epoch=41
03/18/2022 21:46:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003030 on epoch=42
03/18/2022 21:46:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005140 on epoch=42
03/18/2022 21:46:40 - INFO - __main__ - Global step 600 Train loss 0.003492 Classification-F1 0.6797729438970169 on epoch=42
03/18/2022 21:46:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.007929 on epoch=43
03/18/2022 21:46:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.009111 on epoch=44
03/18/2022 21:46:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.009305 on epoch=44
03/18/2022 21:47:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.011899 on epoch=45
03/18/2022 21:47:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001235 on epoch=46
03/18/2022 21:47:09 - INFO - __main__ - Global step 650 Train loss 0.007896 Classification-F1 0.7131698498633983 on epoch=46
03/18/2022 21:47:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006268 on epoch=47
03/18/2022 21:47:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.086595 on epoch=47
03/18/2022 21:47:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.038049 on epoch=48
03/18/2022 21:47:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.029145 on epoch=49
03/18/2022 21:47:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.003508 on epoch=49
03/18/2022 21:47:38 - INFO - __main__ - Global step 700 Train loss 0.032713 Classification-F1 0.6790050302497483 on epoch=49
03/18/2022 21:47:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.004713 on epoch=50
03/18/2022 21:47:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001033 on epoch=51
03/18/2022 21:47:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000695 on epoch=52
03/18/2022 21:47:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000334 on epoch=52
03/18/2022 21:48:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.041801 on epoch=53
03/18/2022 21:48:07 - INFO - __main__ - Global step 750 Train loss 0.009715 Classification-F1 0.757963700220017 on epoch=53
03/18/2022 21:48:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.003575 on epoch=54
03/18/2022 21:48:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001090 on epoch=54
03/18/2022 21:48:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003365 on epoch=55
03/18/2022 21:48:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008702 on epoch=56
03/18/2022 21:48:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004593 on epoch=57
03/18/2022 21:48:36 - INFO - __main__ - Global step 800 Train loss 0.004265 Classification-F1 0.5584216342207042 on epoch=57
03/18/2022 21:48:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004226 on epoch=57
03/18/2022 21:48:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001285 on epoch=58
03/18/2022 21:48:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.002125 on epoch=59
03/18/2022 21:48:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.005169 on epoch=59
03/18/2022 21:49:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000448 on epoch=60
03/18/2022 21:49:05 - INFO - __main__ - Global step 850 Train loss 0.002650 Classification-F1 0.6111453201970443 on epoch=60
03/18/2022 21:49:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000734 on epoch=61
03/18/2022 21:49:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006833 on epoch=62
03/18/2022 21:49:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000737 on epoch=62
03/18/2022 21:49:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001217 on epoch=63
03/18/2022 21:49:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001897 on epoch=64
03/18/2022 21:49:35 - INFO - __main__ - Global step 900 Train loss 0.002283 Classification-F1 0.6168120036510771 on epoch=64
03/18/2022 21:49:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000590 on epoch=64
03/18/2022 21:49:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000158 on epoch=65
03/18/2022 21:49:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000147 on epoch=66
03/18/2022 21:49:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001329 on epoch=67
03/18/2022 21:50:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000860 on epoch=67
03/18/2022 21:50:04 - INFO - __main__ - Global step 950 Train loss 0.000617 Classification-F1 0.5207003467657667 on epoch=67
03/18/2022 21:50:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000467 on epoch=68
03/18/2022 21:50:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.006878 on epoch=69
03/18/2022 21:50:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.006587 on epoch=69
03/18/2022 21:50:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000062 on epoch=70
03/18/2022 21:50:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000153 on epoch=71
03/18/2022 21:50:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:50:32 - INFO - __main__ - Printing 3 examples
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 21:50:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:50:32 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:50:32 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:50:32 - INFO - __main__ - Printing 3 examples
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:50:32 - INFO - __main__ - ['Film']
03/18/2022 21:50:32 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:50:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:50:32 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:50:34 - INFO - __main__ - Global step 1000 Train loss 0.002830 Classification-F1 0.7635364556379737 on epoch=71
03/18/2022 21:50:34 - INFO - __main__ - save last model!
03/18/2022 21:50:41 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 21:50:42 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 21:50:42 - INFO - __main__ - Printing 3 examples
03/18/2022 21:50:42 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 21:50:42 - INFO - __main__ - ['Animal']
03/18/2022 21:50:42 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 21:50:42 - INFO - __main__ - ['Animal']
03/18/2022 21:50:42 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 21:50:42 - INFO - __main__ - ['Village']
03/18/2022 21:50:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:50:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:50:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:50:45 - INFO - __main__ - Starting training!
03/18/2022 21:50:47 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 21:51:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_87_0.0002_8_predictions.txt
03/18/2022 21:51:57 - INFO - __main__ - Classification-F1 on test data: 0.3222
03/18/2022 21:51:57 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.0002, bsz=8, dev_performance=0.8285312001594896, test_performance=0.322246939091555
03/18/2022 21:51:57 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.0001, bsz=8 ...
03/18/2022 21:51:58 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:51:58 - INFO - __main__ - Printing 3 examples
03/18/2022 21:51:58 - INFO - __main__ -  [dbpedia_14] Aib The Movie ( -- ! 42.195km ) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aib.
03/18/2022 21:51:58 - INFO - __main__ - ['Film']
03/18/2022 21:51:58 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shjo ( lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 21:51:58 - INFO - __main__ - ['Film']
03/18/2022 21:51:58 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 21:51:58 - INFO - __main__ - ['Film']
03/18/2022 21:51:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 21:51:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:51:59 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 21:51:59 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 21:51:59 - INFO - __main__ - Printing 3 examples
03/18/2022 21:51:59 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 21:51:59 - INFO - __main__ - ['Film']
03/18/2022 21:51:59 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres  Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres  Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between Franois a French actor and Kay an American woman.
03/18/2022 21:51:59 - INFO - __main__ - ['Film']
03/18/2022 21:51:59 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 21:51:59 - INFO - __main__ - ['Film']
03/18/2022 21:51:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 21:51:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 21:51:59 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 21:52:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 21:52:10 - INFO - __main__ - Starting training!
03/18/2022 21:52:14 - INFO - __main__ - Step 10 Global step 10 Train loss 21.767750 on epoch=0
03/18/2022 21:52:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.487116 on epoch=1
03/18/2022 21:52:24 - INFO - __main__ - Step 30 Global step 30 Train loss 17.865549 on epoch=2
03/18/2022 21:52:29 - INFO - __main__ - Step 40 Global step 40 Train loss 15.887495 on epoch=2
03/18/2022 21:52:34 - INFO - __main__ - Step 50 Global step 50 Train loss 13.830965 on epoch=3
03/18/2022 21:53:02 - INFO - __main__ - Global step 50 Train loss 17.767775 Classification-F1 0.001142204454597373 on epoch=3
03/18/2022 21:53:08 - INFO - __main__ - Step 60 Global step 60 Train loss 13.031897 on epoch=4
03/18/2022 21:53:13 - INFO - __main__ - Step 70 Global step 70 Train loss 12.744516 on epoch=4
03/18/2022 21:53:18 - INFO - __main__ - Step 80 Global step 80 Train loss 11.217990 on epoch=5
03/18/2022 21:53:23 - INFO - __main__ - Step 90 Global step 90 Train loss 11.017534 on epoch=6
03/18/2022 21:53:28 - INFO - __main__ - Step 100 Global step 100 Train loss 11.332471 on epoch=7
03/18/2022 21:53:36 - INFO - __main__ - Global step 100 Train loss 11.868882 Classification-F1 0.0 on epoch=7
03/18/2022 21:53:41 - INFO - __main__ - Step 110 Global step 110 Train loss 11.273524 on epoch=7
03/18/2022 21:53:46 - INFO - __main__ - Step 120 Global step 120 Train loss 10.211219 on epoch=8
03/18/2022 21:53:51 - INFO - __main__ - Step 130 Global step 130 Train loss 10.857519 on epoch=9
03/18/2022 21:53:56 - INFO - __main__ - Step 140 Global step 140 Train loss 11.265519 on epoch=9
03/18/2022 21:54:01 - INFO - __main__ - Step 150 Global step 150 Train loss 9.719211 on epoch=10
03/18/2022 21:54:07 - INFO - __main__ - Global step 150 Train loss 10.665398 Classification-F1 0.0 on epoch=10
03/18/2022 21:54:12 - INFO - __main__ - Step 160 Global step 160 Train loss 10.124092 on epoch=11
03/18/2022 21:54:17 - INFO - __main__ - Step 170 Global step 170 Train loss 9.846386 on epoch=12
03/18/2022 21:54:22 - INFO - __main__ - Step 180 Global step 180 Train loss 10.010607 on epoch=12
03/18/2022 21:54:27 - INFO - __main__ - Step 190 Global step 190 Train loss 9.146703 on epoch=13
03/18/2022 21:54:32 - INFO - __main__ - Step 200 Global step 200 Train loss 9.444730 on epoch=14
03/18/2022 21:54:37 - INFO - __main__ - Global step 200 Train loss 9.714504 Classification-F1 0.0 on epoch=14
03/18/2022 21:54:42 - INFO - __main__ - Step 210 Global step 210 Train loss 9.338226 on epoch=14
03/18/2022 21:54:47 - INFO - __main__ - Step 220 Global step 220 Train loss 8.206298 on epoch=15
03/18/2022 21:54:52 - INFO - __main__ - Step 230 Global step 230 Train loss 9.060012 on epoch=16
03/18/2022 21:54:57 - INFO - __main__ - Step 240 Global step 240 Train loss 8.327858 on epoch=17
03/18/2022 21:55:02 - INFO - __main__ - Step 250 Global step 250 Train loss 8.934423 on epoch=17
03/18/2022 21:55:07 - INFO - __main__ - Global step 250 Train loss 8.773363 Classification-F1 0.0 on epoch=17
03/18/2022 21:55:12 - INFO - __main__ - Step 260 Global step 260 Train loss 7.836595 on epoch=18
03/18/2022 21:55:17 - INFO - __main__ - Step 270 Global step 270 Train loss 8.736198 on epoch=19
03/18/2022 21:55:22 - INFO - __main__ - Step 280 Global step 280 Train loss 8.662572 on epoch=19
03/18/2022 21:55:27 - INFO - __main__ - Step 290 Global step 290 Train loss 7.388814 on epoch=20
03/18/2022 21:55:32 - INFO - __main__ - Step 300 Global step 300 Train loss 8.149626 on epoch=21
03/18/2022 21:55:36 - INFO - __main__ - Global step 300 Train loss 8.154760 Classification-F1 0.0 on epoch=21
03/18/2022 21:55:41 - INFO - __main__ - Step 310 Global step 310 Train loss 7.254391 on epoch=22
03/18/2022 21:55:47 - INFO - __main__ - Step 320 Global step 320 Train loss 7.240891 on epoch=22
03/18/2022 21:55:52 - INFO - __main__ - Step 330 Global step 330 Train loss 6.870502 on epoch=23
03/18/2022 21:55:57 - INFO - __main__ - Step 340 Global step 340 Train loss 6.848145 on epoch=24
03/18/2022 21:56:02 - INFO - __main__ - Step 350 Global step 350 Train loss 6.691089 on epoch=24
03/18/2022 21:56:06 - INFO - __main__ - Global step 350 Train loss 6.981003 Classification-F1 0.0 on epoch=24
03/18/2022 21:56:11 - INFO - __main__ - Step 360 Global step 360 Train loss 5.887792 on epoch=25
03/18/2022 21:56:16 - INFO - __main__ - Step 370 Global step 370 Train loss 5.936843 on epoch=26
03/18/2022 21:56:21 - INFO - __main__ - Step 380 Global step 380 Train loss 5.416312 on epoch=27
03/18/2022 21:56:27 - INFO - __main__ - Step 390 Global step 390 Train loss 5.241303 on epoch=27
03/18/2022 21:56:32 - INFO - __main__ - Step 400 Global step 400 Train loss 4.227304 on epoch=28
03/18/2022 21:56:36 - INFO - __main__ - Global step 400 Train loss 5.341911 Classification-F1 0.0 on epoch=28
03/18/2022 21:56:41 - INFO - __main__ - Step 410 Global step 410 Train loss 4.437906 on epoch=29
03/18/2022 21:56:47 - INFO - __main__ - Step 420 Global step 420 Train loss 4.395803 on epoch=29
03/18/2022 21:56:52 - INFO - __main__ - Step 430 Global step 430 Train loss 3.537862 on epoch=30
03/18/2022 21:56:57 - INFO - __main__ - Step 440 Global step 440 Train loss 4.215529 on epoch=31
03/18/2022 21:57:02 - INFO - __main__ - Step 450 Global step 450 Train loss 3.549274 on epoch=32
03/18/2022 21:57:06 - INFO - __main__ - Global step 450 Train loss 4.027276 Classification-F1 0.08268386785628165 on epoch=32
03/18/2022 21:57:12 - INFO - __main__ - Step 460 Global step 460 Train loss 3.688863 on epoch=32
03/18/2022 21:57:17 - INFO - __main__ - Step 470 Global step 470 Train loss 3.053701 on epoch=33
03/18/2022 21:57:22 - INFO - __main__ - Step 480 Global step 480 Train loss 3.430093 on epoch=34
03/18/2022 21:57:27 - INFO - __main__ - Step 490 Global step 490 Train loss 3.091962 on epoch=34
03/18/2022 21:57:33 - INFO - __main__ - Step 500 Global step 500 Train loss 3.093615 on epoch=35
03/18/2022 21:57:36 - INFO - __main__ - Global step 500 Train loss 3.271647 Classification-F1 0.15079806319979044 on epoch=35
03/18/2022 21:57:42 - INFO - __main__ - Step 510 Global step 510 Train loss 3.406515 on epoch=36
03/18/2022 21:57:47 - INFO - __main__ - Step 520 Global step 520 Train loss 2.943475 on epoch=37
03/18/2022 21:57:52 - INFO - __main__ - Step 530 Global step 530 Train loss 3.111297 on epoch=37
03/18/2022 21:57:58 - INFO - __main__ - Step 540 Global step 540 Train loss 3.113623 on epoch=38
03/18/2022 21:58:03 - INFO - __main__ - Step 550 Global step 550 Train loss 2.454062 on epoch=39
03/18/2022 21:58:06 - INFO - __main__ - Global step 550 Train loss 3.005794 Classification-F1 0.38165243311866476 on epoch=39
03/18/2022 21:58:12 - INFO - __main__ - Step 560 Global step 560 Train loss 3.091618 on epoch=39
03/18/2022 21:58:17 - INFO - __main__ - Step 570 Global step 570 Train loss 2.650379 on epoch=40
03/18/2022 21:58:22 - INFO - __main__ - Step 580 Global step 580 Train loss 2.560245 on epoch=41
03/18/2022 21:58:28 - INFO - __main__ - Step 590 Global step 590 Train loss 2.909565 on epoch=42
03/18/2022 21:58:33 - INFO - __main__ - Step 600 Global step 600 Train loss 2.851430 on epoch=42
03/18/2022 21:58:36 - INFO - __main__ - Global step 600 Train loss 2.812647 Classification-F1 0.479556790975617 on epoch=42
03/18/2022 21:58:42 - INFO - __main__ - Step 610 Global step 610 Train loss 2.629140 on epoch=43
03/18/2022 21:58:47 - INFO - __main__ - Step 620 Global step 620 Train loss 2.745697 on epoch=44
03/18/2022 21:58:52 - INFO - __main__ - Step 630 Global step 630 Train loss 2.982967 on epoch=44
03/18/2022 21:58:58 - INFO - __main__ - Step 640 Global step 640 Train loss 2.534688 on epoch=45
03/18/2022 21:59:03 - INFO - __main__ - Step 650 Global step 650 Train loss 2.869385 on epoch=46
03/18/2022 21:59:06 - INFO - __main__ - Global step 650 Train loss 2.752376 Classification-F1 0.5489616308795761 on epoch=46
03/18/2022 21:59:12 - INFO - __main__ - Step 660 Global step 660 Train loss 2.395803 on epoch=47
03/18/2022 21:59:17 - INFO - __main__ - Step 670 Global step 670 Train loss 2.164336 on epoch=47
03/18/2022 21:59:22 - INFO - __main__ - Step 680 Global step 680 Train loss 2.254187 on epoch=48
03/18/2022 21:59:28 - INFO - __main__ - Step 690 Global step 690 Train loss 2.390533 on epoch=49
03/18/2022 21:59:33 - INFO - __main__ - Step 700 Global step 700 Train loss 2.338016 on epoch=49
03/18/2022 21:59:36 - INFO - __main__ - Global step 700 Train loss 2.308575 Classification-F1 0.4142778898569148 on epoch=49
03/18/2022 21:59:41 - INFO - __main__ - Step 710 Global step 710 Train loss 2.357323 on epoch=50
03/18/2022 21:59:46 - INFO - __main__ - Step 720 Global step 720 Train loss 2.202770 on epoch=51
03/18/2022 21:59:51 - INFO - __main__ - Step 730 Global step 730 Train loss 1.894796 on epoch=52
03/18/2022 21:59:57 - INFO - __main__ - Step 740 Global step 740 Train loss 2.183473 on epoch=52
03/18/2022 22:00:02 - INFO - __main__ - Step 750 Global step 750 Train loss 1.880505 on epoch=53
03/18/2022 22:00:05 - INFO - __main__ - Global step 750 Train loss 2.103773 Classification-F1 0.531134576263306 on epoch=53
03/18/2022 22:00:10 - INFO - __main__ - Step 760 Global step 760 Train loss 1.682531 on epoch=54
03/18/2022 22:00:15 - INFO - __main__ - Step 770 Global step 770 Train loss 1.802794 on epoch=54
03/18/2022 22:00:21 - INFO - __main__ - Step 780 Global step 780 Train loss 1.656519 on epoch=55
03/18/2022 22:00:26 - INFO - __main__ - Step 790 Global step 790 Train loss 1.404971 on epoch=56
03/18/2022 22:00:31 - INFO - __main__ - Step 800 Global step 800 Train loss 1.381915 on epoch=57
03/18/2022 22:00:34 - INFO - __main__ - Global step 800 Train loss 1.585746 Classification-F1 0.5554144717036332 on epoch=57
03/18/2022 22:00:40 - INFO - __main__ - Step 810 Global step 810 Train loss 1.526516 on epoch=57
03/18/2022 22:00:46 - INFO - __main__ - Step 820 Global step 820 Train loss 1.679620 on epoch=58
03/18/2022 22:00:51 - INFO - __main__ - Step 830 Global step 830 Train loss 1.624551 on epoch=59
03/18/2022 22:00:56 - INFO - __main__ - Step 840 Global step 840 Train loss 1.388050 on epoch=59
03/18/2022 22:01:01 - INFO - __main__ - Step 850 Global step 850 Train loss 1.566142 on epoch=60
03/18/2022 22:01:05 - INFO - __main__ - Global step 850 Train loss 1.556976 Classification-F1 0.5690804570804571 on epoch=60
03/18/2022 22:01:11 - INFO - __main__ - Step 860 Global step 860 Train loss 1.208529 on epoch=61
03/18/2022 22:01:16 - INFO - __main__ - Step 870 Global step 870 Train loss 1.078500 on epoch=62
03/18/2022 22:01:21 - INFO - __main__ - Step 880 Global step 880 Train loss 1.467234 on epoch=62
03/18/2022 22:01:26 - INFO - __main__ - Step 890 Global step 890 Train loss 1.635583 on epoch=63
03/18/2022 22:01:32 - INFO - __main__ - Step 900 Global step 900 Train loss 1.252706 on epoch=64
03/18/2022 22:01:35 - INFO - __main__ - Global step 900 Train loss 1.328510 Classification-F1 0.5086093611573769 on epoch=64
03/18/2022 22:01:40 - INFO - __main__ - Step 910 Global step 910 Train loss 1.046819 on epoch=64
03/18/2022 22:01:46 - INFO - __main__ - Step 920 Global step 920 Train loss 1.531189 on epoch=65
03/18/2022 22:01:51 - INFO - __main__ - Step 930 Global step 930 Train loss 1.151543 on epoch=66
03/18/2022 22:01:56 - INFO - __main__ - Step 940 Global step 940 Train loss 1.212374 on epoch=67
03/18/2022 22:02:01 - INFO - __main__ - Step 950 Global step 950 Train loss 1.581964 on epoch=67
03/18/2022 22:02:05 - INFO - __main__ - Global step 950 Train loss 1.304778 Classification-F1 0.5916736007241713 on epoch=67
03/18/2022 22:02:11 - INFO - __main__ - Step 960 Global step 960 Train loss 1.139297 on epoch=68
03/18/2022 22:02:16 - INFO - __main__ - Step 970 Global step 970 Train loss 1.129933 on epoch=69
03/18/2022 22:02:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.925187 on epoch=69
03/18/2022 22:02:27 - INFO - __main__ - Step 990 Global step 990 Train loss 1.106771 on epoch=70
03/18/2022 22:02:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.893214 on epoch=71
03/18/2022 22:02:35 - INFO - __main__ - Global step 1000 Train loss 1.038880 Classification-F1 0.5811617576323459 on epoch=71
03/18/2022 22:02:35 - INFO - __main__ - save last model!
03/18/2022 22:02:42 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:02:43 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 22:02:43 - INFO - __main__ - Printing 3 examples
03/18/2022 22:02:43 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)
03/18/2022 22:02:43 - INFO - __main__ - ['Animal']
03/18/2022 22:02:43 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 22:02:43 - INFO - __main__ - ['Animal']
03/18/2022 22:02:43 - INFO - __main__ -  [dbpedia_14] Strzeczonka [sttnka] is a village in the administrative district of Gmina Debrzno within Czuchw County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Czuchw and 130 km (81 mi) south-west of the regional capital Gdask.For details of the history of the region see History of Pomerania.
03/18/2022 22:02:43 - INFO - __main__ - ['Village']
03/18/2022 22:02:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:02:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:02:48 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 22:03:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-dbpedia_14/dbpedia_14_16_87_0.0001_8_predictions.txt
03/18/2022 22:03:44 - INFO - __main__ - Classification-F1 on test data: 0.4701
03/18/2022 22:03:44 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.0001, bsz=8, dev_performance=0.5916736007241713, test_performance=0.470053022991446
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22093): No such process
Task: wiki_qa, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/18/2022 22:03:49 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 22:03:49 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-wiki_qa
Output directory () already exists and is not empty.
03/18/2022 22:03:49 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/18/2022 22:03:49 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-wiki_qa
03/18/2022 22:03:52 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 22:03:52 - INFO - __main__ - args.device: cuda:0
03/18/2022 22:03:52 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 22:03:52 - INFO - __main__ - Using 2 gpus
03/18/2022 22:03:52 - INFO - __main__ - args.device: cuda:1
03/18/2022 22:03:52 - INFO - __main__ - Using 2 gpus
03/18/2022 22:03:52 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/18/2022 22:03:52 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/18/2022 22:03:57 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.0005, bsz=8 ...
03/18/2022 22:03:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:03:58 - INFO - __main__ - Printing 3 examples
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:03:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:03:58 - INFO - __main__ - Printing 3 examples
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:03:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:03:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:03:58 - INFO - __main__ - Printing 3 examples
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:03:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:03:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:03:58 - INFO - __main__ - Printing 3 examples
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:03:58 - INFO - __main__ - ['false']
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:03:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:03:58 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:03:58 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:04:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:04:11 - INFO - __main__ - Starting training!
03/18/2022 22:04:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:04:11 - INFO - __main__ - Starting training!
03/18/2022 22:04:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.057171 on epoch=4
03/18/2022 22:04:21 - INFO - __main__ - Step 20 Global step 20 Train loss 17.881603 on epoch=9
03/18/2022 22:04:25 - INFO - __main__ - Step 30 Global step 30 Train loss 15.314314 on epoch=14
03/18/2022 22:04:30 - INFO - __main__ - Step 40 Global step 40 Train loss 13.255699 on epoch=19
03/18/2022 22:04:35 - INFO - __main__ - Step 50 Global step 50 Train loss 11.113268 on epoch=24
03/18/2022 22:04:36 - INFO - __main__ - Global step 50 Train loss 16.124413 Classification-F1 0.05555555555555555 on epoch=24
03/18/2022 22:04:42 - INFO - __main__ - Step 60 Global step 60 Train loss 4.030759 on epoch=29
03/18/2022 22:04:47 - INFO - __main__ - Step 70 Global step 70 Train loss 1.980856 on epoch=34
03/18/2022 22:04:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.600710 on epoch=39
03/18/2022 22:04:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.396363 on epoch=44
03/18/2022 22:05:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.348172 on epoch=49
03/18/2022 22:05:02 - INFO - __main__ - Global step 100 Train loss 1.471372 Classification-F1 0.3992490613266583 on epoch=49
03/18/2022 22:05:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.311209 on epoch=54
03/18/2022 22:05:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.153726 on epoch=59
03/18/2022 22:05:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.336732 on epoch=64
03/18/2022 22:05:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.066028 on epoch=69
03/18/2022 22:05:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.009226 on epoch=74
03/18/2022 22:05:29 - INFO - __main__ - Global step 150 Train loss 0.175384 Classification-F1 0.5901477832512315 on epoch=74
03/18/2022 22:05:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.011263 on epoch=79
03/18/2022 22:05:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.002488 on epoch=84
03/18/2022 22:05:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.002249 on epoch=89
03/18/2022 22:05:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.000969 on epoch=94
03/18/2022 22:05:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001619 on epoch=99
03/18/2022 22:05:55 - INFO - __main__ - Global step 200 Train loss 0.003717 Classification-F1 0.6190476190476191 on epoch=99
03/18/2022 22:06:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001286 on epoch=104
03/18/2022 22:06:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002219 on epoch=109
03/18/2022 22:06:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.110666 on epoch=114
03/18/2022 22:06:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001564 on epoch=119
03/18/2022 22:06:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001249 on epoch=124
03/18/2022 22:06:21 - INFO - __main__ - Global step 250 Train loss 0.023397 Classification-F1 0.5270935960591133 on epoch=124
03/18/2022 22:06:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001473 on epoch=129
03/18/2022 22:06:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.018091 on epoch=134
03/18/2022 22:06:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001238 on epoch=139
03/18/2022 22:06:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000336 on epoch=144
03/18/2022 22:06:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000250 on epoch=149
03/18/2022 22:06:45 - INFO - __main__ - Global step 300 Train loss 0.004278 Classification-F1 0.5901477832512315 on epoch=149
03/18/2022 22:06:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000272 on epoch=154
03/18/2022 22:06:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000590 on epoch=159
03/18/2022 22:07:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000190 on epoch=164
03/18/2022 22:07:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000417 on epoch=169
03/18/2022 22:07:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000163 on epoch=174
03/18/2022 22:07:09 - INFO - __main__ - Global step 350 Train loss 0.000326 Classification-F1 0.5835835835835835 on epoch=174
03/18/2022 22:07:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000064 on epoch=179
03/18/2022 22:07:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000153 on epoch=184
03/18/2022 22:07:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000047 on epoch=189
03/18/2022 22:07:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000151 on epoch=194
03/18/2022 22:07:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000308 on epoch=199
03/18/2022 22:07:34 - INFO - __main__ - Global step 400 Train loss 0.000145 Classification-F1 0.4980392156862745 on epoch=199
03/18/2022 22:07:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000102 on epoch=204
03/18/2022 22:07:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000108 on epoch=209
03/18/2022 22:07:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000093 on epoch=214
03/18/2022 22:07:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000044 on epoch=219
03/18/2022 22:07:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000046 on epoch=224
03/18/2022 22:07:59 - INFO - __main__ - Global step 450 Train loss 0.000079 Classification-F1 0.5 on epoch=224
03/18/2022 22:08:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000042 on epoch=229
03/18/2022 22:08:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000087 on epoch=234
03/18/2022 22:08:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000026 on epoch=239
03/18/2022 22:08:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000046 on epoch=244
03/18/2022 22:08:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000054 on epoch=249
03/18/2022 22:08:23 - INFO - __main__ - Global step 500 Train loss 0.000051 Classification-F1 0.5555555555555556 on epoch=249
03/18/2022 22:08:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000010 on epoch=254
03/18/2022 22:08:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000220 on epoch=259
03/18/2022 22:08:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000046 on epoch=264
03/18/2022 22:08:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000040 on epoch=269
03/18/2022 22:08:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000303 on epoch=274
03/18/2022 22:08:48 - INFO - __main__ - Global step 550 Train loss 0.000124 Classification-F1 0.5307917888563051 on epoch=274
03/18/2022 22:08:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000113 on epoch=279
03/18/2022 22:08:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000010 on epoch=284
03/18/2022 22:09:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000069 on epoch=289
03/18/2022 22:09:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000021 on epoch=294
03/18/2022 22:09:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000005 on epoch=299
03/18/2022 22:09:12 - INFO - __main__ - Global step 600 Train loss 0.000044 Classification-F1 0.5555555555555556 on epoch=299
03/18/2022 22:09:12 - INFO - __main__ - save last model!
03/18/2022 22:09:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:09:13 - INFO - __main__ - Printing 3 examples
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:09:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:09:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:09:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:09:13 - INFO - __main__ - Printing 3 examples
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:09:13 - INFO - __main__ - ['false']
03/18/2022 22:09:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:09:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:09:13 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:09:19 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:09:20 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:09:20 - INFO - __main__ - Printing 3 examples
03/18/2022 22:09:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:09:20 - INFO - __main__ - ['false']
03/18/2022 22:09:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:09:20 - INFO - __main__ - ['false']
03/18/2022 22:09:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:09:20 - INFO - __main__ - ['false']
03/18/2022 22:09:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:09:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:09:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:09:24 - INFO - __main__ - Starting training!
03/18/2022 22:09:24 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:09:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_100_0.0005_8_predictions.txt
03/18/2022 22:09:53 - INFO - __main__ - Classification-F1 on test data: 0.3752
03/18/2022 22:09:53 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.0005, bsz=8, dev_performance=0.6190476190476191, test_performance=0.37515298547404047
03/18/2022 22:09:53 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.0003, bsz=8 ...
03/18/2022 22:09:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:09:54 - INFO - __main__ - Printing 3 examples
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:09:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:09:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:09:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:09:54 - INFO - __main__ - Printing 3 examples
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:09:54 - INFO - __main__ - ['false']
03/18/2022 22:09:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:09:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:09:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:10:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:10:05 - INFO - __main__ - Starting training!
03/18/2022 22:10:09 - INFO - __main__ - Step 10 Global step 10 Train loss 22.095274 on epoch=4
03/18/2022 22:10:14 - INFO - __main__ - Step 20 Global step 20 Train loss 17.015585 on epoch=9
03/18/2022 22:10:19 - INFO - __main__ - Step 30 Global step 30 Train loss 16.635725 on epoch=14
03/18/2022 22:10:23 - INFO - __main__ - Step 40 Global step 40 Train loss 15.508509 on epoch=19
03/18/2022 22:10:28 - INFO - __main__ - Step 50 Global step 50 Train loss 14.189145 on epoch=24
03/18/2022 22:10:29 - INFO - __main__ - Global step 50 Train loss 17.088848 Classification-F1 0.0 on epoch=24
03/18/2022 22:10:34 - INFO - __main__ - Step 60 Global step 60 Train loss 12.673387 on epoch=29
03/18/2022 22:10:39 - INFO - __main__ - Step 70 Global step 70 Train loss 7.942295 on epoch=34
03/18/2022 22:10:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.901204 on epoch=39
03/18/2022 22:10:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.606186 on epoch=44
03/18/2022 22:10:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.384967 on epoch=49
03/18/2022 22:10:54 - INFO - __main__ - Global step 100 Train loss 4.701608 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 22:10:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.394468 on epoch=54
03/18/2022 22:11:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.474480 on epoch=59
03/18/2022 22:11:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.403292 on epoch=64
03/18/2022 22:11:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.348839 on epoch=69
03/18/2022 22:11:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.517383 on epoch=74
03/18/2022 22:11:19 - INFO - __main__ - Global step 150 Train loss 0.427692 Classification-F1 0.34401709401709396 on epoch=74
03/18/2022 22:11:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.431590 on epoch=79
03/18/2022 22:11:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.246876 on epoch=84
03/18/2022 22:11:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.279760 on epoch=89
03/18/2022 22:11:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.226277 on epoch=94
03/18/2022 22:11:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.232053 on epoch=99
03/18/2022 22:11:46 - INFO - __main__ - Global step 200 Train loss 0.283311 Classification-F1 0.4682306940371457 on epoch=99
03/18/2022 22:11:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.103230 on epoch=104
03/18/2022 22:11:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.084155 on epoch=109
03/18/2022 22:12:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.181523 on epoch=114
03/18/2022 22:12:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.101078 on epoch=119
03/18/2022 22:12:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.030596 on epoch=124
03/18/2022 22:12:13 - INFO - __main__ - Global step 250 Train loss 0.100116 Classification-F1 0.4420512820512821 on epoch=124
03/18/2022 22:12:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003793 on epoch=129
03/18/2022 22:12:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007656 on epoch=134
03/18/2022 22:12:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.019163 on epoch=139
03/18/2022 22:12:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002855 on epoch=144
03/18/2022 22:12:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.013261 on epoch=149
03/18/2022 22:12:39 - INFO - __main__ - Global step 300 Train loss 0.009346 Classification-F1 0.4920634920634921 on epoch=149
03/18/2022 22:12:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001762 on epoch=154
03/18/2022 22:12:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000637 on epoch=159
03/18/2022 22:12:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001425 on epoch=164
03/18/2022 22:13:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001116 on epoch=169
03/18/2022 22:13:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000433 on epoch=174
03/18/2022 22:13:06 - INFO - __main__ - Global step 350 Train loss 0.001075 Classification-F1 0.41700404858299595 on epoch=174
03/18/2022 22:13:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.105975 on epoch=179
03/18/2022 22:13:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001349 on epoch=184
03/18/2022 22:13:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.056333 on epoch=189
03/18/2022 22:13:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000487 on epoch=194
03/18/2022 22:13:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003823 on epoch=199
03/18/2022 22:13:32 - INFO - __main__ - Global step 400 Train loss 0.033593 Classification-F1 0.5625 on epoch=199
03/18/2022 22:13:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002519 on epoch=204
03/18/2022 22:13:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.020730 on epoch=209
03/18/2022 22:13:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.019822 on epoch=214
03/18/2022 22:13:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003624 on epoch=219
03/18/2022 22:13:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000801 on epoch=224
03/18/2022 22:13:58 - INFO - __main__ - Global step 450 Train loss 0.009499 Classification-F1 0.4009852216748768 on epoch=224
03/18/2022 22:14:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000426 on epoch=229
03/18/2022 22:14:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000262 on epoch=234
03/18/2022 22:14:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000266 on epoch=239
03/18/2022 22:14:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000135 on epoch=244
03/18/2022 22:14:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000301 on epoch=249
03/18/2022 22:14:24 - INFO - __main__ - Global step 500 Train loss 0.000278 Classification-F1 0.4009852216748768 on epoch=249
03/18/2022 22:14:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000426 on epoch=254
03/18/2022 22:14:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000250 on epoch=259
03/18/2022 22:14:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000153 on epoch=264
03/18/2022 22:14:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000117 on epoch=269
03/18/2022 22:14:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000063 on epoch=274
03/18/2022 22:14:50 - INFO - __main__ - Global step 550 Train loss 0.000202 Classification-F1 0.4817813765182186 on epoch=274
03/18/2022 22:14:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000061 on epoch=279
03/18/2022 22:15:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000049 on epoch=284
03/18/2022 22:15:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000028 on epoch=289
03/18/2022 22:15:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000092 on epoch=294
03/18/2022 22:15:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000038 on epoch=299
03/18/2022 22:15:16 - INFO - __main__ - Global step 600 Train loss 0.000054 Classification-F1 0.4285714285714286 on epoch=299
03/18/2022 22:15:16 - INFO - __main__ - save last model!
03/18/2022 22:15:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:15:18 - INFO - __main__ - Printing 3 examples
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:15:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:15:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:15:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:15:18 - INFO - __main__ - Printing 3 examples
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:15:18 - INFO - __main__ - ['false']
03/18/2022 22:15:18 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:15:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:15:18 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:15:23 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:15:23 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:15:23 - INFO - __main__ - Printing 3 examples
03/18/2022 22:15:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:15:23 - INFO - __main__ - ['false']
03/18/2022 22:15:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:15:23 - INFO - __main__ - ['false']
03/18/2022 22:15:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:15:23 - INFO - __main__ - ['false']
03/18/2022 22:15:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:15:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:15:27 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:15:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:15:31 - INFO - __main__ - Starting training!
03/18/2022 22:15:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_100_0.0003_8_predictions.txt
03/18/2022 22:15:56 - INFO - __main__ - Classification-F1 on test data: 0.1484
03/18/2022 22:15:56 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.0003, bsz=8, dev_performance=0.5625, test_performance=0.1484470301600275
03/18/2022 22:15:56 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.0002, bsz=8 ...
03/18/2022 22:15:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:15:57 - INFO - __main__ - Printing 3 examples
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:15:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:15:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:15:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:15:57 - INFO - __main__ - Printing 3 examples
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:15:57 - INFO - __main__ - ['false']
03/18/2022 22:15:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:15:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:15:57 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:16:10 - INFO - __main__ - Starting training!
03/18/2022 22:16:15 - INFO - __main__ - Step 10 Global step 10 Train loss 22.503849 on epoch=4
03/18/2022 22:16:20 - INFO - __main__ - Step 20 Global step 20 Train loss 18.257523 on epoch=9
03/18/2022 22:16:25 - INFO - __main__ - Step 30 Global step 30 Train loss 18.096279 on epoch=14
03/18/2022 22:16:30 - INFO - __main__ - Step 40 Global step 40 Train loss 15.902084 on epoch=19
03/18/2022 22:16:35 - INFO - __main__ - Step 50 Global step 50 Train loss 15.182444 on epoch=24
03/18/2022 22:16:38 - INFO - __main__ - Global step 50 Train loss 17.988436 Classification-F1 0.0 on epoch=24
03/18/2022 22:16:44 - INFO - __main__ - Step 60 Global step 60 Train loss 14.374710 on epoch=29
03/18/2022 22:16:49 - INFO - __main__ - Step 70 Global step 70 Train loss 14.073621 on epoch=34
03/18/2022 22:16:54 - INFO - __main__ - Step 80 Global step 80 Train loss 12.929541 on epoch=39
03/18/2022 22:16:59 - INFO - __main__ - Step 90 Global step 90 Train loss 12.077821 on epoch=44
03/18/2022 22:17:04 - INFO - __main__ - Step 100 Global step 100 Train loss 10.285601 on epoch=49
03/18/2022 22:17:04 - INFO - __main__ - Global step 100 Train loss 12.748260 Classification-F1 0.0 on epoch=49
03/18/2022 22:17:09 - INFO - __main__ - Step 110 Global step 110 Train loss 4.043729 on epoch=54
03/18/2022 22:17:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.570600 on epoch=59
03/18/2022 22:17:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.412494 on epoch=64
03/18/2022 22:17:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.291893 on epoch=69
03/18/2022 22:17:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.236346 on epoch=74
03/18/2022 22:17:30 - INFO - __main__ - Global step 150 Train loss 1.111012 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 22:17:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.165627 on epoch=79
03/18/2022 22:17:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.069089 on epoch=84
03/18/2022 22:17:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.035901 on epoch=89
03/18/2022 22:17:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.026610 on epoch=94
03/18/2022 22:17:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.022411 on epoch=99
03/18/2022 22:17:56 - INFO - __main__ - Global step 200 Train loss 0.063928 Classification-F1 0.4385964912280702 on epoch=99
03/18/2022 22:18:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.025894 on epoch=104
03/18/2022 22:18:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005426 on epoch=109
03/18/2022 22:18:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.007536 on epoch=114
03/18/2022 22:18:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.002936 on epoch=119
03/18/2022 22:18:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.010580 on epoch=124
03/18/2022 22:18:23 - INFO - __main__ - Global step 250 Train loss 0.010474 Classification-F1 0.36374269005847953 on epoch=124
03/18/2022 22:18:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.002869 on epoch=129
03/18/2022 22:18:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002964 on epoch=134
03/18/2022 22:18:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001702 on epoch=139
03/18/2022 22:18:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002083 on epoch=144
03/18/2022 22:18:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001608 on epoch=149
03/18/2022 22:18:48 - INFO - __main__ - Global step 300 Train loss 0.002245 Classification-F1 0.4385964912280702 on epoch=149
03/18/2022 22:18:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002118 on epoch=154
03/18/2022 22:18:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001268 on epoch=159
03/18/2022 22:19:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001484 on epoch=164
03/18/2022 22:19:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002907 on epoch=169
03/18/2022 22:19:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.003265 on epoch=174
03/18/2022 22:19:14 - INFO - __main__ - Global step 350 Train loss 0.002209 Classification-F1 0.6113360323886641 on epoch=174
03/18/2022 22:19:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000343 on epoch=179
03/18/2022 22:19:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000841 on epoch=184
03/18/2022 22:19:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000233 on epoch=189
03/18/2022 22:19:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000298 on epoch=194
03/18/2022 22:19:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001289 on epoch=199
03/18/2022 22:19:40 - INFO - __main__ - Global step 400 Train loss 0.000601 Classification-F1 0.539313399778516 on epoch=199
03/18/2022 22:19:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001591 on epoch=204
03/18/2022 22:19:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000226 on epoch=209
03/18/2022 22:19:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001236 on epoch=214
03/18/2022 22:20:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000101 on epoch=219
03/18/2022 22:20:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000276 on epoch=224
03/18/2022 22:20:06 - INFO - __main__ - Global step 450 Train loss 0.000686 Classification-F1 0.539313399778516 on epoch=224
03/18/2022 22:20:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000323 on epoch=229
03/18/2022 22:20:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000095 on epoch=234
03/18/2022 22:20:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.031630 on epoch=239
03/18/2022 22:20:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000657 on epoch=244
03/18/2022 22:20:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000037 on epoch=249
03/18/2022 22:20:31 - INFO - __main__ - Global step 500 Train loss 0.006548 Classification-F1 0.539313399778516 on epoch=249
03/18/2022 22:20:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000119 on epoch=254
03/18/2022 22:20:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000905 on epoch=259
03/18/2022 22:20:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000326 on epoch=264
03/18/2022 22:20:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000057 on epoch=269
03/18/2022 22:20:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000099 on epoch=274
03/18/2022 22:20:57 - INFO - __main__ - Global step 550 Train loss 0.000301 Classification-F1 0.5844155844155844 on epoch=274
03/18/2022 22:21:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000029 on epoch=279
03/18/2022 22:21:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.025188 on epoch=284
03/18/2022 22:21:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011895 on epoch=289
03/18/2022 22:21:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001479 on epoch=294
03/18/2022 22:21:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000066 on epoch=299
03/18/2022 22:21:22 - INFO - __main__ - Global step 600 Train loss 0.007731 Classification-F1 0.5333333333333333 on epoch=299
03/18/2022 22:21:22 - INFO - __main__ - save last model!
03/18/2022 22:21:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:21:23 - INFO - __main__ - Printing 3 examples
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:21:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:21:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:21:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:21:23 - INFO - __main__ - Printing 3 examples
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:21:23 - INFO - __main__ - ['false']
03/18/2022 22:21:23 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:21:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:21:23 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:21:29 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:21:30 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:21:30 - INFO - __main__ - Printing 3 examples
03/18/2022 22:21:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:21:30 - INFO - __main__ - ['false']
03/18/2022 22:21:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:21:30 - INFO - __main__ - ['false']
03/18/2022 22:21:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:21:30 - INFO - __main__ - ['false']
03/18/2022 22:21:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:21:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:21:34 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:21:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:21:36 - INFO - __main__ - Starting training!
03/18/2022 22:22:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_100_0.0002_8_predictions.txt
03/18/2022 22:22:02 - INFO - __main__ - Classification-F1 on test data: 0.3724
03/18/2022 22:22:03 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.0002, bsz=8, dev_performance=0.6113360323886641, test_performance=0.37244930762000694
03/18/2022 22:22:03 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.0001, bsz=8 ...
03/18/2022 22:22:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:22:03 - INFO - __main__ - Printing 3 examples
03/18/2022 22:22:03 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 22:22:03 - INFO - __main__ - ['false']
03/18/2022 22:22:03 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 22:22:03 - INFO - __main__ - ['false']
03/18/2022 22:22:03 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 22:22:03 - INFO - __main__ - ['false']
03/18/2022 22:22:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:22:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:22:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:22:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:22:04 - INFO - __main__ - Printing 3 examples
03/18/2022 22:22:04 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 22:22:04 - INFO - __main__ - ['false']
03/18/2022 22:22:04 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 22:22:04 - INFO - __main__ - ['false']
03/18/2022 22:22:04 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 22:22:04 - INFO - __main__ - ['false']
03/18/2022 22:22:04 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:22:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:22:04 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:22:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:22:14 - INFO - __main__ - Starting training!
03/18/2022 22:22:19 - INFO - __main__ - Step 10 Global step 10 Train loss 21.781040 on epoch=4
03/18/2022 22:22:24 - INFO - __main__ - Step 20 Global step 20 Train loss 20.178492 on epoch=9
03/18/2022 22:22:29 - INFO - __main__ - Step 30 Global step 30 Train loss 19.019901 on epoch=14
03/18/2022 22:22:34 - INFO - __main__ - Step 40 Global step 40 Train loss 17.875937 on epoch=19
03/18/2022 22:22:39 - INFO - __main__ - Step 50 Global step 50 Train loss 17.225609 on epoch=24
03/18/2022 22:22:42 - INFO - __main__ - Global step 50 Train loss 19.216194 Classification-F1 0.0 on epoch=24
03/18/2022 22:22:48 - INFO - __main__ - Step 60 Global step 60 Train loss 16.993134 on epoch=29
03/18/2022 22:22:53 - INFO - __main__ - Step 70 Global step 70 Train loss 15.876546 on epoch=34
03/18/2022 22:22:58 - INFO - __main__ - Step 80 Global step 80 Train loss 15.660772 on epoch=39
03/18/2022 22:23:03 - INFO - __main__ - Step 90 Global step 90 Train loss 15.345212 on epoch=44
03/18/2022 22:23:08 - INFO - __main__ - Step 100 Global step 100 Train loss 14.297091 on epoch=49
03/18/2022 22:23:09 - INFO - __main__ - Global step 100 Train loss 15.634552 Classification-F1 0.0 on epoch=49
03/18/2022 22:23:14 - INFO - __main__ - Step 110 Global step 110 Train loss 14.710838 on epoch=54
03/18/2022 22:23:19 - INFO - __main__ - Step 120 Global step 120 Train loss 14.476291 on epoch=59
03/18/2022 22:23:24 - INFO - __main__ - Step 130 Global step 130 Train loss 14.185633 on epoch=64
03/18/2022 22:23:29 - INFO - __main__ - Step 140 Global step 140 Train loss 13.026874 on epoch=69
03/18/2022 22:23:34 - INFO - __main__ - Step 150 Global step 150 Train loss 12.885089 on epoch=74
03/18/2022 22:23:37 - INFO - __main__ - Global step 150 Train loss 13.856943 Classification-F1 0.0 on epoch=74
03/18/2022 22:23:43 - INFO - __main__ - Step 160 Global step 160 Train loss 11.935171 on epoch=79
03/18/2022 22:23:48 - INFO - __main__ - Step 170 Global step 170 Train loss 11.358534 on epoch=84
03/18/2022 22:23:53 - INFO - __main__ - Step 180 Global step 180 Train loss 6.537076 on epoch=89
03/18/2022 22:23:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.467518 on epoch=94
03/18/2022 22:24:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.835089 on epoch=99
03/18/2022 22:24:03 - INFO - __main__ - Global step 200 Train loss 6.426678 Classification-F1 0.6476476476476476 on epoch=99
03/18/2022 22:24:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.776615 on epoch=104
03/18/2022 22:24:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.385139 on epoch=109
03/18/2022 22:24:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.631292 on epoch=114
03/18/2022 22:24:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.316672 on epoch=119
03/18/2022 22:24:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.307153 on epoch=124
03/18/2022 22:24:29 - INFO - __main__ - Global step 250 Train loss 0.483374 Classification-F1 0.5 on epoch=124
03/18/2022 22:24:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.274093 on epoch=129
03/18/2022 22:24:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.241846 on epoch=134
03/18/2022 22:24:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.180550 on epoch=139
03/18/2022 22:24:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.219416 on epoch=144
03/18/2022 22:24:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.103344 on epoch=149
03/18/2022 22:24:55 - INFO - __main__ - Global step 300 Train loss 0.203850 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 22:25:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.168190 on epoch=154
03/18/2022 22:25:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.107435 on epoch=159
03/18/2022 22:25:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.072653 on epoch=164
03/18/2022 22:25:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.034203 on epoch=169
03/18/2022 22:25:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.069567 on epoch=174
03/18/2022 22:25:21 - INFO - __main__ - Global step 350 Train loss 0.090410 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 22:25:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.049725 on epoch=179
03/18/2022 22:25:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.027350 on epoch=184
03/18/2022 22:25:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.018240 on epoch=189
03/18/2022 22:25:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025226 on epoch=194
03/18/2022 22:25:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.023228 on epoch=199
03/18/2022 22:25:47 - INFO - __main__ - Global step 400 Train loss 0.028754 Classification-F1 0.3992490613266583 on epoch=199
03/18/2022 22:25:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.013074 on epoch=204
03/18/2022 22:25:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.023792 on epoch=209
03/18/2022 22:26:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.019740 on epoch=214
03/18/2022 22:26:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005531 on epoch=219
03/18/2022 22:26:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.028299 on epoch=224
03/18/2022 22:26:12 - INFO - __main__ - Global step 450 Train loss 0.018087 Classification-F1 0.4385964912280702 on epoch=224
03/18/2022 22:26:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008383 on epoch=229
03/18/2022 22:26:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.007710 on epoch=234
03/18/2022 22:26:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006716 on epoch=239
03/18/2022 22:26:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004245 on epoch=244
03/18/2022 22:26:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005799 on epoch=249
03/18/2022 22:26:38 - INFO - __main__ - Global step 500 Train loss 0.006571 Classification-F1 0.4385964912280702 on epoch=249
03/18/2022 22:26:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.004971 on epoch=254
03/18/2022 22:26:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004755 on epoch=259
03/18/2022 22:26:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003140 on epoch=264
03/18/2022 22:26:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003748 on epoch=269
03/18/2022 22:27:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001377 on epoch=274
03/18/2022 22:27:04 - INFO - __main__ - Global step 550 Train loss 0.003598 Classification-F1 0.4458874458874459 on epoch=274
03/18/2022 22:27:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017028 on epoch=279
03/18/2022 22:27:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004413 on epoch=284
03/18/2022 22:27:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003372 on epoch=289
03/18/2022 22:27:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001370 on epoch=294
03/18/2022 22:27:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007658 on epoch=299
03/18/2022 22:27:29 - INFO - __main__ - Global step 600 Train loss 0.006768 Classification-F1 0.4420512820512821 on epoch=299
03/18/2022 22:27:29 - INFO - __main__ - save last model!
03/18/2022 22:27:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:27:30 - INFO - __main__ - Printing 3 examples
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:27:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:27:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:27:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:27:30 - INFO - __main__ - Printing 3 examples
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:27:30 - INFO - __main__ - ['false']
03/18/2022 22:27:30 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:27:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:27:30 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:27:37 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:27:37 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:27:37 - INFO - __main__ - Printing 3 examples
03/18/2022 22:27:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:27:37 - INFO - __main__ - ['false']
03/18/2022 22:27:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:27:37 - INFO - __main__ - ['false']
03/18/2022 22:27:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:27:37 - INFO - __main__ - ['false']
03/18/2022 22:27:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:27:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:27:41 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:27:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:27:43 - INFO - __main__ - Starting training!
03/18/2022 22:28:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_100_0.0001_8_predictions.txt
03/18/2022 22:28:08 - INFO - __main__ - Classification-F1 on test data: 0.2036
03/18/2022 22:28:08 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.0001, bsz=8, dev_performance=0.6476476476476476, test_performance=0.2036376809406651
03/18/2022 22:28:08 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.0005, bsz=8 ...
03/18/2022 22:28:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:28:09 - INFO - __main__ - Printing 3 examples
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:28:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:28:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:28:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:28:09 - INFO - __main__ - Printing 3 examples
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:28:09 - INFO - __main__ - ['false']
03/18/2022 22:28:09 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:28:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:28:09 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:28:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:28:22 - INFO - __main__ - Starting training!
03/18/2022 22:28:27 - INFO - __main__ - Step 10 Global step 10 Train loss 22.458256 on epoch=4
03/18/2022 22:28:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.246389 on epoch=9
03/18/2022 22:28:38 - INFO - __main__ - Step 30 Global step 30 Train loss 15.668315 on epoch=14
03/18/2022 22:28:44 - INFO - __main__ - Step 40 Global step 40 Train loss 13.917418 on epoch=19
03/18/2022 22:28:49 - INFO - __main__ - Step 50 Global step 50 Train loss 11.373755 on epoch=24
03/18/2022 22:29:02 - INFO - __main__ - Global step 50 Train loss 16.132828 Classification-F1 0.0 on epoch=24
03/18/2022 22:29:08 - INFO - __main__ - Step 60 Global step 60 Train loss 4.976944 on epoch=29
03/18/2022 22:29:13 - INFO - __main__ - Step 70 Global step 70 Train loss 3.307683 on epoch=34
03/18/2022 22:29:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.982704 on epoch=39
03/18/2022 22:29:24 - INFO - __main__ - Step 90 Global step 90 Train loss 1.039079 on epoch=44
03/18/2022 22:29:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.502670 on epoch=49
03/18/2022 22:29:30 - INFO - __main__ - Global step 100 Train loss 2.161816 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 22:29:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.447348 on epoch=54
03/18/2022 22:29:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.401501 on epoch=59
03/18/2022 22:29:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.346403 on epoch=64
03/18/2022 22:29:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.694360 on epoch=69
03/18/2022 22:29:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.288760 on epoch=74
03/18/2022 22:29:58 - INFO - __main__ - Global step 150 Train loss 0.435674 Classification-F1 0.464039408866995 on epoch=74
03/18/2022 22:30:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.202438 on epoch=79
03/18/2022 22:30:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.482437 on epoch=84
03/18/2022 22:30:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.234619 on epoch=89
03/18/2022 22:30:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.556113 on epoch=94
03/18/2022 22:30:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.341032 on epoch=99
03/18/2022 22:30:27 - INFO - __main__ - Global step 200 Train loss 0.363328 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 22:30:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.301145 on epoch=104
03/18/2022 22:30:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.129196 on epoch=109
03/18/2022 22:30:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.078407 on epoch=114
03/18/2022 22:30:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.028041 on epoch=119
03/18/2022 22:30:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.009089 on epoch=124
03/18/2022 22:30:55 - INFO - __main__ - Global step 250 Train loss 0.109176 Classification-F1 0.40566959921798634 on epoch=124
03/18/2022 22:31:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.017935 on epoch=129
03/18/2022 22:31:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001794 on epoch=134
03/18/2022 22:31:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001860 on epoch=139
03/18/2022 22:31:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002124 on epoch=144
03/18/2022 22:31:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001828 on epoch=149
03/18/2022 22:31:22 - INFO - __main__ - Global step 300 Train loss 0.005108 Classification-F1 0.40566959921798634 on epoch=149
03/18/2022 22:31:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001989 on epoch=154
03/18/2022 22:31:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000713 on epoch=159
03/18/2022 22:31:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000495 on epoch=164
03/18/2022 22:31:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.007348 on epoch=169
03/18/2022 22:31:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.003371 on epoch=174
03/18/2022 22:31:50 - INFO - __main__ - Global step 350 Train loss 0.002783 Classification-F1 0.4682306940371457 on epoch=174
03/18/2022 22:31:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001450 on epoch=179
03/18/2022 22:32:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000779 on epoch=184
03/18/2022 22:32:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000324 on epoch=189
03/18/2022 22:32:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.049429 on epoch=194
03/18/2022 22:32:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000610 on epoch=199
03/18/2022 22:32:19 - INFO - __main__ - Global step 400 Train loss 0.010518 Classification-F1 0.43529411764705883 on epoch=199
03/18/2022 22:32:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003122 on epoch=204
03/18/2022 22:32:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.045849 on epoch=209
03/18/2022 22:32:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000623 on epoch=214
03/18/2022 22:32:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000440 on epoch=219
03/18/2022 22:32:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000254 on epoch=224
03/18/2022 22:32:46 - INFO - __main__ - Global step 450 Train loss 0.010058 Classification-F1 0.40566959921798634 on epoch=224
03/18/2022 22:32:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000183 on epoch=229
03/18/2022 22:32:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000134 on epoch=234
03/18/2022 22:33:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001486 on epoch=239
03/18/2022 22:33:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001088 on epoch=244
03/18/2022 22:33:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000124 on epoch=249
03/18/2022 22:33:14 - INFO - __main__ - Global step 500 Train loss 0.000603 Classification-F1 0.43529411764705883 on epoch=249
03/18/2022 22:33:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000159 on epoch=254
03/18/2022 22:33:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000158 on epoch=259
03/18/2022 22:33:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000112 on epoch=264
03/18/2022 22:33:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000453 on epoch=269
03/18/2022 22:33:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000523 on epoch=274
03/18/2022 22:33:42 - INFO - __main__ - Global step 550 Train loss 0.000281 Classification-F1 0.39139139139139134 on epoch=274
03/18/2022 22:33:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000376 on epoch=279
03/18/2022 22:33:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000914 on epoch=284
03/18/2022 22:33:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000157 on epoch=289
03/18/2022 22:34:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000194 on epoch=294
03/18/2022 22:34:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000120 on epoch=299
03/18/2022 22:34:10 - INFO - __main__ - Global step 600 Train loss 0.000352 Classification-F1 0.3650793650793651 on epoch=299
03/18/2022 22:34:10 - INFO - __main__ - save last model!
03/18/2022 22:34:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:34:11 - INFO - __main__ - Printing 3 examples
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:34:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:34:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:34:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:34:11 - INFO - __main__ - Printing 3 examples
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:34:11 - INFO - __main__ - ['false']
03/18/2022 22:34:11 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:34:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:34:11 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:34:17 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:34:18 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:34:18 - INFO - __main__ - Printing 3 examples
03/18/2022 22:34:18 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:34:18 - INFO - __main__ - ['false']
03/18/2022 22:34:18 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:34:18 - INFO - __main__ - ['false']
03/18/2022 22:34:18 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:34:18 - INFO - __main__ - ['false']
03/18/2022 22:34:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:34:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:34:22 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:34:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:34:24 - INFO - __main__ - Starting training!
03/18/2022 22:34:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_13_0.0005_8_predictions.txt
03/18/2022 22:34:51 - INFO - __main__ - Classification-F1 on test data: 0.3966
03/18/2022 22:34:51 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.0005, bsz=8, dev_performance=0.4682306940371457, test_performance=0.39664551948215937
03/18/2022 22:34:51 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.0003, bsz=8 ...
03/18/2022 22:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:34:52 - INFO - __main__ - Printing 3 examples
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:34:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:34:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:34:52 - INFO - __main__ - Printing 3 examples
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:34:52 - INFO - __main__ - ['false']
03/18/2022 22:34:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:34:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:34:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:35:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:35:05 - INFO - __main__ - Starting training!
03/18/2022 22:35:10 - INFO - __main__ - Step 10 Global step 10 Train loss 22.626759 on epoch=4
03/18/2022 22:35:15 - INFO - __main__ - Step 20 Global step 20 Train loss 18.363354 on epoch=9
03/18/2022 22:35:20 - INFO - __main__ - Step 30 Global step 30 Train loss 18.105576 on epoch=14
03/18/2022 22:35:26 - INFO - __main__ - Step 40 Global step 40 Train loss 15.897319 on epoch=19
03/18/2022 22:35:31 - INFO - __main__ - Step 50 Global step 50 Train loss 15.094182 on epoch=24
03/18/2022 22:35:33 - INFO - __main__ - Global step 50 Train loss 18.017439 Classification-F1 0.0 on epoch=24
03/18/2022 22:35:39 - INFO - __main__ - Step 60 Global step 60 Train loss 14.829193 on epoch=29
03/18/2022 22:35:44 - INFO - __main__ - Step 70 Global step 70 Train loss 13.068494 on epoch=34
03/18/2022 22:35:50 - INFO - __main__ - Step 80 Global step 80 Train loss 11.565722 on epoch=39
03/18/2022 22:35:55 - INFO - __main__ - Step 90 Global step 90 Train loss 10.114523 on epoch=44
03/18/2022 22:36:01 - INFO - __main__ - Step 100 Global step 100 Train loss 8.001406 on epoch=49
03/18/2022 22:36:01 - INFO - __main__ - Global step 100 Train loss 11.515868 Classification-F1 0.0 on epoch=49
03/18/2022 22:36:07 - INFO - __main__ - Step 110 Global step 110 Train loss 4.175738 on epoch=54
03/18/2022 22:36:12 - INFO - __main__ - Step 120 Global step 120 Train loss 2.518936 on epoch=59
03/18/2022 22:36:18 - INFO - __main__ - Step 130 Global step 130 Train loss 2.607246 on epoch=64
03/18/2022 22:36:23 - INFO - __main__ - Step 140 Global step 140 Train loss 3.030168 on epoch=69
03/18/2022 22:36:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.147666 on epoch=74
03/18/2022 22:36:29 - INFO - __main__ - Global step 150 Train loss 2.895951 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 22:36:35 - INFO - __main__ - Step 160 Global step 160 Train loss 2.793868 on epoch=79
03/18/2022 22:36:40 - INFO - __main__ - Step 170 Global step 170 Train loss 2.491478 on epoch=84
03/18/2022 22:36:46 - INFO - __main__ - Step 180 Global step 180 Train loss 1.732847 on epoch=89
03/18/2022 22:36:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.963328 on epoch=94
03/18/2022 22:36:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.699931 on epoch=99
03/18/2022 22:36:57 - INFO - __main__ - Global step 200 Train loss 1.736291 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 22:37:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.515721 on epoch=104
03/18/2022 22:37:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.382746 on epoch=109
03/18/2022 22:37:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.419339 on epoch=114
03/18/2022 22:37:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.463057 on epoch=119
03/18/2022 22:37:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.416634 on epoch=124
03/18/2022 22:37:25 - INFO - __main__ - Global step 250 Train loss 0.439500 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 22:37:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.334787 on epoch=129
03/18/2022 22:37:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.383578 on epoch=134
03/18/2022 22:37:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.328642 on epoch=139
03/18/2022 22:37:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.344009 on epoch=144
03/18/2022 22:37:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.316290 on epoch=149
03/18/2022 22:37:53 - INFO - __main__ - Global step 300 Train loss 0.341461 Classification-F1 0.2873806998939555 on epoch=149
03/18/2022 22:37:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.324633 on epoch=154
03/18/2022 22:38:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.349768 on epoch=159
03/18/2022 22:38:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.236405 on epoch=164
03/18/2022 22:38:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.202654 on epoch=169
03/18/2022 22:38:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.212207 on epoch=174
03/18/2022 22:38:21 - INFO - __main__ - Global step 350 Train loss 0.265134 Classification-F1 0.3073593073593074 on epoch=174
03/18/2022 22:38:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.223270 on epoch=179
03/18/2022 22:38:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.168742 on epoch=184
03/18/2022 22:38:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.141883 on epoch=189
03/18/2022 22:38:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.060964 on epoch=194
03/18/2022 22:38:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.047161 on epoch=199
03/18/2022 22:38:49 - INFO - __main__ - Global step 400 Train loss 0.128404 Classification-F1 0.37254901960784315 on epoch=199
03/18/2022 22:38:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.041884 on epoch=204
03/18/2022 22:39:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.074179 on epoch=209
03/18/2022 22:39:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011922 on epoch=214
03/18/2022 22:39:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.050832 on epoch=219
03/18/2022 22:39:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.032884 on epoch=224
03/18/2022 22:39:18 - INFO - __main__ - Global step 450 Train loss 0.042340 Classification-F1 0.4285714285714286 on epoch=224
03/18/2022 22:39:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.042368 on epoch=229
03/18/2022 22:39:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.022227 on epoch=234
03/18/2022 22:39:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.018934 on epoch=239
03/18/2022 22:39:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.010575 on epoch=244
03/18/2022 22:39:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016156 on epoch=249
03/18/2022 22:39:46 - INFO - __main__ - Global step 500 Train loss 0.022052 Classification-F1 0.4554554554554554 on epoch=249
03/18/2022 22:39:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002596 on epoch=254
03/18/2022 22:39:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.021599 on epoch=259
03/18/2022 22:40:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.015419 on epoch=264
03/18/2022 22:40:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.008913 on epoch=269
03/18/2022 22:40:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002146 on epoch=274
03/18/2022 22:40:14 - INFO - __main__ - Global step 550 Train loss 0.010135 Classification-F1 0.464039408866995 on epoch=274
03/18/2022 22:40:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004422 on epoch=279
03/18/2022 22:40:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002783 on epoch=284
03/18/2022 22:40:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002356 on epoch=289
03/18/2022 22:40:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003161 on epoch=294
03/18/2022 22:40:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001542 on epoch=299
03/18/2022 22:40:43 - INFO - __main__ - Global step 600 Train loss 0.002853 Classification-F1 0.43529411764705883 on epoch=299
03/18/2022 22:40:43 - INFO - __main__ - save last model!
03/18/2022 22:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:40:44 - INFO - __main__ - Printing 3 examples
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:40:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:40:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:40:44 - INFO - __main__ - Printing 3 examples
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:40:44 - INFO - __main__ - ['false']
03/18/2022 22:40:44 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:40:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:40:44 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:40:50 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:40:50 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:40:50 - INFO - __main__ - Printing 3 examples
03/18/2022 22:40:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:40:50 - INFO - __main__ - ['false']
03/18/2022 22:40:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:40:50 - INFO - __main__ - ['false']
03/18/2022 22:40:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:40:50 - INFO - __main__ - ['false']
03/18/2022 22:40:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:40:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:40:54 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:40:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:40:55 - INFO - __main__ - Starting training!
03/18/2022 22:41:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_13_0.0003_8_predictions.txt
03/18/2022 22:41:23 - INFO - __main__ - Classification-F1 on test data: 0.4374
03/18/2022 22:41:24 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.0003, bsz=8, dev_performance=0.464039408866995, test_performance=0.43740015449345865
03/18/2022 22:41:24 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.0002, bsz=8 ...
03/18/2022 22:41:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:41:25 - INFO - __main__ - Printing 3 examples
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:41:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:41:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:41:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:41:25 - INFO - __main__ - Printing 3 examples
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:41:25 - INFO - __main__ - ['false']
03/18/2022 22:41:25 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:41:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:41:25 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:41:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:41:37 - INFO - __main__ - Starting training!
03/18/2022 22:41:42 - INFO - __main__ - Step 10 Global step 10 Train loss 22.995213 on epoch=4
03/18/2022 22:41:48 - INFO - __main__ - Step 20 Global step 20 Train loss 19.530874 on epoch=9
03/18/2022 22:41:53 - INFO - __main__ - Step 30 Global step 30 Train loss 17.995682 on epoch=14
03/18/2022 22:41:59 - INFO - __main__ - Step 40 Global step 40 Train loss 17.172628 on epoch=19
03/18/2022 22:42:04 - INFO - __main__ - Step 50 Global step 50 Train loss 16.504980 on epoch=24
03/18/2022 22:42:09 - INFO - __main__ - Global step 50 Train loss 18.839876 Classification-F1 0.0 on epoch=24
03/18/2022 22:42:15 - INFO - __main__ - Step 60 Global step 60 Train loss 15.715251 on epoch=29
03/18/2022 22:42:21 - INFO - __main__ - Step 70 Global step 70 Train loss 14.660960 on epoch=34
03/18/2022 22:42:26 - INFO - __main__ - Step 80 Global step 80 Train loss 13.931888 on epoch=39
03/18/2022 22:42:32 - INFO - __main__ - Step 90 Global step 90 Train loss 13.268791 on epoch=44
03/18/2022 22:42:37 - INFO - __main__ - Step 100 Global step 100 Train loss 11.851698 on epoch=49
03/18/2022 22:42:37 - INFO - __main__ - Global step 100 Train loss 13.885716 Classification-F1 0.0 on epoch=49
03/18/2022 22:42:43 - INFO - __main__ - Step 110 Global step 110 Train loss 8.480669 on epoch=54
03/18/2022 22:42:48 - INFO - __main__ - Step 120 Global step 120 Train loss 1.580702 on epoch=59
03/18/2022 22:42:54 - INFO - __main__ - Step 130 Global step 130 Train loss 2.701477 on epoch=64
03/18/2022 22:42:59 - INFO - __main__ - Step 140 Global step 140 Train loss 3.319012 on epoch=69
03/18/2022 22:43:04 - INFO - __main__ - Step 150 Global step 150 Train loss 1.691406 on epoch=74
03/18/2022 22:43:05 - INFO - __main__ - Global step 150 Train loss 3.554653 Classification-F1 0.2923976608187135 on epoch=74
03/18/2022 22:43:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.617256 on epoch=79
03/18/2022 22:43:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.759807 on epoch=84
03/18/2022 22:43:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.573890 on epoch=89
03/18/2022 22:43:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.461684 on epoch=94
03/18/2022 22:43:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.513564 on epoch=99
03/18/2022 22:43:33 - INFO - __main__ - Global step 200 Train loss 0.585240 Classification-F1 0.39999999999999997 on epoch=99
03/18/2022 22:43:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.400530 on epoch=104
03/18/2022 22:43:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.426479 on epoch=109
03/18/2022 22:43:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.308144 on epoch=114
03/18/2022 22:43:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.315795 on epoch=119
03/18/2022 22:44:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.295797 on epoch=124
03/18/2022 22:44:02 - INFO - __main__ - Global step 250 Train loss 0.349349 Classification-F1 0.40566959921798634 on epoch=124
03/18/2022 22:44:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.274634 on epoch=129
03/18/2022 22:44:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.268938 on epoch=134
03/18/2022 22:44:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.212461 on epoch=139
03/18/2022 22:44:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.218885 on epoch=144
03/18/2022 22:44:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.283852 on epoch=149
03/18/2022 22:44:30 - INFO - __main__ - Global step 300 Train loss 0.251754 Classification-F1 0.5195195195195195 on epoch=149
03/18/2022 22:44:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.345287 on epoch=154
03/18/2022 22:44:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.369793 on epoch=159
03/18/2022 22:44:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.268524 on epoch=164
03/18/2022 22:44:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.306203 on epoch=169
03/18/2022 22:44:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.268076 on epoch=174
03/18/2022 22:44:59 - INFO - __main__ - Global step 350 Train loss 0.311577 Classification-F1 0.464039408866995 on epoch=174
03/18/2022 22:45:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.280825 on epoch=179
03/18/2022 22:45:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.280086 on epoch=184
03/18/2022 22:45:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.284849 on epoch=189
03/18/2022 22:45:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.216790 on epoch=194
03/18/2022 22:45:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.156959 on epoch=199
03/18/2022 22:45:27 - INFO - __main__ - Global step 400 Train loss 0.243902 Classification-F1 0.4285714285714286 on epoch=199
03/18/2022 22:45:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.163747 on epoch=204
03/18/2022 22:45:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.109739 on epoch=209
03/18/2022 22:45:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.107650 on epoch=214
03/18/2022 22:45:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.078785 on epoch=219
03/18/2022 22:45:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.070766 on epoch=224
03/18/2022 22:45:55 - INFO - __main__ - Global step 450 Train loss 0.106137 Classification-F1 0.37254901960784315 on epoch=224
03/18/2022 22:46:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.025534 on epoch=229
03/18/2022 22:46:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.051798 on epoch=234
03/18/2022 22:46:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.051423 on epoch=239
03/18/2022 22:46:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.044950 on epoch=244
03/18/2022 22:46:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.040114 on epoch=249
03/18/2022 22:46:23 - INFO - __main__ - Global step 500 Train loss 0.042764 Classification-F1 0.3273273273273273 on epoch=249
03/18/2022 22:46:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.049997 on epoch=254
03/18/2022 22:46:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.050696 on epoch=259
03/18/2022 22:46:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.027641 on epoch=264
03/18/2022 22:46:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.114275 on epoch=269
03/18/2022 22:46:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022989 on epoch=274
03/18/2022 22:46:51 - INFO - __main__ - Global step 550 Train loss 0.053120 Classification-F1 0.40566959921798634 on epoch=274
03/18/2022 22:46:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018744 on epoch=279
03/18/2022 22:47:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015799 on epoch=284
03/18/2022 22:47:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.031188 on epoch=289
03/18/2022 22:47:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004642 on epoch=294
03/18/2022 22:47:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.048072 on epoch=299
03/18/2022 22:47:19 - INFO - __main__ - Global step 600 Train loss 0.023689 Classification-F1 0.3650793650793651 on epoch=299
03/18/2022 22:47:19 - INFO - __main__ - save last model!
03/18/2022 22:47:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:47:20 - INFO - __main__ - Printing 3 examples
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:47:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:47:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:47:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:47:20 - INFO - __main__ - Printing 3 examples
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:47:20 - INFO - __main__ - ['false']
03/18/2022 22:47:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:47:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:47:20 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:47:25 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:47:26 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:47:26 - INFO - __main__ - Printing 3 examples
03/18/2022 22:47:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:47:26 - INFO - __main__ - ['false']
03/18/2022 22:47:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:47:26 - INFO - __main__ - ['false']
03/18/2022 22:47:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:47:26 - INFO - __main__ - ['false']
03/18/2022 22:47:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:47:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:47:30 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:47:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:47:33 - INFO - __main__ - Starting training!
03/18/2022 22:47:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_13_0.0002_8_predictions.txt
03/18/2022 22:47:59 - INFO - __main__ - Classification-F1 on test data: 0.2964
03/18/2022 22:47:59 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.0002, bsz=8, dev_performance=0.5195195195195195, test_performance=0.2963698579235116
03/18/2022 22:47:59 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.0001, bsz=8 ...
03/18/2022 22:48:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:48:00 - INFO - __main__ - Printing 3 examples
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:48:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:48:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:48:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:48:00 - INFO - __main__ - Printing 3 examples
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 22:48:00 - INFO - __main__ - ['false']
03/18/2022 22:48:00 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:48:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:48:00 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:48:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:48:13 - INFO - __main__ - Starting training!
03/18/2022 22:48:18 - INFO - __main__ - Step 10 Global step 10 Train loss 23.021410 on epoch=4
03/18/2022 22:48:23 - INFO - __main__ - Step 20 Global step 20 Train loss 20.499968 on epoch=9
03/18/2022 22:48:29 - INFO - __main__ - Step 30 Global step 30 Train loss 19.847395 on epoch=14
03/18/2022 22:48:34 - INFO - __main__ - Step 40 Global step 40 Train loss 17.626398 on epoch=19
03/18/2022 22:48:40 - INFO - __main__ - Step 50 Global step 50 Train loss 17.234516 on epoch=24
03/18/2022 22:48:44 - INFO - __main__ - Global step 50 Train loss 19.645937 Classification-F1 0.0 on epoch=24
03/18/2022 22:48:51 - INFO - __main__ - Step 60 Global step 60 Train loss 16.885469 on epoch=29
03/18/2022 22:48:56 - INFO - __main__ - Step 70 Global step 70 Train loss 16.512516 on epoch=34
03/18/2022 22:49:02 - INFO - __main__ - Step 80 Global step 80 Train loss 16.507339 on epoch=39
03/18/2022 22:49:07 - INFO - __main__ - Step 90 Global step 90 Train loss 16.309937 on epoch=44
03/18/2022 22:49:13 - INFO - __main__ - Step 100 Global step 100 Train loss 15.249681 on epoch=49
03/18/2022 22:49:18 - INFO - __main__ - Global step 100 Train loss 16.292988 Classification-F1 0.0 on epoch=49
03/18/2022 22:49:23 - INFO - __main__ - Step 110 Global step 110 Train loss 14.827219 on epoch=54
03/18/2022 22:49:29 - INFO - __main__ - Step 120 Global step 120 Train loss 14.485090 on epoch=59
03/18/2022 22:49:34 - INFO - __main__ - Step 130 Global step 130 Train loss 14.429735 on epoch=64
03/18/2022 22:49:40 - INFO - __main__ - Step 140 Global step 140 Train loss 13.807483 on epoch=69
03/18/2022 22:49:45 - INFO - __main__ - Step 150 Global step 150 Train loss 14.064611 on epoch=74
03/18/2022 22:49:51 - INFO - __main__ - Global step 150 Train loss 14.322827 Classification-F1 0.0 on epoch=74
03/18/2022 22:49:57 - INFO - __main__ - Step 160 Global step 160 Train loss 13.068674 on epoch=79
03/18/2022 22:50:02 - INFO - __main__ - Step 170 Global step 170 Train loss 12.577671 on epoch=84
03/18/2022 22:50:08 - INFO - __main__ - Step 180 Global step 180 Train loss 12.588361 on epoch=89
03/18/2022 22:50:13 - INFO - __main__ - Step 190 Global step 190 Train loss 11.209448 on epoch=94
03/18/2022 22:50:19 - INFO - __main__ - Step 200 Global step 200 Train loss 10.417534 on epoch=99
03/18/2022 22:50:29 - INFO - __main__ - Global step 200 Train loss 11.972338 Classification-F1 0.031746031746031744 on epoch=99
03/18/2022 22:50:35 - INFO - __main__ - Step 210 Global step 210 Train loss 7.949310 on epoch=104
03/18/2022 22:50:40 - INFO - __main__ - Step 220 Global step 220 Train loss 2.643078 on epoch=109
03/18/2022 22:50:46 - INFO - __main__ - Step 230 Global step 230 Train loss 1.210668 on epoch=114
03/18/2022 22:50:51 - INFO - __main__ - Step 240 Global step 240 Train loss 1.556969 on epoch=119
03/18/2022 22:50:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.012069 on epoch=124
03/18/2022 22:50:57 - INFO - __main__ - Global step 250 Train loss 2.874419 Classification-F1 0.3992490613266583 on epoch=124
03/18/2022 22:51:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.876547 on epoch=129
03/18/2022 22:51:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.609125 on epoch=134
03/18/2022 22:51:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.771421 on epoch=139
03/18/2022 22:51:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.020150 on epoch=144
03/18/2022 22:51:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.852953 on epoch=149
03/18/2022 22:51:26 - INFO - __main__ - Global step 300 Train loss 0.826039 Classification-F1 0.5625 on epoch=149
03/18/2022 22:51:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.846666 on epoch=154
03/18/2022 22:51:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.429846 on epoch=159
03/18/2022 22:51:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.364831 on epoch=164
03/18/2022 22:51:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.308663 on epoch=169
03/18/2022 22:51:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.293132 on epoch=174
03/18/2022 22:51:54 - INFO - __main__ - Global step 350 Train loss 0.448628 Classification-F1 0.39756367663344405 on epoch=174
03/18/2022 22:52:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314558 on epoch=179
03/18/2022 22:52:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.285994 on epoch=184
03/18/2022 22:52:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.171413 on epoch=189
03/18/2022 22:52:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.209088 on epoch=194
03/18/2022 22:52:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.076179 on epoch=199
03/18/2022 22:52:22 - INFO - __main__ - Global step 400 Train loss 0.211446 Classification-F1 0.4375 on epoch=199
03/18/2022 22:52:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.093754 on epoch=204
03/18/2022 22:52:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.068041 on epoch=209
03/18/2022 22:52:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.045405 on epoch=214
03/18/2022 22:52:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.018123 on epoch=219
03/18/2022 22:52:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.025356 on epoch=224
03/18/2022 22:52:50 - INFO - __main__ - Global step 450 Train loss 0.050136 Classification-F1 0.4375 on epoch=224
03/18/2022 22:52:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.027820 on epoch=229
03/18/2022 22:53:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010783 on epoch=234
03/18/2022 22:53:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.016521 on epoch=239
03/18/2022 22:53:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.023315 on epoch=244
03/18/2022 22:53:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017361 on epoch=249
03/18/2022 22:53:18 - INFO - __main__ - Global step 500 Train loss 0.019160 Classification-F1 0.40566959921798634 on epoch=249
03/18/2022 22:53:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007753 on epoch=254
03/18/2022 22:53:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.006470 on epoch=259
03/18/2022 22:53:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005988 on epoch=264
03/18/2022 22:53:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.012312 on epoch=269
03/18/2022 22:53:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004703 on epoch=274
03/18/2022 22:53:46 - INFO - __main__ - Global step 550 Train loss 0.007445 Classification-F1 0.4285714285714286 on epoch=274
03/18/2022 22:53:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004306 on epoch=279
03/18/2022 22:53:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006513 on epoch=284
03/18/2022 22:54:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018906 on epoch=289
03/18/2022 22:54:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004367 on epoch=294
03/18/2022 22:54:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001918 on epoch=299
03/18/2022 22:54:14 - INFO - __main__ - Global step 600 Train loss 0.007202 Classification-F1 0.4285714285714286 on epoch=299
03/18/2022 22:54:14 - INFO - __main__ - save last model!
03/18/2022 22:54:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:54:15 - INFO - __main__ - Printing 3 examples
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 22:54:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:54:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:54:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:54:15 - INFO - __main__ - Printing 3 examples
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 22:54:15 - INFO - __main__ - ['false']
03/18/2022 22:54:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:54:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:54:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:54:21 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 22:54:21 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 22:54:21 - INFO - __main__ - Printing 3 examples
03/18/2022 22:54:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 22:54:21 - INFO - __main__ - ['false']
03/18/2022 22:54:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 22:54:21 - INFO - __main__ - ['false']
03/18/2022 22:54:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 22:54:21 - INFO - __main__ - ['false']
03/18/2022 22:54:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:54:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:54:25 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 22:54:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:54:28 - INFO - __main__ - Starting training!
03/18/2022 22:54:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_13_0.0001_8_predictions.txt
03/18/2022 22:54:53 - INFO - __main__ - Classification-F1 on test data: 0.4451
03/18/2022 22:54:53 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.44511032352110924
03/18/2022 22:54:53 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.0005, bsz=8 ...
03/18/2022 22:54:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:54:54 - INFO - __main__ - Printing 3 examples
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 22:54:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:54:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 22:54:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 22:54:54 - INFO - __main__ - Printing 3 examples
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 22:54:54 - INFO - __main__ - ['false']
03/18/2022 22:54:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 22:54:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 22:54:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 22:55:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 22:55:07 - INFO - __main__ - Starting training!
03/18/2022 22:55:11 - INFO - __main__ - Step 10 Global step 10 Train loss 22.036278 on epoch=4
03/18/2022 22:55:16 - INFO - __main__ - Step 20 Global step 20 Train loss 17.744383 on epoch=9
03/18/2022 22:55:21 - INFO - __main__ - Step 30 Global step 30 Train loss 15.384610 on epoch=14
03/18/2022 22:55:26 - INFO - __main__ - Step 40 Global step 40 Train loss 13.923094 on epoch=19
03/18/2022 22:55:31 - INFO - __main__ - Step 50 Global step 50 Train loss 8.838972 on epoch=24
03/18/2022 22:55:31 - INFO - __main__ - Global step 50 Train loss 15.585467 Classification-F1 0.0 on epoch=24
03/18/2022 22:55:37 - INFO - __main__ - Step 60 Global step 60 Train loss 7.382360 on epoch=29
03/18/2022 22:55:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.807220 on epoch=34
03/18/2022 22:55:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.486068 on epoch=39
03/18/2022 22:55:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.424442 on epoch=44
03/18/2022 22:55:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.394767 on epoch=49
03/18/2022 22:55:57 - INFO - __main__ - Global step 100 Train loss 1.898972 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 22:56:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.284569 on epoch=54
03/18/2022 22:56:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.385421 on epoch=59
03/18/2022 22:56:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.470668 on epoch=64
03/18/2022 22:56:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.318801 on epoch=69
03/18/2022 22:56:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.535974 on epoch=74
03/18/2022 22:56:23 - INFO - __main__ - Global step 150 Train loss 0.399087 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 22:56:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.420118 on epoch=79
03/18/2022 22:56:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.172838 on epoch=84
03/18/2022 22:56:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.296661 on epoch=89
03/18/2022 22:56:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.216509 on epoch=94
03/18/2022 22:56:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.371044 on epoch=99
03/18/2022 22:56:48 - INFO - __main__ - Global step 200 Train loss 0.295434 Classification-F1 0.5607843137254902 on epoch=99
03/18/2022 22:56:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.226260 on epoch=104
03/18/2022 22:56:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.051686 on epoch=109
03/18/2022 22:57:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.015726 on epoch=114
03/18/2022 22:57:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.012455 on epoch=119
03/18/2022 22:57:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.007914 on epoch=124
03/18/2022 22:57:14 - INFO - __main__ - Global step 250 Train loss 0.062808 Classification-F1 0.5625 on epoch=124
03/18/2022 22:57:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.006265 on epoch=129
03/18/2022 22:57:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.005475 on epoch=134
03/18/2022 22:57:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002323 on epoch=139
03/18/2022 22:57:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002528 on epoch=144
03/18/2022 22:57:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000936 on epoch=149
03/18/2022 22:57:41 - INFO - __main__ - Global step 300 Train loss 0.003505 Classification-F1 0.4458874458874459 on epoch=149
03/18/2022 22:57:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002108 on epoch=154
03/18/2022 22:57:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002077 on epoch=159
03/18/2022 22:57:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001140 on epoch=164
03/18/2022 22:58:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.173946 on epoch=169
03/18/2022 22:58:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.005420 on epoch=174
03/18/2022 22:58:06 - INFO - __main__ - Global step 350 Train loss 0.036938 Classification-F1 0.4920634920634921 on epoch=174
03/18/2022 22:58:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.018363 on epoch=179
03/18/2022 22:58:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009031 on epoch=184
03/18/2022 22:58:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.046712 on epoch=189
03/18/2022 22:58:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.005621 on epoch=194
03/18/2022 22:58:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.010831 on epoch=199
03/18/2022 22:58:31 - INFO - __main__ - Global step 400 Train loss 0.018112 Classification-F1 0.5733333333333335 on epoch=199
03/18/2022 22:58:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.005324 on epoch=204
03/18/2022 22:58:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.136278 on epoch=209
03/18/2022 22:58:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001594 on epoch=214
03/18/2022 22:58:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000271 on epoch=219
03/18/2022 22:58:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007002 on epoch=224
03/18/2022 22:58:58 - INFO - __main__ - Global step 450 Train loss 0.030094 Classification-F1 0.37254901960784315 on epoch=224
03/18/2022 22:59:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003199 on epoch=229
03/18/2022 22:59:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.118885 on epoch=234
03/18/2022 22:59:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002379 on epoch=239
03/18/2022 22:59:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001482 on epoch=244
03/18/2022 22:59:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000236 on epoch=249
03/18/2022 22:59:23 - INFO - __main__ - Global step 500 Train loss 0.025236 Classification-F1 0.39999999999999997 on epoch=249
03/18/2022 22:59:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000561 on epoch=254
03/18/2022 22:59:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000364 on epoch=259
03/18/2022 22:59:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000064 on epoch=264
03/18/2022 22:59:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000080 on epoch=269
03/18/2022 22:59:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000071 on epoch=274
03/18/2022 22:59:49 - INFO - __main__ - Global step 550 Train loss 0.000228 Classification-F1 0.4420512820512821 on epoch=274
03/18/2022 22:59:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000044 on epoch=279
03/18/2022 22:59:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000141 on epoch=284
03/18/2022 23:00:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000016 on epoch=289
03/18/2022 23:00:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002425 on epoch=294
03/18/2022 23:00:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000089 on epoch=299
03/18/2022 23:00:14 - INFO - __main__ - Global step 600 Train loss 0.000543 Classification-F1 0.39756367663344405 on epoch=299
03/18/2022 23:00:14 - INFO - __main__ - save last model!
03/18/2022 23:00:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:00:15 - INFO - __main__ - Printing 3 examples
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:00:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:00:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:00:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:00:15 - INFO - __main__ - Printing 3 examples
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:00:15 - INFO - __main__ - ['false']
03/18/2022 23:00:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:00:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:00:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:00:21 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:00:22 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:00:22 - INFO - __main__ - Printing 3 examples
03/18/2022 23:00:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:00:22 - INFO - __main__ - ['false']
03/18/2022 23:00:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:00:22 - INFO - __main__ - ['false']
03/18/2022 23:00:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:00:22 - INFO - __main__ - ['false']
03/18/2022 23:00:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:00:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:00:26 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:00:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:00:26 - INFO - __main__ - Starting training!
03/18/2022 23:00:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_21_0.0005_8_predictions.txt
03/18/2022 23:00:56 - INFO - __main__ - Classification-F1 on test data: 0.0218
03/18/2022 23:00:56 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.0005, bsz=8, dev_performance=0.5733333333333335, test_performance=0.021842645365532452
03/18/2022 23:00:56 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.0003, bsz=8 ...
03/18/2022 23:00:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:00:57 - INFO - __main__ - Printing 3 examples
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:00:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:00:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:00:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:00:57 - INFO - __main__ - Printing 3 examples
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:00:57 - INFO - __main__ - ['false']
03/18/2022 23:00:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:00:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:00:57 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:01:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:01:10 - INFO - __main__ - Starting training!
03/18/2022 23:01:14 - INFO - __main__ - Step 10 Global step 10 Train loss 23.192184 on epoch=4
03/18/2022 23:01:19 - INFO - __main__ - Step 20 Global step 20 Train loss 18.141922 on epoch=9
03/18/2022 23:01:24 - INFO - __main__ - Step 30 Global step 30 Train loss 16.340712 on epoch=14
03/18/2022 23:01:29 - INFO - __main__ - Step 40 Global step 40 Train loss 15.430008 on epoch=19
03/18/2022 23:01:34 - INFO - __main__ - Step 50 Global step 50 Train loss 14.229326 on epoch=24
03/18/2022 23:01:38 - INFO - __main__ - Global step 50 Train loss 17.466831 Classification-F1 0.0 on epoch=24
03/18/2022 23:01:43 - INFO - __main__ - Step 60 Global step 60 Train loss 13.292986 on epoch=29
03/18/2022 23:01:49 - INFO - __main__ - Step 70 Global step 70 Train loss 11.311197 on epoch=34
03/18/2022 23:01:54 - INFO - __main__ - Step 80 Global step 80 Train loss 4.127142 on epoch=39
03/18/2022 23:01:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.958953 on epoch=44
03/18/2022 23:02:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.636598 on epoch=49
03/18/2022 23:02:03 - INFO - __main__ - Global step 100 Train loss 6.065375 Classification-F1 0.5588547189819725 on epoch=49
03/18/2022 23:02:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.489132 on epoch=54
03/18/2022 23:02:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.372425 on epoch=59
03/18/2022 23:02:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.326295 on epoch=64
03/18/2022 23:02:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.320043 on epoch=69
03/18/2022 23:02:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.179430 on epoch=74
03/18/2022 23:02:29 - INFO - __main__ - Global step 150 Train loss 0.337465 Classification-F1 0.5733333333333335 on epoch=74
03/18/2022 23:02:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.189420 on epoch=79
03/18/2022 23:02:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.115032 on epoch=84
03/18/2022 23:02:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.159062 on epoch=89
03/18/2022 23:02:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.431248 on epoch=94
03/18/2022 23:02:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.712862 on epoch=99
03/18/2022 23:02:54 - INFO - __main__ - Global step 200 Train loss 0.321525 Classification-F1 0.37662337662337664 on epoch=99
03/18/2022 23:02:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.291372 on epoch=104
03/18/2022 23:03:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.562478 on epoch=109
03/18/2022 23:03:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.454364 on epoch=114
03/18/2022 23:03:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.382446 on epoch=119
03/18/2022 23:03:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.504380 on epoch=124
03/18/2022 23:03:19 - INFO - __main__ - Global step 250 Train loss 0.439008 Classification-F1 0.4222222222222223 on epoch=124
03/18/2022 23:03:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.370364 on epoch=129
03/18/2022 23:03:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.416277 on epoch=134
03/18/2022 23:03:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.327240 on epoch=139
03/18/2022 23:03:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.341739 on epoch=144
03/18/2022 23:03:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.363122 on epoch=149
03/18/2022 23:03:44 - INFO - __main__ - Global step 300 Train loss 0.363749 Classification-F1 0.4231177094379639 on epoch=149
03/18/2022 23:03:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323776 on epoch=154
03/18/2022 23:03:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.301476 on epoch=159
03/18/2022 23:03:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.224699 on epoch=164
03/18/2022 23:04:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.299820 on epoch=169
03/18/2022 23:04:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.349718 on epoch=174
03/18/2022 23:04:09 - INFO - __main__ - Global step 350 Train loss 0.299898 Classification-F1 0.33793103448275863 on epoch=174
03/18/2022 23:04:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.289427 on epoch=179
03/18/2022 23:04:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.238118 on epoch=184
03/18/2022 23:04:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.283989 on epoch=189
03/18/2022 23:04:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.219781 on epoch=194
03/18/2022 23:04:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.174920 on epoch=199
03/18/2022 23:04:34 - INFO - __main__ - Global step 400 Train loss 0.241247 Classification-F1 0.37662337662337664 on epoch=199
03/18/2022 23:04:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.179003 on epoch=204
03/18/2022 23:04:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.268092 on epoch=209
03/18/2022 23:04:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.161387 on epoch=214
03/18/2022 23:04:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.205751 on epoch=219
03/18/2022 23:04:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.184031 on epoch=224
03/18/2022 23:05:00 - INFO - __main__ - Global step 450 Train loss 0.199653 Classification-F1 0.4666666666666667 on epoch=224
03/18/2022 23:05:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.219553 on epoch=229
03/18/2022 23:05:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.188732 on epoch=234
03/18/2022 23:05:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.156263 on epoch=239
03/18/2022 23:05:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.155537 on epoch=244
03/18/2022 23:05:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.146533 on epoch=249
03/18/2022 23:05:25 - INFO - __main__ - Global step 500 Train loss 0.173324 Classification-F1 0.4980392156862745 on epoch=249
03/18/2022 23:05:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.189656 on epoch=254
03/18/2022 23:05:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.114586 on epoch=259
03/18/2022 23:05:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.212323 on epoch=264
03/18/2022 23:05:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.107727 on epoch=269
03/18/2022 23:05:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.058773 on epoch=274
03/18/2022 23:05:50 - INFO - __main__ - Global step 550 Train loss 0.136613 Classification-F1 0.3191489361702127 on epoch=274
03/18/2022 23:05:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.025261 on epoch=279
03/18/2022 23:06:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.105037 on epoch=284
03/18/2022 23:06:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.038346 on epoch=289
03/18/2022 23:06:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.058768 on epoch=294
03/18/2022 23:06:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.044534 on epoch=299
03/18/2022 23:06:15 - INFO - __main__ - Global step 600 Train loss 0.054389 Classification-F1 0.5076923076923077 on epoch=299
03/18/2022 23:06:15 - INFO - __main__ - save last model!
03/18/2022 23:06:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:06:16 - INFO - __main__ - Printing 3 examples
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:06:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:06:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:06:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:06:16 - INFO - __main__ - Printing 3 examples
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:06:16 - INFO - __main__ - ['false']
03/18/2022 23:06:16 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:06:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:06:16 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:06:22 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:06:23 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:06:23 - INFO - __main__ - Printing 3 examples
03/18/2022 23:06:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:06:23 - INFO - __main__ - ['false']
03/18/2022 23:06:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:06:23 - INFO - __main__ - ['false']
03/18/2022 23:06:23 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:06:23 - INFO - __main__ - ['false']
03/18/2022 23:06:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:06:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:06:27 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:06:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:06:27 - INFO - __main__ - Starting training!
03/18/2022 23:06:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_21_0.0003_8_predictions.txt
03/18/2022 23:06:56 - INFO - __main__ - Classification-F1 on test data: 0.1143
03/18/2022 23:06:56 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.0003, bsz=8, dev_performance=0.5733333333333335, test_performance=0.11430455695696799
03/18/2022 23:06:56 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.0002, bsz=8 ...
03/18/2022 23:06:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:06:57 - INFO - __main__ - Printing 3 examples
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:06:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:06:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:06:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:06:57 - INFO - __main__ - Printing 3 examples
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:06:57 - INFO - __main__ - ['false']
03/18/2022 23:06:57 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:06:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:06:57 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:07:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:07:10 - INFO - __main__ - Starting training!
03/18/2022 23:07:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.136385 on epoch=4
03/18/2022 23:07:19 - INFO - __main__ - Step 20 Global step 20 Train loss 18.841949 on epoch=9
03/18/2022 23:07:23 - INFO - __main__ - Step 30 Global step 30 Train loss 18.294558 on epoch=14
03/18/2022 23:07:28 - INFO - __main__ - Step 40 Global step 40 Train loss 16.843843 on epoch=19
03/18/2022 23:07:33 - INFO - __main__ - Step 50 Global step 50 Train loss 16.537899 on epoch=24
03/18/2022 23:07:43 - INFO - __main__ - Global step 50 Train loss 18.930927 Classification-F1 0.0 on epoch=24
03/18/2022 23:07:48 - INFO - __main__ - Step 60 Global step 60 Train loss 15.159777 on epoch=29
03/18/2022 23:07:53 - INFO - __main__ - Step 70 Global step 70 Train loss 14.643191 on epoch=34
03/18/2022 23:07:58 - INFO - __main__ - Step 80 Global step 80 Train loss 13.628171 on epoch=39
03/18/2022 23:08:03 - INFO - __main__ - Step 90 Global step 90 Train loss 11.679693 on epoch=44
03/18/2022 23:08:08 - INFO - __main__ - Step 100 Global step 100 Train loss 10.935859 on epoch=49
03/18/2022 23:08:08 - INFO - __main__ - Global step 100 Train loss 13.209338 Classification-F1 0.0 on epoch=49
03/18/2022 23:08:13 - INFO - __main__ - Step 110 Global step 110 Train loss 3.636657 on epoch=54
03/18/2022 23:08:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.972297 on epoch=59
03/18/2022 23:08:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.655819 on epoch=64
03/18/2022 23:08:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.900287 on epoch=69
03/18/2022 23:08:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.622092 on epoch=74
03/18/2022 23:08:33 - INFO - __main__ - Global step 150 Train loss 1.357430 Classification-F1 0.22727272727272727 on epoch=74
03/18/2022 23:08:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.736611 on epoch=79
03/18/2022 23:08:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.597152 on epoch=84
03/18/2022 23:08:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.431662 on epoch=89
03/18/2022 23:08:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.463132 on epoch=94
03/18/2022 23:08:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.564259 on epoch=99
03/18/2022 23:08:59 - INFO - __main__ - Global step 200 Train loss 0.558563 Classification-F1 0.17777777777777778 on epoch=99
03/18/2022 23:09:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.580000 on epoch=104
03/18/2022 23:09:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.377030 on epoch=109
03/18/2022 23:09:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.410964 on epoch=114
03/18/2022 23:09:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.408275 on epoch=119
03/18/2022 23:09:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.405204 on epoch=124
03/18/2022 23:09:24 - INFO - __main__ - Global step 250 Train loss 0.436295 Classification-F1 0.21505376344086022 on epoch=124
03/18/2022 23:09:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.369217 on epoch=129
03/18/2022 23:09:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.336727 on epoch=134
03/18/2022 23:09:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.410095 on epoch=139
03/18/2022 23:09:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.392523 on epoch=144
03/18/2022 23:09:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.426791 on epoch=149
03/18/2022 23:09:49 - INFO - __main__ - Global step 300 Train loss 0.387071 Classification-F1 0.4589371980676329 on epoch=149
03/18/2022 23:09:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.383276 on epoch=154
03/18/2022 23:10:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.410698 on epoch=159
03/18/2022 23:10:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.375708 on epoch=164
03/18/2022 23:10:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.365304 on epoch=169
03/18/2022 23:10:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.379030 on epoch=174
03/18/2022 23:10:15 - INFO - __main__ - Global step 350 Train loss 0.382803 Classification-F1 0.4385964912280702 on epoch=174
03/18/2022 23:10:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.364878 on epoch=179
03/18/2022 23:10:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.337872 on epoch=184
03/18/2022 23:10:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.350279 on epoch=189
03/18/2022 23:10:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.363548 on epoch=194
03/18/2022 23:10:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.347405 on epoch=199
03/18/2022 23:10:40 - INFO - __main__ - Global step 400 Train loss 0.352797 Classification-F1 0.3191489361702127 on epoch=199
03/18/2022 23:10:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.361626 on epoch=204
03/18/2022 23:10:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.364460 on epoch=209
03/18/2022 23:10:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.388908 on epoch=214
03/18/2022 23:11:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.389036 on epoch=219
03/18/2022 23:11:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.368919 on epoch=224
03/18/2022 23:11:06 - INFO - __main__ - Global step 450 Train loss 0.374590 Classification-F1 0.18604651162790697 on epoch=224
03/18/2022 23:11:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.347171 on epoch=229
03/18/2022 23:11:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.384284 on epoch=234
03/18/2022 23:11:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.388146 on epoch=239
03/18/2022 23:11:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.341881 on epoch=244
03/18/2022 23:11:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.400097 on epoch=249
03/18/2022 23:11:31 - INFO - __main__ - Global step 500 Train loss 0.372316 Classification-F1 0.21276595744680848 on epoch=249
03/18/2022 23:11:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.334273 on epoch=254
03/18/2022 23:11:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.343993 on epoch=259
03/18/2022 23:11:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.322944 on epoch=264
03/18/2022 23:11:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.328424 on epoch=269
03/18/2022 23:11:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.323275 on epoch=274
03/18/2022 23:11:56 - INFO - __main__ - Global step 550 Train loss 0.330582 Classification-F1 0.4181818181818182 on epoch=274
03/18/2022 23:12:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.320144 on epoch=279
03/18/2022 23:12:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.296890 on epoch=284
03/18/2022 23:12:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.305442 on epoch=289
03/18/2022 23:12:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.285320 on epoch=294
03/18/2022 23:12:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.288635 on epoch=299
03/18/2022 23:12:21 - INFO - __main__ - Global step 600 Train loss 0.299286 Classification-F1 0.4009852216748768 on epoch=299
03/18/2022 23:12:21 - INFO - __main__ - save last model!
03/18/2022 23:12:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:12:22 - INFO - __main__ - Printing 3 examples
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:12:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:12:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:12:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:12:22 - INFO - __main__ - Printing 3 examples
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:12:22 - INFO - __main__ - ['false']
03/18/2022 23:12:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:12:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:12:22 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:12:28 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:12:28 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:12:28 - INFO - __main__ - Printing 3 examples
03/18/2022 23:12:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:12:28 - INFO - __main__ - ['false']
03/18/2022 23:12:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:12:28 - INFO - __main__ - ['false']
03/18/2022 23:12:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:12:28 - INFO - __main__ - ['false']
03/18/2022 23:12:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:12:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:12:32 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:12:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:12:35 - INFO - __main__ - Starting training!
03/18/2022 23:13:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_21_0.0002_8_predictions.txt
03/18/2022 23:13:19 - INFO - __main__ - Classification-F1 on test data: 0.2044
03/18/2022 23:13:20 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.0002, bsz=8, dev_performance=0.4589371980676329, test_performance=0.20436916777590283
03/18/2022 23:13:20 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.0001, bsz=8 ...
03/18/2022 23:13:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:13:21 - INFO - __main__ - Printing 3 examples
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:13:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:13:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:13:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:13:21 - INFO - __main__ - Printing 3 examples
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of WisconsinGreen Bay .
03/18/2022 23:13:21 - INFO - __main__ - ['false']
03/18/2022 23:13:21 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:13:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:13:21 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:13:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:13:32 - INFO - __main__ - Starting training!
03/18/2022 23:13:36 - INFO - __main__ - Step 10 Global step 10 Train loss 23.697638 on epoch=4
03/18/2022 23:13:41 - INFO - __main__ - Step 20 Global step 20 Train loss 19.861858 on epoch=9
03/18/2022 23:13:46 - INFO - __main__ - Step 30 Global step 30 Train loss 18.223120 on epoch=14
03/18/2022 23:13:51 - INFO - __main__ - Step 40 Global step 40 Train loss 18.650051 on epoch=19
03/18/2022 23:13:56 - INFO - __main__ - Step 50 Global step 50 Train loss 17.288794 on epoch=24
03/18/2022 23:14:05 - INFO - __main__ - Global step 50 Train loss 19.544292 Classification-F1 0.0 on epoch=24
03/18/2022 23:14:11 - INFO - __main__ - Step 60 Global step 60 Train loss 16.982502 on epoch=29
03/18/2022 23:14:16 - INFO - __main__ - Step 70 Global step 70 Train loss 16.295872 on epoch=34
03/18/2022 23:14:21 - INFO - __main__ - Step 80 Global step 80 Train loss 15.577583 on epoch=39
03/18/2022 23:14:26 - INFO - __main__ - Step 90 Global step 90 Train loss 15.583185 on epoch=44
03/18/2022 23:14:31 - INFO - __main__ - Step 100 Global step 100 Train loss 16.302084 on epoch=49
03/18/2022 23:14:36 - INFO - __main__ - Global step 100 Train loss 16.148245 Classification-F1 0.0 on epoch=49
03/18/2022 23:14:41 - INFO - __main__ - Step 110 Global step 110 Train loss 14.708858 on epoch=54
03/18/2022 23:14:46 - INFO - __main__ - Step 120 Global step 120 Train loss 14.899397 on epoch=59
03/18/2022 23:14:52 - INFO - __main__ - Step 130 Global step 130 Train loss 14.652349 on epoch=64
03/18/2022 23:14:57 - INFO - __main__ - Step 140 Global step 140 Train loss 14.371332 on epoch=69
03/18/2022 23:15:02 - INFO - __main__ - Step 150 Global step 150 Train loss 13.642252 on epoch=74
03/18/2022 23:15:06 - INFO - __main__ - Global step 150 Train loss 14.454837 Classification-F1 0.0 on epoch=74
03/18/2022 23:15:11 - INFO - __main__ - Step 160 Global step 160 Train loss 12.838959 on epoch=79
03/18/2022 23:15:16 - INFO - __main__ - Step 170 Global step 170 Train loss 12.346292 on epoch=84
03/18/2022 23:15:22 - INFO - __main__ - Step 180 Global step 180 Train loss 12.302771 on epoch=89
03/18/2022 23:15:27 - INFO - __main__ - Step 190 Global step 190 Train loss 11.075781 on epoch=94
03/18/2022 23:15:32 - INFO - __main__ - Step 200 Global step 200 Train loss 8.689818 on epoch=99
03/18/2022 23:15:41 - INFO - __main__ - Global step 200 Train loss 11.450726 Classification-F1 0.0 on epoch=99
03/18/2022 23:15:46 - INFO - __main__ - Step 210 Global step 210 Train loss 4.031852 on epoch=104
03/18/2022 23:15:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.838344 on epoch=109
03/18/2022 23:15:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.602468 on epoch=114
03/18/2022 23:16:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.499244 on epoch=119
03/18/2022 23:16:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.455683 on epoch=124
03/18/2022 23:16:07 - INFO - __main__ - Global step 250 Train loss 1.285518 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 23:16:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.590297 on epoch=129
03/18/2022 23:16:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.438257 on epoch=134
03/18/2022 23:16:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.460068 on epoch=139
03/18/2022 23:16:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.436295 on epoch=144
03/18/2022 23:16:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.325488 on epoch=149
03/18/2022 23:16:33 - INFO - __main__ - Global step 300 Train loss 0.450081 Classification-F1 0.3816425120772947 on epoch=149
03/18/2022 23:16:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.428950 on epoch=154
03/18/2022 23:16:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.303905 on epoch=159
03/18/2022 23:16:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.316245 on epoch=164
03/18/2022 23:16:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.282272 on epoch=169
03/18/2022 23:16:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.391021 on epoch=174
03/18/2022 23:17:00 - INFO - __main__ - Global step 350 Train loss 0.344479 Classification-F1 0.3191489361702127 on epoch=174
03/18/2022 23:17:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.280866 on epoch=179
03/18/2022 23:17:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.289668 on epoch=184
03/18/2022 23:17:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.161713 on epoch=189
03/18/2022 23:17:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.189220 on epoch=194
03/18/2022 23:17:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.136644 on epoch=199
03/18/2022 23:17:25 - INFO - __main__ - Global step 400 Train loss 0.211623 Classification-F1 0.4920634920634921 on epoch=199
03/18/2022 23:17:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.109692 on epoch=204
03/18/2022 23:17:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.104469 on epoch=209
03/18/2022 23:17:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.154588 on epoch=214
03/18/2022 23:17:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.088500 on epoch=219
03/18/2022 23:17:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.057330 on epoch=224
03/18/2022 23:17:51 - INFO - __main__ - Global step 450 Train loss 0.102916 Classification-F1 0.4666666666666667 on epoch=224
03/18/2022 23:17:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.083140 on epoch=229
03/18/2022 23:18:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034003 on epoch=234
03/18/2022 23:18:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.056970 on epoch=239
03/18/2022 23:18:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.023689 on epoch=244
03/18/2022 23:18:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.025670 on epoch=249
03/18/2022 23:18:17 - INFO - __main__ - Global step 500 Train loss 0.044694 Classification-F1 0.5195195195195195 on epoch=249
03/18/2022 23:18:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.079282 on epoch=254
03/18/2022 23:18:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.143969 on epoch=259
03/18/2022 23:18:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.011503 on epoch=264
03/18/2022 23:18:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.020894 on epoch=269
03/18/2022 23:18:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022070 on epoch=274
03/18/2022 23:18:43 - INFO - __main__ - Global step 550 Train loss 0.055544 Classification-F1 0.4920634920634921 on epoch=274
03/18/2022 23:18:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008198 on epoch=279
03/18/2022 23:18:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.027331 on epoch=284
03/18/2022 23:18:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.052629 on epoch=289
03/18/2022 23:19:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.007628 on epoch=294
03/18/2022 23:19:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003044 on epoch=299
03/18/2022 23:19:08 - INFO - __main__ - Global step 600 Train loss 0.019766 Classification-F1 0.5195195195195195 on epoch=299
03/18/2022 23:19:08 - INFO - __main__ - save last model!
03/18/2022 23:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:19:09 - INFO - __main__ - Printing 3 examples
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:19:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:19:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:19:09 - INFO - __main__ - Printing 3 examples
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:19:09 - INFO - __main__ - ['false']
03/18/2022 23:19:09 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:19:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:19:09 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:19:15 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:19:16 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:19:16 - INFO - __main__ - Printing 3 examples
03/18/2022 23:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:19:16 - INFO - __main__ - ['false']
03/18/2022 23:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:19:16 - INFO - __main__ - ['false']
03/18/2022 23:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:19:16 - INFO - __main__ - ['false']
03/18/2022 23:19:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:19:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:19:20 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:19:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:19:22 - INFO - __main__ - Starting training!
03/18/2022 23:19:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_21_0.0001_8_predictions.txt
03/18/2022 23:19:49 - INFO - __main__ - Classification-F1 on test data: 0.2523
03/18/2022 23:19:50 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.0001, bsz=8, dev_performance=0.5195195195195195, test_performance=0.2522640168896656
03/18/2022 23:19:50 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.0005, bsz=8 ...
03/18/2022 23:19:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:19:51 - INFO - __main__ - Printing 3 examples
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:19:51 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:19:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:19:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:19:51 - INFO - __main__ - Printing 3 examples
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:19:51 - INFO - __main__ - ['false']
03/18/2022 23:19:51 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:19:51 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:19:51 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:20:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:20:01 - INFO - __main__ - Starting training!
03/18/2022 23:20:06 - INFO - __main__ - Step 10 Global step 10 Train loss 22.258427 on epoch=4
03/18/2022 23:20:10 - INFO - __main__ - Step 20 Global step 20 Train loss 18.678082 on epoch=9
03/18/2022 23:20:15 - INFO - __main__ - Step 30 Global step 30 Train loss 16.441980 on epoch=14
03/18/2022 23:20:21 - INFO - __main__ - Step 40 Global step 40 Train loss 14.715620 on epoch=19
03/18/2022 23:20:26 - INFO - __main__ - Step 50 Global step 50 Train loss 12.246337 on epoch=24
03/18/2022 23:20:29 - INFO - __main__ - Global step 50 Train loss 16.868088 Classification-F1 0.010025062656641603 on epoch=24
03/18/2022 23:20:34 - INFO - __main__ - Step 60 Global step 60 Train loss 6.385662 on epoch=29
03/18/2022 23:20:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.906226 on epoch=34
03/18/2022 23:20:45 - INFO - __main__ - Step 80 Global step 80 Train loss 2.408628 on epoch=39
03/18/2022 23:20:50 - INFO - __main__ - Step 90 Global step 90 Train loss 2.320304 on epoch=44
03/18/2022 23:20:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.629875 on epoch=49
03/18/2022 23:20:55 - INFO - __main__ - Global step 100 Train loss 3.130139 Classification-F1 0.4666666666666667 on epoch=49
03/18/2022 23:21:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.816034 on epoch=54
03/18/2022 23:21:06 - INFO - __main__ - Step 120 Global step 120 Train loss 2.645248 on epoch=59
03/18/2022 23:21:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.618944 on epoch=64
03/18/2022 23:21:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.005373 on epoch=69
03/18/2022 23:21:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.949400 on epoch=74
03/18/2022 23:21:22 - INFO - __main__ - Global step 150 Train loss 1.607000 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 23:21:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.457077 on epoch=79
03/18/2022 23:21:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.467709 on epoch=84
03/18/2022 23:21:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.375277 on epoch=89
03/18/2022 23:21:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.377260 on epoch=94
03/18/2022 23:21:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.358756 on epoch=99
03/18/2022 23:21:47 - INFO - __main__ - Global step 200 Train loss 0.407216 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 23:21:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.341988 on epoch=104
03/18/2022 23:21:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.343243 on epoch=109
03/18/2022 23:22:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.308180 on epoch=114
03/18/2022 23:22:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.234740 on epoch=119
03/18/2022 23:22:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.292624 on epoch=124
03/18/2022 23:22:13 - INFO - __main__ - Global step 250 Train loss 0.304155 Classification-F1 0.49090909090909085 on epoch=124
03/18/2022 23:22:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.286298 on epoch=129
03/18/2022 23:22:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.344242 on epoch=134
03/18/2022 23:22:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.251409 on epoch=139
03/18/2022 23:22:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.188951 on epoch=144
03/18/2022 23:22:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.306049 on epoch=149
03/18/2022 23:22:40 - INFO - __main__ - Global step 300 Train loss 0.275390 Classification-F1 0.49090909090909085 on epoch=149
03/18/2022 23:22:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.316486 on epoch=154
03/18/2022 23:22:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.235701 on epoch=159
03/18/2022 23:22:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.163785 on epoch=164
03/18/2022 23:23:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.167296 on epoch=169
03/18/2022 23:23:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.134928 on epoch=174
03/18/2022 23:23:06 - INFO - __main__ - Global step 350 Train loss 0.203639 Classification-F1 0.6235294117647059 on epoch=174
03/18/2022 23:23:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.096722 on epoch=179
03/18/2022 23:23:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.272759 on epoch=184
03/18/2022 23:23:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.092419 on epoch=189
03/18/2022 23:23:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.093770 on epoch=194
03/18/2022 23:23:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.121212 on epoch=199
03/18/2022 23:23:33 - INFO - __main__ - Global step 400 Train loss 0.135376 Classification-F1 0.6825396825396826 on epoch=199
03/18/2022 23:23:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.179895 on epoch=204
03/18/2022 23:23:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.115842 on epoch=209
03/18/2022 23:23:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.197382 on epoch=214
03/18/2022 23:23:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.062731 on epoch=219
03/18/2022 23:23:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.050310 on epoch=224
03/18/2022 23:24:00 - INFO - __main__ - Global step 450 Train loss 0.121232 Classification-F1 0.6559139784946237 on epoch=224
03/18/2022 23:24:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.064132 on epoch=229
03/18/2022 23:24:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116580 on epoch=234
03/18/2022 23:24:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.121752 on epoch=239
03/18/2022 23:24:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.031026 on epoch=244
03/18/2022 23:24:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.048211 on epoch=249
03/18/2022 23:24:26 - INFO - __main__ - Global step 500 Train loss 0.076340 Classification-F1 0.6875 on epoch=249
03/18/2022 23:24:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.041355 on epoch=254
03/18/2022 23:24:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.183259 on epoch=259
03/18/2022 23:24:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.088338 on epoch=264
03/18/2022 23:24:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.082120 on epoch=269
03/18/2022 23:24:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.041410 on epoch=274
03/18/2022 23:24:52 - INFO - __main__ - Global step 550 Train loss 0.087296 Classification-F1 0.5607843137254902 on epoch=274
03/18/2022 23:24:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.022299 on epoch=279
03/18/2022 23:25:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.013159 on epoch=284
03/18/2022 23:25:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021203 on epoch=289
03/18/2022 23:25:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.026199 on epoch=294
03/18/2022 23:25:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.038430 on epoch=299
03/18/2022 23:25:18 - INFO - __main__ - Global step 600 Train loss 0.024258 Classification-F1 0.6235294117647059 on epoch=299
03/18/2022 23:25:18 - INFO - __main__ - save last model!
03/18/2022 23:25:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:25:19 - INFO - __main__ - Printing 3 examples
03/18/2022 23:25:19 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:25:19 - INFO - __main__ - ['false']
03/18/2022 23:25:19 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:25:19 - INFO - __main__ - ['false']
03/18/2022 23:25:19 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:25:19 - INFO - __main__ - ['false']
03/18/2022 23:25:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:25:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:25:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:25:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:25:20 - INFO - __main__ - Printing 3 examples
03/18/2022 23:25:20 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:25:20 - INFO - __main__ - ['false']
03/18/2022 23:25:20 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:25:20 - INFO - __main__ - ['false']
03/18/2022 23:25:20 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:25:20 - INFO - __main__ - ['false']
03/18/2022 23:25:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:25:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:25:20 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:25:25 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:25:26 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:25:26 - INFO - __main__ - Printing 3 examples
03/18/2022 23:25:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:25:26 - INFO - __main__ - ['false']
03/18/2022 23:25:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:25:26 - INFO - __main__ - ['false']
03/18/2022 23:25:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:25:26 - INFO - __main__ - ['false']
03/18/2022 23:25:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:25:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:25:30 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:25:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:25:31 - INFO - __main__ - Starting training!
03/18/2022 23:25:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_42_0.0005_8_predictions.txt
03/18/2022 23:25:59 - INFO - __main__ - Classification-F1 on test data: 0.4314
03/18/2022 23:25:59 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.43137433151012144
03/18/2022 23:25:59 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.0003, bsz=8 ...
03/18/2022 23:26:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:26:00 - INFO - __main__ - Printing 3 examples
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:26:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:26:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:26:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:26:00 - INFO - __main__ - Printing 3 examples
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:26:00 - INFO - __main__ - ['false']
03/18/2022 23:26:00 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:26:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:26:00 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:26:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:26:13 - INFO - __main__ - Starting training!
03/18/2022 23:26:17 - INFO - __main__ - Step 10 Global step 10 Train loss 22.431967 on epoch=4
03/18/2022 23:26:22 - INFO - __main__ - Step 20 Global step 20 Train loss 18.658504 on epoch=9
03/18/2022 23:26:27 - INFO - __main__ - Step 30 Global step 30 Train loss 16.709791 on epoch=14
03/18/2022 23:26:32 - INFO - __main__ - Step 40 Global step 40 Train loss 14.844675 on epoch=19
03/18/2022 23:26:37 - INFO - __main__ - Step 50 Global step 50 Train loss 13.796237 on epoch=24
03/18/2022 23:26:40 - INFO - __main__ - Global step 50 Train loss 17.288235 Classification-F1 0.0 on epoch=24
03/18/2022 23:26:45 - INFO - __main__ - Step 60 Global step 60 Train loss 12.896459 on epoch=29
03/18/2022 23:26:50 - INFO - __main__ - Step 70 Global step 70 Train loss 8.625734 on epoch=34
03/18/2022 23:26:55 - INFO - __main__ - Step 80 Global step 80 Train loss 2.864511 on epoch=39
03/18/2022 23:27:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.860282 on epoch=44
03/18/2022 23:27:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.448122 on epoch=49
03/18/2022 23:27:06 - INFO - __main__ - Global step 100 Train loss 5.139021 Classification-F1 0.4589371980676329 on epoch=49
03/18/2022 23:27:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.270876 on epoch=54
03/18/2022 23:27:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.096840 on epoch=59
03/18/2022 23:27:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.097179 on epoch=64
03/18/2022 23:27:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.041050 on epoch=69
03/18/2022 23:27:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.040837 on epoch=74
03/18/2022 23:27:32 - INFO - __main__ - Global step 150 Train loss 0.109356 Classification-F1 0.6190476190476191 on epoch=74
03/18/2022 23:27:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.031824 on epoch=79
03/18/2022 23:27:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.182568 on epoch=84
03/18/2022 23:27:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.394159 on epoch=89
03/18/2022 23:27:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.452653 on epoch=94
03/18/2022 23:27:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.354563 on epoch=99
03/18/2022 23:27:58 - INFO - __main__ - Global step 200 Train loss 0.283153 Classification-F1 0.6389743589743591 on epoch=99
03/18/2022 23:28:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.338296 on epoch=104
03/18/2022 23:28:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.309696 on epoch=109
03/18/2022 23:28:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.380216 on epoch=114
03/18/2022 23:28:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.381967 on epoch=119
03/18/2022 23:28:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.348721 on epoch=124
03/18/2022 23:28:24 - INFO - __main__ - Global step 250 Train loss 0.351779 Classification-F1 0.4231177094379639 on epoch=124
03/18/2022 23:28:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.280567 on epoch=129
03/18/2022 23:28:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.308719 on epoch=134
03/18/2022 23:28:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.298515 on epoch=139
03/18/2022 23:28:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.256820 on epoch=144
03/18/2022 23:28:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.249467 on epoch=149
03/18/2022 23:28:50 - INFO - __main__ - Global step 300 Train loss 0.278818 Classification-F1 0.3816425120772947 on epoch=149
03/18/2022 23:28:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.313958 on epoch=154
03/18/2022 23:29:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.316720 on epoch=159
03/18/2022 23:29:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.175613 on epoch=164
03/18/2022 23:29:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.111356 on epoch=169
03/18/2022 23:29:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.083005 on epoch=174
03/18/2022 23:29:16 - INFO - __main__ - Global step 350 Train loss 0.200130 Classification-F1 0.4980392156862745 on epoch=174
03/18/2022 23:29:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.073063 on epoch=179
03/18/2022 23:29:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.101908 on epoch=184
03/18/2022 23:29:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.029316 on epoch=189
03/18/2022 23:29:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.035565 on epoch=194
03/18/2022 23:29:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.026202 on epoch=199
03/18/2022 23:29:41 - INFO - __main__ - Global step 400 Train loss 0.053211 Classification-F1 0.5195195195195195 on epoch=199
03/18/2022 23:29:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.018941 on epoch=204
03/18/2022 23:29:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.083643 on epoch=209
03/18/2022 23:29:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.025079 on epoch=214
03/18/2022 23:30:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.020307 on epoch=219
03/18/2022 23:30:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.039586 on epoch=224
03/18/2022 23:30:07 - INFO - __main__ - Global step 450 Train loss 0.037511 Classification-F1 0.5625 on epoch=224
03/18/2022 23:30:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.055119 on epoch=229
03/18/2022 23:30:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.012257 on epoch=234
03/18/2022 23:30:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.046751 on epoch=239
03/18/2022 23:30:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.018172 on epoch=244
03/18/2022 23:30:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.023737 on epoch=249
03/18/2022 23:30:32 - INFO - __main__ - Global step 500 Train loss 0.031207 Classification-F1 0.5933528836754642 on epoch=249
03/18/2022 23:30:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005760 on epoch=254
03/18/2022 23:30:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.038469 on epoch=259
03/18/2022 23:30:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.036380 on epoch=264
03/18/2022 23:30:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.059711 on epoch=269
03/18/2022 23:30:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.062097 on epoch=274
03/18/2022 23:30:58 - INFO - __main__ - Global step 550 Train loss 0.040483 Classification-F1 0.5195195195195195 on epoch=274
03/18/2022 23:31:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008440 on epoch=279
03/18/2022 23:31:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.008781 on epoch=284
03/18/2022 23:31:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003980 on epoch=289
03/18/2022 23:31:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001322 on epoch=294
03/18/2022 23:31:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.023822 on epoch=299
03/18/2022 23:31:24 - INFO - __main__ - Global step 600 Train loss 0.009269 Classification-F1 0.5307917888563051 on epoch=299
03/18/2022 23:31:24 - INFO - __main__ - save last model!
03/18/2022 23:31:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:31:24 - INFO - __main__ - Printing 3 examples
03/18/2022 23:31:24 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:31:24 - INFO - __main__ - ['false']
03/18/2022 23:31:24 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:31:24 - INFO - __main__ - ['false']
03/18/2022 23:31:24 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:31:24 - INFO - __main__ - ['false']
03/18/2022 23:31:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:31:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:31:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:31:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:31:25 - INFO - __main__ - Printing 3 examples
03/18/2022 23:31:25 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:31:25 - INFO - __main__ - ['false']
03/18/2022 23:31:25 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:31:25 - INFO - __main__ - ['false']
03/18/2022 23:31:25 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:31:25 - INFO - __main__ - ['false']
03/18/2022 23:31:25 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:31:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:31:25 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:31:31 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:31:31 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:31:31 - INFO - __main__ - Printing 3 examples
03/18/2022 23:31:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:31:31 - INFO - __main__ - ['false']
03/18/2022 23:31:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:31:31 - INFO - __main__ - ['false']
03/18/2022 23:31:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:31:31 - INFO - __main__ - ['false']
03/18/2022 23:31:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:31:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:31:35 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:31:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:31:37 - INFO - __main__ - Starting training!
03/18/2022 23:32:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_42_0.0003_8_predictions.txt
03/18/2022 23:32:05 - INFO - __main__ - Classification-F1 on test data: 0.4568
03/18/2022 23:32:05 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.0003, bsz=8, dev_performance=0.6389743589743591, test_performance=0.45676894553107616
03/18/2022 23:32:05 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.0002, bsz=8 ...
03/18/2022 23:32:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:32:06 - INFO - __main__ - Printing 3 examples
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:32:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:32:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:32:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:32:06 - INFO - __main__ - Printing 3 examples
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:32:06 - INFO - __main__ - ['false']
03/18/2022 23:32:06 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:32:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:32:06 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:32:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:32:17 - INFO - __main__ - Starting training!
03/18/2022 23:32:23 - INFO - __main__ - Step 10 Global step 10 Train loss 23.728340 on epoch=4
03/18/2022 23:32:28 - INFO - __main__ - Step 20 Global step 20 Train loss 18.911280 on epoch=9
03/18/2022 23:32:33 - INFO - __main__ - Step 30 Global step 30 Train loss 17.259668 on epoch=14
03/18/2022 23:32:38 - INFO - __main__ - Step 40 Global step 40 Train loss 16.898275 on epoch=19
03/18/2022 23:32:43 - INFO - __main__ - Step 50 Global step 50 Train loss 15.866475 on epoch=24
03/18/2022 23:32:44 - INFO - __main__ - Global step 50 Train loss 18.532806 Classification-F1 0.0 on epoch=24
03/18/2022 23:32:50 - INFO - __main__ - Step 60 Global step 60 Train loss 14.251544 on epoch=29
03/18/2022 23:32:55 - INFO - __main__ - Step 70 Global step 70 Train loss 14.653707 on epoch=34
03/18/2022 23:33:00 - INFO - __main__ - Step 80 Global step 80 Train loss 13.939911 on epoch=39
03/18/2022 23:33:05 - INFO - __main__ - Step 90 Global step 90 Train loss 13.740560 on epoch=44
03/18/2022 23:33:10 - INFO - __main__ - Step 100 Global step 100 Train loss 12.510347 on epoch=49
03/18/2022 23:33:10 - INFO - __main__ - Global step 100 Train loss 13.819214 Classification-F1 0.0 on epoch=49
03/18/2022 23:33:15 - INFO - __main__ - Step 110 Global step 110 Train loss 9.321265 on epoch=54
03/18/2022 23:33:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.600734 on epoch=59
03/18/2022 23:33:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.659446 on epoch=64
03/18/2022 23:33:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.547065 on epoch=69
03/18/2022 23:33:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.361272 on epoch=74
03/18/2022 23:33:36 - INFO - __main__ - Global step 150 Train loss 2.497957 Classification-F1 0.3650793650793651 on epoch=74
03/18/2022 23:33:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.444571 on epoch=79
03/18/2022 23:33:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.194823 on epoch=84
03/18/2022 23:33:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.277786 on epoch=89
03/18/2022 23:33:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.380409 on epoch=94
03/18/2022 23:34:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.366694 on epoch=99
03/18/2022 23:34:02 - INFO - __main__ - Global step 200 Train loss 0.332857 Classification-F1 0.4980392156862745 on epoch=99
03/18/2022 23:34:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.553510 on epoch=104
03/18/2022 23:34:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.181865 on epoch=109
03/18/2022 23:34:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.085750 on epoch=114
03/18/2022 23:34:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.071450 on epoch=119
03/18/2022 23:34:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.033465 on epoch=124
03/18/2022 23:34:28 - INFO - __main__ - Global step 250 Train loss 0.185208 Classification-F1 0.4980392156862745 on epoch=124
03/18/2022 23:34:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.012534 on epoch=129
03/18/2022 23:34:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.025882 on epoch=134
03/18/2022 23:34:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.028006 on epoch=139
03/18/2022 23:34:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.014419 on epoch=144
03/18/2022 23:34:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.006522 on epoch=149
03/18/2022 23:34:53 - INFO - __main__ - Global step 300 Train loss 0.017473 Classification-F1 0.5835835835835835 on epoch=149
03/18/2022 23:35:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003123 on epoch=154
03/18/2022 23:35:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003393 on epoch=159
03/18/2022 23:35:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001780 on epoch=164
03/18/2022 23:35:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001796 on epoch=169
03/18/2022 23:35:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001568 on epoch=174
03/18/2022 23:35:20 - INFO - __main__ - Global step 350 Train loss 0.002332 Classification-F1 0.5076923076923077 on epoch=174
03/18/2022 23:35:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006707 on epoch=179
03/18/2022 23:35:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.005766 on epoch=184
03/18/2022 23:35:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003999 on epoch=189
03/18/2022 23:35:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000628 on epoch=194
03/18/2022 23:35:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001875 on epoch=199
03/18/2022 23:35:45 - INFO - __main__ - Global step 400 Train loss 0.003795 Classification-F1 0.5901477832512315 on epoch=199
03/18/2022 23:35:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002918 on epoch=204
03/18/2022 23:35:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000410 on epoch=209
03/18/2022 23:36:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001140 on epoch=214
03/18/2022 23:36:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000364 on epoch=219
03/18/2022 23:36:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000264 on epoch=224
03/18/2022 23:36:12 - INFO - __main__ - Global step 450 Train loss 0.001019 Classification-F1 0.5607843137254902 on epoch=224
03/18/2022 23:36:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002432 on epoch=229
03/18/2022 23:36:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001482 on epoch=234
03/18/2022 23:36:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008174 on epoch=239
03/18/2022 23:36:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000536 on epoch=244
03/18/2022 23:36:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000424 on epoch=249
03/18/2022 23:36:37 - INFO - __main__ - Global step 500 Train loss 0.002610 Classification-F1 0.6235294117647059 on epoch=249
03/18/2022 23:36:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000299 on epoch=254
03/18/2022 23:36:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000414 on epoch=259
03/18/2022 23:36:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000445 on epoch=264
03/18/2022 23:36:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000545 on epoch=269
03/18/2022 23:37:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000751 on epoch=274
03/18/2022 23:37:03 - INFO - __main__ - Global step 550 Train loss 0.000491 Classification-F1 0.5607843137254902 on epoch=274
03/18/2022 23:37:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000265 on epoch=279
03/18/2022 23:37:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000411 on epoch=284
03/18/2022 23:37:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002740 on epoch=289
03/18/2022 23:37:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000183 on epoch=294
03/18/2022 23:37:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000212 on epoch=299
03/18/2022 23:37:29 - INFO - __main__ - Global step 600 Train loss 0.000762 Classification-F1 0.4980392156862745 on epoch=299
03/18/2022 23:37:29 - INFO - __main__ - save last model!
03/18/2022 23:37:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:37:30 - INFO - __main__ - Printing 3 examples
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:37:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:37:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:37:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:37:30 - INFO - __main__ - Printing 3 examples
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:37:30 - INFO - __main__ - ['false']
03/18/2022 23:37:30 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:37:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:37:30 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:37:36 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:37:37 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:37:37 - INFO - __main__ - Printing 3 examples
03/18/2022 23:37:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:37:37 - INFO - __main__ - ['false']
03/18/2022 23:37:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:37:37 - INFO - __main__ - ['false']
03/18/2022 23:37:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:37:37 - INFO - __main__ - ['false']
03/18/2022 23:37:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:37:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:37:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:37:41 - INFO - __main__ - Starting training!
03/18/2022 23:37:41 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:38:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_42_0.0002_8_predictions.txt
03/18/2022 23:38:12 - INFO - __main__ - Classification-F1 on test data: 0.0970
03/18/2022 23:38:12 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.0002, bsz=8, dev_performance=0.6235294117647059, test_performance=0.09700539444600527
03/18/2022 23:38:12 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.0001, bsz=8 ...
03/18/2022 23:38:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:38:13 - INFO - __main__ - Printing 3 examples
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:38:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:38:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:38:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:38:13 - INFO - __main__ - Printing 3 examples
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 23:38:13 - INFO - __main__ - ['false']
03/18/2022 23:38:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:38:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:38:13 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:38:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:38:24 - INFO - __main__ - Starting training!
03/18/2022 23:38:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.010180 on epoch=4
03/18/2022 23:38:33 - INFO - __main__ - Step 20 Global step 20 Train loss 20.247078 on epoch=9
03/18/2022 23:38:38 - INFO - __main__ - Step 30 Global step 30 Train loss 17.313637 on epoch=14
03/18/2022 23:38:43 - INFO - __main__ - Step 40 Global step 40 Train loss 17.400436 on epoch=19
03/18/2022 23:38:48 - INFO - __main__ - Step 50 Global step 50 Train loss 17.747471 on epoch=24
03/18/2022 23:38:53 - INFO - __main__ - Global step 50 Train loss 19.143759 Classification-F1 0.0 on epoch=24
03/18/2022 23:38:59 - INFO - __main__ - Step 60 Global step 60 Train loss 16.107342 on epoch=29
03/18/2022 23:39:04 - INFO - __main__ - Step 70 Global step 70 Train loss 16.119787 on epoch=34
03/18/2022 23:39:09 - INFO - __main__ - Step 80 Global step 80 Train loss 15.768122 on epoch=39
03/18/2022 23:39:14 - INFO - __main__ - Step 90 Global step 90 Train loss 15.041908 on epoch=44
03/18/2022 23:39:19 - INFO - __main__ - Step 100 Global step 100 Train loss 15.792549 on epoch=49
03/18/2022 23:39:27 - INFO - __main__ - Global step 100 Train loss 15.765943 Classification-F1 0.0 on epoch=49
03/18/2022 23:39:32 - INFO - __main__ - Step 110 Global step 110 Train loss 14.482320 on epoch=54
03/18/2022 23:39:37 - INFO - __main__ - Step 120 Global step 120 Train loss 14.143576 on epoch=59
03/18/2022 23:39:42 - INFO - __main__ - Step 130 Global step 130 Train loss 13.907404 on epoch=64
03/18/2022 23:39:47 - INFO - __main__ - Step 140 Global step 140 Train loss 12.483773 on epoch=69
03/18/2022 23:39:52 - INFO - __main__ - Step 150 Global step 150 Train loss 12.925145 on epoch=74
03/18/2022 23:39:59 - INFO - __main__ - Global step 150 Train loss 13.588444 Classification-F1 0.0 on epoch=74
03/18/2022 23:40:05 - INFO - __main__ - Step 160 Global step 160 Train loss 12.652988 on epoch=79
03/18/2022 23:40:10 - INFO - __main__ - Step 170 Global step 170 Train loss 11.653440 on epoch=84
03/18/2022 23:40:15 - INFO - __main__ - Step 180 Global step 180 Train loss 10.267258 on epoch=89
03/18/2022 23:40:20 - INFO - __main__ - Step 190 Global step 190 Train loss 5.779150 on epoch=94
03/18/2022 23:40:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.980385 on epoch=99
03/18/2022 23:40:25 - INFO - __main__ - Global step 200 Train loss 8.266644 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 23:40:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.714315 on epoch=104
03/18/2022 23:40:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.497350 on epoch=109
03/18/2022 23:40:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.448490 on epoch=114
03/18/2022 23:40:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.424729 on epoch=119
03/18/2022 23:40:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.370659 on epoch=124
03/18/2022 23:40:52 - INFO - __main__ - Global step 250 Train loss 0.491109 Classification-F1 0.6761133603238867 on epoch=124
03/18/2022 23:40:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.252287 on epoch=129
03/18/2022 23:41:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.246198 on epoch=134
03/18/2022 23:41:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.180954 on epoch=139
03/18/2022 23:41:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.154087 on epoch=144
03/18/2022 23:41:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.202976 on epoch=149
03/18/2022 23:41:18 - INFO - __main__ - Global step 300 Train loss 0.207300 Classification-F1 0.716256157635468 on epoch=149
03/18/2022 23:41:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.111539 on epoch=154
03/18/2022 23:41:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.066461 on epoch=159
03/18/2022 23:41:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.061699 on epoch=164
03/18/2022 23:41:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.044618 on epoch=169
03/18/2022 23:41:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.033267 on epoch=174
03/18/2022 23:41:45 - INFO - __main__ - Global step 350 Train loss 0.063517 Classification-F1 0.6559139784946237 on epoch=174
03/18/2022 23:41:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.022846 on epoch=179
03/18/2022 23:41:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.027378 on epoch=184
03/18/2022 23:42:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025879 on epoch=189
03/18/2022 23:42:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.023875 on epoch=194
03/18/2022 23:42:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015763 on epoch=199
03/18/2022 23:42:10 - INFO - __main__ - Global step 400 Train loss 0.023148 Classification-F1 0.625 on epoch=199
03/18/2022 23:42:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.009014 on epoch=204
03/18/2022 23:42:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.027952 on epoch=209
03/18/2022 23:42:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.006897 on epoch=214
03/18/2022 23:42:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006805 on epoch=219
03/18/2022 23:42:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004480 on epoch=224
03/18/2022 23:42:36 - INFO - __main__ - Global step 450 Train loss 0.011029 Classification-F1 0.625 on epoch=224
03/18/2022 23:42:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.006825 on epoch=229
03/18/2022 23:42:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002777 on epoch=234
03/18/2022 23:42:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004516 on epoch=239
03/18/2022 23:42:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.006194 on epoch=244
03/18/2022 23:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.009358 on epoch=249
03/18/2022 23:43:02 - INFO - __main__ - Global step 500 Train loss 0.005934 Classification-F1 0.625 on epoch=249
03/18/2022 23:43:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001379 on epoch=254
03/18/2022 23:43:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004025 on epoch=259
03/18/2022 23:43:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003224 on epoch=264
03/18/2022 23:43:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001437 on epoch=269
03/18/2022 23:43:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003481 on epoch=274
03/18/2022 23:43:27 - INFO - __main__ - Global step 550 Train loss 0.002709 Classification-F1 0.5607843137254902 on epoch=274
03/18/2022 23:43:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007267 on epoch=279
03/18/2022 23:43:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001596 on epoch=284
03/18/2022 23:43:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.139988 on epoch=289
03/18/2022 23:43:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001443 on epoch=294
03/18/2022 23:43:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001034 on epoch=299
03/18/2022 23:43:53 - INFO - __main__ - Global step 600 Train loss 0.030266 Classification-F1 0.5607843137254902 on epoch=299
03/18/2022 23:43:53 - INFO - __main__ - save last model!
03/18/2022 23:43:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:43:54 - INFO - __main__ - Printing 3 examples
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:43:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:43:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:43:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:43:54 - INFO - __main__ - Printing 3 examples
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:43:54 - INFO - __main__ - ['false']
03/18/2022 23:43:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:43:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:43:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:44:00 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:44:00 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:44:00 - INFO - __main__ - Printing 3 examples
03/18/2022 23:44:00 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:44:00 - INFO - __main__ - ['false']
03/18/2022 23:44:00 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:44:00 - INFO - __main__ - ['false']
03/18/2022 23:44:00 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:44:00 - INFO - __main__ - ['false']
03/18/2022 23:44:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:44:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:44:05 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:44:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:44:06 - INFO - __main__ - Starting training!
03/18/2022 23:44:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_42_0.0001_8_predictions.txt
03/18/2022 23:44:35 - INFO - __main__ - Classification-F1 on test data: 0.2347
03/18/2022 23:44:35 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.0001, bsz=8, dev_performance=0.716256157635468, test_performance=0.23473670288795498
03/18/2022 23:44:35 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.0005, bsz=8 ...
03/18/2022 23:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:44:36 - INFO - __main__ - Printing 3 examples
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:44:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:44:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:44:36 - INFO - __main__ - Printing 3 examples
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:44:36 - INFO - __main__ - ['false']
03/18/2022 23:44:36 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:44:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:44:36 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:44:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:44:49 - INFO - __main__ - Starting training!
03/18/2022 23:44:54 - INFO - __main__ - Step 10 Global step 10 Train loss 22.911121 on epoch=4
03/18/2022 23:44:59 - INFO - __main__ - Step 20 Global step 20 Train loss 18.359507 on epoch=9
03/18/2022 23:45:04 - INFO - __main__ - Step 30 Global step 30 Train loss 15.829916 on epoch=14
03/18/2022 23:45:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.217108 on epoch=19
03/18/2022 23:45:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.797812 on epoch=24
03/18/2022 23:45:28 - INFO - __main__ - Global step 50 Train loss 16.623095 Classification-F1 0.0 on epoch=24
03/18/2022 23:45:34 - INFO - __main__ - Step 60 Global step 60 Train loss 5.976547 on epoch=29
03/18/2022 23:45:39 - INFO - __main__ - Step 70 Global step 70 Train loss 3.232598 on epoch=34
03/18/2022 23:45:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.705507 on epoch=39
03/18/2022 23:45:49 - INFO - __main__ - Step 90 Global step 90 Train loss 2.962477 on epoch=44
03/18/2022 23:45:54 - INFO - __main__ - Step 100 Global step 100 Train loss 2.891619 on epoch=49
03/18/2022 23:45:54 - INFO - __main__ - Global step 100 Train loss 3.553750 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 23:46:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.859576 on epoch=54
03/18/2022 23:46:06 - INFO - __main__ - Step 120 Global step 120 Train loss 2.182469 on epoch=59
03/18/2022 23:46:11 - INFO - __main__ - Step 130 Global step 130 Train loss 3.026161 on epoch=64
03/18/2022 23:46:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.636488 on epoch=69
03/18/2022 23:46:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.502214 on epoch=74
03/18/2022 23:46:21 - INFO - __main__ - Global step 150 Train loss 1.641382 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 23:46:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.459276 on epoch=79
03/18/2022 23:46:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.392807 on epoch=84
03/18/2022 23:46:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.363885 on epoch=89
03/18/2022 23:46:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.353493 on epoch=94
03/18/2022 23:46:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.334005 on epoch=99
03/18/2022 23:46:47 - INFO - __main__ - Global step 200 Train loss 0.380693 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 23:46:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.339980 on epoch=104
03/18/2022 23:46:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.375373 on epoch=109
03/18/2022 23:47:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.360214 on epoch=114
03/18/2022 23:47:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.337162 on epoch=119
03/18/2022 23:47:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.336057 on epoch=124
03/18/2022 23:47:12 - INFO - __main__ - Global step 250 Train loss 0.349757 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 23:47:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.330050 on epoch=129
03/18/2022 23:47:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.295382 on epoch=134
03/18/2022 23:47:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.355825 on epoch=139
03/18/2022 23:47:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.344424 on epoch=144
03/18/2022 23:47:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.397807 on epoch=149
03/18/2022 23:47:38 - INFO - __main__ - Global step 300 Train loss 0.344698 Classification-F1 0.5555555555555556 on epoch=149
03/18/2022 23:47:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.333730 on epoch=154
03/18/2022 23:47:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.389545 on epoch=159
03/18/2022 23:47:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.330654 on epoch=164
03/18/2022 23:47:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.405929 on epoch=169
03/18/2022 23:48:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.319203 on epoch=174
03/18/2022 23:48:04 - INFO - __main__ - Global step 350 Train loss 0.355812 Classification-F1 0.4666666666666667 on epoch=174
03/18/2022 23:48:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.285859 on epoch=179
03/18/2022 23:48:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.320049 on epoch=184
03/18/2022 23:48:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.209116 on epoch=189
03/18/2022 23:48:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.114014 on epoch=194
03/18/2022 23:48:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.092087 on epoch=199
03/18/2022 23:48:30 - INFO - __main__ - Global step 400 Train loss 0.204225 Classification-F1 0.4909862142099682 on epoch=199
03/18/2022 23:48:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003186 on epoch=204
03/18/2022 23:48:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001524 on epoch=209
03/18/2022 23:48:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001249 on epoch=214
03/18/2022 23:48:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000785 on epoch=219
03/18/2022 23:48:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000674 on epoch=224
03/18/2022 23:48:55 - INFO - __main__ - Global step 450 Train loss 0.001484 Classification-F1 0.5151515151515151 on epoch=224
03/18/2022 23:49:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000810 on epoch=229
03/18/2022 23:49:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000385 on epoch=234
03/18/2022 23:49:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.194471 on epoch=239
03/18/2022 23:49:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.015011 on epoch=244
03/18/2022 23:49:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003903 on epoch=249
03/18/2022 23:49:21 - INFO - __main__ - Global step 500 Train loss 0.042916 Classification-F1 0.5588547189819725 on epoch=249
03/18/2022 23:49:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002199 on epoch=254
03/18/2022 23:49:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001779 on epoch=259
03/18/2022 23:49:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001180 on epoch=264
03/18/2022 23:49:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000991 on epoch=269
03/18/2022 23:49:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002022 on epoch=274
03/18/2022 23:49:47 - INFO - __main__ - Global step 550 Train loss 0.001634 Classification-F1 0.6000000000000001 on epoch=274
03/18/2022 23:49:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000552 on epoch=279
03/18/2022 23:49:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000544 on epoch=284
03/18/2022 23:50:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000429 on epoch=289
03/18/2022 23:50:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000352 on epoch=294
03/18/2022 23:50:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000289 on epoch=299
03/18/2022 23:50:14 - INFO - __main__ - Global step 600 Train loss 0.000433 Classification-F1 0.5588547189819725 on epoch=299
03/18/2022 23:50:14 - INFO - __main__ - save last model!
03/18/2022 23:50:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:50:14 - INFO - __main__ - Printing 3 examples
03/18/2022 23:50:14 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:50:14 - INFO - __main__ - ['false']
03/18/2022 23:50:14 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:50:14 - INFO - __main__ - ['false']
03/18/2022 23:50:14 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:50:14 - INFO - __main__ - ['false']
03/18/2022 23:50:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:50:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:50:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:50:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:50:15 - INFO - __main__ - Printing 3 examples
03/18/2022 23:50:15 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:50:15 - INFO - __main__ - ['false']
03/18/2022 23:50:15 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:50:15 - INFO - __main__ - ['false']
03/18/2022 23:50:15 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:50:15 - INFO - __main__ - ['false']
03/18/2022 23:50:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:50:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:50:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:50:21 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:50:21 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:50:21 - INFO - __main__ - Printing 3 examples
03/18/2022 23:50:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:50:21 - INFO - __main__ - ['false']
03/18/2022 23:50:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:50:21 - INFO - __main__ - ['false']
03/18/2022 23:50:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:50:21 - INFO - __main__ - ['false']
03/18/2022 23:50:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:50:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:50:25 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:50:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:50:27 - INFO - __main__ - Starting training!
03/18/2022 23:50:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_87_0.0005_8_predictions.txt
03/18/2022 23:50:54 - INFO - __main__ - Classification-F1 on test data: 0.2353
03/18/2022 23:50:55 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.0005, bsz=8, dev_performance=0.6000000000000001, test_performance=0.2352698955985002
03/18/2022 23:50:55 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.0003, bsz=8 ...
03/18/2022 23:50:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:50:56 - INFO - __main__ - Printing 3 examples
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:50:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:50:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:50:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:50:56 - INFO - __main__ - Printing 3 examples
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:50:56 - INFO - __main__ - ['false']
03/18/2022 23:50:56 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:50:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:50:56 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:51:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:51:08 - INFO - __main__ - Starting training!
03/18/2022 23:51:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.944214 on epoch=4
03/18/2022 23:51:17 - INFO - __main__ - Step 20 Global step 20 Train loss 17.567028 on epoch=9
03/18/2022 23:51:22 - INFO - __main__ - Step 30 Global step 30 Train loss 18.849216 on epoch=14
03/18/2022 23:51:27 - INFO - __main__ - Step 40 Global step 40 Train loss 15.776860 on epoch=19
03/18/2022 23:51:33 - INFO - __main__ - Step 50 Global step 50 Train loss 15.453367 on epoch=24
03/18/2022 23:51:34 - INFO - __main__ - Global step 50 Train loss 18.318138 Classification-F1 0.0 on epoch=24
03/18/2022 23:51:39 - INFO - __main__ - Step 60 Global step 60 Train loss 13.579921 on epoch=29
03/18/2022 23:51:44 - INFO - __main__ - Step 70 Global step 70 Train loss 13.149371 on epoch=34
03/18/2022 23:51:49 - INFO - __main__ - Step 80 Global step 80 Train loss 11.612365 on epoch=39
03/18/2022 23:51:55 - INFO - __main__ - Step 90 Global step 90 Train loss 8.847824 on epoch=44
03/18/2022 23:52:00 - INFO - __main__ - Step 100 Global step 100 Train loss 2.405651 on epoch=49
03/18/2022 23:52:00 - INFO - __main__ - Global step 100 Train loss 9.919025 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 23:52:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.215597 on epoch=54
03/18/2022 23:52:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.478071 on epoch=59
03/18/2022 23:52:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.485436 on epoch=64
03/18/2022 23:52:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.392843 on epoch=69
03/18/2022 23:52:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.370425 on epoch=74
03/18/2022 23:52:27 - INFO - __main__ - Global step 150 Train loss 0.588474 Classification-F1 0.5933528836754642 on epoch=74
03/18/2022 23:52:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.437491 on epoch=79
03/18/2022 23:52:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.281326 on epoch=84
03/18/2022 23:52:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.174980 on epoch=89
03/18/2022 23:52:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.217426 on epoch=94
03/18/2022 23:52:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.081313 on epoch=99
03/18/2022 23:52:53 - INFO - __main__ - Global step 200 Train loss 0.238507 Classification-F1 0.539313399778516 on epoch=99
03/18/2022 23:52:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.065598 on epoch=104
03/18/2022 23:53:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.039117 on epoch=109
03/18/2022 23:53:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.014923 on epoch=114
03/18/2022 23:53:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.028523 on epoch=119
03/18/2022 23:53:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.043799 on epoch=124
03/18/2022 23:53:19 - INFO - __main__ - Global step 250 Train loss 0.038392 Classification-F1 0.5901477832512315 on epoch=124
03/18/2022 23:53:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.006226 on epoch=129
03/18/2022 23:53:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.011643 on epoch=134
03/18/2022 23:53:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.577397 on epoch=139
03/18/2022 23:53:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.301103 on epoch=144
03/18/2022 23:53:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.356684 on epoch=149
03/18/2022 23:53:44 - INFO - __main__ - Global step 300 Train loss 0.250611 Classification-F1 0.5270935960591133 on epoch=149
03/18/2022 23:53:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.153973 on epoch=154
03/18/2022 23:53:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.264801 on epoch=159
03/18/2022 23:53:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.169408 on epoch=164
03/18/2022 23:54:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.108431 on epoch=169
03/18/2022 23:54:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.109943 on epoch=174
03/18/2022 23:54:10 - INFO - __main__ - Global step 350 Train loss 0.161311 Classification-F1 0.539313399778516 on epoch=174
03/18/2022 23:54:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.073878 on epoch=179
03/18/2022 23:54:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.237361 on epoch=184
03/18/2022 23:54:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.091205 on epoch=189
03/18/2022 23:54:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.153792 on epoch=194
03/18/2022 23:54:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.044634 on epoch=199
03/18/2022 23:54:35 - INFO - __main__ - Global step 400 Train loss 0.120174 Classification-F1 0.5588547189819725 on epoch=199
03/18/2022 23:54:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.078917 on epoch=204
03/18/2022 23:54:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.058902 on epoch=209
03/18/2022 23:54:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.019348 on epoch=214
03/18/2022 23:54:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.029766 on epoch=219
03/18/2022 23:55:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.102939 on epoch=224
03/18/2022 23:55:01 - INFO - __main__ - Global step 450 Train loss 0.057975 Classification-F1 0.5835835835835835 on epoch=224
03/18/2022 23:55:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.035010 on epoch=229
03/18/2022 23:55:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.039383 on epoch=234
03/18/2022 23:55:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.018546 on epoch=239
03/18/2022 23:55:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.006238 on epoch=244
03/18/2022 23:55:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.020274 on epoch=249
03/18/2022 23:55:26 - INFO - __main__ - Global step 500 Train loss 0.023890 Classification-F1 0.4909862142099682 on epoch=249
03/18/2022 23:55:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011442 on epoch=254
03/18/2022 23:55:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.041997 on epoch=259
03/18/2022 23:55:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009045 on epoch=264
03/18/2022 23:55:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.031781 on epoch=269
03/18/2022 23:55:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015950 on epoch=274
03/18/2022 23:55:52 - INFO - __main__ - Global step 550 Train loss 0.022043 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 23:55:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005109 on epoch=279
03/18/2022 23:56:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.013095 on epoch=284
03/18/2022 23:56:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006627 on epoch=289
03/18/2022 23:56:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004690 on epoch=294
03/18/2022 23:56:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004510 on epoch=299
03/18/2022 23:56:17 - INFO - __main__ - Global step 600 Train loss 0.006806 Classification-F1 0.5333333333333333 on epoch=299
03/18/2022 23:56:17 - INFO - __main__ - save last model!
03/18/2022 23:56:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:56:19 - INFO - __main__ - Printing 3 examples
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 23:56:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:56:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:56:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:56:19 - INFO - __main__ - Printing 3 examples
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:56:19 - INFO - __main__ - ['false']
03/18/2022 23:56:19 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:56:19 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:56:19 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:56:25 - INFO - __main__ - Loading checkpoint on the fly
03/18/2022 23:56:25 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 23:56:25 - INFO - __main__ - Printing 3 examples
03/18/2022 23:56:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 23:56:25 - INFO - __main__ - ['false']
03/18/2022 23:56:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 23:56:25 - INFO - __main__ - ['false']
03/18/2022 23:56:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 23:56:25 - INFO - __main__ - ['false']
03/18/2022 23:56:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:56:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:56:29 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 23:56:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:56:32 - INFO - __main__ - Starting training!
03/18/2022 23:56:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_87_0.0003_8_predictions.txt
03/18/2022 23:56:58 - INFO - __main__ - Classification-F1 on test data: 0.3330
03/18/2022 23:56:58 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.0003, bsz=8, dev_performance=0.5933528836754642, test_performance=0.3330007119147048
03/18/2022 23:56:58 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.0002, bsz=8 ...
03/18/2022 23:56:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:56:59 - INFO - __main__ - Printing 3 examples
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 23:56:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:56:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 23:56:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 23:56:59 - INFO - __main__ - Printing 3 examples
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 23:56:59 - INFO - __main__ - ['false']
03/18/2022 23:56:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 23:56:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 23:56:59 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 23:57:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/18/2022 23:57:12 - INFO - __main__ - Starting training!
03/18/2022 23:57:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.394005 on epoch=4
03/18/2022 23:57:22 - INFO - __main__ - Step 20 Global step 20 Train loss 18.671413 on epoch=9
03/18/2022 23:57:27 - INFO - __main__ - Step 30 Global step 30 Train loss 18.492985 on epoch=14
03/18/2022 23:57:32 - INFO - __main__ - Step 40 Global step 40 Train loss 16.644117 on epoch=19
03/18/2022 23:57:37 - INFO - __main__ - Step 50 Global step 50 Train loss 15.618980 on epoch=24
03/18/2022 23:57:40 - INFO - __main__ - Global step 50 Train loss 18.564299 Classification-F1 0.0 on epoch=24
03/18/2022 23:57:45 - INFO - __main__ - Step 60 Global step 60 Train loss 14.817517 on epoch=29
03/18/2022 23:57:50 - INFO - __main__ - Step 70 Global step 70 Train loss 15.138205 on epoch=34
03/18/2022 23:57:55 - INFO - __main__ - Step 80 Global step 80 Train loss 14.848833 on epoch=39
03/18/2022 23:58:00 - INFO - __main__ - Step 90 Global step 90 Train loss 13.716326 on epoch=44
03/18/2022 23:58:06 - INFO - __main__ - Step 100 Global step 100 Train loss 12.518402 on epoch=49
03/18/2022 23:58:07 - INFO - __main__ - Global step 100 Train loss 14.207856 Classification-F1 0.0 on epoch=49
03/18/2022 23:58:12 - INFO - __main__ - Step 110 Global step 110 Train loss 10.790576 on epoch=54
03/18/2022 23:58:17 - INFO - __main__ - Step 120 Global step 120 Train loss 11.009775 on epoch=59
03/18/2022 23:58:22 - INFO - __main__ - Step 130 Global step 130 Train loss 9.525323 on epoch=64
03/18/2022 23:58:27 - INFO - __main__ - Step 140 Global step 140 Train loss 8.722262 on epoch=69
03/18/2022 23:58:32 - INFO - __main__ - Step 150 Global step 150 Train loss 5.679746 on epoch=74
03/18/2022 23:58:40 - INFO - __main__ - Global step 150 Train loss 9.145537 Classification-F1 0.0 on epoch=74
03/18/2022 23:58:45 - INFO - __main__ - Step 160 Global step 160 Train loss 3.752326 on epoch=79
03/18/2022 23:58:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.222283 on epoch=84
03/18/2022 23:58:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.623282 on epoch=89
03/18/2022 23:59:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.489781 on epoch=94
03/18/2022 23:59:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.449840 on epoch=99
03/18/2022 23:59:05 - INFO - __main__ - Global step 200 Train loss 1.307502 Classification-F1 0.5733333333333335 on epoch=99
03/18/2022 23:59:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.333435 on epoch=104
03/18/2022 23:59:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.423051 on epoch=109
03/18/2022 23:59:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.511686 on epoch=114
03/18/2022 23:59:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.394321 on epoch=119
03/18/2022 23:59:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.109850 on epoch=124
03/18/2022 23:59:32 - INFO - __main__ - Global step 250 Train loss 0.354469 Classification-F1 0.49090909090909085 on epoch=124
03/18/2022 23:59:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.070635 on epoch=129
03/18/2022 23:59:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.093584 on epoch=134
03/18/2022 23:59:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100764 on epoch=139
03/18/2022 23:59:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280884 on epoch=144
03/18/2022 23:59:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.242835 on epoch=149
03/18/2022 23:59:57 - INFO - __main__ - Global step 300 Train loss 0.157740 Classification-F1 0.6476476476476476 on epoch=149
03/19/2022 00:00:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.333041 on epoch=154
03/19/2022 00:00:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.178402 on epoch=159
03/19/2022 00:00:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.151442 on epoch=164
03/19/2022 00:00:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.153570 on epoch=169
03/19/2022 00:00:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004386 on epoch=174
03/19/2022 00:00:23 - INFO - __main__ - Global step 350 Train loss 0.164168 Classification-F1 0.6113360323886641 on epoch=174
03/19/2022 00:00:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.008041 on epoch=179
03/19/2022 00:00:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011868 on epoch=184
03/19/2022 00:00:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003470 on epoch=189
03/19/2022 00:00:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001041 on epoch=194
03/19/2022 00:00:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002875 on epoch=199
03/19/2022 00:00:49 - INFO - __main__ - Global step 400 Train loss 0.005459 Classification-F1 0.5733333333333335 on epoch=199
03/19/2022 00:00:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002970 on epoch=204
03/19/2022 00:00:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001367 on epoch=209
03/19/2022 00:01:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001441 on epoch=214
03/19/2022 00:01:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001028 on epoch=219
03/19/2022 00:01:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002663 on epoch=224
03/19/2022 00:01:14 - INFO - __main__ - Global step 450 Train loss 0.001894 Classification-F1 0.5588547189819725 on epoch=224
03/19/2022 00:01:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.449209 on epoch=229
03/19/2022 00:01:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019676 on epoch=234
03/19/2022 00:01:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012399 on epoch=239
03/19/2022 00:01:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001009 on epoch=244
03/19/2022 00:01:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000250 on epoch=249
03/19/2022 00:01:40 - INFO - __main__ - Global step 500 Train loss 0.096508 Classification-F1 0.5733333333333335 on epoch=249
03/19/2022 00:01:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000374 on epoch=254
03/19/2022 00:01:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001606 on epoch=259
03/19/2022 00:01:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001121 on epoch=264
03/19/2022 00:02:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000277 on epoch=269
03/19/2022 00:02:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.006604 on epoch=274
03/19/2022 00:02:06 - INFO - __main__ - Global step 550 Train loss 0.001996 Classification-F1 0.5333333333333333 on epoch=274
03/19/2022 00:02:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000649 on epoch=279
03/19/2022 00:02:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000407 on epoch=284
03/19/2022 00:02:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000120 on epoch=289
03/19/2022 00:02:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001262 on epoch=294
03/19/2022 00:02:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000215 on epoch=299
03/19/2022 00:02:32 - INFO - __main__ - Global step 600 Train loss 0.000531 Classification-F1 0.5733333333333335 on epoch=299
03/19/2022 00:02:32 - INFO - __main__ - save last model!
03/19/2022 00:02:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 00:02:32 - INFO - __main__ - Printing 3 examples
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:02:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:02:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 00:02:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 00:02:32 - INFO - __main__ - Printing 3 examples
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/19/2022 00:02:32 - INFO - __main__ - ['false']
03/19/2022 00:02:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:02:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:02:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 00:02:38 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:02:39 - INFO - __main__ - Start tokenizing ... 2733 instances
03/19/2022 00:02:39 - INFO - __main__ - Printing 3 examples
03/19/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/19/2022 00:02:39 - INFO - __main__ - ['false']
03/19/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/19/2022 00:02:39 - INFO - __main__ - ['false']
03/19/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/19/2022 00:02:39 - INFO - __main__ - ['false']
03/19/2022 00:02:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:02:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:02:43 - INFO - __main__ - Loaded 2733 examples from test data
03/19/2022 00:02:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:02:45 - INFO - __main__ - Starting training!
03/19/2022 00:03:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_87_0.0002_8_predictions.txt
03/19/2022 00:03:13 - INFO - __main__ - Classification-F1 on test data: 0.3328
03/19/2022 00:03:13 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.0002, bsz=8, dev_performance=0.6476476476476476, test_performance=0.3328003681157047
03/19/2022 00:03:13 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.0001, bsz=8 ...
03/19/2022 00:03:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 00:03:14 - INFO - __main__ - Printing 3 examples
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:03:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:03:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 00:03:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 00:03:14 - INFO - __main__ - Printing 3 examples
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/19/2022 00:03:14 - INFO - __main__ - ['false']
03/19/2022 00:03:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:03:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:03:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 00:03:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:03:27 - INFO - __main__ - Starting training!
03/19/2022 00:03:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.176846 on epoch=4
03/19/2022 00:03:36 - INFO - __main__ - Step 20 Global step 20 Train loss 20.567127 on epoch=9
03/19/2022 00:03:41 - INFO - __main__ - Step 30 Global step 30 Train loss 17.905331 on epoch=14
03/19/2022 00:03:46 - INFO - __main__ - Step 40 Global step 40 Train loss 18.088017 on epoch=19
03/19/2022 00:03:51 - INFO - __main__ - Step 50 Global step 50 Train loss 18.011864 on epoch=24
03/19/2022 00:04:00 - INFO - __main__ - Global step 50 Train loss 19.549837 Classification-F1 0.0 on epoch=24
03/19/2022 00:04:06 - INFO - __main__ - Step 60 Global step 60 Train loss 17.238899 on epoch=29
03/19/2022 00:04:11 - INFO - __main__ - Step 70 Global step 70 Train loss 16.255091 on epoch=34
03/19/2022 00:04:16 - INFO - __main__ - Step 80 Global step 80 Train loss 15.889890 on epoch=39
03/19/2022 00:04:21 - INFO - __main__ - Step 90 Global step 90 Train loss 15.651854 on epoch=44
03/19/2022 00:04:26 - INFO - __main__ - Step 100 Global step 100 Train loss 15.485133 on epoch=49
03/19/2022 00:04:31 - INFO - __main__ - Global step 100 Train loss 16.104172 Classification-F1 0.0 on epoch=49
03/19/2022 00:04:36 - INFO - __main__ - Step 110 Global step 110 Train loss 15.470810 on epoch=54
03/19/2022 00:04:41 - INFO - __main__ - Step 120 Global step 120 Train loss 14.639954 on epoch=59
03/19/2022 00:04:46 - INFO - __main__ - Step 130 Global step 130 Train loss 13.543947 on epoch=64
03/19/2022 00:04:51 - INFO - __main__ - Step 140 Global step 140 Train loss 13.841937 on epoch=69
03/19/2022 00:04:56 - INFO - __main__ - Step 150 Global step 150 Train loss 14.148390 on epoch=74
03/19/2022 00:04:59 - INFO - __main__ - Global step 150 Train loss 14.329006 Classification-F1 0.0 on epoch=74
03/19/2022 00:05:04 - INFO - __main__ - Step 160 Global step 160 Train loss 13.073482 on epoch=79
03/19/2022 00:05:09 - INFO - __main__ - Step 170 Global step 170 Train loss 12.179792 on epoch=84
03/19/2022 00:05:14 - INFO - __main__ - Step 180 Global step 180 Train loss 11.686574 on epoch=89
03/19/2022 00:05:19 - INFO - __main__ - Step 190 Global step 190 Train loss 10.832970 on epoch=94
03/19/2022 00:05:24 - INFO - __main__ - Step 200 Global step 200 Train loss 8.806033 on epoch=99
03/19/2022 00:05:25 - INFO - __main__ - Global step 200 Train loss 11.315771 Classification-F1 0.021164021164021163 on epoch=99
03/19/2022 00:05:31 - INFO - __main__ - Step 210 Global step 210 Train loss 4.011072 on epoch=104
03/19/2022 00:05:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.152716 on epoch=109
03/19/2022 00:05:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.941644 on epoch=114
03/19/2022 00:05:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.020852 on epoch=119
03/19/2022 00:05:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.887056 on epoch=124
03/19/2022 00:05:51 - INFO - __main__ - Global step 250 Train loss 1.602668 Classification-F1 0.39756367663344405 on epoch=124
03/19/2022 00:05:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.409110 on epoch=129
03/19/2022 00:06:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.919384 on epoch=134
03/19/2022 00:06:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.400656 on epoch=139
03/19/2022 00:06:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.298389 on epoch=144
03/19/2022 00:06:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.414130 on epoch=149
03/19/2022 00:06:17 - INFO - __main__ - Global step 300 Train loss 0.488334 Classification-F1 0.5733333333333335 on epoch=149
03/19/2022 00:06:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.277191 on epoch=154
03/19/2022 00:06:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.418291 on epoch=159
03/19/2022 00:06:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.211919 on epoch=164
03/19/2022 00:06:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.202841 on epoch=169
03/19/2022 00:06:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.268026 on epoch=174
03/19/2022 00:06:43 - INFO - __main__ - Global step 350 Train loss 0.275654 Classification-F1 0.5195195195195195 on epoch=174
03/19/2022 00:06:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.180675 on epoch=179
03/19/2022 00:06:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.149561 on epoch=184
03/19/2022 00:06:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.130983 on epoch=189
03/19/2022 00:07:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.215755 on epoch=194
03/19/2022 00:07:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.098880 on epoch=199
03/19/2022 00:07:09 - INFO - __main__ - Global step 400 Train loss 0.155171 Classification-F1 0.5625 on epoch=199
03/19/2022 00:07:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.168747 on epoch=204
03/19/2022 00:07:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.114442 on epoch=209
03/19/2022 00:07:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.146286 on epoch=214
03/19/2022 00:07:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.044659 on epoch=219
03/19/2022 00:07:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.017744 on epoch=224
03/19/2022 00:07:34 - INFO - __main__ - Global step 450 Train loss 0.098376 Classification-F1 0.625 on epoch=224
03/19/2022 00:07:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.038984 on epoch=229
03/19/2022 00:07:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.017929 on epoch=234
03/19/2022 00:07:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011452 on epoch=239
03/19/2022 00:07:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.015083 on epoch=244
03/19/2022 00:08:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011218 on epoch=249
03/19/2022 00:08:00 - INFO - __main__ - Global step 500 Train loss 0.018933 Classification-F1 0.5933528836754642 on epoch=249
03/19/2022 00:08:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006128 on epoch=254
03/19/2022 00:08:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.015346 on epoch=259
03/19/2022 00:08:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.020115 on epoch=264
03/19/2022 00:08:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.008476 on epoch=269
03/19/2022 00:08:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.011664 on epoch=274
03/19/2022 00:08:26 - INFO - __main__ - Global step 550 Train loss 0.012346 Classification-F1 0.4666666666666667 on epoch=274
03/19/2022 00:08:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026575 on epoch=279
03/19/2022 00:08:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003417 on epoch=284
03/19/2022 00:08:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011904 on epoch=289
03/19/2022 00:08:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.005030 on epoch=294
03/19/2022 00:08:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003082 on epoch=299
03/19/2022 00:08:51 - INFO - __main__ - Global step 600 Train loss 0.010002 Classification-F1 0.5901477832512315 on epoch=299
03/19/2022 00:08:51 - INFO - __main__ - save last model!
03/19/2022 00:08:58 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:08:59 - INFO - __main__ - Start tokenizing ... 2733 instances
03/19/2022 00:08:59 - INFO - __main__ - Printing 3 examples
03/19/2022 00:08:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/19/2022 00:08:59 - INFO - __main__ - ['false']
03/19/2022 00:08:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/19/2022 00:08:59 - INFO - __main__ - ['false']
03/19/2022 00:08:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/19/2022 00:08:59 - INFO - __main__ - ['false']
03/19/2022 00:08:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:09:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:09:03 - INFO - __main__ - Loaded 2733 examples from test data
03/19/2022 00:09:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-wiki_qa/wiki_qa_16_87_0.0001_8_predictions.txt
03/19/2022 00:09:32 - INFO - __main__ - Classification-F1 on test data: 0.3602
03/19/2022 00:09:33 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.0001, bsz=8, dev_performance=0.625, test_performance=0.36017077577665096
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22119): No such process
Task: emo, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 00:09:38 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 00:09:38 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-emo
03/19/2022 00:09:38 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 00:09:38 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-emo
03/19/2022 00:09:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 00:09:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 00:09:39 - INFO - __main__ - args.device: cuda:0
03/19/2022 00:09:39 - INFO - __main__ - Using 2 gpus
03/19/2022 00:09:39 - INFO - __main__ - args.device: cuda:1
03/19/2022 00:09:39 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/19/2022 00:09:39 - INFO - __main__ - Using 2 gpus
03/19/2022 00:09:39 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/19/2022 00:09:44 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0005, bsz=8 ...
03/19/2022 00:09:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:09:45 - INFO - __main__ - Printing 3 examples
03/19/2022 00:09:45 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:09:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:09:45 - INFO - __main__ - Printing 3 examples
03/19/2022 00:09:45 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:09:45 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:09:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:09:45 - INFO - __main__ - Printing 3 examples
03/19/2022 00:09:45 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:09:45 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:09:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:09:45 - INFO - __main__ - Printing 3 examples
03/19/2022 00:09:45 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:09:45 - INFO - __main__ - ['others']
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:09:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:09:45 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:09:45 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:09:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:09:57 - INFO - __main__ - Starting training!
03/19/2022 00:09:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:09:58 - INFO - __main__ - Starting training!
03/19/2022 00:10:02 - INFO - __main__ - Step 10 Global step 10 Train loss 24.519060 on epoch=2
03/19/2022 00:10:07 - INFO - __main__ - Step 20 Global step 20 Train loss 18.893360 on epoch=4
03/19/2022 00:10:12 - INFO - __main__ - Step 30 Global step 30 Train loss 16.136152 on epoch=7
03/19/2022 00:10:17 - INFO - __main__ - Step 40 Global step 40 Train loss 14.414729 on epoch=9
03/19/2022 00:10:22 - INFO - __main__ - Step 50 Global step 50 Train loss 13.090100 on epoch=12
03/19/2022 00:10:23 - INFO - __main__ - Global step 50 Train loss 17.410681 Classification-F1 0.0 on epoch=12
03/19/2022 00:10:29 - INFO - __main__ - Step 60 Global step 60 Train loss 11.925522 on epoch=14
03/19/2022 00:10:34 - INFO - __main__ - Step 70 Global step 70 Train loss 11.015587 on epoch=17
03/19/2022 00:10:38 - INFO - __main__ - Step 80 Global step 80 Train loss 7.900140 on epoch=19
03/19/2022 00:10:43 - INFO - __main__ - Step 90 Global step 90 Train loss 3.145200 on epoch=22
03/19/2022 00:10:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.133615 on epoch=24
03/19/2022 00:10:49 - INFO - __main__ - Global step 100 Train loss 7.024013 Classification-F1 0.10126582278481013 on epoch=24
03/19/2022 00:10:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.934047 on epoch=27
03/19/2022 00:11:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.873441 on epoch=29
03/19/2022 00:11:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.748888 on epoch=32
03/19/2022 00:11:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.847226 on epoch=34
03/19/2022 00:11:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.764994 on epoch=37
03/19/2022 00:11:16 - INFO - __main__ - Global step 150 Train loss 0.833719 Classification-F1 0.24751918158567776 on epoch=37
03/19/2022 00:11:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.793690 on epoch=39
03/19/2022 00:11:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.755583 on epoch=42
03/19/2022 00:11:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.684435 on epoch=44
03/19/2022 00:11:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.771998 on epoch=47
03/19/2022 00:11:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.704302 on epoch=49
03/19/2022 00:11:42 - INFO - __main__ - Global step 200 Train loss 0.742001 Classification-F1 0.19933796756041047 on epoch=49
03/19/2022 00:11:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.866151 on epoch=52
03/19/2022 00:11:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.739269 on epoch=54
03/19/2022 00:11:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.762964 on epoch=57
03/19/2022 00:12:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.747744 on epoch=59
03/19/2022 00:12:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.570045 on epoch=62
03/19/2022 00:12:07 - INFO - __main__ - Global step 250 Train loss 0.737235 Classification-F1 0.3270166352793471 on epoch=62
03/19/2022 00:12:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.426186 on epoch=64
03/19/2022 00:12:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.501772 on epoch=67
03/19/2022 00:12:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.284166 on epoch=69
03/19/2022 00:12:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.335332 on epoch=72
03/19/2022 00:12:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.220079 on epoch=74
03/19/2022 00:12:33 - INFO - __main__ - Global step 300 Train loss 0.353507 Classification-F1 0.39919670846394983 on epoch=74
03/19/2022 00:12:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.184523 on epoch=77
03/19/2022 00:12:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.131980 on epoch=79
03/19/2022 00:12:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.163140 on epoch=82
03/19/2022 00:12:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.075107 on epoch=84
03/19/2022 00:12:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.055452 on epoch=87
03/19/2022 00:12:59 - INFO - __main__ - Global step 350 Train loss 0.122040 Classification-F1 0.4399829563676212 on epoch=87
03/19/2022 00:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.067115 on epoch=89
03/19/2022 00:13:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.084387 on epoch=92
03/19/2022 00:13:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.051819 on epoch=94
03/19/2022 00:13:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.030491 on epoch=97
03/19/2022 00:13:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.024118 on epoch=99
03/19/2022 00:13:25 - INFO - __main__ - Global step 400 Train loss 0.051586 Classification-F1 0.4444444444444445 on epoch=99
03/19/2022 00:13:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.176710 on epoch=102
03/19/2022 00:13:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.024591 on epoch=104
03/19/2022 00:13:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.056212 on epoch=107
03/19/2022 00:13:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.056132 on epoch=109
03/19/2022 00:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.024530 on epoch=112
03/19/2022 00:13:51 - INFO - __main__ - Global step 450 Train loss 0.067635 Classification-F1 0.3945040466779597 on epoch=112
03/19/2022 00:13:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.025153 on epoch=114
03/19/2022 00:14:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019129 on epoch=117
03/19/2022 00:14:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011675 on epoch=119
03/19/2022 00:14:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.039528 on epoch=122
03/19/2022 00:14:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.015534 on epoch=124
03/19/2022 00:14:16 - INFO - __main__ - Global step 500 Train loss 0.022204 Classification-F1 0.3129578754578754 on epoch=124
03/19/2022 00:14:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.016001 on epoch=127
03/19/2022 00:14:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005031 on epoch=129
03/19/2022 00:14:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005292 on epoch=132
03/19/2022 00:14:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.005172 on epoch=134
03/19/2022 00:14:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023409 on epoch=137
03/19/2022 00:14:41 - INFO - __main__ - Global step 550 Train loss 0.010981 Classification-F1 0.2761973261271999 on epoch=137
03/19/2022 00:14:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008191 on epoch=139
03/19/2022 00:14:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017510 on epoch=142
03/19/2022 00:14:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.032040 on epoch=144
03/19/2022 00:15:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014752 on epoch=147
03/19/2022 00:15:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.011242 on epoch=149
03/19/2022 00:15:06 - INFO - __main__ - Global step 600 Train loss 0.016747 Classification-F1 0.3237527716186252 on epoch=149
03/19/2022 00:15:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.026358 on epoch=152
03/19/2022 00:15:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.003525 on epoch=154
03/19/2022 00:15:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.066417 on epoch=157
03/19/2022 00:15:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001605 on epoch=159
03/19/2022 00:15:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007457 on epoch=162
03/19/2022 00:15:31 - INFO - __main__ - Global step 650 Train loss 0.021072 Classification-F1 0.3322440087145969 on epoch=162
03/19/2022 00:15:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000840 on epoch=164
03/19/2022 00:15:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001435 on epoch=167
03/19/2022 00:15:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000541 on epoch=169
03/19/2022 00:15:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000882 on epoch=172
03/19/2022 00:15:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000172 on epoch=174
03/19/2022 00:15:56 - INFO - __main__ - Global step 700 Train loss 0.000774 Classification-F1 0.35495679960289767 on epoch=174
03/19/2022 00:16:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000939 on epoch=177
03/19/2022 00:16:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.025363 on epoch=179
03/19/2022 00:16:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001592 on epoch=182
03/19/2022 00:16:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.011392 on epoch=184
03/19/2022 00:16:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003070 on epoch=187
03/19/2022 00:16:21 - INFO - __main__ - Global step 750 Train loss 0.008471 Classification-F1 0.4140546218487395 on epoch=187
03/19/2022 00:16:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000761 on epoch=189
03/19/2022 00:16:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000647 on epoch=192
03/19/2022 00:16:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000374 on epoch=194
03/19/2022 00:16:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000603 on epoch=197
03/19/2022 00:16:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000210 on epoch=199
03/19/2022 00:16:46 - INFO - __main__ - Global step 800 Train loss 0.000519 Classification-F1 0.27656207644348063 on epoch=199
03/19/2022 00:16:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000092 on epoch=202
03/19/2022 00:16:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000630 on epoch=204
03/19/2022 00:17:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.040473 on epoch=207
03/19/2022 00:17:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001630 on epoch=209
03/19/2022 00:17:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000292 on epoch=212
03/19/2022 00:17:11 - INFO - __main__ - Global step 850 Train loss 0.008623 Classification-F1 0.42141812865497075 on epoch=212
03/19/2022 00:17:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000141 on epoch=214
03/19/2022 00:17:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000924 on epoch=217
03/19/2022 00:17:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000162 on epoch=219
03/19/2022 00:17:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002548 on epoch=222
03/19/2022 00:17:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000801 on epoch=224
03/19/2022 00:17:37 - INFO - __main__ - Global step 900 Train loss 0.000915 Classification-F1 0.423270697167756 on epoch=224
03/19/2022 00:17:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000602 on epoch=227
03/19/2022 00:17:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000379 on epoch=229
03/19/2022 00:17:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001262 on epoch=232
03/19/2022 00:17:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.009015 on epoch=234
03/19/2022 00:18:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.020114 on epoch=237
03/19/2022 00:18:02 - INFO - __main__ - Global step 950 Train loss 0.006275 Classification-F1 0.47254224270353296 on epoch=237
03/19/2022 00:18:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.010635 on epoch=239
03/19/2022 00:18:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001432 on epoch=242
03/19/2022 00:18:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001125 on epoch=244
03/19/2022 00:18:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001297 on epoch=247
03/19/2022 00:18:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.016292 on epoch=249
03/19/2022 00:18:28 - INFO - __main__ - Global step 1000 Train loss 0.006156 Classification-F1 0.3442194246791948 on epoch=249
03/19/2022 00:18:28 - INFO - __main__ - save last model!
03/19/2022 00:18:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:18:29 - INFO - __main__ - Printing 3 examples
03/19/2022 00:18:29 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:18:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:18:29 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:18:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:18:29 - INFO - __main__ - Printing 3 examples
03/19/2022 00:18:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:18:29 - INFO - __main__ - ['others']
03/19/2022 00:18:29 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:18:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:18:29 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:18:35 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:18:36 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 00:18:36 - INFO - __main__ - Printing 3 examples
03/19/2022 00:18:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 00:18:36 - INFO - __main__ - ['others']
03/19/2022 00:18:36 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 00:18:36 - INFO - __main__ - ['others']
03/19/2022 00:18:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 00:18:36 - INFO - __main__ - ['others']
03/19/2022 00:18:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:18:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:18:40 - INFO - __main__ - Starting training!
03/19/2022 00:18:43 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 00:19:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_100_0.0005_8_predictions.txt
03/19/2022 00:19:35 - INFO - __main__ - Classification-F1 on test data: 0.0186
03/19/2022 00:19:35 - INFO - __main__ - prefix=emo_16_100, lr=0.0005, bsz=8, dev_performance=0.47254224270353296, test_performance=0.01860362964587614
03/19/2022 00:19:35 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0003, bsz=8 ...
03/19/2022 00:19:36 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:19:36 - INFO - __main__ - Printing 3 examples
03/19/2022 00:19:36 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:19:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:19:36 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:19:36 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:19:36 - INFO - __main__ - Printing 3 examples
03/19/2022 00:19:36 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:19:36 - INFO - __main__ - ['others']
03/19/2022 00:19:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:19:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:19:36 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:19:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:19:49 - INFO - __main__ - Starting training!
03/19/2022 00:19:54 - INFO - __main__ - Step 10 Global step 10 Train loss 25.081684 on epoch=2
03/19/2022 00:19:58 - INFO - __main__ - Step 20 Global step 20 Train loss 19.522635 on epoch=4
03/19/2022 00:20:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.062458 on epoch=7
03/19/2022 00:20:08 - INFO - __main__ - Step 40 Global step 40 Train loss 16.319256 on epoch=9
03/19/2022 00:20:13 - INFO - __main__ - Step 50 Global step 50 Train loss 15.551822 on epoch=12
03/19/2022 00:20:15 - INFO - __main__ - Global step 50 Train loss 18.707571 Classification-F1 0.0 on epoch=12
03/19/2022 00:20:21 - INFO - __main__ - Step 60 Global step 60 Train loss 14.199026 on epoch=14
03/19/2022 00:20:26 - INFO - __main__ - Step 70 Global step 70 Train loss 13.173207 on epoch=17
03/19/2022 00:20:31 - INFO - __main__ - Step 80 Global step 80 Train loss 12.827138 on epoch=19
03/19/2022 00:20:36 - INFO - __main__ - Step 90 Global step 90 Train loss 11.095229 on epoch=22
03/19/2022 00:20:41 - INFO - __main__ - Step 100 Global step 100 Train loss 9.746382 on epoch=24
03/19/2022 00:20:42 - INFO - __main__ - Global step 100 Train loss 12.208197 Classification-F1 0.0 on epoch=24
03/19/2022 00:20:47 - INFO - __main__ - Step 110 Global step 110 Train loss 8.335711 on epoch=27
03/19/2022 00:20:52 - INFO - __main__ - Step 120 Global step 120 Train loss 6.232844 on epoch=29
03/19/2022 00:20:57 - INFO - __main__ - Step 130 Global step 130 Train loss 4.889640 on epoch=32
03/19/2022 00:21:02 - INFO - __main__ - Step 140 Global step 140 Train loss 4.122478 on epoch=34
03/19/2022 00:21:07 - INFO - __main__ - Step 150 Global step 150 Train loss 4.172137 on epoch=37
03/19/2022 00:21:07 - INFO - __main__ - Global step 150 Train loss 5.550562 Classification-F1 0.1 on epoch=37
03/19/2022 00:21:13 - INFO - __main__ - Step 160 Global step 160 Train loss 3.495847 on epoch=39
03/19/2022 00:21:18 - INFO - __main__ - Step 170 Global step 170 Train loss 3.296650 on epoch=42
03/19/2022 00:21:23 - INFO - __main__ - Step 180 Global step 180 Train loss 3.742307 on epoch=44
03/19/2022 00:21:28 - INFO - __main__ - Step 190 Global step 190 Train loss 2.949938 on epoch=47
03/19/2022 00:21:33 - INFO - __main__ - Step 200 Global step 200 Train loss 3.123427 on epoch=49
03/19/2022 00:21:33 - INFO - __main__ - Global step 200 Train loss 3.321634 Classification-F1 0.1 on epoch=49
03/19/2022 00:21:38 - INFO - __main__ - Step 210 Global step 210 Train loss 2.445085 on epoch=52
03/19/2022 00:21:43 - INFO - __main__ - Step 220 Global step 220 Train loss 2.649805 on epoch=54
03/19/2022 00:21:48 - INFO - __main__ - Step 230 Global step 230 Train loss 2.226973 on epoch=57
03/19/2022 00:21:53 - INFO - __main__ - Step 240 Global step 240 Train loss 2.299628 on epoch=59
03/19/2022 00:21:58 - INFO - __main__ - Step 250 Global step 250 Train loss 2.079012 on epoch=62
03/19/2022 00:21:59 - INFO - __main__ - Global step 250 Train loss 2.340100 Classification-F1 0.1 on epoch=62
03/19/2022 00:22:04 - INFO - __main__ - Step 260 Global step 260 Train loss 2.448069 on epoch=64
03/19/2022 00:22:09 - INFO - __main__ - Step 270 Global step 270 Train loss 2.054519 on epoch=67
03/19/2022 00:22:14 - INFO - __main__ - Step 280 Global step 280 Train loss 2.079138 on epoch=69
03/19/2022 00:22:19 - INFO - __main__ - Step 290 Global step 290 Train loss 2.654957 on epoch=72
03/19/2022 00:22:24 - INFO - __main__ - Step 300 Global step 300 Train loss 2.670717 on epoch=74
03/19/2022 00:22:24 - INFO - __main__ - Global step 300 Train loss 2.381480 Classification-F1 0.17344312918167784 on epoch=74
03/19/2022 00:22:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.970421 on epoch=77
03/19/2022 00:22:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.944311 on epoch=79
03/19/2022 00:22:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.994614 on epoch=82
03/19/2022 00:22:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.772877 on epoch=84
03/19/2022 00:22:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.799403 on epoch=87
03/19/2022 00:22:50 - INFO - __main__ - Global step 350 Train loss 0.896325 Classification-F1 0.37912087912087916 on epoch=87
03/19/2022 00:22:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.678669 on epoch=89
03/19/2022 00:23:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.830380 on epoch=92
03/19/2022 00:23:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.870672 on epoch=94
03/19/2022 00:23:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.823426 on epoch=97
03/19/2022 00:23:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.678398 on epoch=99
03/19/2022 00:23:16 - INFO - __main__ - Global step 400 Train loss 0.776309 Classification-F1 0.4458713920350749 on epoch=99
03/19/2022 00:23:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.723691 on epoch=102
03/19/2022 00:23:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.654875 on epoch=104
03/19/2022 00:23:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.641280 on epoch=107
03/19/2022 00:23:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.574993 on epoch=109
03/19/2022 00:23:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.546247 on epoch=112
03/19/2022 00:23:42 - INFO - __main__ - Global step 450 Train loss 0.628217 Classification-F1 0.5376678020436787 on epoch=112
03/19/2022 00:23:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.575337 on epoch=114
03/19/2022 00:23:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.532513 on epoch=117
03/19/2022 00:23:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.426113 on epoch=119
03/19/2022 00:24:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.417534 on epoch=122
03/19/2022 00:24:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.453694 on epoch=124
03/19/2022 00:24:08 - INFO - __main__ - Global step 500 Train loss 0.481038 Classification-F1 0.483547366029474 on epoch=124
03/19/2022 00:24:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.432742 on epoch=127
03/19/2022 00:24:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.331102 on epoch=129
03/19/2022 00:24:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.345607 on epoch=132
03/19/2022 00:24:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.378635 on epoch=134
03/19/2022 00:24:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.260218 on epoch=137
03/19/2022 00:24:33 - INFO - __main__ - Global step 550 Train loss 0.349661 Classification-F1 0.5929129129129129 on epoch=137
03/19/2022 00:24:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.187448 on epoch=139
03/19/2022 00:24:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.232435 on epoch=142
03/19/2022 00:24:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.151718 on epoch=144
03/19/2022 00:24:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.208872 on epoch=147
03/19/2022 00:24:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.185883 on epoch=149
03/19/2022 00:24:58 - INFO - __main__ - Global step 600 Train loss 0.193271 Classification-F1 0.4268611767794507 on epoch=149
03/19/2022 00:25:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.240191 on epoch=152
03/19/2022 00:25:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.135804 on epoch=154
03/19/2022 00:25:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.219240 on epoch=157
03/19/2022 00:25:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.084878 on epoch=159
03/19/2022 00:25:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.072640 on epoch=162
03/19/2022 00:25:23 - INFO - __main__ - Global step 650 Train loss 0.150551 Classification-F1 0.5785223903885162 on epoch=162
03/19/2022 00:25:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.048394 on epoch=164
03/19/2022 00:25:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.112503 on epoch=167
03/19/2022 00:25:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.066120 on epoch=169
03/19/2022 00:25:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.056392 on epoch=172
03/19/2022 00:25:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.053553 on epoch=174
03/19/2022 00:25:49 - INFO - __main__ - Global step 700 Train loss 0.067392 Classification-F1 0.5860485415894203 on epoch=174
03/19/2022 00:25:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.043176 on epoch=177
03/19/2022 00:25:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.022634 on epoch=179
03/19/2022 00:26:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.029709 on epoch=182
03/19/2022 00:26:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.007389 on epoch=184
03/19/2022 00:26:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.032838 on epoch=187
03/19/2022 00:26:14 - INFO - __main__ - Global step 750 Train loss 0.027149 Classification-F1 0.5078670634920635 on epoch=187
03/19/2022 00:26:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.021368 on epoch=189
03/19/2022 00:26:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.052775 on epoch=192
03/19/2022 00:26:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.036615 on epoch=194
03/19/2022 00:26:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.030791 on epoch=197
03/19/2022 00:26:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.030886 on epoch=199
03/19/2022 00:26:39 - INFO - __main__ - Global step 800 Train loss 0.034487 Classification-F1 0.5942260775277853 on epoch=199
03/19/2022 00:26:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.044416 on epoch=202
03/19/2022 00:26:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.021117 on epoch=204
03/19/2022 00:26:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.007422 on epoch=207
03/19/2022 00:27:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.030763 on epoch=209
03/19/2022 00:27:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.007102 on epoch=212
03/19/2022 00:27:05 - INFO - __main__ - Global step 850 Train loss 0.022164 Classification-F1 0.5980362046538517 on epoch=212
03/19/2022 00:27:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.010199 on epoch=214
03/19/2022 00:27:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.012771 on epoch=217
03/19/2022 00:27:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.027311 on epoch=219
03/19/2022 00:27:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.041924 on epoch=222
03/19/2022 00:27:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.014525 on epoch=224
03/19/2022 00:27:31 - INFO - __main__ - Global step 900 Train loss 0.021346 Classification-F1 0.46078021595262975 on epoch=224
03/19/2022 00:27:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.004630 on epoch=227
03/19/2022 00:27:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002921 on epoch=229
03/19/2022 00:27:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.028756 on epoch=232
03/19/2022 00:27:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.004389 on epoch=234
03/19/2022 00:27:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.020696 on epoch=237
03/19/2022 00:27:56 - INFO - __main__ - Global step 950 Train loss 0.012278 Classification-F1 0.4968137254901961 on epoch=237
03/19/2022 00:28:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.028923 on epoch=239
03/19/2022 00:28:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.015534 on epoch=242
03/19/2022 00:28:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002940 on epoch=244
03/19/2022 00:28:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.036469 on epoch=247
03/19/2022 00:28:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.009910 on epoch=249
03/19/2022 00:28:22 - INFO - __main__ - Global step 1000 Train loss 0.018756 Classification-F1 0.5808284457478006 on epoch=249
03/19/2022 00:28:22 - INFO - __main__ - save last model!
03/19/2022 00:28:22 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:28:22 - INFO - __main__ - Printing 3 examples
03/19/2022 00:28:22 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:28:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:28:22 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:28:22 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:28:22 - INFO - __main__ - Printing 3 examples
03/19/2022 00:28:22 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:28:22 - INFO - __main__ - ['others']
03/19/2022 00:28:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:28:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:28:22 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:28:29 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:28:29 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 00:28:29 - INFO - __main__ - Printing 3 examples
03/19/2022 00:28:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 00:28:29 - INFO - __main__ - ['others']
03/19/2022 00:28:29 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 00:28:29 - INFO - __main__ - ['others']
03/19/2022 00:28:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 00:28:29 - INFO - __main__ - ['others']
03/19/2022 00:28:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:28:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:28:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:28:35 - INFO - __main__ - Starting training!
03/19/2022 00:28:37 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 00:29:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_100_0.0003_8_predictions.txt
03/19/2022 00:29:30 - INFO - __main__ - Classification-F1 on test data: 0.0145
03/19/2022 00:29:31 - INFO - __main__ - prefix=emo_16_100, lr=0.0003, bsz=8, dev_performance=0.5980362046538517, test_performance=0.014542648644727435
03/19/2022 00:29:31 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0002, bsz=8 ...
03/19/2022 00:29:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:29:32 - INFO - __main__ - Printing 3 examples
03/19/2022 00:29:32 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:29:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:29:32 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:29:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:29:32 - INFO - __main__ - Printing 3 examples
03/19/2022 00:29:32 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:29:32 - INFO - __main__ - ['others']
03/19/2022 00:29:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:29:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:29:32 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:29:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:29:44 - INFO - __main__ - Starting training!
03/19/2022 00:29:49 - INFO - __main__ - Step 10 Global step 10 Train loss 24.624876 on epoch=2
03/19/2022 00:29:53 - INFO - __main__ - Step 20 Global step 20 Train loss 20.089159 on epoch=4
03/19/2022 00:29:58 - INFO - __main__ - Step 30 Global step 30 Train loss 17.813833 on epoch=7
03/19/2022 00:30:03 - INFO - __main__ - Step 40 Global step 40 Train loss 17.237215 on epoch=9
03/19/2022 00:30:08 - INFO - __main__ - Step 50 Global step 50 Train loss 16.589636 on epoch=12
03/19/2022 00:30:09 - INFO - __main__ - Global step 50 Train loss 19.270943 Classification-F1 0.0 on epoch=12
03/19/2022 00:30:14 - INFO - __main__ - Step 60 Global step 60 Train loss 15.883078 on epoch=14
03/19/2022 00:30:19 - INFO - __main__ - Step 70 Global step 70 Train loss 15.125984 on epoch=17
03/19/2022 00:30:24 - INFO - __main__ - Step 80 Global step 80 Train loss 14.212782 on epoch=19
03/19/2022 00:30:29 - INFO - __main__ - Step 90 Global step 90 Train loss 14.138060 on epoch=22
03/19/2022 00:30:34 - INFO - __main__ - Step 100 Global step 100 Train loss 12.267702 on epoch=24
03/19/2022 00:30:34 - INFO - __main__ - Global step 100 Train loss 14.325522 Classification-F1 0.0 on epoch=24
03/19/2022 00:30:39 - INFO - __main__ - Step 110 Global step 110 Train loss 12.842054 on epoch=27
03/19/2022 00:30:44 - INFO - __main__ - Step 120 Global step 120 Train loss 11.839302 on epoch=29
03/19/2022 00:30:49 - INFO - __main__ - Step 130 Global step 130 Train loss 11.544286 on epoch=32
03/19/2022 00:30:54 - INFO - __main__ - Step 140 Global step 140 Train loss 10.556746 on epoch=34
03/19/2022 00:30:59 - INFO - __main__ - Step 150 Global step 150 Train loss 10.284463 on epoch=37
03/19/2022 00:30:59 - INFO - __main__ - Global step 150 Train loss 11.413370 Classification-F1 0.0 on epoch=37
03/19/2022 00:31:04 - INFO - __main__ - Step 160 Global step 160 Train loss 8.670319 on epoch=39
03/19/2022 00:31:09 - INFO - __main__ - Step 170 Global step 170 Train loss 7.499544 on epoch=42
03/19/2022 00:31:14 - INFO - __main__ - Step 180 Global step 180 Train loss 4.530419 on epoch=44
03/19/2022 00:31:19 - INFO - __main__ - Step 190 Global step 190 Train loss 2.114173 on epoch=47
03/19/2022 00:31:24 - INFO - __main__ - Step 200 Global step 200 Train loss 1.547654 on epoch=49
03/19/2022 00:31:25 - INFO - __main__ - Global step 200 Train loss 4.872422 Classification-F1 0.3573512377418222 on epoch=49
03/19/2022 00:31:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.543277 on epoch=52
03/19/2022 00:31:35 - INFO - __main__ - Step 220 Global step 220 Train loss 1.085111 on epoch=54
03/19/2022 00:31:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.210030 on epoch=57
03/19/2022 00:31:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.737742 on epoch=59
03/19/2022 00:31:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.780507 on epoch=62
03/19/2022 00:31:50 - INFO - __main__ - Global step 250 Train loss 1.071334 Classification-F1 0.5709150326797385 on epoch=62
03/19/2022 00:31:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.483023 on epoch=64
03/19/2022 00:32:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.399772 on epoch=67
03/19/2022 00:32:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.321171 on epoch=69
03/19/2022 00:32:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.369893 on epoch=72
03/19/2022 00:32:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.230990 on epoch=74
03/19/2022 00:32:16 - INFO - __main__ - Global step 300 Train loss 0.360970 Classification-F1 0.6414039589442815 on epoch=74
03/19/2022 00:32:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.324986 on epoch=77
03/19/2022 00:32:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.362147 on epoch=79
03/19/2022 00:32:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.252432 on epoch=82
03/19/2022 00:32:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.207736 on epoch=84
03/19/2022 00:32:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.207019 on epoch=87
03/19/2022 00:32:42 - INFO - __main__ - Global step 350 Train loss 0.270864 Classification-F1 0.623946294060146 on epoch=87
03/19/2022 00:32:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.127795 on epoch=89
03/19/2022 00:32:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.299692 on epoch=92
03/19/2022 00:32:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.098397 on epoch=94
03/19/2022 00:33:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.288975 on epoch=97
03/19/2022 00:33:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.296447 on epoch=99
03/19/2022 00:33:07 - INFO - __main__ - Global step 400 Train loss 0.222261 Classification-F1 0.5643039049235994 on epoch=99
03/19/2022 00:33:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.175227 on epoch=102
03/19/2022 00:33:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.207686 on epoch=104
03/19/2022 00:33:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.369029 on epoch=107
03/19/2022 00:33:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.285534 on epoch=109
03/19/2022 00:33:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.159010 on epoch=112
03/19/2022 00:33:32 - INFO - __main__ - Global step 450 Train loss 0.239297 Classification-F1 0.5149572649572649 on epoch=112
03/19/2022 00:33:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.120016 on epoch=114
03/19/2022 00:33:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.130473 on epoch=117
03/19/2022 00:33:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.092267 on epoch=119
03/19/2022 00:33:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.133178 on epoch=122
03/19/2022 00:33:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.048832 on epoch=124
03/19/2022 00:33:57 - INFO - __main__ - Global step 500 Train loss 0.104953 Classification-F1 0.5123563218390804 on epoch=124
03/19/2022 00:34:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.098394 on epoch=127
03/19/2022 00:34:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.156169 on epoch=129
03/19/2022 00:34:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.167637 on epoch=132
03/19/2022 00:34:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.130828 on epoch=134
03/19/2022 00:34:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.085860 on epoch=137
03/19/2022 00:34:22 - INFO - __main__ - Global step 550 Train loss 0.127777 Classification-F1 0.40979020979020975 on epoch=137
03/19/2022 00:34:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.041091 on epoch=139
03/19/2022 00:34:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011148 on epoch=142
03/19/2022 00:34:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.059803 on epoch=144
03/19/2022 00:34:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.133604 on epoch=147
03/19/2022 00:34:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.057811 on epoch=149
03/19/2022 00:34:47 - INFO - __main__ - Global step 600 Train loss 0.060692 Classification-F1 0.5598484848484848 on epoch=149
03/19/2022 00:34:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.078651 on epoch=152
03/19/2022 00:34:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.107808 on epoch=154
03/19/2022 00:35:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.062005 on epoch=157
03/19/2022 00:35:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.021562 on epoch=159
03/19/2022 00:35:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005217 on epoch=162
03/19/2022 00:35:12 - INFO - __main__ - Global step 650 Train loss 0.055048 Classification-F1 0.3585978835978836 on epoch=162
03/19/2022 00:35:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.022143 on epoch=164
03/19/2022 00:35:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.052413 on epoch=167
03/19/2022 00:35:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.018875 on epoch=169
03/19/2022 00:35:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.018977 on epoch=172
03/19/2022 00:35:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.013479 on epoch=174
03/19/2022 00:35:37 - INFO - __main__ - Global step 700 Train loss 0.025177 Classification-F1 0.46942954529161424 on epoch=174
03/19/2022 00:35:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.032005 on epoch=177
03/19/2022 00:35:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.020315 on epoch=179
03/19/2022 00:35:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007763 on epoch=182
03/19/2022 00:35:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.188119 on epoch=184
03/19/2022 00:36:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.081304 on epoch=187
03/19/2022 00:36:02 - INFO - __main__ - Global step 750 Train loss 0.065901 Classification-F1 0.5261457109283196 on epoch=187
03/19/2022 00:36:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.049132 on epoch=189
03/19/2022 00:36:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.022761 on epoch=192
03/19/2022 00:36:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.038424 on epoch=194
03/19/2022 00:36:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.064515 on epoch=197
03/19/2022 00:36:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.006237 on epoch=199
03/19/2022 00:36:27 - INFO - __main__ - Global step 800 Train loss 0.036214 Classification-F1 0.3682988251409304 on epoch=199
03/19/2022 00:36:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.018929 on epoch=202
03/19/2022 00:36:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.048884 on epoch=204
03/19/2022 00:36:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.042244 on epoch=207
03/19/2022 00:36:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.060012 on epoch=209
03/19/2022 00:36:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003363 on epoch=212
03/19/2022 00:36:52 - INFO - __main__ - Global step 850 Train loss 0.034687 Classification-F1 0.41051282051282045 on epoch=212
03/19/2022 00:36:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006176 on epoch=214
03/19/2022 00:37:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.007428 on epoch=217
03/19/2022 00:37:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.031483 on epoch=219
03/19/2022 00:37:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.038628 on epoch=222
03/19/2022 00:37:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.021554 on epoch=224
03/19/2022 00:37:17 - INFO - __main__ - Global step 900 Train loss 0.021054 Classification-F1 0.4481698841698842 on epoch=224
03/19/2022 00:37:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.043821 on epoch=227
03/19/2022 00:37:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.040698 on epoch=229
03/19/2022 00:37:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.042627 on epoch=232
03/19/2022 00:37:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.043929 on epoch=234
03/19/2022 00:37:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.099026 on epoch=237
03/19/2022 00:37:42 - INFO - __main__ - Global step 950 Train loss 0.054020 Classification-F1 0.2920910973084886 on epoch=237
03/19/2022 00:37:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.090678 on epoch=239
03/19/2022 00:37:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.111124 on epoch=242
03/19/2022 00:37:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.049228 on epoch=244
03/19/2022 00:38:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.034137 on epoch=247
03/19/2022 00:38:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.134875 on epoch=249
03/19/2022 00:38:07 - INFO - __main__ - Global step 1000 Train loss 0.084008 Classification-F1 0.5478176847662142 on epoch=249
03/19/2022 00:38:07 - INFO - __main__ - save last model!
03/19/2022 00:38:07 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:38:07 - INFO - __main__ - Printing 3 examples
03/19/2022 00:38:07 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:38:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:38:07 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:38:07 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:38:07 - INFO - __main__ - Printing 3 examples
03/19/2022 00:38:07 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:38:07 - INFO - __main__ - ['others']
03/19/2022 00:38:07 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:38:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:38:08 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:38:14 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:38:14 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 00:38:14 - INFO - __main__ - Printing 3 examples
03/19/2022 00:38:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 00:38:14 - INFO - __main__ - ['others']
03/19/2022 00:38:14 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 00:38:14 - INFO - __main__ - ['others']
03/19/2022 00:38:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 00:38:14 - INFO - __main__ - ['others']
03/19/2022 00:38:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:38:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:38:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:38:19 - INFO - __main__ - Starting training!
03/19/2022 00:38:22 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 00:39:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_100_0.0002_8_predictions.txt
03/19/2022 00:39:04 - INFO - __main__ - Classification-F1 on test data: 0.0644
03/19/2022 00:39:04 - INFO - __main__ - prefix=emo_16_100, lr=0.0002, bsz=8, dev_performance=0.6414039589442815, test_performance=0.06436166156665508
03/19/2022 00:39:04 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0001, bsz=8 ...
03/19/2022 00:39:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:39:05 - INFO - __main__ - Printing 3 examples
03/19/2022 00:39:05 - INFO - __main__ -  [emo] how cause yes am listening
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:39:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:39:05 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:39:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:39:05 - INFO - __main__ - Printing 3 examples
03/19/2022 00:39:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/19/2022 00:39:05 - INFO - __main__ - ['others']
03/19/2022 00:39:05 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:39:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:39:05 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:39:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:39:18 - INFO - __main__ - Starting training!
03/19/2022 00:39:23 - INFO - __main__ - Step 10 Global step 10 Train loss 24.821789 on epoch=2
03/19/2022 00:39:28 - INFO - __main__ - Step 20 Global step 20 Train loss 22.044819 on epoch=4
03/19/2022 00:39:33 - INFO - __main__ - Step 30 Global step 30 Train loss 19.611414 on epoch=7
03/19/2022 00:39:38 - INFO - __main__ - Step 40 Global step 40 Train loss 17.439199 on epoch=9
03/19/2022 00:39:43 - INFO - __main__ - Step 50 Global step 50 Train loss 17.934198 on epoch=12
03/19/2022 00:40:06 - INFO - __main__ - Global step 50 Train loss 20.370285 Classification-F1 0.0 on epoch=12
03/19/2022 00:40:12 - INFO - __main__ - Step 60 Global step 60 Train loss 17.006310 on epoch=14
03/19/2022 00:40:17 - INFO - __main__ - Step 70 Global step 70 Train loss 17.400408 on epoch=17
03/19/2022 00:40:22 - INFO - __main__ - Step 80 Global step 80 Train loss 15.964197 on epoch=19
03/19/2022 00:40:27 - INFO - __main__ - Step 90 Global step 90 Train loss 15.954338 on epoch=22
03/19/2022 00:40:32 - INFO - __main__ - Step 100 Global step 100 Train loss 15.686893 on epoch=24
03/19/2022 00:40:49 - INFO - __main__ - Global step 100 Train loss 16.402430 Classification-F1 0.0 on epoch=24
03/19/2022 00:40:54 - INFO - __main__ - Step 110 Global step 110 Train loss 15.379395 on epoch=27
03/19/2022 00:40:59 - INFO - __main__ - Step 120 Global step 120 Train loss 14.941630 on epoch=29
03/19/2022 00:41:04 - INFO - __main__ - Step 130 Global step 130 Train loss 14.879976 on epoch=32
03/19/2022 00:41:09 - INFO - __main__ - Step 140 Global step 140 Train loss 14.276898 on epoch=34
03/19/2022 00:41:14 - INFO - __main__ - Step 150 Global step 150 Train loss 14.333746 on epoch=37
03/19/2022 00:41:26 - INFO - __main__ - Global step 150 Train loss 14.762329 Classification-F1 0.0 on epoch=37
03/19/2022 00:41:31 - INFO - __main__ - Step 160 Global step 160 Train loss 14.134581 on epoch=39
03/19/2022 00:41:36 - INFO - __main__ - Step 170 Global step 170 Train loss 14.000061 on epoch=42
03/19/2022 00:41:41 - INFO - __main__ - Step 180 Global step 180 Train loss 13.465242 on epoch=44
03/19/2022 00:41:46 - INFO - __main__ - Step 190 Global step 190 Train loss 13.442713 on epoch=47
03/19/2022 00:41:51 - INFO - __main__ - Step 200 Global step 200 Train loss 12.670464 on epoch=49
03/19/2022 00:42:01 - INFO - __main__ - Global step 200 Train loss 13.542612 Classification-F1 0.0 on epoch=49
03/19/2022 00:42:06 - INFO - __main__ - Step 210 Global step 210 Train loss 11.725866 on epoch=52
03/19/2022 00:42:11 - INFO - __main__ - Step 220 Global step 220 Train loss 12.364584 on epoch=54
03/19/2022 00:42:16 - INFO - __main__ - Step 230 Global step 230 Train loss 11.426554 on epoch=57
03/19/2022 00:42:21 - INFO - __main__ - Step 240 Global step 240 Train loss 11.296721 on epoch=59
03/19/2022 00:42:26 - INFO - __main__ - Step 250 Global step 250 Train loss 10.944151 on epoch=62
03/19/2022 00:42:32 - INFO - __main__ - Global step 250 Train loss 11.551575 Classification-F1 0.0 on epoch=62
03/19/2022 00:42:37 - INFO - __main__ - Step 260 Global step 260 Train loss 10.940700 on epoch=64
03/19/2022 00:42:42 - INFO - __main__ - Step 270 Global step 270 Train loss 10.260787 on epoch=67
03/19/2022 00:42:47 - INFO - __main__ - Step 280 Global step 280 Train loss 9.750816 on epoch=69
03/19/2022 00:42:52 - INFO - __main__ - Step 290 Global step 290 Train loss 10.099482 on epoch=72
03/19/2022 00:42:57 - INFO - __main__ - Step 300 Global step 300 Train loss 9.442506 on epoch=74
03/19/2022 00:43:02 - INFO - __main__ - Global step 300 Train loss 10.098859 Classification-F1 0.0 on epoch=74
03/19/2022 00:43:07 - INFO - __main__ - Step 310 Global step 310 Train loss 8.396490 on epoch=77
03/19/2022 00:43:12 - INFO - __main__ - Step 320 Global step 320 Train loss 6.593293 on epoch=79
03/19/2022 00:43:17 - INFO - __main__ - Step 330 Global step 330 Train loss 5.765989 on epoch=82
03/19/2022 00:43:22 - INFO - __main__ - Step 340 Global step 340 Train loss 5.596438 on epoch=84
03/19/2022 00:43:27 - INFO - __main__ - Step 350 Global step 350 Train loss 7.066698 on epoch=87
03/19/2022 00:43:27 - INFO - __main__ - Global step 350 Train loss 6.683781 Classification-F1 0.11019607843137255 on epoch=87
03/19/2022 00:43:33 - INFO - __main__ - Step 360 Global step 360 Train loss 4.054815 on epoch=89
03/19/2022 00:43:38 - INFO - __main__ - Step 370 Global step 370 Train loss 3.705626 on epoch=92
03/19/2022 00:43:43 - INFO - __main__ - Step 380 Global step 380 Train loss 3.657420 on epoch=94
03/19/2022 00:43:48 - INFO - __main__ - Step 390 Global step 390 Train loss 3.564708 on epoch=97
03/19/2022 00:43:53 - INFO - __main__ - Step 400 Global step 400 Train loss 3.053997 on epoch=99
03/19/2022 00:43:53 - INFO - __main__ - Global step 400 Train loss 3.607313 Classification-F1 0.2802770562770563 on epoch=99
03/19/2022 00:43:59 - INFO - __main__ - Step 410 Global step 410 Train loss 2.877324 on epoch=102
03/19/2022 00:44:04 - INFO - __main__ - Step 420 Global step 420 Train loss 2.269507 on epoch=104
03/19/2022 00:44:09 - INFO - __main__ - Step 430 Global step 430 Train loss 2.399545 on epoch=107
03/19/2022 00:44:14 - INFO - __main__ - Step 440 Global step 440 Train loss 2.404231 on epoch=109
03/19/2022 00:44:19 - INFO - __main__ - Step 450 Global step 450 Train loss 2.180876 on epoch=112
03/19/2022 00:44:19 - INFO - __main__ - Global step 450 Train loss 2.426297 Classification-F1 0.2421950720087366 on epoch=112
03/19/2022 00:44:24 - INFO - __main__ - Step 460 Global step 460 Train loss 1.494281 on epoch=114
03/19/2022 00:44:29 - INFO - __main__ - Step 470 Global step 470 Train loss 1.560442 on epoch=117
03/19/2022 00:44:34 - INFO - __main__ - Step 480 Global step 480 Train loss 1.346955 on epoch=119
03/19/2022 00:44:39 - INFO - __main__ - Step 490 Global step 490 Train loss 1.123168 on epoch=122
03/19/2022 00:44:44 - INFO - __main__ - Step 500 Global step 500 Train loss 1.159127 on epoch=124
03/19/2022 00:44:44 - INFO - __main__ - Global step 500 Train loss 1.336795 Classification-F1 0.4193700433026809 on epoch=124
03/19/2022 00:44:50 - INFO - __main__ - Step 510 Global step 510 Train loss 1.534239 on epoch=127
03/19/2022 00:44:55 - INFO - __main__ - Step 520 Global step 520 Train loss 1.007859 on epoch=129
03/19/2022 00:45:00 - INFO - __main__ - Step 530 Global step 530 Train loss 1.239858 on epoch=132
03/19/2022 00:45:05 - INFO - __main__ - Step 540 Global step 540 Train loss 1.194573 on epoch=134
03/19/2022 00:45:10 - INFO - __main__ - Step 550 Global step 550 Train loss 1.125073 on epoch=137
03/19/2022 00:45:10 - INFO - __main__ - Global step 550 Train loss 1.220321 Classification-F1 0.3304855691812214 on epoch=137
03/19/2022 00:45:15 - INFO - __main__ - Step 560 Global step 560 Train loss 1.029674 on epoch=139
03/19/2022 00:45:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.787571 on epoch=142
03/19/2022 00:45:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.596593 on epoch=144
03/19/2022 00:45:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.623356 on epoch=147
03/19/2022 00:45:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.721640 on epoch=149
03/19/2022 00:45:36 - INFO - __main__ - Global step 600 Train loss 0.751767 Classification-F1 0.3977596477596478 on epoch=149
03/19/2022 00:45:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.641882 on epoch=152
03/19/2022 00:45:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.676308 on epoch=154
03/19/2022 00:45:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.587058 on epoch=157
03/19/2022 00:45:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.685273 on epoch=159
03/19/2022 00:46:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.650917 on epoch=162
03/19/2022 00:46:01 - INFO - __main__ - Global step 650 Train loss 0.648288 Classification-F1 0.6860328002168609 on epoch=162
03/19/2022 00:46:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.621536 on epoch=164
03/19/2022 00:46:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.459930 on epoch=167
03/19/2022 00:46:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.598817 on epoch=169
03/19/2022 00:46:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.496645 on epoch=172
03/19/2022 00:46:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.410107 on epoch=174
03/19/2022 00:46:27 - INFO - __main__ - Global step 700 Train loss 0.517407 Classification-F1 0.5595117845117845 on epoch=174
03/19/2022 00:46:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.490573 on epoch=177
03/19/2022 00:46:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.396510 on epoch=179
03/19/2022 00:46:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.460661 on epoch=182
03/19/2022 00:46:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.483931 on epoch=184
03/19/2022 00:46:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.347384 on epoch=187
03/19/2022 00:46:52 - INFO - __main__ - Global step 750 Train loss 0.435812 Classification-F1 0.7301193543587052 on epoch=187
03/19/2022 00:46:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.313109 on epoch=189
03/19/2022 00:47:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.331296 on epoch=192
03/19/2022 00:47:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.297601 on epoch=194
03/19/2022 00:47:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.413873 on epoch=197
03/19/2022 00:47:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.343420 on epoch=199
03/19/2022 00:47:17 - INFO - __main__ - Global step 800 Train loss 0.339860 Classification-F1 0.7033264473753906 on epoch=199
03/19/2022 00:47:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.321934 on epoch=202
03/19/2022 00:47:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.377070 on epoch=204
03/19/2022 00:47:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.339744 on epoch=207
03/19/2022 00:47:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.228230 on epoch=209
03/19/2022 00:47:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.297416 on epoch=212
03/19/2022 00:47:43 - INFO - __main__ - Global step 850 Train loss 0.312879 Classification-F1 0.7018572386219445 on epoch=212
03/19/2022 00:47:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.227277 on epoch=214
03/19/2022 00:47:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.352494 on epoch=217
03/19/2022 00:47:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.362318 on epoch=219
03/19/2022 00:48:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.260626 on epoch=222
03/19/2022 00:48:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.239428 on epoch=224
03/19/2022 00:48:08 - INFO - __main__ - Global step 900 Train loss 0.288429 Classification-F1 0.718348089468779 on epoch=224
03/19/2022 00:48:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.223927 on epoch=227
03/19/2022 00:48:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.240017 on epoch=229
03/19/2022 00:48:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.271771 on epoch=232
03/19/2022 00:48:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.190438 on epoch=234
03/19/2022 00:48:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.188122 on epoch=237
03/19/2022 00:48:33 - INFO - __main__ - Global step 950 Train loss 0.222855 Classification-F1 0.6859318996415771 on epoch=237
03/19/2022 00:48:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.209824 on epoch=239
03/19/2022 00:48:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.280978 on epoch=242
03/19/2022 00:48:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.234158 on epoch=244
03/19/2022 00:48:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.188695 on epoch=247
03/19/2022 00:48:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.283831 on epoch=249
03/19/2022 00:48:58 - INFO - __main__ - Global step 1000 Train loss 0.239497 Classification-F1 0.6590996168582376 on epoch=249
03/19/2022 00:48:58 - INFO - __main__ - save last model!
03/19/2022 00:48:59 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:48:59 - INFO - __main__ - Printing 3 examples
03/19/2022 00:48:59 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:48:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:48:59 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:48:59 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:48:59 - INFO - __main__ - Printing 3 examples
03/19/2022 00:48:59 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 00:48:59 - INFO - __main__ - ['others']
03/19/2022 00:48:59 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:48:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:48:59 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:49:05 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:49:06 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 00:49:06 - INFO - __main__ - Printing 3 examples
03/19/2022 00:49:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 00:49:06 - INFO - __main__ - ['others']
03/19/2022 00:49:06 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 00:49:06 - INFO - __main__ - ['others']
03/19/2022 00:49:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 00:49:06 - INFO - __main__ - ['others']
03/19/2022 00:49:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:49:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:49:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:49:12 - INFO - __main__ - Starting training!
03/19/2022 00:49:13 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 00:49:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_100_0.0001_8_predictions.txt
03/19/2022 00:49:54 - INFO - __main__ - Classification-F1 on test data: 0.2433
03/19/2022 00:49:55 - INFO - __main__ - prefix=emo_16_100, lr=0.0001, bsz=8, dev_performance=0.7301193543587052, test_performance=0.24328068262612335
03/19/2022 00:49:55 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0005, bsz=8 ...
03/19/2022 00:49:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:49:56 - INFO - __main__ - Printing 3 examples
03/19/2022 00:49:56 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:49:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:49:56 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:49:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:49:56 - INFO - __main__ - Printing 3 examples
03/19/2022 00:49:56 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 00:49:56 - INFO - __main__ - ['others']
03/19/2022 00:49:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:49:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:49:56 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:50:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:50:09 - INFO - __main__ - Starting training!
03/19/2022 00:50:13 - INFO - __main__ - Step 10 Global step 10 Train loss 24.004360 on epoch=2
03/19/2022 00:50:18 - INFO - __main__ - Step 20 Global step 20 Train loss 18.039078 on epoch=4
03/19/2022 00:50:23 - INFO - __main__ - Step 30 Global step 30 Train loss 15.737963 on epoch=7
03/19/2022 00:50:27 - INFO - __main__ - Step 40 Global step 40 Train loss 14.316673 on epoch=9
03/19/2022 00:50:32 - INFO - __main__ - Step 50 Global step 50 Train loss 13.595488 on epoch=12
03/19/2022 00:50:33 - INFO - __main__ - Global step 50 Train loss 17.138712 Classification-F1 0.0 on epoch=12
03/19/2022 00:50:38 - INFO - __main__ - Step 60 Global step 60 Train loss 11.825974 on epoch=14
03/19/2022 00:50:43 - INFO - __main__ - Step 70 Global step 70 Train loss 10.007213 on epoch=17
03/19/2022 00:50:48 - INFO - __main__ - Step 80 Global step 80 Train loss 6.825598 on epoch=19
03/19/2022 00:50:53 - INFO - __main__ - Step 90 Global step 90 Train loss 4.973505 on epoch=22
03/19/2022 00:50:58 - INFO - __main__ - Step 100 Global step 100 Train loss 3.930703 on epoch=24
03/19/2022 00:50:59 - INFO - __main__ - Global step 100 Train loss 7.512599 Classification-F1 0.07999999999999999 on epoch=24
03/19/2022 00:51:04 - INFO - __main__ - Step 110 Global step 110 Train loss 2.383581 on epoch=27
03/19/2022 00:51:09 - INFO - __main__ - Step 120 Global step 120 Train loss 2.428811 on epoch=29
03/19/2022 00:51:14 - INFO - __main__ - Step 130 Global step 130 Train loss 2.595486 on epoch=32
03/19/2022 00:51:19 - INFO - __main__ - Step 140 Global step 140 Train loss 3.076547 on epoch=34
03/19/2022 00:51:24 - INFO - __main__ - Step 150 Global step 150 Train loss 2.944641 on epoch=37
03/19/2022 00:51:25 - INFO - __main__ - Global step 150 Train loss 2.685813 Classification-F1 0.1 on epoch=37
03/19/2022 00:51:30 - INFO - __main__ - Step 160 Global step 160 Train loss 2.655725 on epoch=39
03/19/2022 00:51:35 - INFO - __main__ - Step 170 Global step 170 Train loss 2.651944 on epoch=42
03/19/2022 00:51:40 - INFO - __main__ - Step 180 Global step 180 Train loss 2.262702 on epoch=44
03/19/2022 00:51:45 - INFO - __main__ - Step 190 Global step 190 Train loss 2.176881 on epoch=47
03/19/2022 00:51:50 - INFO - __main__ - Step 200 Global step 200 Train loss 1.938896 on epoch=49
03/19/2022 00:51:50 - INFO - __main__ - Global step 200 Train loss 2.337229 Classification-F1 0.14621798689696247 on epoch=49
03/19/2022 00:51:56 - INFO - __main__ - Step 210 Global step 210 Train loss 1.752233 on epoch=52
03/19/2022 00:52:01 - INFO - __main__ - Step 220 Global step 220 Train loss 1.492250 on epoch=54
03/19/2022 00:52:05 - INFO - __main__ - Step 230 Global step 230 Train loss 1.484900 on epoch=57
03/19/2022 00:52:10 - INFO - __main__ - Step 240 Global step 240 Train loss 1.190826 on epoch=59
03/19/2022 00:52:15 - INFO - __main__ - Step 250 Global step 250 Train loss 1.166506 on epoch=62
03/19/2022 00:52:16 - INFO - __main__ - Global step 250 Train loss 1.417343 Classification-F1 0.1476190476190476 on epoch=62
03/19/2022 00:52:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.389757 on epoch=64
03/19/2022 00:52:26 - INFO - __main__ - Step 270 Global step 270 Train loss 1.107723 on epoch=67
03/19/2022 00:52:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.991754 on epoch=69
03/19/2022 00:52:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.122052 on epoch=72
03/19/2022 00:52:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.982907 on epoch=74
03/19/2022 00:52:41 - INFO - __main__ - Global step 300 Train loss 1.118839 Classification-F1 0.1 on epoch=74
03/19/2022 00:52:46 - INFO - __main__ - Step 310 Global step 310 Train loss 1.008536 on epoch=77
03/19/2022 00:52:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.905627 on epoch=79
03/19/2022 00:52:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.987800 on epoch=82
03/19/2022 00:53:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.833356 on epoch=84
03/19/2022 00:53:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.954629 on epoch=87
03/19/2022 00:53:06 - INFO - __main__ - Global step 350 Train loss 0.937989 Classification-F1 0.27356942014029106 on epoch=87
03/19/2022 00:53:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.900516 on epoch=89
03/19/2022 00:53:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.783481 on epoch=92
03/19/2022 00:53:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.868252 on epoch=94
03/19/2022 00:53:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.879179 on epoch=97
03/19/2022 00:53:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.801359 on epoch=99
03/19/2022 00:53:31 - INFO - __main__ - Global step 400 Train loss 0.846557 Classification-F1 0.30779187758592796 on epoch=99
03/19/2022 00:53:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.784796 on epoch=102
03/19/2022 00:53:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.775598 on epoch=104
03/19/2022 00:53:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.734384 on epoch=107
03/19/2022 00:53:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.808566 on epoch=109
03/19/2022 00:53:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.743688 on epoch=112
03/19/2022 00:53:57 - INFO - __main__ - Global step 450 Train loss 0.769406 Classification-F1 0.3695580186097428 on epoch=112
03/19/2022 00:54:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.722671 on epoch=114
03/19/2022 00:54:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.720291 on epoch=117
03/19/2022 00:54:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.643788 on epoch=119
03/19/2022 00:54:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.628122 on epoch=122
03/19/2022 00:54:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.708120 on epoch=124
03/19/2022 00:54:22 - INFO - __main__ - Global step 500 Train loss 0.684598 Classification-F1 0.5505536130536131 on epoch=124
03/19/2022 00:54:28 - INFO - __main__ - Step 510 Global step 510 Train loss 1.102083 on epoch=127
03/19/2022 00:54:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.933230 on epoch=129
03/19/2022 00:54:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.458889 on epoch=132
03/19/2022 00:54:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.431357 on epoch=134
03/19/2022 00:54:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.501867 on epoch=137
03/19/2022 00:54:48 - INFO - __main__ - Global step 550 Train loss 0.685485 Classification-F1 0.5197335840058693 on epoch=137
03/19/2022 00:54:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.400050 on epoch=139
03/19/2022 00:54:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.370977 on epoch=142
03/19/2022 00:55:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.314610 on epoch=144
03/19/2022 00:55:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.309902 on epoch=147
03/19/2022 00:55:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.333280 on epoch=149
03/19/2022 00:55:13 - INFO - __main__ - Global step 600 Train loss 0.345764 Classification-F1 0.616592204092204 on epoch=149
03/19/2022 00:55:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.159564 on epoch=152
03/19/2022 00:55:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.290637 on epoch=154
03/19/2022 00:55:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.199004 on epoch=157
03/19/2022 00:55:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.211219 on epoch=159
03/19/2022 00:55:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.095010 on epoch=162
03/19/2022 00:55:39 - INFO - __main__ - Global step 650 Train loss 0.191087 Classification-F1 0.6160493517775996 on epoch=162
03/19/2022 00:55:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.130836 on epoch=164
03/19/2022 00:55:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.125605 on epoch=167
03/19/2022 00:55:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.086208 on epoch=169
03/19/2022 00:55:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.065161 on epoch=172
03/19/2022 00:56:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.144113 on epoch=174
03/19/2022 00:56:04 - INFO - __main__ - Global step 700 Train loss 0.110385 Classification-F1 0.7478838728838728 on epoch=174
03/19/2022 00:56:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.060359 on epoch=177
03/19/2022 00:56:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.095568 on epoch=179
03/19/2022 00:56:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.078328 on epoch=182
03/19/2022 00:56:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.086341 on epoch=184
03/19/2022 00:56:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.012240 on epoch=187
03/19/2022 00:56:30 - INFO - __main__ - Global step 750 Train loss 0.066567 Classification-F1 0.6935775335775336 on epoch=187
03/19/2022 00:56:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.020571 on epoch=189
03/19/2022 00:56:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.021879 on epoch=192
03/19/2022 00:56:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.021695 on epoch=194
03/19/2022 00:56:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.022664 on epoch=197
03/19/2022 00:56:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.068314 on epoch=199
03/19/2022 00:56:56 - INFO - __main__ - Global step 800 Train loss 0.031024 Classification-F1 0.6954954954954955 on epoch=199
03/19/2022 00:57:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007429 on epoch=202
03/19/2022 00:57:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.015224 on epoch=204
03/19/2022 00:57:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.006138 on epoch=207
03/19/2022 00:57:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.058303 on epoch=209
03/19/2022 00:57:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008228 on epoch=212
03/19/2022 00:57:21 - INFO - __main__ - Global step 850 Train loss 0.019064 Classification-F1 0.6657738095238096 on epoch=212
03/19/2022 00:57:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.007712 on epoch=214
03/19/2022 00:57:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.037733 on epoch=217
03/19/2022 00:57:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003154 on epoch=219
03/19/2022 00:57:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000456 on epoch=222
03/19/2022 00:57:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.063684 on epoch=224
03/19/2022 00:57:47 - INFO - __main__ - Global step 900 Train loss 0.022548 Classification-F1 0.6394033865150682 on epoch=224
03/19/2022 00:57:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.017699 on epoch=227
03/19/2022 00:57:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.003315 on epoch=229
03/19/2022 00:58:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002375 on epoch=232
03/19/2022 00:58:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.011459 on epoch=234
03/19/2022 00:58:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.003804 on epoch=237
03/19/2022 00:58:12 - INFO - __main__ - Global step 950 Train loss 0.007730 Classification-F1 0.6994864612511672 on epoch=237
03/19/2022 00:58:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001504 on epoch=239
03/19/2022 00:58:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.010338 on epoch=242
03/19/2022 00:58:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.072023 on epoch=244
03/19/2022 00:58:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.006438 on epoch=247
03/19/2022 00:58:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003454 on epoch=249
03/19/2022 00:58:37 - INFO - __main__ - Global step 1000 Train loss 0.018751 Classification-F1 0.7043186450207322 on epoch=249
03/19/2022 00:58:37 - INFO - __main__ - save last model!
03/19/2022 00:58:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:58:38 - INFO - __main__ - Printing 3 examples
03/19/2022 00:58:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 00:58:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:58:38 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:58:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:58:38 - INFO - __main__ - Printing 3 examples
03/19/2022 00:58:38 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 00:58:38 - INFO - __main__ - ['others']
03/19/2022 00:58:38 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:58:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:58:38 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:58:44 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 00:58:45 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 00:58:45 - INFO - __main__ - Printing 3 examples
03/19/2022 00:58:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 00:58:45 - INFO - __main__ - ['others']
03/19/2022 00:58:45 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 00:58:45 - INFO - __main__ - ['others']
03/19/2022 00:58:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 00:58:45 - INFO - __main__ - ['others']
03/19/2022 00:58:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:58:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:58:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:58:51 - INFO - __main__ - Starting training!
03/19/2022 00:58:52 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 00:59:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_13_0.0005_8_predictions.txt
03/19/2022 00:59:42 - INFO - __main__ - Classification-F1 on test data: 0.1544
03/19/2022 00:59:43 - INFO - __main__ - prefix=emo_16_13, lr=0.0005, bsz=8, dev_performance=0.7478838728838728, test_performance=0.1544454066083761
03/19/2022 00:59:43 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0003, bsz=8 ...
03/19/2022 00:59:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:59:44 - INFO - __main__ - Printing 3 examples
03/19/2022 00:59:44 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 00:59:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:59:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 00:59:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 00:59:44 - INFO - __main__ - Printing 3 examples
03/19/2022 00:59:44 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 00:59:44 - INFO - __main__ - ['others']
03/19/2022 00:59:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 00:59:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 00:59:44 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 00:59:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 00:59:56 - INFO - __main__ - Starting training!
03/19/2022 01:00:01 - INFO - __main__ - Step 10 Global step 10 Train loss 24.529341 on epoch=2
03/19/2022 01:00:05 - INFO - __main__ - Step 20 Global step 20 Train loss 18.637053 on epoch=4
03/19/2022 01:00:10 - INFO - __main__ - Step 30 Global step 30 Train loss 17.343058 on epoch=7
03/19/2022 01:00:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.920980 on epoch=9
03/19/2022 01:00:20 - INFO - __main__ - Step 50 Global step 50 Train loss 16.042126 on epoch=12
03/19/2022 01:00:27 - INFO - __main__ - Global step 50 Train loss 18.694513 Classification-F1 0.004524886877828054 on epoch=12
03/19/2022 01:00:32 - INFO - __main__ - Step 60 Global step 60 Train loss 15.003095 on epoch=14
03/19/2022 01:00:37 - INFO - __main__ - Step 70 Global step 70 Train loss 14.034811 on epoch=17
03/19/2022 01:00:42 - INFO - __main__ - Step 80 Global step 80 Train loss 12.937452 on epoch=19
03/19/2022 01:00:47 - INFO - __main__ - Step 90 Global step 90 Train loss 11.137117 on epoch=22
03/19/2022 01:00:52 - INFO - __main__ - Step 100 Global step 100 Train loss 10.756793 on epoch=24
03/19/2022 01:00:53 - INFO - __main__ - Global step 100 Train loss 12.773853 Classification-F1 0.0106951871657754 on epoch=24
03/19/2022 01:00:59 - INFO - __main__ - Step 110 Global step 110 Train loss 9.828654 on epoch=27
03/19/2022 01:01:03 - INFO - __main__ - Step 120 Global step 120 Train loss 7.223470 on epoch=29
03/19/2022 01:01:09 - INFO - __main__ - Step 130 Global step 130 Train loss 5.802352 on epoch=32
03/19/2022 01:01:14 - INFO - __main__ - Step 140 Global step 140 Train loss 4.619003 on epoch=34
03/19/2022 01:01:18 - INFO - __main__ - Step 150 Global step 150 Train loss 3.872597 on epoch=37
03/19/2022 01:01:19 - INFO - __main__ - Global step 150 Train loss 6.269216 Classification-F1 0.14290271132376398 on epoch=37
03/19/2022 01:01:25 - INFO - __main__ - Step 160 Global step 160 Train loss 4.066551 on epoch=39
03/19/2022 01:01:30 - INFO - __main__ - Step 170 Global step 170 Train loss 3.453806 on epoch=42
03/19/2022 01:01:35 - INFO - __main__ - Step 180 Global step 180 Train loss 3.300191 on epoch=44
03/19/2022 01:01:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.572437 on epoch=47
03/19/2022 01:01:45 - INFO - __main__ - Step 200 Global step 200 Train loss 2.831638 on epoch=49
03/19/2022 01:01:46 - INFO - __main__ - Global step 200 Train loss 3.244925 Classification-F1 0.19957983193277312 on epoch=49
03/19/2022 01:01:51 - INFO - __main__ - Step 210 Global step 210 Train loss 2.615227 on epoch=52
03/19/2022 01:01:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.228044 on epoch=54
03/19/2022 01:02:01 - INFO - __main__ - Step 230 Global step 230 Train loss 3.001079 on epoch=57
03/19/2022 01:02:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.091937 on epoch=59
03/19/2022 01:02:11 - INFO - __main__ - Step 250 Global step 250 Train loss 2.034389 on epoch=62
03/19/2022 01:02:12 - INFO - __main__ - Global step 250 Train loss 2.394135 Classification-F1 0.1 on epoch=62
03/19/2022 01:02:17 - INFO - __main__ - Step 260 Global step 260 Train loss 2.556459 on epoch=64
03/19/2022 01:02:22 - INFO - __main__ - Step 270 Global step 270 Train loss 2.461917 on epoch=67
03/19/2022 01:02:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.568311 on epoch=69
03/19/2022 01:02:32 - INFO - __main__ - Step 290 Global step 290 Train loss 1.844150 on epoch=72
03/19/2022 01:02:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.018792 on epoch=74
03/19/2022 01:02:37 - INFO - __main__ - Global step 300 Train loss 2.089926 Classification-F1 0.1346749226006192 on epoch=74
03/19/2022 01:02:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.908988 on epoch=77
03/19/2022 01:02:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.766474 on epoch=79
03/19/2022 01:02:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.801393 on epoch=82
03/19/2022 01:02:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.220295 on epoch=84
03/19/2022 01:03:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.687922 on epoch=87
03/19/2022 01:03:02 - INFO - __main__ - Global step 350 Train loss 1.477014 Classification-F1 0.4094131881427707 on epoch=87
03/19/2022 01:03:08 - INFO - __main__ - Step 360 Global step 360 Train loss 1.181492 on epoch=89
03/19/2022 01:03:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.997755 on epoch=92
03/19/2022 01:03:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.892242 on epoch=94
03/19/2022 01:03:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.736841 on epoch=97
03/19/2022 01:03:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.635091 on epoch=99
03/19/2022 01:03:28 - INFO - __main__ - Global step 400 Train loss 0.888684 Classification-F1 0.4852352416167283 on epoch=99
03/19/2022 01:03:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.664020 on epoch=102
03/19/2022 01:03:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.631657 on epoch=104
03/19/2022 01:03:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.651856 on epoch=107
03/19/2022 01:03:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.598768 on epoch=109
03/19/2022 01:03:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.454789 on epoch=112
03/19/2022 01:03:54 - INFO - __main__ - Global step 450 Train loss 0.600218 Classification-F1 0.5478927775123428 on epoch=112
03/19/2022 01:04:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.654590 on epoch=114
03/19/2022 01:04:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.619494 on epoch=117
03/19/2022 01:04:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.560915 on epoch=119
03/19/2022 01:04:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.476434 on epoch=122
03/19/2022 01:04:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.496912 on epoch=124
03/19/2022 01:04:20 - INFO - __main__ - Global step 500 Train loss 0.561669 Classification-F1 0.4614057239057239 on epoch=124
03/19/2022 01:04:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.436815 on epoch=127
03/19/2022 01:04:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.467679 on epoch=129
03/19/2022 01:04:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.356627 on epoch=132
03/19/2022 01:04:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.411996 on epoch=134
03/19/2022 01:04:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.431360 on epoch=137
03/19/2022 01:04:45 - INFO - __main__ - Global step 550 Train loss 0.420895 Classification-F1 0.547187019969278 on epoch=137
03/19/2022 01:04:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.338461 on epoch=139
03/19/2022 01:04:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.438645 on epoch=142
03/19/2022 01:04:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.334733 on epoch=144
03/19/2022 01:05:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.323986 on epoch=147
03/19/2022 01:05:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.297380 on epoch=149
03/19/2022 01:05:09 - INFO - __main__ - Global step 600 Train loss 0.346641 Classification-F1 0.619017094017094 on epoch=149
03/19/2022 01:05:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.191973 on epoch=152
03/19/2022 01:05:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.244496 on epoch=154
03/19/2022 01:05:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.210201 on epoch=157
03/19/2022 01:05:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.275473 on epoch=159
03/19/2022 01:05:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.291899 on epoch=162
03/19/2022 01:05:35 - INFO - __main__ - Global step 650 Train loss 0.242808 Classification-F1 0.6072667342799188 on epoch=162
03/19/2022 01:05:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.291574 on epoch=164
03/19/2022 01:05:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.240460 on epoch=167
03/19/2022 01:05:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.164427 on epoch=169
03/19/2022 01:05:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.163114 on epoch=172
03/19/2022 01:05:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.228294 on epoch=174
03/19/2022 01:06:00 - INFO - __main__ - Global step 700 Train loss 0.217573 Classification-F1 0.6994152453343221 on epoch=174
03/19/2022 01:06:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.169203 on epoch=177
03/19/2022 01:06:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.141419 on epoch=179
03/19/2022 01:06:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.221593 on epoch=182
03/19/2022 01:06:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.131562 on epoch=184
03/19/2022 01:06:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.074077 on epoch=187
03/19/2022 01:06:26 - INFO - __main__ - Global step 750 Train loss 0.147571 Classification-F1 0.730249265698182 on epoch=187
03/19/2022 01:06:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.155279 on epoch=189
03/19/2022 01:06:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.087503 on epoch=192
03/19/2022 01:06:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.122596 on epoch=194
03/19/2022 01:06:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.098397 on epoch=197
03/19/2022 01:06:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.132583 on epoch=199
03/19/2022 01:06:51 - INFO - __main__ - Global step 800 Train loss 0.119271 Classification-F1 0.6537492584117298 on epoch=199
03/19/2022 01:06:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.056816 on epoch=202
03/19/2022 01:07:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.107937 on epoch=204
03/19/2022 01:07:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.087582 on epoch=207
03/19/2022 01:07:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.156374 on epoch=209
03/19/2022 01:07:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.136338 on epoch=212
03/19/2022 01:07:16 - INFO - __main__ - Global step 850 Train loss 0.109009 Classification-F1 0.7430069365553237 on epoch=212
03/19/2022 01:07:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.059944 on epoch=214
03/19/2022 01:07:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.091751 on epoch=217
03/19/2022 01:07:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.068371 on epoch=219
03/19/2022 01:07:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.045891 on epoch=222
03/19/2022 01:07:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.058430 on epoch=224
03/19/2022 01:07:42 - INFO - __main__ - Global step 900 Train loss 0.064877 Classification-F1 0.5422551101540829 on epoch=224
03/19/2022 01:07:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.030798 on epoch=227
03/19/2022 01:07:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.044403 on epoch=229
03/19/2022 01:07:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.040517 on epoch=232
03/19/2022 01:08:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.108895 on epoch=234
03/19/2022 01:08:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.014791 on epoch=237
03/19/2022 01:08:06 - INFO - __main__ - Global step 950 Train loss 0.047880 Classification-F1 0.7299601896844446 on epoch=237
03/19/2022 01:08:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.034596 on epoch=239
03/19/2022 01:08:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.076504 on epoch=242
03/19/2022 01:08:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.099528 on epoch=244
03/19/2022 01:08:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.053012 on epoch=247
03/19/2022 01:08:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.037574 on epoch=249
03/19/2022 01:08:31 - INFO - __main__ - Global step 1000 Train loss 0.060243 Classification-F1 0.6738636363636364 on epoch=249
03/19/2022 01:08:31 - INFO - __main__ - save last model!
03/19/2022 01:08:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:08:32 - INFO - __main__ - Printing 3 examples
03/19/2022 01:08:32 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:08:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:08:32 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:08:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:08:32 - INFO - __main__ - Printing 3 examples
03/19/2022 01:08:32 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 01:08:32 - INFO - __main__ - ['others']
03/19/2022 01:08:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:08:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:08:32 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:08:38 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:08:39 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:08:39 - INFO - __main__ - Printing 3 examples
03/19/2022 01:08:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:08:39 - INFO - __main__ - ['others']
03/19/2022 01:08:39 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:08:39 - INFO - __main__ - ['others']
03/19/2022 01:08:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:08:39 - INFO - __main__ - ['others']
03/19/2022 01:08:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:08:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:08:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:08:45 - INFO - __main__ - Starting training!
03/19/2022 01:08:46 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 01:09:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_13_0.0003_8_predictions.txt
03/19/2022 01:09:29 - INFO - __main__ - Classification-F1 on test data: 0.0916
03/19/2022 01:09:29 - INFO - __main__ - prefix=emo_16_13, lr=0.0003, bsz=8, dev_performance=0.7430069365553237, test_performance=0.09159610924242367
03/19/2022 01:09:29 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0002, bsz=8 ...
03/19/2022 01:09:30 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:09:30 - INFO - __main__ - Printing 3 examples
03/19/2022 01:09:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:09:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:09:30 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:09:30 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:09:30 - INFO - __main__ - Printing 3 examples
03/19/2022 01:09:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 01:09:30 - INFO - __main__ - ['others']
03/19/2022 01:09:30 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:09:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:09:30 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:09:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:09:43 - INFO - __main__ - Starting training!
03/19/2022 01:09:47 - INFO - __main__ - Step 10 Global step 10 Train loss 25.300188 on epoch=2
03/19/2022 01:09:52 - INFO - __main__ - Step 20 Global step 20 Train loss 21.467037 on epoch=4
03/19/2022 01:09:57 - INFO - __main__ - Step 30 Global step 30 Train loss 18.950905 on epoch=7
03/19/2022 01:10:02 - INFO - __main__ - Step 40 Global step 40 Train loss 17.148701 on epoch=9
03/19/2022 01:10:07 - INFO - __main__ - Step 50 Global step 50 Train loss 17.197723 on epoch=12
03/19/2022 01:10:16 - INFO - __main__ - Global step 50 Train loss 20.012911 Classification-F1 0.0 on epoch=12
03/19/2022 01:10:22 - INFO - __main__ - Step 60 Global step 60 Train loss 14.292996 on epoch=14
03/19/2022 01:10:27 - INFO - __main__ - Step 70 Global step 70 Train loss 15.186623 on epoch=17
03/19/2022 01:10:31 - INFO - __main__ - Step 80 Global step 80 Train loss 14.705218 on epoch=19
03/19/2022 01:10:36 - INFO - __main__ - Step 90 Global step 90 Train loss 14.585933 on epoch=22
03/19/2022 01:10:41 - INFO - __main__ - Step 100 Global step 100 Train loss 13.127194 on epoch=24
03/19/2022 01:10:48 - INFO - __main__ - Global step 100 Train loss 14.379592 Classification-F1 0.0 on epoch=24
03/19/2022 01:10:53 - INFO - __main__ - Step 110 Global step 110 Train loss 12.566326 on epoch=27
03/19/2022 01:10:58 - INFO - __main__ - Step 120 Global step 120 Train loss 12.392630 on epoch=29
03/19/2022 01:11:03 - INFO - __main__ - Step 130 Global step 130 Train loss 11.916698 on epoch=32
03/19/2022 01:11:08 - INFO - __main__ - Step 140 Global step 140 Train loss 10.276508 on epoch=34
03/19/2022 01:11:13 - INFO - __main__ - Step 150 Global step 150 Train loss 9.901960 on epoch=37
03/19/2022 01:11:16 - INFO - __main__ - Global step 150 Train loss 11.410824 Classification-F1 0.0 on epoch=37
03/19/2022 01:11:21 - INFO - __main__ - Step 160 Global step 160 Train loss 9.448565 on epoch=39
03/19/2022 01:11:26 - INFO - __main__ - Step 170 Global step 170 Train loss 8.607574 on epoch=42
03/19/2022 01:11:31 - INFO - __main__ - Step 180 Global step 180 Train loss 7.861180 on epoch=44
03/19/2022 01:11:36 - INFO - __main__ - Step 190 Global step 190 Train loss 6.041973 on epoch=47
03/19/2022 01:11:41 - INFO - __main__ - Step 200 Global step 200 Train loss 5.472357 on epoch=49
03/19/2022 01:11:44 - INFO - __main__ - Global step 200 Train loss 7.486330 Classification-F1 0.0 on epoch=49
03/19/2022 01:11:49 - INFO - __main__ - Step 210 Global step 210 Train loss 3.993319 on epoch=52
03/19/2022 01:11:53 - INFO - __main__ - Step 220 Global step 220 Train loss 3.845657 on epoch=54
03/19/2022 01:11:58 - INFO - __main__ - Step 230 Global step 230 Train loss 3.638049 on epoch=57
03/19/2022 01:12:03 - INFO - __main__ - Step 240 Global step 240 Train loss 3.301685 on epoch=59
03/19/2022 01:12:08 - INFO - __main__ - Step 250 Global step 250 Train loss 4.202084 on epoch=62
03/19/2022 01:12:09 - INFO - __main__ - Global step 250 Train loss 3.796159 Classification-F1 0.2861111111111111 on epoch=62
03/19/2022 01:12:14 - INFO - __main__ - Step 260 Global step 260 Train loss 3.913849 on epoch=64
03/19/2022 01:12:19 - INFO - __main__ - Step 270 Global step 270 Train loss 3.561993 on epoch=67
03/19/2022 01:12:24 - INFO - __main__ - Step 280 Global step 280 Train loss 2.774388 on epoch=69
03/19/2022 01:12:29 - INFO - __main__ - Step 290 Global step 290 Train loss 3.922546 on epoch=72
03/19/2022 01:12:34 - INFO - __main__ - Step 300 Global step 300 Train loss 3.244195 on epoch=74
03/19/2022 01:12:35 - INFO - __main__ - Global step 300 Train loss 3.483394 Classification-F1 0.19654556283502084 on epoch=74
03/19/2022 01:12:40 - INFO - __main__ - Step 310 Global step 310 Train loss 2.796278 on epoch=77
03/19/2022 01:12:45 - INFO - __main__ - Step 320 Global step 320 Train loss 2.710356 on epoch=79
03/19/2022 01:12:50 - INFO - __main__ - Step 330 Global step 330 Train loss 2.744750 on epoch=82
03/19/2022 01:12:55 - INFO - __main__ - Step 340 Global step 340 Train loss 2.647433 on epoch=84
03/19/2022 01:13:00 - INFO - __main__ - Step 350 Global step 350 Train loss 3.092466 on epoch=87
03/19/2022 01:13:00 - INFO - __main__ - Global step 350 Train loss 2.798256 Classification-F1 0.1 on epoch=87
03/19/2022 01:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 2.393641 on epoch=89
03/19/2022 01:13:10 - INFO - __main__ - Step 370 Global step 370 Train loss 2.183240 on epoch=92
03/19/2022 01:13:15 - INFO - __main__ - Step 380 Global step 380 Train loss 2.384714 on epoch=94
03/19/2022 01:13:20 - INFO - __main__ - Step 390 Global step 390 Train loss 2.231220 on epoch=97
03/19/2022 01:13:25 - INFO - __main__ - Step 400 Global step 400 Train loss 2.623443 on epoch=99
03/19/2022 01:13:25 - INFO - __main__ - Global step 400 Train loss 2.363251 Classification-F1 0.1 on epoch=99
03/19/2022 01:13:30 - INFO - __main__ - Step 410 Global step 410 Train loss 2.214236 on epoch=102
03/19/2022 01:13:35 - INFO - __main__ - Step 420 Global step 420 Train loss 1.685393 on epoch=104
03/19/2022 01:13:40 - INFO - __main__ - Step 430 Global step 430 Train loss 1.857040 on epoch=107
03/19/2022 01:13:45 - INFO - __main__ - Step 440 Global step 440 Train loss 2.715850 on epoch=109
03/19/2022 01:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 2.203452 on epoch=112
03/19/2022 01:13:51 - INFO - __main__ - Global step 450 Train loss 2.135194 Classification-F1 0.33779904306220093 on epoch=112
03/19/2022 01:13:56 - INFO - __main__ - Step 460 Global step 460 Train loss 2.205940 on epoch=114
03/19/2022 01:14:01 - INFO - __main__ - Step 470 Global step 470 Train loss 1.800897 on epoch=117
03/19/2022 01:14:06 - INFO - __main__ - Step 480 Global step 480 Train loss 1.351373 on epoch=119
03/19/2022 01:14:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.599001 on epoch=122
03/19/2022 01:14:16 - INFO - __main__ - Step 500 Global step 500 Train loss 1.396644 on epoch=124
03/19/2022 01:14:17 - INFO - __main__ - Global step 500 Train loss 1.670771 Classification-F1 0.32564935064935063 on epoch=124
03/19/2022 01:14:22 - INFO - __main__ - Step 510 Global step 510 Train loss 1.302805 on epoch=127
03/19/2022 01:14:27 - INFO - __main__ - Step 520 Global step 520 Train loss 1.310003 on epoch=129
03/19/2022 01:14:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.060244 on epoch=132
03/19/2022 01:14:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.322863 on epoch=134
03/19/2022 01:14:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.004004 on epoch=137
03/19/2022 01:14:42 - INFO - __main__ - Global step 550 Train loss 1.199984 Classification-F1 0.35645021645021646 on epoch=137
03/19/2022 01:14:48 - INFO - __main__ - Step 560 Global step 560 Train loss 1.034381 on epoch=139
03/19/2022 01:14:52 - INFO - __main__ - Step 570 Global step 570 Train loss 1.056481 on epoch=142
03/19/2022 01:14:57 - INFO - __main__ - Step 580 Global step 580 Train loss 1.075034 on epoch=144
03/19/2022 01:15:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.782281 on epoch=147
03/19/2022 01:15:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.904674 on epoch=149
03/19/2022 01:15:07 - INFO - __main__ - Global step 600 Train loss 0.970570 Classification-F1 0.4075534266764923 on epoch=149
03/19/2022 01:15:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.809847 on epoch=152
03/19/2022 01:15:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.731487 on epoch=154
03/19/2022 01:15:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.538636 on epoch=157
03/19/2022 01:15:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.605837 on epoch=159
03/19/2022 01:15:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.562826 on epoch=162
03/19/2022 01:15:33 - INFO - __main__ - Global step 650 Train loss 0.649727 Classification-F1 0.605391792891793 on epoch=162
03/19/2022 01:15:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.573321 on epoch=164
03/19/2022 01:15:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.431666 on epoch=167
03/19/2022 01:15:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.463558 on epoch=169
03/19/2022 01:15:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.545483 on epoch=172
03/19/2022 01:15:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.273393 on epoch=174
03/19/2022 01:15:58 - INFO - __main__ - Global step 700 Train loss 0.457484 Classification-F1 0.7440446650124071 on epoch=174
03/19/2022 01:16:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.350096 on epoch=177
03/19/2022 01:16:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.284433 on epoch=179
03/19/2022 01:16:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.268184 on epoch=182
03/19/2022 01:16:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.413583 on epoch=184
03/19/2022 01:16:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.208060 on epoch=187
03/19/2022 01:16:24 - INFO - __main__ - Global step 750 Train loss 0.304871 Classification-F1 0.674732853353543 on epoch=187
03/19/2022 01:16:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.207897 on epoch=189
03/19/2022 01:16:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.153290 on epoch=192
03/19/2022 01:16:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.346831 on epoch=194
03/19/2022 01:16:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.378823 on epoch=197
03/19/2022 01:16:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.249691 on epoch=199
03/19/2022 01:16:49 - INFO - __main__ - Global step 800 Train loss 0.267307 Classification-F1 0.7306012200165426 on epoch=199
03/19/2022 01:16:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.149954 on epoch=202
03/19/2022 01:16:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.129917 on epoch=204
03/19/2022 01:17:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.120409 on epoch=207
03/19/2022 01:17:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.221880 on epoch=209
03/19/2022 01:17:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.073799 on epoch=212
03/19/2022 01:17:14 - INFO - __main__ - Global step 850 Train loss 0.139192 Classification-F1 0.793459008097166 on epoch=212
03/19/2022 01:17:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.079066 on epoch=214
03/19/2022 01:17:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.087463 on epoch=217
03/19/2022 01:17:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.082815 on epoch=219
03/19/2022 01:17:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.042346 on epoch=222
03/19/2022 01:17:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.077474 on epoch=224
03/19/2022 01:17:39 - INFO - __main__ - Global step 900 Train loss 0.073833 Classification-F1 0.6759834368530021 on epoch=224
03/19/2022 01:17:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.146004 on epoch=227
03/19/2022 01:17:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.151671 on epoch=229
03/19/2022 01:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.158920 on epoch=232
03/19/2022 01:17:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.182136 on epoch=234
03/19/2022 01:18:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.181171 on epoch=237
03/19/2022 01:18:04 - INFO - __main__ - Global step 950 Train loss 0.163980 Classification-F1 0.7302492656981822 on epoch=237
03/19/2022 01:18:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.154208 on epoch=239
03/19/2022 01:18:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.131139 on epoch=242
03/19/2022 01:18:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.108304 on epoch=244
03/19/2022 01:18:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.119801 on epoch=247
03/19/2022 01:18:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.101969 on epoch=249
03/19/2022 01:18:29 - INFO - __main__ - Global step 1000 Train loss 0.123084 Classification-F1 0.7105827110071592 on epoch=249
03/19/2022 01:18:29 - INFO - __main__ - save last model!
03/19/2022 01:18:30 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:18:30 - INFO - __main__ - Printing 3 examples
03/19/2022 01:18:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:18:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:18:30 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:18:30 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:18:30 - INFO - __main__ - Printing 3 examples
03/19/2022 01:18:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 01:18:30 - INFO - __main__ - ['others']
03/19/2022 01:18:30 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:18:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:18:30 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:18:36 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:18:37 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:18:37 - INFO - __main__ - Printing 3 examples
03/19/2022 01:18:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:18:37 - INFO - __main__ - ['others']
03/19/2022 01:18:37 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:18:37 - INFO - __main__ - ['others']
03/19/2022 01:18:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:18:37 - INFO - __main__ - ['others']
03/19/2022 01:18:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:18:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:18:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:18:42 - INFO - __main__ - Starting training!
03/19/2022 01:18:45 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 01:19:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_13_0.0002_8_predictions.txt
03/19/2022 01:19:30 - INFO - __main__ - Classification-F1 on test data: 0.3993
03/19/2022 01:19:30 - INFO - __main__ - prefix=emo_16_13, lr=0.0002, bsz=8, dev_performance=0.793459008097166, test_performance=0.3993063716280652
03/19/2022 01:19:30 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0001, bsz=8 ...
03/19/2022 01:19:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:19:31 - INFO - __main__ - Printing 3 examples
03/19/2022 01:19:31 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:19:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:19:31 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:19:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:19:31 - INFO - __main__ - Printing 3 examples
03/19/2022 01:19:31 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/19/2022 01:19:31 - INFO - __main__ - ['others']
03/19/2022 01:19:31 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:19:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:19:31 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:19:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:19:42 - INFO - __main__ - Starting training!
03/19/2022 01:19:46 - INFO - __main__ - Step 10 Global step 10 Train loss 24.569000 on epoch=2
03/19/2022 01:19:51 - INFO - __main__ - Step 20 Global step 20 Train loss 22.605347 on epoch=4
03/19/2022 01:19:56 - INFO - __main__ - Step 30 Global step 30 Train loss 18.798965 on epoch=7
03/19/2022 01:20:01 - INFO - __main__ - Step 40 Global step 40 Train loss 19.319031 on epoch=9
03/19/2022 01:20:06 - INFO - __main__ - Step 50 Global step 50 Train loss 17.250010 on epoch=12
03/19/2022 01:20:25 - INFO - __main__ - Global step 50 Train loss 20.508472 Classification-F1 0.0 on epoch=12
03/19/2022 01:20:31 - INFO - __main__ - Step 60 Global step 60 Train loss 18.368740 on epoch=14
03/19/2022 01:20:36 - INFO - __main__ - Step 70 Global step 70 Train loss 17.300709 on epoch=17
03/19/2022 01:20:41 - INFO - __main__ - Step 80 Global step 80 Train loss 16.454737 on epoch=19
03/19/2022 01:20:45 - INFO - __main__ - Step 90 Global step 90 Train loss 16.761688 on epoch=22
03/19/2022 01:20:50 - INFO - __main__ - Step 100 Global step 100 Train loss 15.184477 on epoch=24
03/19/2022 01:21:06 - INFO - __main__ - Global step 100 Train loss 16.814072 Classification-F1 0.0 on epoch=24
03/19/2022 01:21:11 - INFO - __main__ - Step 110 Global step 110 Train loss 15.144526 on epoch=27
03/19/2022 01:21:16 - INFO - __main__ - Step 120 Global step 120 Train loss 14.554846 on epoch=29
03/19/2022 01:21:21 - INFO - __main__ - Step 130 Global step 130 Train loss 14.460652 on epoch=32
03/19/2022 01:21:25 - INFO - __main__ - Step 140 Global step 140 Train loss 14.642711 on epoch=34
03/19/2022 01:21:30 - INFO - __main__ - Step 150 Global step 150 Train loss 14.363693 on epoch=37
03/19/2022 01:21:45 - INFO - __main__ - Global step 150 Train loss 14.633286 Classification-F1 0.0 on epoch=37
03/19/2022 01:21:50 - INFO - __main__ - Step 160 Global step 160 Train loss 13.833693 on epoch=39
03/19/2022 01:21:55 - INFO - __main__ - Step 170 Global step 170 Train loss 13.652698 on epoch=42
03/19/2022 01:22:00 - INFO - __main__ - Step 180 Global step 180 Train loss 13.205403 on epoch=44
03/19/2022 01:22:04 - INFO - __main__ - Step 190 Global step 190 Train loss 12.996511 on epoch=47
03/19/2022 01:22:09 - INFO - __main__ - Step 200 Global step 200 Train loss 12.830200 on epoch=49
03/19/2022 01:22:17 - INFO - __main__ - Global step 200 Train loss 13.303701 Classification-F1 0.0 on epoch=49
03/19/2022 01:22:22 - INFO - __main__ - Step 210 Global step 210 Train loss 12.853773 on epoch=52
03/19/2022 01:22:27 - INFO - __main__ - Step 220 Global step 220 Train loss 12.101626 on epoch=54
03/19/2022 01:22:32 - INFO - __main__ - Step 230 Global step 230 Train loss 12.161000 on epoch=57
03/19/2022 01:22:37 - INFO - __main__ - Step 240 Global step 240 Train loss 11.161257 on epoch=59
03/19/2022 01:22:42 - INFO - __main__ - Step 250 Global step 250 Train loss 11.246739 on epoch=62
03/19/2022 01:22:48 - INFO - __main__ - Global step 250 Train loss 11.904878 Classification-F1 0.0 on epoch=62
03/19/2022 01:22:53 - INFO - __main__ - Step 260 Global step 260 Train loss 10.378471 on epoch=64
03/19/2022 01:22:58 - INFO - __main__ - Step 270 Global step 270 Train loss 10.603321 on epoch=67
03/19/2022 01:23:03 - INFO - __main__ - Step 280 Global step 280 Train loss 9.990735 on epoch=69
03/19/2022 01:23:08 - INFO - __main__ - Step 290 Global step 290 Train loss 9.874112 on epoch=72
03/19/2022 01:23:13 - INFO - __main__ - Step 300 Global step 300 Train loss 9.484915 on epoch=74
03/19/2022 01:23:18 - INFO - __main__ - Global step 300 Train loss 10.066311 Classification-F1 0.0 on epoch=74
03/19/2022 01:23:23 - INFO - __main__ - Step 310 Global step 310 Train loss 8.871323 on epoch=77
03/19/2022 01:23:28 - INFO - __main__ - Step 320 Global step 320 Train loss 7.529714 on epoch=79
03/19/2022 01:23:33 - INFO - __main__ - Step 330 Global step 330 Train loss 7.320636 on epoch=82
03/19/2022 01:23:38 - INFO - __main__ - Step 340 Global step 340 Train loss 5.763120 on epoch=84
03/19/2022 01:23:42 - INFO - __main__ - Step 350 Global step 350 Train loss 5.250021 on epoch=87
03/19/2022 01:23:43 - INFO - __main__ - Global step 350 Train loss 6.946962 Classification-F1 0.07999999999999999 on epoch=87
03/19/2022 01:23:49 - INFO - __main__ - Step 360 Global step 360 Train loss 4.705221 on epoch=89
03/19/2022 01:23:54 - INFO - __main__ - Step 370 Global step 370 Train loss 4.551095 on epoch=92
03/19/2022 01:23:58 - INFO - __main__ - Step 380 Global step 380 Train loss 4.789698 on epoch=94
03/19/2022 01:24:03 - INFO - __main__ - Step 390 Global step 390 Train loss 3.799857 on epoch=97
03/19/2022 01:24:08 - INFO - __main__ - Step 400 Global step 400 Train loss 3.885694 on epoch=99
03/19/2022 01:24:09 - INFO - __main__ - Global step 400 Train loss 4.346313 Classification-F1 0.1 on epoch=99
03/19/2022 01:24:14 - INFO - __main__ - Step 410 Global step 410 Train loss 3.926101 on epoch=102
03/19/2022 01:24:19 - INFO - __main__ - Step 420 Global step 420 Train loss 4.422311 on epoch=104
03/19/2022 01:24:24 - INFO - __main__ - Step 430 Global step 430 Train loss 3.195721 on epoch=107
03/19/2022 01:24:29 - INFO - __main__ - Step 440 Global step 440 Train loss 3.387288 on epoch=109
03/19/2022 01:24:34 - INFO - __main__ - Step 450 Global step 450 Train loss 3.571575 on epoch=112
03/19/2022 01:24:34 - INFO - __main__ - Global step 450 Train loss 3.700599 Classification-F1 0.1581196581196581 on epoch=112
03/19/2022 01:24:40 - INFO - __main__ - Step 460 Global step 460 Train loss 3.303077 on epoch=114
03/19/2022 01:24:45 - INFO - __main__ - Step 470 Global step 470 Train loss 3.894318 on epoch=117
03/19/2022 01:24:50 - INFO - __main__ - Step 480 Global step 480 Train loss 3.110713 on epoch=119
03/19/2022 01:24:55 - INFO - __main__ - Step 490 Global step 490 Train loss 3.211852 on epoch=122
03/19/2022 01:25:00 - INFO - __main__ - Step 500 Global step 500 Train loss 3.107682 on epoch=124
03/19/2022 01:25:00 - INFO - __main__ - Global step 500 Train loss 3.325528 Classification-F1 0.19444444444444445 on epoch=124
03/19/2022 01:25:06 - INFO - __main__ - Step 510 Global step 510 Train loss 3.158092 on epoch=127
03/19/2022 01:25:11 - INFO - __main__ - Step 520 Global step 520 Train loss 2.749223 on epoch=129
03/19/2022 01:25:15 - INFO - __main__ - Step 530 Global step 530 Train loss 2.795582 on epoch=132
03/19/2022 01:25:20 - INFO - __main__ - Step 540 Global step 540 Train loss 2.880266 on epoch=134
03/19/2022 01:25:25 - INFO - __main__ - Step 550 Global step 550 Train loss 3.627666 on epoch=137
03/19/2022 01:25:26 - INFO - __main__ - Global step 550 Train loss 3.042166 Classification-F1 0.13034188034188032 on epoch=137
03/19/2022 01:25:31 - INFO - __main__ - Step 560 Global step 560 Train loss 2.852939 on epoch=139
03/19/2022 01:25:36 - INFO - __main__ - Step 570 Global step 570 Train loss 2.678934 on epoch=142
03/19/2022 01:25:41 - INFO - __main__ - Step 580 Global step 580 Train loss 3.322574 on epoch=144
03/19/2022 01:25:46 - INFO - __main__ - Step 590 Global step 590 Train loss 2.918935 on epoch=147
03/19/2022 01:25:51 - INFO - __main__ - Step 600 Global step 600 Train loss 2.161334 on epoch=149
03/19/2022 01:25:51 - INFO - __main__ - Global step 600 Train loss 2.786943 Classification-F1 0.1302118933697881 on epoch=149
03/19/2022 01:25:57 - INFO - __main__ - Step 610 Global step 610 Train loss 2.530173 on epoch=152
03/19/2022 01:26:02 - INFO - __main__ - Step 620 Global step 620 Train loss 2.559923 on epoch=154
03/19/2022 01:26:07 - INFO - __main__ - Step 630 Global step 630 Train loss 2.771564 on epoch=157
03/19/2022 01:26:12 - INFO - __main__ - Step 640 Global step 640 Train loss 2.459402 on epoch=159
03/19/2022 01:26:17 - INFO - __main__ - Step 650 Global step 650 Train loss 2.610933 on epoch=162
03/19/2022 01:26:17 - INFO - __main__ - Global step 650 Train loss 2.586399 Classification-F1 0.1 on epoch=162
03/19/2022 01:26:22 - INFO - __main__ - Step 660 Global step 660 Train loss 2.614114 on epoch=164
03/19/2022 01:26:27 - INFO - __main__ - Step 670 Global step 670 Train loss 2.336098 on epoch=167
03/19/2022 01:26:32 - INFO - __main__ - Step 680 Global step 680 Train loss 2.263671 on epoch=169
03/19/2022 01:26:37 - INFO - __main__ - Step 690 Global step 690 Train loss 1.982430 on epoch=172
03/19/2022 01:26:42 - INFO - __main__ - Step 700 Global step 700 Train loss 2.036989 on epoch=174
03/19/2022 01:26:42 - INFO - __main__ - Global step 700 Train loss 2.246660 Classification-F1 0.1 on epoch=174
03/19/2022 01:26:47 - INFO - __main__ - Step 710 Global step 710 Train loss 2.381748 on epoch=177
03/19/2022 01:26:52 - INFO - __main__ - Step 720 Global step 720 Train loss 2.217028 on epoch=179
03/19/2022 01:26:57 - INFO - __main__ - Step 730 Global step 730 Train loss 2.143380 on epoch=182
03/19/2022 01:27:02 - INFO - __main__ - Step 740 Global step 740 Train loss 1.449650 on epoch=184
03/19/2022 01:27:07 - INFO - __main__ - Step 750 Global step 750 Train loss 2.356703 on epoch=187
03/19/2022 01:27:08 - INFO - __main__ - Global step 750 Train loss 2.109702 Classification-F1 0.2924887346242845 on epoch=187
03/19/2022 01:27:13 - INFO - __main__ - Step 760 Global step 760 Train loss 2.369427 on epoch=189
03/19/2022 01:27:18 - INFO - __main__ - Step 770 Global step 770 Train loss 1.762242 on epoch=192
03/19/2022 01:27:23 - INFO - __main__ - Step 780 Global step 780 Train loss 1.964771 on epoch=194
03/19/2022 01:27:28 - INFO - __main__ - Step 790 Global step 790 Train loss 1.750615 on epoch=197
03/19/2022 01:27:33 - INFO - __main__ - Step 800 Global step 800 Train loss 1.672939 on epoch=199
03/19/2022 01:27:34 - INFO - __main__ - Global step 800 Train loss 1.903999 Classification-F1 0.40944618599791016 on epoch=199
03/19/2022 01:27:40 - INFO - __main__ - Step 810 Global step 810 Train loss 1.880968 on epoch=202
03/19/2022 01:27:44 - INFO - __main__ - Step 820 Global step 820 Train loss 1.899201 on epoch=204
03/19/2022 01:27:49 - INFO - __main__ - Step 830 Global step 830 Train loss 1.756369 on epoch=207
03/19/2022 01:27:54 - INFO - __main__ - Step 840 Global step 840 Train loss 1.650920 on epoch=209
03/19/2022 01:27:59 - INFO - __main__ - Step 850 Global step 850 Train loss 1.308758 on epoch=212
03/19/2022 01:28:00 - INFO - __main__ - Global step 850 Train loss 1.699243 Classification-F1 0.34324872201656975 on epoch=212
03/19/2022 01:28:05 - INFO - __main__ - Step 860 Global step 860 Train loss 1.693196 on epoch=214
03/19/2022 01:28:10 - INFO - __main__ - Step 870 Global step 870 Train loss 1.688649 on epoch=217
03/19/2022 01:28:15 - INFO - __main__ - Step 880 Global step 880 Train loss 1.059639 on epoch=219
03/19/2022 01:28:20 - INFO - __main__ - Step 890 Global step 890 Train loss 1.280398 on epoch=222
03/19/2022 01:28:25 - INFO - __main__ - Step 900 Global step 900 Train loss 1.242125 on epoch=224
03/19/2022 01:28:25 - INFO - __main__ - Global step 900 Train loss 1.392801 Classification-F1 0.4078164835674222 on epoch=224
03/19/2022 01:28:30 - INFO - __main__ - Step 910 Global step 910 Train loss 1.366127 on epoch=227
03/19/2022 01:28:35 - INFO - __main__ - Step 920 Global step 920 Train loss 1.330499 on epoch=229
03/19/2022 01:28:40 - INFO - __main__ - Step 930 Global step 930 Train loss 1.108438 on epoch=232
03/19/2022 01:28:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.892744 on epoch=234
03/19/2022 01:28:50 - INFO - __main__ - Step 950 Global step 950 Train loss 1.056090 on epoch=237
03/19/2022 01:28:51 - INFO - __main__ - Global step 950 Train loss 1.150780 Classification-F1 0.594205630970337 on epoch=237
03/19/2022 01:28:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.663688 on epoch=239
03/19/2022 01:29:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.747547 on epoch=242
03/19/2022 01:29:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.650735 on epoch=244
03/19/2022 01:29:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.589223 on epoch=247
03/19/2022 01:29:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.481858 on epoch=249
03/19/2022 01:29:17 - INFO - __main__ - Global step 1000 Train loss 0.626610 Classification-F1 0.6357906799083269 on epoch=249
03/19/2022 01:29:18 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:29:18 - INFO - __main__ - Printing 3 examples
03/19/2022 01:29:18 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:29:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:29:18 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:29:18 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:29:18 - INFO - __main__ - Printing 3 examples
03/19/2022 01:29:18 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:29:18 - INFO - __main__ - ['sad']
03/19/2022 01:29:18 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:29:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:29:18 - INFO - __main__ - save last model!
03/19/2022 01:29:18 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:29:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:29:25 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:29:25 - INFO - __main__ - Printing 3 examples
03/19/2022 01:29:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:29:25 - INFO - __main__ - ['others']
03/19/2022 01:29:25 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:29:25 - INFO - __main__ - ['others']
03/19/2022 01:29:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:29:25 - INFO - __main__ - ['others']
03/19/2022 01:29:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:29:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:29:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:29:31 - INFO - __main__ - Starting training!
03/19/2022 01:29:33 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 01:30:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_13_0.0001_8_predictions.txt
03/19/2022 01:30:15 - INFO - __main__ - Classification-F1 on test data: 0.3250
03/19/2022 01:30:15 - INFO - __main__ - prefix=emo_16_13, lr=0.0001, bsz=8, dev_performance=0.6357906799083269, test_performance=0.32495294292610133
03/19/2022 01:30:15 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0005, bsz=8 ...
03/19/2022 01:30:16 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:30:16 - INFO - __main__ - Printing 3 examples
03/19/2022 01:30:16 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:30:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:30:16 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:30:16 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:30:16 - INFO - __main__ - Printing 3 examples
03/19/2022 01:30:16 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:30:16 - INFO - __main__ - ['sad']
03/19/2022 01:30:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:30:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:30:16 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:30:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:30:27 - INFO - __main__ - Starting training!
03/19/2022 01:30:31 - INFO - __main__ - Step 10 Global step 10 Train loss 25.204792 on epoch=2
03/19/2022 01:30:36 - INFO - __main__ - Step 20 Global step 20 Train loss 19.343737 on epoch=4
03/19/2022 01:30:40 - INFO - __main__ - Step 30 Global step 30 Train loss 16.875288 on epoch=7
03/19/2022 01:30:45 - INFO - __main__ - Step 40 Global step 40 Train loss 14.071314 on epoch=9
03/19/2022 01:30:50 - INFO - __main__ - Step 50 Global step 50 Train loss 13.097128 on epoch=12
03/19/2022 01:30:51 - INFO - __main__ - Global step 50 Train loss 17.718451 Classification-F1 0.0 on epoch=12
03/19/2022 01:30:57 - INFO - __main__ - Step 60 Global step 60 Train loss 11.919928 on epoch=14
03/19/2022 01:31:02 - INFO - __main__ - Step 70 Global step 70 Train loss 10.184436 on epoch=17
03/19/2022 01:31:07 - INFO - __main__ - Step 80 Global step 80 Train loss 6.664835 on epoch=19
03/19/2022 01:31:12 - INFO - __main__ - Step 90 Global step 90 Train loss 3.537349 on epoch=22
03/19/2022 01:31:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.809028 on epoch=24
03/19/2022 01:31:18 - INFO - __main__ - Global step 100 Train loss 6.623116 Classification-F1 0.5398514174523966 on epoch=24
03/19/2022 01:31:23 - INFO - __main__ - Step 110 Global step 110 Train loss 1.825189 on epoch=27
03/19/2022 01:31:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.781364 on epoch=29
03/19/2022 01:31:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.497025 on epoch=32
03/19/2022 01:31:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.576367 on epoch=34
03/19/2022 01:31:43 - INFO - __main__ - Step 150 Global step 150 Train loss 1.053975 on epoch=37
03/19/2022 01:31:44 - INFO - __main__ - Global step 150 Train loss 0.946784 Classification-F1 0.2766884531590414 on epoch=37
03/19/2022 01:31:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.604810 on epoch=39
03/19/2022 01:31:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.542346 on epoch=42
03/19/2022 01:31:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.523390 on epoch=44
03/19/2022 01:32:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.369841 on epoch=47
03/19/2022 01:32:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.373047 on epoch=49
03/19/2022 01:32:09 - INFO - __main__ - Global step 200 Train loss 0.482687 Classification-F1 0.6511904761904762 on epoch=49
03/19/2022 01:32:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.229720 on epoch=52
03/19/2022 01:32:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.313815 on epoch=54
03/19/2022 01:32:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.724596 on epoch=57
03/19/2022 01:32:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.784718 on epoch=59
03/19/2022 01:32:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.664856 on epoch=62
03/19/2022 01:32:36 - INFO - __main__ - Global step 250 Train loss 0.543541 Classification-F1 0.37515870782124655 on epoch=62
03/19/2022 01:32:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.691095 on epoch=64
03/19/2022 01:32:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.714614 on epoch=67
03/19/2022 01:32:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.525215 on epoch=69
03/19/2022 01:32:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.435463 on epoch=72
03/19/2022 01:33:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.403960 on epoch=74
03/19/2022 01:33:01 - INFO - __main__ - Global step 300 Train loss 0.554069 Classification-F1 0.6496739130434783 on epoch=74
03/19/2022 01:33:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.400264 on epoch=77
03/19/2022 01:33:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.333424 on epoch=79
03/19/2022 01:33:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.405400 on epoch=82
03/19/2022 01:33:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.244139 on epoch=84
03/19/2022 01:33:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.171255 on epoch=87
03/19/2022 01:33:27 - INFO - __main__ - Global step 350 Train loss 0.310896 Classification-F1 0.44558309292770343 on epoch=87
03/19/2022 01:33:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.062052 on epoch=89
03/19/2022 01:33:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.246495 on epoch=92
03/19/2022 01:33:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.188029 on epoch=94
03/19/2022 01:33:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.424594 on epoch=97
03/19/2022 01:33:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.247609 on epoch=99
03/19/2022 01:33:53 - INFO - __main__ - Global step 400 Train loss 0.233756 Classification-F1 0.6653639846743296 on epoch=99
03/19/2022 01:33:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.168431 on epoch=102
03/19/2022 01:34:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.363192 on epoch=104
03/19/2022 01:34:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.170932 on epoch=107
03/19/2022 01:34:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.204847 on epoch=109
03/19/2022 01:34:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.179701 on epoch=112
03/19/2022 01:34:20 - INFO - __main__ - Global step 450 Train loss 0.217421 Classification-F1 0.5207294866737591 on epoch=112
03/19/2022 01:34:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.105337 on epoch=114
03/19/2022 01:34:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.063192 on epoch=117
03/19/2022 01:34:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.058930 on epoch=119
03/19/2022 01:34:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.132341 on epoch=122
03/19/2022 01:34:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.066689 on epoch=124
03/19/2022 01:34:46 - INFO - __main__ - Global step 500 Train loss 0.085298 Classification-F1 0.478423645320197 on epoch=124
03/19/2022 01:34:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.066936 on epoch=127
03/19/2022 01:34:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.036835 on epoch=129
03/19/2022 01:35:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.050978 on epoch=132
03/19/2022 01:35:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.076389 on epoch=134
03/19/2022 01:35:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.381832 on epoch=137
03/19/2022 01:35:11 - INFO - __main__ - Global step 550 Train loss 0.122594 Classification-F1 0.560536223036223 on epoch=137
03/19/2022 01:35:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.044421 on epoch=139
03/19/2022 01:35:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.112630 on epoch=142
03/19/2022 01:35:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.054293 on epoch=144
03/19/2022 01:35:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.039070 on epoch=147
03/19/2022 01:35:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.062761 on epoch=149
03/19/2022 01:35:37 - INFO - __main__ - Global step 600 Train loss 0.062635 Classification-F1 0.5055232124197642 on epoch=149
03/19/2022 01:35:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.029116 on epoch=152
03/19/2022 01:35:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.063120 on epoch=154
03/19/2022 01:35:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.131423 on epoch=157
03/19/2022 01:35:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.060427 on epoch=159
03/19/2022 01:36:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.149368 on epoch=162
03/19/2022 01:36:03 - INFO - __main__ - Global step 650 Train loss 0.086691 Classification-F1 0.6217894913547087 on epoch=162
03/19/2022 01:36:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.092153 on epoch=164
03/19/2022 01:36:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.094541 on epoch=167
03/19/2022 01:36:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.024857 on epoch=169
03/19/2022 01:36:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.007257 on epoch=172
03/19/2022 01:36:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008851 on epoch=174
03/19/2022 01:36:29 - INFO - __main__ - Global step 700 Train loss 0.045532 Classification-F1 0.42099358974358975 on epoch=174
03/19/2022 01:36:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.003871 on epoch=177
03/19/2022 01:36:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.032990 on epoch=179
03/19/2022 01:36:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.032432 on epoch=182
03/19/2022 01:36:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.007371 on epoch=184
03/19/2022 01:36:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.008317 on epoch=187
03/19/2022 01:36:55 - INFO - __main__ - Global step 750 Train loss 0.016996 Classification-F1 0.4666390956713537 on epoch=187
03/19/2022 01:37:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.031430 on epoch=189
03/19/2022 01:37:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.057963 on epoch=192
03/19/2022 01:37:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003179 on epoch=194
03/19/2022 01:37:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003335 on epoch=197
03/19/2022 01:37:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002038 on epoch=199
03/19/2022 01:37:20 - INFO - __main__ - Global step 800 Train loss 0.019589 Classification-F1 0.4199596630256301 on epoch=199
03/19/2022 01:37:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004634 on epoch=202
03/19/2022 01:37:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001910 on epoch=204
03/19/2022 01:37:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001565 on epoch=207
03/19/2022 01:37:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.005218 on epoch=209
03/19/2022 01:37:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001145 on epoch=212
03/19/2022 01:37:46 - INFO - __main__ - Global step 850 Train loss 0.002894 Classification-F1 0.30766439909297055 on epoch=212
03/19/2022 01:37:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.004871 on epoch=214
03/19/2022 01:37:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.004074 on epoch=217
03/19/2022 01:38:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000867 on epoch=219
03/19/2022 01:38:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000971 on epoch=222
03/19/2022 01:38:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.028983 on epoch=224
03/19/2022 01:38:11 - INFO - __main__ - Global step 900 Train loss 0.007953 Classification-F1 0.4705319605319605 on epoch=224
03/19/2022 01:38:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.018064 on epoch=227
03/19/2022 01:38:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.011917 on epoch=229
03/19/2022 01:38:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.041745 on epoch=232
03/19/2022 01:38:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.034195 on epoch=234
03/19/2022 01:38:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.017122 on epoch=237
03/19/2022 01:38:36 - INFO - __main__ - Global step 950 Train loss 0.024609 Classification-F1 0.68165077327465 on epoch=237
03/19/2022 01:38:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003971 on epoch=239
03/19/2022 01:38:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001573 on epoch=242
03/19/2022 01:38:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.006755 on epoch=244
03/19/2022 01:38:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.073492 on epoch=247
03/19/2022 01:39:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004209 on epoch=249
03/19/2022 01:39:02 - INFO - __main__ - Global step 1000 Train loss 0.018000 Classification-F1 0.41483696927046676 on epoch=249
03/19/2022 01:39:02 - INFO - __main__ - save last model!
03/19/2022 01:39:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:39:03 - INFO - __main__ - Printing 3 examples
03/19/2022 01:39:03 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:39:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:39:03 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:39:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:39:03 - INFO - __main__ - Printing 3 examples
03/19/2022 01:39:03 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:39:03 - INFO - __main__ - ['sad']
03/19/2022 01:39:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:39:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:39:03 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:39:09 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:39:10 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:39:10 - INFO - __main__ - Printing 3 examples
03/19/2022 01:39:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:39:10 - INFO - __main__ - ['others']
03/19/2022 01:39:10 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:39:10 - INFO - __main__ - ['others']
03/19/2022 01:39:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:39:10 - INFO - __main__ - ['others']
03/19/2022 01:39:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:39:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:39:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:39:15 - INFO - __main__ - Starting training!
03/19/2022 01:39:18 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 01:40:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_21_0.0005_8_predictions.txt
03/19/2022 01:40:11 - INFO - __main__ - Classification-F1 on test data: 0.0284
03/19/2022 01:40:11 - INFO - __main__ - prefix=emo_16_21, lr=0.0005, bsz=8, dev_performance=0.68165077327465, test_performance=0.028400811542764758
03/19/2022 01:40:11 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0003, bsz=8 ...
03/19/2022 01:40:12 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:40:12 - INFO - __main__ - Printing 3 examples
03/19/2022 01:40:12 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:40:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:40:12 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:40:12 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:40:12 - INFO - __main__ - Printing 3 examples
03/19/2022 01:40:12 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:40:12 - INFO - __main__ - ['sad']
03/19/2022 01:40:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:40:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:40:13 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:40:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:40:23 - INFO - __main__ - Starting training!
03/19/2022 01:40:27 - INFO - __main__ - Step 10 Global step 10 Train loss 25.383900 on epoch=2
03/19/2022 01:40:32 - INFO - __main__ - Step 20 Global step 20 Train loss 19.712423 on epoch=4
03/19/2022 01:40:37 - INFO - __main__ - Step 30 Global step 30 Train loss 18.215723 on epoch=7
03/19/2022 01:40:42 - INFO - __main__ - Step 40 Global step 40 Train loss 16.127703 on epoch=9
03/19/2022 01:40:47 - INFO - __main__ - Step 50 Global step 50 Train loss 15.341532 on epoch=12
03/19/2022 01:40:52 - INFO - __main__ - Global step 50 Train loss 18.956255 Classification-F1 0.0 on epoch=12
03/19/2022 01:40:58 - INFO - __main__ - Step 60 Global step 60 Train loss 15.192772 on epoch=14
03/19/2022 01:41:03 - INFO - __main__ - Step 70 Global step 70 Train loss 13.427112 on epoch=17
03/19/2022 01:41:08 - INFO - __main__ - Step 80 Global step 80 Train loss 13.356108 on epoch=19
03/19/2022 01:41:13 - INFO - __main__ - Step 90 Global step 90 Train loss 13.245275 on epoch=22
03/19/2022 01:41:18 - INFO - __main__ - Step 100 Global step 100 Train loss 11.470185 on epoch=24
03/19/2022 01:41:19 - INFO - __main__ - Global step 100 Train loss 13.338291 Classification-F1 0.0 on epoch=24
03/19/2022 01:41:24 - INFO - __main__ - Step 110 Global step 110 Train loss 8.872015 on epoch=27
03/19/2022 01:41:29 - INFO - __main__ - Step 120 Global step 120 Train loss 7.154475 on epoch=29
03/19/2022 01:41:34 - INFO - __main__ - Step 130 Global step 130 Train loss 3.484275 on epoch=32
03/19/2022 01:41:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.640611 on epoch=34
03/19/2022 01:41:44 - INFO - __main__ - Step 150 Global step 150 Train loss 1.327659 on epoch=37
03/19/2022 01:41:45 - INFO - __main__ - Global step 150 Train loss 4.495807 Classification-F1 0.4235765244312828 on epoch=37
03/19/2022 01:41:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.660984 on epoch=39
03/19/2022 01:41:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.775893 on epoch=42
03/19/2022 01:42:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.415154 on epoch=44
03/19/2022 01:42:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.296031 on epoch=47
03/19/2022 01:42:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.249806 on epoch=49
03/19/2022 01:42:11 - INFO - __main__ - Global step 200 Train loss 0.479573 Classification-F1 0.6993644615348469 on epoch=49
03/19/2022 01:42:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.234434 on epoch=52
03/19/2022 01:42:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.125188 on epoch=54
03/19/2022 01:42:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.169819 on epoch=57
03/19/2022 01:42:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.101560 on epoch=59
03/19/2022 01:42:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.202577 on epoch=62
03/19/2022 01:42:37 - INFO - __main__ - Global step 250 Train loss 0.166716 Classification-F1 0.7128033205619413 on epoch=62
03/19/2022 01:42:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.084829 on epoch=64
03/19/2022 01:42:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.055556 on epoch=67
03/19/2022 01:42:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.091862 on epoch=69
03/19/2022 01:42:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.112685 on epoch=72
03/19/2022 01:43:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.051529 on epoch=74
03/19/2022 01:43:03 - INFO - __main__ - Global step 300 Train loss 0.079292 Classification-F1 0.7425824175824176 on epoch=74
03/19/2022 01:43:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035070 on epoch=77
03/19/2022 01:43:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.032371 on epoch=79
03/19/2022 01:43:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.030703 on epoch=82
03/19/2022 01:43:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.007971 on epoch=84
03/19/2022 01:43:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.039590 on epoch=87
03/19/2022 01:43:29 - INFO - __main__ - Global step 350 Train loss 0.029141 Classification-F1 0.7260607023677103 on epoch=87
03/19/2022 01:43:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.005948 on epoch=89
03/19/2022 01:43:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.016007 on epoch=92
03/19/2022 01:43:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003601 on epoch=94
03/19/2022 01:43:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.014472 on epoch=97
03/19/2022 01:43:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007000 on epoch=99
03/19/2022 01:43:54 - INFO - __main__ - Global step 400 Train loss 0.009405 Classification-F1 0.7417326589652566 on epoch=99
03/19/2022 01:43:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.007547 on epoch=102
03/19/2022 01:44:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005249 on epoch=104
03/19/2022 01:44:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004610 on epoch=107
03/19/2022 01:44:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.036869 on epoch=109
03/19/2022 01:44:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011905 on epoch=112
03/19/2022 01:44:19 - INFO - __main__ - Global step 450 Train loss 0.013236 Classification-F1 0.7426219512195121 on epoch=112
03/19/2022 01:44:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001266 on epoch=114
03/19/2022 01:44:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.027650 on epoch=117
03/19/2022 01:44:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005761 on epoch=119
03/19/2022 01:44:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004542 on epoch=122
03/19/2022 01:44:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.008776 on epoch=124
03/19/2022 01:44:45 - INFO - __main__ - Global step 500 Train loss 0.009599 Classification-F1 0.7583477576711252 on epoch=124
03/19/2022 01:44:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000736 on epoch=127
03/19/2022 01:44:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000479 on epoch=129
03/19/2022 01:45:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003516 on epoch=132
03/19/2022 01:45:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.016362 on epoch=134
03/19/2022 01:45:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001304 on epoch=137
03/19/2022 01:45:11 - INFO - __main__ - Global step 550 Train loss 0.004479 Classification-F1 0.7234062980030723 on epoch=137
03/19/2022 01:45:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000784 on epoch=139
03/19/2022 01:45:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001993 on epoch=142
03/19/2022 01:45:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.015889 on epoch=144
03/19/2022 01:45:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.173704 on epoch=147
03/19/2022 01:45:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001820 on epoch=149
03/19/2022 01:45:36 - INFO - __main__ - Global step 600 Train loss 0.038838 Classification-F1 0.7218181818181818 on epoch=149
03/19/2022 01:45:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003639 on epoch=152
03/19/2022 01:45:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004055 on epoch=154
03/19/2022 01:45:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000714 on epoch=157
03/19/2022 01:45:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.010649 on epoch=159
03/19/2022 01:46:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001571 on epoch=162
03/19/2022 01:46:01 - INFO - __main__ - Global step 650 Train loss 0.004126 Classification-F1 0.7725490196078432 on epoch=162
03/19/2022 01:46:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001338 on epoch=164
03/19/2022 01:46:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.027204 on epoch=167
03/19/2022 01:46:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.002313 on epoch=169
03/19/2022 01:46:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000446 on epoch=172
03/19/2022 01:46:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001817 on epoch=174
03/19/2022 01:46:26 - INFO - __main__ - Global step 700 Train loss 0.006624 Classification-F1 0.7556477460711333 on epoch=174
03/19/2022 01:46:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000302 on epoch=177
03/19/2022 01:46:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000134 on epoch=179
03/19/2022 01:46:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.004895 on epoch=182
03/19/2022 01:46:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000536 on epoch=184
03/19/2022 01:46:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000576 on epoch=187
03/19/2022 01:46:52 - INFO - __main__ - Global step 750 Train loss 0.001288 Classification-F1 0.7561517783291977 on epoch=187
03/19/2022 01:46:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005082 on epoch=189
03/19/2022 01:47:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003252 on epoch=192
03/19/2022 01:47:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000232 on epoch=194
03/19/2022 01:47:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.017439 on epoch=197
03/19/2022 01:47:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002683 on epoch=199
03/19/2022 01:47:17 - INFO - __main__ - Global step 800 Train loss 0.005738 Classification-F1 0.7382469512195121 on epoch=199
03/19/2022 01:47:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000322 on epoch=202
03/19/2022 01:47:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000107 on epoch=204
03/19/2022 01:47:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001285 on epoch=207
03/19/2022 01:47:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010189 on epoch=209
03/19/2022 01:47:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001980 on epoch=212
03/19/2022 01:47:42 - INFO - __main__ - Global step 850 Train loss 0.002776 Classification-F1 0.7425164921624403 on epoch=212
03/19/2022 01:47:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001491 on epoch=214
03/19/2022 01:47:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.015661 on epoch=217
03/19/2022 01:47:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002272 on epoch=219
03/19/2022 01:48:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000440 on epoch=222
03/19/2022 01:48:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000086 on epoch=224
03/19/2022 01:48:07 - INFO - __main__ - Global step 900 Train loss 0.003990 Classification-F1 0.7322038501527435 on epoch=224
03/19/2022 01:48:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000368 on epoch=227
03/19/2022 01:48:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.023898 on epoch=229
03/19/2022 01:48:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002763 on epoch=232
03/19/2022 01:48:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000117 on epoch=234
03/19/2022 01:48:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000965 on epoch=237
03/19/2022 01:48:32 - INFO - __main__ - Global step 950 Train loss 0.005622 Classification-F1 0.7574645748987854 on epoch=237
03/19/2022 01:48:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000298 on epoch=239
03/19/2022 01:48:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002303 on epoch=242
03/19/2022 01:48:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012960 on epoch=244
03/19/2022 01:48:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000728 on epoch=247
03/19/2022 01:48:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006607 on epoch=249
03/19/2022 01:48:57 - INFO - __main__ - Global step 1000 Train loss 0.004579 Classification-F1 0.6985795454545455 on epoch=249
03/19/2022 01:48:57 - INFO - __main__ - save last model!
03/19/2022 01:48:58 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:48:58 - INFO - __main__ - Printing 3 examples
03/19/2022 01:48:58 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:48:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:48:58 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:48:58 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:48:58 - INFO - __main__ - Printing 3 examples
03/19/2022 01:48:58 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:48:58 - INFO - __main__ - ['sad']
03/19/2022 01:48:58 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:48:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:48:58 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:49:05 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:49:05 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:49:05 - INFO - __main__ - Printing 3 examples
03/19/2022 01:49:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:49:05 - INFO - __main__ - ['others']
03/19/2022 01:49:05 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:49:05 - INFO - __main__ - ['others']
03/19/2022 01:49:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:49:05 - INFO - __main__ - ['others']
03/19/2022 01:49:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:49:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:49:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:49:09 - INFO - __main__ - Starting training!
03/19/2022 01:49:13 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 01:49:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_21_0.0003_8_predictions.txt
03/19/2022 01:49:55 - INFO - __main__ - Classification-F1 on test data: 0.1905
03/19/2022 01:49:55 - INFO - __main__ - prefix=emo_16_21, lr=0.0003, bsz=8, dev_performance=0.7725490196078432, test_performance=0.19049985206343373
03/19/2022 01:49:55 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0002, bsz=8 ...
03/19/2022 01:49:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:49:56 - INFO - __main__ - Printing 3 examples
03/19/2022 01:49:56 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:49:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:49:56 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:49:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:49:56 - INFO - __main__ - Printing 3 examples
03/19/2022 01:49:56 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:49:56 - INFO - __main__ - ['sad']
03/19/2022 01:49:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:49:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:49:56 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:50:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:50:07 - INFO - __main__ - Starting training!
03/19/2022 01:50:11 - INFO - __main__ - Step 10 Global step 10 Train loss 24.338810 on epoch=2
03/19/2022 01:50:16 - INFO - __main__ - Step 20 Global step 20 Train loss 21.484499 on epoch=4
03/19/2022 01:50:21 - INFO - __main__ - Step 30 Global step 30 Train loss 19.049761 on epoch=7
03/19/2022 01:50:26 - INFO - __main__ - Step 40 Global step 40 Train loss 17.991581 on epoch=9
03/19/2022 01:50:30 - INFO - __main__ - Step 50 Global step 50 Train loss 16.082388 on epoch=12
03/19/2022 01:50:49 - INFO - __main__ - Global step 50 Train loss 19.789406 Classification-F1 0.0 on epoch=12
03/19/2022 01:50:54 - INFO - __main__ - Step 60 Global step 60 Train loss 16.113010 on epoch=14
03/19/2022 01:50:59 - INFO - __main__ - Step 70 Global step 70 Train loss 14.861402 on epoch=17
03/19/2022 01:51:04 - INFO - __main__ - Step 80 Global step 80 Train loss 14.892985 on epoch=19
03/19/2022 01:51:09 - INFO - __main__ - Step 90 Global step 90 Train loss 14.081596 on epoch=22
03/19/2022 01:51:14 - INFO - __main__ - Step 100 Global step 100 Train loss 13.859883 on epoch=24
03/19/2022 01:51:17 - INFO - __main__ - Global step 100 Train loss 14.761774 Classification-F1 0.0 on epoch=24
03/19/2022 01:51:22 - INFO - __main__ - Step 110 Global step 110 Train loss 13.443372 on epoch=27
03/19/2022 01:51:27 - INFO - __main__ - Step 120 Global step 120 Train loss 12.250338 on epoch=29
03/19/2022 01:51:32 - INFO - __main__ - Step 130 Global step 130 Train loss 12.164341 on epoch=32
03/19/2022 01:51:37 - INFO - __main__ - Step 140 Global step 140 Train loss 10.906017 on epoch=34
03/19/2022 01:51:42 - INFO - __main__ - Step 150 Global step 150 Train loss 11.017937 on epoch=37
03/19/2022 01:51:45 - INFO - __main__ - Global step 150 Train loss 11.956400 Classification-F1 0.0 on epoch=37
03/19/2022 01:51:50 - INFO - __main__ - Step 160 Global step 160 Train loss 9.070704 on epoch=39
03/19/2022 01:51:55 - INFO - __main__ - Step 170 Global step 170 Train loss 8.831232 on epoch=42
03/19/2022 01:52:00 - INFO - __main__ - Step 180 Global step 180 Train loss 5.134852 on epoch=44
03/19/2022 01:52:05 - INFO - __main__ - Step 190 Global step 190 Train loss 5.155861 on epoch=47
03/19/2022 01:52:10 - INFO - __main__ - Step 200 Global step 200 Train loss 3.989375 on epoch=49
03/19/2022 01:52:10 - INFO - __main__ - Global step 200 Train loss 6.436404 Classification-F1 0.20571428571428568 on epoch=49
03/19/2022 01:52:16 - INFO - __main__ - Step 210 Global step 210 Train loss 4.300002 on epoch=52
03/19/2022 01:52:21 - INFO - __main__ - Step 220 Global step 220 Train loss 4.133904 on epoch=54
03/19/2022 01:52:26 - INFO - __main__ - Step 230 Global step 230 Train loss 3.054193 on epoch=57
03/19/2022 01:52:31 - INFO - __main__ - Step 240 Global step 240 Train loss 3.873477 on epoch=59
03/19/2022 01:52:36 - INFO - __main__ - Step 250 Global step 250 Train loss 3.325447 on epoch=62
03/19/2022 01:52:37 - INFO - __main__ - Global step 250 Train loss 3.737405 Classification-F1 0.1 on epoch=62
03/19/2022 01:52:42 - INFO - __main__ - Step 260 Global step 260 Train loss 3.586286 on epoch=64
03/19/2022 01:52:46 - INFO - __main__ - Step 270 Global step 270 Train loss 3.635682 on epoch=67
03/19/2022 01:52:51 - INFO - __main__ - Step 280 Global step 280 Train loss 3.385202 on epoch=69
03/19/2022 01:52:56 - INFO - __main__ - Step 290 Global step 290 Train loss 3.334121 on epoch=72
03/19/2022 01:53:01 - INFO - __main__ - Step 300 Global step 300 Train loss 2.852558 on epoch=74
03/19/2022 01:53:02 - INFO - __main__ - Global step 300 Train loss 3.358770 Classification-F1 0.1312577833125778 on epoch=74
03/19/2022 01:53:07 - INFO - __main__ - Step 310 Global step 310 Train loss 2.694474 on epoch=77
03/19/2022 01:53:12 - INFO - __main__ - Step 320 Global step 320 Train loss 2.540524 on epoch=79
03/19/2022 01:53:17 - INFO - __main__ - Step 330 Global step 330 Train loss 3.015225 on epoch=82
03/19/2022 01:53:22 - INFO - __main__ - Step 340 Global step 340 Train loss 2.777884 on epoch=84
03/19/2022 01:53:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.664932 on epoch=87
03/19/2022 01:53:27 - INFO - __main__ - Global step 350 Train loss 2.738608 Classification-F1 0.10126582278481013 on epoch=87
03/19/2022 01:53:32 - INFO - __main__ - Step 360 Global step 360 Train loss 2.392311 on epoch=89
03/19/2022 01:53:37 - INFO - __main__ - Step 370 Global step 370 Train loss 2.590562 on epoch=92
03/19/2022 01:53:42 - INFO - __main__ - Step 380 Global step 380 Train loss 2.423928 on epoch=94
03/19/2022 01:53:47 - INFO - __main__ - Step 390 Global step 390 Train loss 2.490495 on epoch=97
03/19/2022 01:53:52 - INFO - __main__ - Step 400 Global step 400 Train loss 2.765397 on epoch=99
03/19/2022 01:53:52 - INFO - __main__ - Global step 400 Train loss 2.532539 Classification-F1 0.1570048309178744 on epoch=99
03/19/2022 01:53:57 - INFO - __main__ - Step 410 Global step 410 Train loss 2.205624 on epoch=102
03/19/2022 01:54:02 - INFO - __main__ - Step 420 Global step 420 Train loss 2.012006 on epoch=104
03/19/2022 01:54:07 - INFO - __main__ - Step 430 Global step 430 Train loss 1.625626 on epoch=107
03/19/2022 01:54:12 - INFO - __main__ - Step 440 Global step 440 Train loss 1.612430 on epoch=109
03/19/2022 01:54:17 - INFO - __main__ - Step 450 Global step 450 Train loss 2.020485 on epoch=112
03/19/2022 01:54:17 - INFO - __main__ - Global step 450 Train loss 1.895234 Classification-F1 0.13194444444444445 on epoch=112
03/19/2022 01:54:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.912662 on epoch=114
03/19/2022 01:54:27 - INFO - __main__ - Step 470 Global step 470 Train loss 1.884996 on epoch=117
03/19/2022 01:54:32 - INFO - __main__ - Step 480 Global step 480 Train loss 1.690462 on epoch=119
03/19/2022 01:54:37 - INFO - __main__ - Step 490 Global step 490 Train loss 2.044959 on epoch=122
03/19/2022 01:54:42 - INFO - __main__ - Step 500 Global step 500 Train loss 1.661626 on epoch=124
03/19/2022 01:54:42 - INFO - __main__ - Global step 500 Train loss 1.838941 Classification-F1 0.1 on epoch=124
03/19/2022 01:54:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.672278 on epoch=127
03/19/2022 01:54:52 - INFO - __main__ - Step 520 Global step 520 Train loss 1.554197 on epoch=129
03/19/2022 01:54:57 - INFO - __main__ - Step 530 Global step 530 Train loss 1.415981 on epoch=132
03/19/2022 01:55:02 - INFO - __main__ - Step 540 Global step 540 Train loss 1.311202 on epoch=134
03/19/2022 01:55:07 - INFO - __main__ - Step 550 Global step 550 Train loss 1.439352 on epoch=137
03/19/2022 01:55:08 - INFO - __main__ - Global step 550 Train loss 1.478602 Classification-F1 0.29623655913978497 on epoch=137
03/19/2022 01:55:14 - INFO - __main__ - Step 560 Global step 560 Train loss 1.245983 on epoch=139
03/19/2022 01:55:18 - INFO - __main__ - Step 570 Global step 570 Train loss 1.242047 on epoch=142
03/19/2022 01:55:23 - INFO - __main__ - Step 580 Global step 580 Train loss 1.303012 on epoch=144
03/19/2022 01:55:28 - INFO - __main__ - Step 590 Global step 590 Train loss 1.009226 on epoch=147
03/19/2022 01:55:33 - INFO - __main__ - Step 600 Global step 600 Train loss 1.076364 on epoch=149
03/19/2022 01:55:34 - INFO - __main__ - Global step 600 Train loss 1.175327 Classification-F1 0.34537612146307795 on epoch=149
03/19/2022 01:55:40 - INFO - __main__ - Step 610 Global step 610 Train loss 1.063798 on epoch=152
03/19/2022 01:55:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.925437 on epoch=154
03/19/2022 01:55:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.956789 on epoch=157
03/19/2022 01:55:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.816570 on epoch=159
03/19/2022 01:56:00 - INFO - __main__ - Step 650 Global step 650 Train loss 1.089757 on epoch=162
03/19/2022 01:56:00 - INFO - __main__ - Global step 650 Train loss 0.970470 Classification-F1 0.32220480668756535 on epoch=162
03/19/2022 01:56:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.176171 on epoch=164
03/19/2022 01:56:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.943368 on epoch=167
03/19/2022 01:56:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.905417 on epoch=169
03/19/2022 01:56:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.861393 on epoch=172
03/19/2022 01:56:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.895169 on epoch=174
03/19/2022 01:56:26 - INFO - __main__ - Global step 700 Train loss 0.956304 Classification-F1 0.2747091001807983 on epoch=174
03/19/2022 01:56:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.761109 on epoch=177
03/19/2022 01:56:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.804707 on epoch=179
03/19/2022 01:56:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.795485 on epoch=182
03/19/2022 01:56:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.697366 on epoch=184
03/19/2022 01:56:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.594941 on epoch=187
03/19/2022 01:56:51 - INFO - __main__ - Global step 750 Train loss 0.730722 Classification-F1 0.3058441558441558 on epoch=187
03/19/2022 01:56:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.650589 on epoch=189
03/19/2022 01:57:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.653225 on epoch=192
03/19/2022 01:57:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.610777 on epoch=194
03/19/2022 01:57:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.580119 on epoch=197
03/19/2022 01:57:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.443043 on epoch=199
03/19/2022 01:57:16 - INFO - __main__ - Global step 800 Train loss 0.587550 Classification-F1 0.5040229885057471 on epoch=199
03/19/2022 01:57:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.407962 on epoch=202
03/19/2022 01:57:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.481062 on epoch=204
03/19/2022 01:57:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.587252 on epoch=207
03/19/2022 01:57:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.486053 on epoch=209
03/19/2022 01:57:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.436066 on epoch=212
03/19/2022 01:57:42 - INFO - __main__ - Global step 850 Train loss 0.479679 Classification-F1 0.375 on epoch=212
03/19/2022 01:57:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.445191 on epoch=214
03/19/2022 01:57:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.361864 on epoch=217
03/19/2022 01:57:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.318382 on epoch=219
03/19/2022 01:58:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.251751 on epoch=222
03/19/2022 01:58:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.184520 on epoch=224
03/19/2022 01:58:07 - INFO - __main__ - Global step 900 Train loss 0.312342 Classification-F1 0.6015198552086941 on epoch=224
03/19/2022 01:58:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.210335 on epoch=227
03/19/2022 01:58:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.182493 on epoch=229
03/19/2022 01:58:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.149093 on epoch=232
03/19/2022 01:58:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.123088 on epoch=234
03/19/2022 01:58:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.081519 on epoch=237
03/19/2022 01:58:33 - INFO - __main__ - Global step 950 Train loss 0.149306 Classification-F1 0.5457940129336879 on epoch=237
03/19/2022 01:58:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.072663 on epoch=239
03/19/2022 01:58:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.070079 on epoch=242
03/19/2022 01:58:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.061814 on epoch=244
03/19/2022 01:58:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.084022 on epoch=247
03/19/2022 01:58:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.101647 on epoch=249
03/19/2022 01:58:59 - INFO - __main__ - Global step 1000 Train loss 0.078045 Classification-F1 0.6728745907368575 on epoch=249
03/19/2022 01:58:59 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:58:59 - INFO - __main__ - Printing 3 examples
03/19/2022 01:58:59 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 01:58:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:58:59 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 01:58:59 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 01:58:59 - INFO - __main__ - Printing 3 examples
03/19/2022 01:58:59 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 01:58:59 - INFO - __main__ - ['sad']
03/19/2022 01:58:59 - INFO - __main__ - Tokenizing Input ...
03/19/2022 01:58:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:58:59 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 01:58:59 - INFO - __main__ - save last model!
03/19/2022 01:59:07 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 01:59:07 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 01:59:07 - INFO - __main__ - Printing 3 examples
03/19/2022 01:59:07 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 01:59:07 - INFO - __main__ - ['others']
03/19/2022 01:59:07 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 01:59:07 - INFO - __main__ - ['others']
03/19/2022 01:59:07 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 01:59:07 - INFO - __main__ - ['others']
03/19/2022 01:59:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 01:59:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 01:59:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 01:59:12 - INFO - __main__ - Starting training!
03/19/2022 01:59:15 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:00:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_21_0.0002_8_predictions.txt
03/19/2022 02:00:01 - INFO - __main__ - Classification-F1 on test data: 0.0299
03/19/2022 02:00:02 - INFO - __main__ - prefix=emo_16_21, lr=0.0002, bsz=8, dev_performance=0.6728745907368575, test_performance=0.029900620064560124
03/19/2022 02:00:02 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0001, bsz=8 ...
03/19/2022 02:00:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:00:03 - INFO - __main__ - Printing 3 examples
03/19/2022 02:00:03 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:00:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:00:03 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:00:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:00:03 - INFO - __main__ - Printing 3 examples
03/19/2022 02:00:03 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/19/2022 02:00:03 - INFO - __main__ - ['sad']
03/19/2022 02:00:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:00:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:00:03 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:00:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:00:14 - INFO - __main__ - Starting training!
03/19/2022 02:00:19 - INFO - __main__ - Step 10 Global step 10 Train loss 25.776602 on epoch=2
03/19/2022 02:00:23 - INFO - __main__ - Step 20 Global step 20 Train loss 21.702887 on epoch=4
03/19/2022 02:00:28 - INFO - __main__ - Step 30 Global step 30 Train loss 19.820065 on epoch=7
03/19/2022 02:00:33 - INFO - __main__ - Step 40 Global step 40 Train loss 18.414211 on epoch=9
03/19/2022 02:00:38 - INFO - __main__ - Step 50 Global step 50 Train loss 18.299763 on epoch=12
03/19/2022 02:00:59 - INFO - __main__ - Global step 50 Train loss 20.802706 Classification-F1 0.0 on epoch=12
03/19/2022 02:01:04 - INFO - __main__ - Step 60 Global step 60 Train loss 17.995869 on epoch=14
03/19/2022 02:01:09 - INFO - __main__ - Step 70 Global step 70 Train loss 16.947567 on epoch=17
03/19/2022 02:01:14 - INFO - __main__ - Step 80 Global step 80 Train loss 16.098726 on epoch=19
03/19/2022 02:01:19 - INFO - __main__ - Step 90 Global step 90 Train loss 16.574148 on epoch=22
03/19/2022 02:01:23 - INFO - __main__ - Step 100 Global step 100 Train loss 16.286243 on epoch=24
03/19/2022 02:01:41 - INFO - __main__ - Global step 100 Train loss 16.780512 Classification-F1 0.0 on epoch=24
03/19/2022 02:01:46 - INFO - __main__ - Step 110 Global step 110 Train loss 16.203447 on epoch=27
03/19/2022 02:01:51 - INFO - __main__ - Step 120 Global step 120 Train loss 15.887011 on epoch=29
03/19/2022 02:01:56 - INFO - __main__ - Step 130 Global step 130 Train loss 16.176935 on epoch=32
03/19/2022 02:02:01 - INFO - __main__ - Step 140 Global step 140 Train loss 14.524035 on epoch=34
03/19/2022 02:02:05 - INFO - __main__ - Step 150 Global step 150 Train loss 14.587728 on epoch=37
03/19/2022 02:02:14 - INFO - __main__ - Global step 150 Train loss 15.475831 Classification-F1 0.0 on epoch=37
03/19/2022 02:02:19 - INFO - __main__ - Step 160 Global step 160 Train loss 13.712942 on epoch=39
03/19/2022 02:02:24 - INFO - __main__ - Step 170 Global step 170 Train loss 13.470291 on epoch=42
03/19/2022 02:02:29 - INFO - __main__ - Step 180 Global step 180 Train loss 13.275371 on epoch=44
03/19/2022 02:02:33 - INFO - __main__ - Step 190 Global step 190 Train loss 13.915092 on epoch=47
03/19/2022 02:02:38 - INFO - __main__ - Step 200 Global step 200 Train loss 12.892540 on epoch=49
03/19/2022 02:02:49 - INFO - __main__ - Global step 200 Train loss 13.453247 Classification-F1 0.004201680672268907 on epoch=49
03/19/2022 02:02:54 - INFO - __main__ - Step 210 Global step 210 Train loss 13.200607 on epoch=52
03/19/2022 02:02:59 - INFO - __main__ - Step 220 Global step 220 Train loss 12.199812 on epoch=54
03/19/2022 02:03:04 - INFO - __main__ - Step 230 Global step 230 Train loss 12.332417 on epoch=57
03/19/2022 02:03:09 - INFO - __main__ - Step 240 Global step 240 Train loss 11.525694 on epoch=59
03/19/2022 02:03:14 - INFO - __main__ - Step 250 Global step 250 Train loss 11.899088 on epoch=62
03/19/2022 02:03:15 - INFO - __main__ - Global step 250 Train loss 12.231524 Classification-F1 0.005115089514066496 on epoch=62
03/19/2022 02:03:21 - INFO - __main__ - Step 260 Global step 260 Train loss 10.759798 on epoch=64
03/19/2022 02:03:26 - INFO - __main__ - Step 270 Global step 270 Train loss 10.676396 on epoch=67
03/19/2022 02:03:31 - INFO - __main__ - Step 280 Global step 280 Train loss 9.761449 on epoch=69
03/19/2022 02:03:36 - INFO - __main__ - Step 290 Global step 290 Train loss 8.454744 on epoch=72
03/19/2022 02:03:41 - INFO - __main__ - Step 300 Global step 300 Train loss 7.893630 on epoch=74
03/19/2022 02:03:41 - INFO - __main__ - Global step 300 Train loss 9.509204 Classification-F1 0.013793103448275862 on epoch=74
03/19/2022 02:03:47 - INFO - __main__ - Step 310 Global step 310 Train loss 6.480143 on epoch=77
03/19/2022 02:03:51 - INFO - __main__ - Step 320 Global step 320 Train loss 5.799727 on epoch=79
03/19/2022 02:03:56 - INFO - __main__ - Step 330 Global step 330 Train loss 4.490824 on epoch=82
03/19/2022 02:04:01 - INFO - __main__ - Step 340 Global step 340 Train loss 2.260664 on epoch=84
03/19/2022 02:04:06 - INFO - __main__ - Step 350 Global step 350 Train loss 1.320856 on epoch=87
03/19/2022 02:04:07 - INFO - __main__ - Global step 350 Train loss 4.070443 Classification-F1 0.3663487133984028 on epoch=87
03/19/2022 02:04:12 - INFO - __main__ - Step 360 Global step 360 Train loss 1.563984 on epoch=89
03/19/2022 02:04:17 - INFO - __main__ - Step 370 Global step 370 Train loss 1.400408 on epoch=92
03/19/2022 02:04:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.788519 on epoch=94
03/19/2022 02:04:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.945408 on epoch=97
03/19/2022 02:04:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.651885 on epoch=99
03/19/2022 02:04:32 - INFO - __main__ - Global step 400 Train loss 1.070041 Classification-F1 0.5042729930887826 on epoch=99
03/19/2022 02:04:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.681453 on epoch=102
03/19/2022 02:04:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.591596 on epoch=104
03/19/2022 02:04:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.566781 on epoch=107
03/19/2022 02:04:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.593662 on epoch=109
03/19/2022 02:04:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.565675 on epoch=112
03/19/2022 02:04:58 - INFO - __main__ - Global step 450 Train loss 0.599833 Classification-F1 0.5467231065057152 on epoch=112
03/19/2022 02:05:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.584093 on epoch=114
03/19/2022 02:05:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.469189 on epoch=117
03/19/2022 02:05:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.493510 on epoch=119
03/19/2022 02:05:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.544641 on epoch=122
03/19/2022 02:05:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.371933 on epoch=124
03/19/2022 02:05:24 - INFO - __main__ - Global step 500 Train loss 0.492673 Classification-F1 0.6319844531051427 on epoch=124
03/19/2022 02:05:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.802952 on epoch=127
03/19/2022 02:05:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.360551 on epoch=129
03/19/2022 02:05:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.477982 on epoch=132
03/19/2022 02:05:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.336709 on epoch=134
03/19/2022 02:05:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.438117 on epoch=137
03/19/2022 02:05:50 - INFO - __main__ - Global step 550 Train loss 0.483262 Classification-F1 0.6450874446632665 on epoch=137
03/19/2022 02:05:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.315354 on epoch=139
03/19/2022 02:06:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.242467 on epoch=142
03/19/2022 02:06:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.291780 on epoch=144
03/19/2022 02:06:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.199224 on epoch=147
03/19/2022 02:06:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.181183 on epoch=149
03/19/2022 02:06:17 - INFO - __main__ - Global step 600 Train loss 0.246002 Classification-F1 0.5709060045266942 on epoch=149
03/19/2022 02:06:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.424235 on epoch=152
03/19/2022 02:06:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.253001 on epoch=154
03/19/2022 02:06:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.279543 on epoch=157
03/19/2022 02:06:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.253099 on epoch=159
03/19/2022 02:06:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.179155 on epoch=162
03/19/2022 02:06:42 - INFO - __main__ - Global step 650 Train loss 0.277807 Classification-F1 0.6107142857142857 on epoch=162
03/19/2022 02:06:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.097341 on epoch=164
03/19/2022 02:06:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.388035 on epoch=167
03/19/2022 02:06:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.181569 on epoch=169
03/19/2022 02:07:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.143405 on epoch=172
03/19/2022 02:07:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.076913 on epoch=174
03/19/2022 02:07:08 - INFO - __main__ - Global step 700 Train loss 0.177453 Classification-F1 0.6380065450789134 on epoch=174
03/19/2022 02:07:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.140583 on epoch=177
03/19/2022 02:07:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.137558 on epoch=179
03/19/2022 02:07:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.100316 on epoch=182
03/19/2022 02:07:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.066424 on epoch=184
03/19/2022 02:07:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.124564 on epoch=187
03/19/2022 02:07:34 - INFO - __main__ - Global step 750 Train loss 0.113889 Classification-F1 0.6421896770733979 on epoch=187
03/19/2022 02:07:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.074408 on epoch=189
03/19/2022 02:07:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.099647 on epoch=192
03/19/2022 02:07:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.054495 on epoch=194
03/19/2022 02:07:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.132857 on epoch=197
03/19/2022 02:07:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.049874 on epoch=199
03/19/2022 02:07:59 - INFO - __main__ - Global step 800 Train loss 0.082256 Classification-F1 0.7025401069518716 on epoch=199
03/19/2022 02:08:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.119460 on epoch=202
03/19/2022 02:08:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.032685 on epoch=204
03/19/2022 02:08:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.082634 on epoch=207
03/19/2022 02:08:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.062847 on epoch=209
03/19/2022 02:08:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.052152 on epoch=212
03/19/2022 02:08:26 - INFO - __main__ - Global step 850 Train loss 0.069956 Classification-F1 0.7153575402214241 on epoch=212
03/19/2022 02:08:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.069480 on epoch=214
03/19/2022 02:08:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.051030 on epoch=217
03/19/2022 02:08:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.089612 on epoch=219
03/19/2022 02:08:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.034594 on epoch=222
03/19/2022 02:08:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.079622 on epoch=224
03/19/2022 02:08:52 - INFO - __main__ - Global step 900 Train loss 0.064867 Classification-F1 0.6543085655314758 on epoch=224
03/19/2022 02:08:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.054930 on epoch=227
03/19/2022 02:09:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.025593 on epoch=229
03/19/2022 02:09:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.047825 on epoch=232
03/19/2022 02:09:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.040148 on epoch=234
03/19/2022 02:09:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.047159 on epoch=237
03/19/2022 02:09:18 - INFO - __main__ - Global step 950 Train loss 0.043131 Classification-F1 0.6962027914614123 on epoch=237
03/19/2022 02:09:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.032448 on epoch=239
03/19/2022 02:09:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.066333 on epoch=242
03/19/2022 02:09:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.043490 on epoch=244
03/19/2022 02:09:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.062450 on epoch=247
03/19/2022 02:09:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.091795 on epoch=249
03/19/2022 02:09:44 - INFO - __main__ - Global step 1000 Train loss 0.059303 Classification-F1 0.7005332066307676 on epoch=249
03/19/2022 02:09:44 - INFO - __main__ - save last model!
03/19/2022 02:09:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:09:44 - INFO - __main__ - Printing 3 examples
03/19/2022 02:09:44 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 02:09:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:09:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:09:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:09:44 - INFO - __main__ - Printing 3 examples
03/19/2022 02:09:44 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:09:44 - INFO - __main__ - ['happy']
03/19/2022 02:09:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:09:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:09:45 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:09:51 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 02:09:52 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 02:09:52 - INFO - __main__ - Printing 3 examples
03/19/2022 02:09:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 02:09:52 - INFO - __main__ - ['others']
03/19/2022 02:09:52 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 02:09:52 - INFO - __main__ - ['others']
03/19/2022 02:09:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 02:09:52 - INFO - __main__ - ['others']
03/19/2022 02:09:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:09:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:09:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:09:57 - INFO - __main__ - Starting training!
03/19/2022 02:09:59 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:10:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_21_0.0001_8_predictions.txt
03/19/2022 02:10:42 - INFO - __main__ - Classification-F1 on test data: 0.0294
03/19/2022 02:10:43 - INFO - __main__ - prefix=emo_16_21, lr=0.0001, bsz=8, dev_performance=0.7153575402214241, test_performance=0.029395976507372117
03/19/2022 02:10:43 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0005, bsz=8 ...
03/19/2022 02:10:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:10:44 - INFO - __main__ - Printing 3 examples
03/19/2022 02:10:44 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:10:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:10:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:10:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:10:44 - INFO - __main__ - Printing 3 examples
03/19/2022 02:10:44 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:10:44 - INFO - __main__ - ['happy']
03/19/2022 02:10:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:10:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:10:44 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:10:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:10:57 - INFO - __main__ - Starting training!
03/19/2022 02:11:01 - INFO - __main__ - Step 10 Global step 10 Train loss 25.682032 on epoch=2
03/19/2022 02:11:06 - INFO - __main__ - Step 20 Global step 20 Train loss 18.951801 on epoch=4
03/19/2022 02:11:11 - INFO - __main__ - Step 30 Global step 30 Train loss 16.018190 on epoch=7
03/19/2022 02:11:16 - INFO - __main__ - Step 40 Global step 40 Train loss 15.130177 on epoch=9
03/19/2022 02:11:21 - INFO - __main__ - Step 50 Global step 50 Train loss 13.237178 on epoch=12
03/19/2022 02:11:21 - INFO - __main__ - Global step 50 Train loss 17.803875 Classification-F1 0.0 on epoch=12
03/19/2022 02:11:27 - INFO - __main__ - Step 60 Global step 60 Train loss 12.087383 on epoch=14
03/19/2022 02:11:32 - INFO - __main__ - Step 70 Global step 70 Train loss 10.424676 on epoch=17
03/19/2022 02:11:37 - INFO - __main__ - Step 80 Global step 80 Train loss 8.079519 on epoch=19
03/19/2022 02:11:42 - INFO - __main__ - Step 90 Global step 90 Train loss 5.543365 on epoch=22
03/19/2022 02:11:47 - INFO - __main__ - Step 100 Global step 100 Train loss 2.309270 on epoch=24
03/19/2022 02:11:47 - INFO - __main__ - Global step 100 Train loss 7.688842 Classification-F1 0.2596512757580406 on epoch=24
03/19/2022 02:11:53 - INFO - __main__ - Step 110 Global step 110 Train loss 1.458897 on epoch=27
03/19/2022 02:11:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.040830 on epoch=29
03/19/2022 02:12:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.705990 on epoch=32
03/19/2022 02:12:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.634785 on epoch=34
03/19/2022 02:12:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.482269 on epoch=37
03/19/2022 02:12:14 - INFO - __main__ - Global step 150 Train loss 0.864554 Classification-F1 0.5329948932890108 on epoch=37
03/19/2022 02:12:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.596111 on epoch=39
03/19/2022 02:12:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.343589 on epoch=42
03/19/2022 02:12:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.316949 on epoch=44
03/19/2022 02:12:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.351140 on epoch=47
03/19/2022 02:12:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.558880 on epoch=49
03/19/2022 02:12:40 - INFO - __main__ - Global step 200 Train loss 0.433334 Classification-F1 0.6778561476837338 on epoch=49
03/19/2022 02:12:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.755684 on epoch=52
03/19/2022 02:12:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.375084 on epoch=54
03/19/2022 02:12:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.711574 on epoch=57
03/19/2022 02:13:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.159814 on epoch=59
03/19/2022 02:13:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.207943 on epoch=62
03/19/2022 02:13:06 - INFO - __main__ - Global step 250 Train loss 0.642020 Classification-F1 0.7082750582750583 on epoch=62
03/19/2022 02:13:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.140082 on epoch=64
03/19/2022 02:13:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.054645 on epoch=67
03/19/2022 02:13:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.072261 on epoch=69
03/19/2022 02:13:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.063969 on epoch=72
03/19/2022 02:13:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.024296 on epoch=74
03/19/2022 02:13:32 - INFO - __main__ - Global step 300 Train loss 0.071050 Classification-F1 0.7106818181818181 on epoch=74
03/19/2022 02:13:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.022991 on epoch=77
03/19/2022 02:13:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.038827 on epoch=79
03/19/2022 02:13:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.028063 on epoch=82
03/19/2022 02:13:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017097 on epoch=84
03/19/2022 02:13:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.107696 on epoch=87
03/19/2022 02:13:58 - INFO - __main__ - Global step 350 Train loss 0.042935 Classification-F1 0.6540533209331163 on epoch=87
03/19/2022 02:14:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.007442 on epoch=89
03/19/2022 02:14:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.021443 on epoch=92
03/19/2022 02:14:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008060 on epoch=94
03/19/2022 02:14:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004415 on epoch=97
03/19/2022 02:14:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.109866 on epoch=99
03/19/2022 02:14:24 - INFO - __main__ - Global step 400 Train loss 0.030245 Classification-F1 0.7023172905525846 on epoch=99
03/19/2022 02:14:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000411 on epoch=102
03/19/2022 02:14:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.002450 on epoch=104
03/19/2022 02:14:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.014610 on epoch=107
03/19/2022 02:14:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.038675 on epoch=109
03/19/2022 02:14:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008766 on epoch=112
03/19/2022 02:14:49 - INFO - __main__ - Global step 450 Train loss 0.012982 Classification-F1 0.7161319073083778 on epoch=112
03/19/2022 02:14:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001858 on epoch=114
03/19/2022 02:15:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001444 on epoch=117
03/19/2022 02:15:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008540 on epoch=119
03/19/2022 02:15:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000619 on epoch=122
03/19/2022 02:15:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.012535 on epoch=124
03/19/2022 02:15:15 - INFO - __main__ - Global step 500 Train loss 0.004999 Classification-F1 0.6988156775502191 on epoch=124
03/19/2022 02:15:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001684 on epoch=127
03/19/2022 02:15:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000756 on epoch=129
03/19/2022 02:15:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000534 on epoch=132
03/19/2022 02:15:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000344 on epoch=134
03/19/2022 02:15:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002032 on epoch=137
03/19/2022 02:15:41 - INFO - __main__ - Global step 550 Train loss 0.001070 Classification-F1 0.6500278473962685 on epoch=137
03/19/2022 02:15:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000199 on epoch=139
03/19/2022 02:15:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000494 on epoch=142
03/19/2022 02:15:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000125 on epoch=144
03/19/2022 02:16:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000678 on epoch=147
03/19/2022 02:16:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004724 on epoch=149
03/19/2022 02:16:06 - INFO - __main__ - Global step 600 Train loss 0.001244 Classification-F1 0.6575396825396825 on epoch=149
03/19/2022 02:16:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.007272 on epoch=152
03/19/2022 02:16:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000073 on epoch=154
03/19/2022 02:16:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000079 on epoch=157
03/19/2022 02:16:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000594 on epoch=159
03/19/2022 02:16:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000509 on epoch=162
03/19/2022 02:16:31 - INFO - __main__ - Global step 650 Train loss 0.001706 Classification-F1 0.6475497475497476 on epoch=162
03/19/2022 02:16:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.007939 on epoch=164
03/19/2022 02:16:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.009693 on epoch=167
03/19/2022 02:16:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000187 on epoch=169
03/19/2022 02:16:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000072 on epoch=172
03/19/2022 02:16:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000605 on epoch=174
03/19/2022 02:16:57 - INFO - __main__ - Global step 700 Train loss 0.003699 Classification-F1 0.7179719395089413 on epoch=174
03/19/2022 02:17:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000048 on epoch=177
03/19/2022 02:17:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000276 on epoch=179
03/19/2022 02:17:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000223 on epoch=182
03/19/2022 02:17:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000265 on epoch=184
03/19/2022 02:17:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000145 on epoch=187
03/19/2022 02:17:23 - INFO - __main__ - Global step 750 Train loss 0.000191 Classification-F1 0.7181747111070134 on epoch=187
03/19/2022 02:17:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002485 on epoch=189
03/19/2022 02:17:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000106 on epoch=192
03/19/2022 02:17:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000084 on epoch=194
03/19/2022 02:17:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.141628 on epoch=197
03/19/2022 02:17:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000330 on epoch=199
03/19/2022 02:17:49 - INFO - __main__ - Global step 800 Train loss 0.028927 Classification-F1 0.6881944444444444 on epoch=199
03/19/2022 02:17:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000753 on epoch=202
03/19/2022 02:17:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000446 on epoch=204
03/19/2022 02:18:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000287 on epoch=207
03/19/2022 02:18:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000303 on epoch=209
03/19/2022 02:18:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000378 on epoch=212
03/19/2022 02:18:15 - INFO - __main__ - Global step 850 Train loss 0.000433 Classification-F1 0.6881944444444444 on epoch=212
03/19/2022 02:18:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000079 on epoch=214
03/19/2022 02:18:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002910 on epoch=217
03/19/2022 02:18:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001022 on epoch=219
03/19/2022 02:18:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000409 on epoch=222
03/19/2022 02:18:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000260 on epoch=224
03/19/2022 02:18:41 - INFO - __main__ - Global step 900 Train loss 0.000936 Classification-F1 0.6545285087719298 on epoch=224
03/19/2022 02:18:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000836 on epoch=227
03/19/2022 02:18:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000090 on epoch=229
03/19/2022 02:18:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000901 on epoch=232
03/19/2022 02:19:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000180 on epoch=234
03/19/2022 02:19:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.003515 on epoch=237
03/19/2022 02:19:06 - INFO - __main__ - Global step 950 Train loss 0.001104 Classification-F1 0.6353354978354978 on epoch=237
03/19/2022 02:19:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000158 on epoch=239
03/19/2022 02:19:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000039 on epoch=242
03/19/2022 02:19:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000156 on epoch=244
03/19/2022 02:19:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000259 on epoch=247
03/19/2022 02:19:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000421 on epoch=249
03/19/2022 02:19:32 - INFO - __main__ - Global step 1000 Train loss 0.000207 Classification-F1 0.6708660146160147 on epoch=249
03/19/2022 02:19:32 - INFO - __main__ - save last model!
03/19/2022 02:19:33 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:19:33 - INFO - __main__ - Printing 3 examples
03/19/2022 02:19:33 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 02:19:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:19:33 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:19:33 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:19:33 - INFO - __main__ - Printing 3 examples
03/19/2022 02:19:33 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:19:33 - INFO - __main__ - ['happy']
03/19/2022 02:19:33 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:19:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:19:33 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:19:39 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 02:19:40 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 02:19:40 - INFO - __main__ - Printing 3 examples
03/19/2022 02:19:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 02:19:40 - INFO - __main__ - ['others']
03/19/2022 02:19:40 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 02:19:40 - INFO - __main__ - ['others']
03/19/2022 02:19:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 02:19:40 - INFO - __main__ - ['others']
03/19/2022 02:19:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:19:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:19:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:19:44 - INFO - __main__ - Starting training!
03/19/2022 02:19:47 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:20:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_42_0.0005_8_predictions.txt
03/19/2022 02:20:35 - INFO - __main__ - Classification-F1 on test data: 0.0379
03/19/2022 02:20:35 - INFO - __main__ - prefix=emo_16_42, lr=0.0005, bsz=8, dev_performance=0.7181747111070134, test_performance=0.03786370950504051
03/19/2022 02:20:36 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0003, bsz=8 ...
03/19/2022 02:20:36 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:20:36 - INFO - __main__ - Printing 3 examples
03/19/2022 02:20:36 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:20:36 - INFO - __main__ - ['happy']
03/19/2022 02:20:36 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:20:36 - INFO - __main__ - ['happy']
03/19/2022 02:20:36 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:20:36 - INFO - __main__ - ['happy']
03/19/2022 02:20:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:20:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:20:37 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:20:37 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:20:37 - INFO - __main__ - Printing 3 examples
03/19/2022 02:20:37 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:20:37 - INFO - __main__ - ['happy']
03/19/2022 02:20:37 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:20:37 - INFO - __main__ - ['happy']
03/19/2022 02:20:37 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:20:37 - INFO - __main__ - ['happy']
03/19/2022 02:20:37 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:20:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:20:37 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:20:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:20:47 - INFO - __main__ - Starting training!
03/19/2022 02:20:52 - INFO - __main__ - Step 10 Global step 10 Train loss 25.384253 on epoch=2
03/19/2022 02:20:56 - INFO - __main__ - Step 20 Global step 20 Train loss 20.716251 on epoch=4
03/19/2022 02:21:01 - INFO - __main__ - Step 30 Global step 30 Train loss 17.793491 on epoch=7
03/19/2022 02:21:06 - INFO - __main__ - Step 40 Global step 40 Train loss 16.826527 on epoch=9
03/19/2022 02:21:11 - INFO - __main__ - Step 50 Global step 50 Train loss 15.579176 on epoch=12
03/19/2022 02:21:16 - INFO - __main__ - Global step 50 Train loss 19.259941 Classification-F1 0.0 on epoch=12
03/19/2022 02:21:22 - INFO - __main__ - Step 60 Global step 60 Train loss 15.002325 on epoch=14
03/19/2022 02:21:27 - INFO - __main__ - Step 70 Global step 70 Train loss 14.175458 on epoch=17
03/19/2022 02:21:32 - INFO - __main__ - Step 80 Global step 80 Train loss 13.341309 on epoch=19
03/19/2022 02:21:37 - INFO - __main__ - Step 90 Global step 90 Train loss 11.433310 on epoch=22
03/19/2022 02:21:42 - INFO - __main__ - Step 100 Global step 100 Train loss 10.700693 on epoch=24
03/19/2022 02:21:43 - INFO - __main__ - Global step 100 Train loss 12.930619 Classification-F1 0.0 on epoch=24
03/19/2022 02:21:48 - INFO - __main__ - Step 110 Global step 110 Train loss 8.726883 on epoch=27
03/19/2022 02:21:53 - INFO - __main__ - Step 120 Global step 120 Train loss 7.282810 on epoch=29
03/19/2022 02:21:58 - INFO - __main__ - Step 130 Global step 130 Train loss 4.879550 on epoch=32
03/19/2022 02:22:03 - INFO - __main__ - Step 140 Global step 140 Train loss 3.680238 on epoch=34
03/19/2022 02:22:08 - INFO - __main__ - Step 150 Global step 150 Train loss 3.403959 on epoch=37
03/19/2022 02:22:09 - INFO - __main__ - Global step 150 Train loss 5.594688 Classification-F1 0.09493670886075949 on epoch=37
03/19/2022 02:22:15 - INFO - __main__ - Step 160 Global step 160 Train loss 4.239798 on epoch=39
03/19/2022 02:22:20 - INFO - __main__ - Step 170 Global step 170 Train loss 3.705098 on epoch=42
03/19/2022 02:22:25 - INFO - __main__ - Step 180 Global step 180 Train loss 3.595728 on epoch=44
03/19/2022 02:22:30 - INFO - __main__ - Step 190 Global step 190 Train loss 1.597907 on epoch=47
03/19/2022 02:22:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.771778 on epoch=49
03/19/2022 02:22:35 - INFO - __main__ - Global step 200 Train loss 2.782062 Classification-F1 0.5381708581799326 on epoch=49
03/19/2022 02:22:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.756394 on epoch=52
03/19/2022 02:22:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.420352 on epoch=54
03/19/2022 02:22:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.371838 on epoch=57
03/19/2022 02:22:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.307794 on epoch=59
03/19/2022 02:23:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.305796 on epoch=62
03/19/2022 02:23:02 - INFO - __main__ - Global step 250 Train loss 0.432435 Classification-F1 0.6976456339359565 on epoch=62
03/19/2022 02:23:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.162436 on epoch=64
03/19/2022 02:23:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.120150 on epoch=67
03/19/2022 02:23:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.193742 on epoch=69
03/19/2022 02:23:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.111379 on epoch=72
03/19/2022 02:23:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.084822 on epoch=74
03/19/2022 02:23:28 - INFO - __main__ - Global step 300 Train loss 0.134506 Classification-F1 0.7468750000000001 on epoch=74
03/19/2022 02:23:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.065003 on epoch=77
03/19/2022 02:23:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.066893 on epoch=79
03/19/2022 02:23:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.035452 on epoch=82
03/19/2022 02:23:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.084247 on epoch=84
03/19/2022 02:23:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.062526 on epoch=87
03/19/2022 02:23:54 - INFO - __main__ - Global step 350 Train loss 0.062824 Classification-F1 0.698054818744474 on epoch=87
03/19/2022 02:23:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.026735 on epoch=89
03/19/2022 02:24:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011694 on epoch=92
03/19/2022 02:24:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008310 on epoch=94
03/19/2022 02:24:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.038059 on epoch=97
03/19/2022 02:24:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015073 on epoch=99
03/19/2022 02:24:20 - INFO - __main__ - Global step 400 Train loss 0.019974 Classification-F1 0.7660539215686275 on epoch=99
03/19/2022 02:24:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.017013 on epoch=102
03/19/2022 02:24:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.011526 on epoch=104
03/19/2022 02:24:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.019241 on epoch=107
03/19/2022 02:24:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.035702 on epoch=109
03/19/2022 02:24:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.025674 on epoch=112
03/19/2022 02:24:46 - INFO - __main__ - Global step 450 Train loss 0.021831 Classification-F1 0.8135069863722615 on epoch=112
03/19/2022 02:24:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.045758 on epoch=114
03/19/2022 02:24:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010148 on epoch=117
03/19/2022 02:25:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004859 on epoch=119
03/19/2022 02:25:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009337 on epoch=122
03/19/2022 02:25:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003084 on epoch=124
03/19/2022 02:25:12 - INFO - __main__ - Global step 500 Train loss 0.014637 Classification-F1 0.7671146953405018 on epoch=124
03/19/2022 02:25:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005875 on epoch=127
03/19/2022 02:25:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.055608 on epoch=129
03/19/2022 02:25:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005218 on epoch=132
03/19/2022 02:25:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.005479 on epoch=134
03/19/2022 02:25:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004083 on epoch=137
03/19/2022 02:25:38 - INFO - __main__ - Global step 550 Train loss 0.015253 Classification-F1 0.7813124039938556 on epoch=137
03/19/2022 02:25:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.041832 on epoch=139
03/19/2022 02:25:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.025737 on epoch=142
03/19/2022 02:25:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005533 on epoch=144
03/19/2022 02:25:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021116 on epoch=147
03/19/2022 02:26:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.032204 on epoch=149
03/19/2022 02:26:03 - INFO - __main__ - Global step 600 Train loss 0.025284 Classification-F1 0.7313831453634085 on epoch=149
03/19/2022 02:26:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001314 on epoch=152
03/19/2022 02:26:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.012631 on epoch=154
03/19/2022 02:26:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.026484 on epoch=157
03/19/2022 02:26:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003121 on epoch=159
03/19/2022 02:26:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.055980 on epoch=162
03/19/2022 02:26:28 - INFO - __main__ - Global step 650 Train loss 0.019906 Classification-F1 0.8105744350526134 on epoch=162
03/19/2022 02:26:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004968 on epoch=164
03/19/2022 02:26:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.010387 on epoch=167
03/19/2022 02:26:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.034798 on epoch=169
03/19/2022 02:26:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.137295 on epoch=172
03/19/2022 02:26:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.121955 on epoch=174
03/19/2022 02:26:53 - INFO - __main__ - Global step 700 Train loss 0.061881 Classification-F1 0.7015587015587016 on epoch=174
03/19/2022 02:26:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.039716 on epoch=177
03/19/2022 02:27:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.027451 on epoch=179
03/19/2022 02:27:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.032369 on epoch=182
03/19/2022 02:27:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.053255 on epoch=184
03/19/2022 02:27:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.056937 on epoch=187
03/19/2022 02:27:19 - INFO - __main__ - Global step 750 Train loss 0.041945 Classification-F1 0.7024725274725275 on epoch=187
03/19/2022 02:27:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.076272 on epoch=189
03/19/2022 02:27:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.051097 on epoch=192
03/19/2022 02:27:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.107445 on epoch=194
03/19/2022 02:27:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.027671 on epoch=197
03/19/2022 02:27:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.125961 on epoch=199
03/19/2022 02:27:44 - INFO - __main__ - Global step 800 Train loss 0.077689 Classification-F1 0.682174843465166 on epoch=199
03/19/2022 02:27:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.068633 on epoch=202
03/19/2022 02:27:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.044798 on epoch=204
03/19/2022 02:27:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.053418 on epoch=207
03/19/2022 02:28:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.009663 on epoch=209
03/19/2022 02:28:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.022457 on epoch=212
03/19/2022 02:28:09 - INFO - __main__ - Global step 850 Train loss 0.039794 Classification-F1 0.6623756360598465 on epoch=212
03/19/2022 02:28:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.017648 on epoch=214
03/19/2022 02:28:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002415 on epoch=217
03/19/2022 02:28:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002769 on epoch=219
03/19/2022 02:28:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.110559 on epoch=222
03/19/2022 02:28:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.033516 on epoch=224
03/19/2022 02:28:35 - INFO - __main__ - Global step 900 Train loss 0.033381 Classification-F1 0.6467948717948718 on epoch=224
03/19/2022 02:28:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.010258 on epoch=227
03/19/2022 02:28:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.009287 on epoch=229
03/19/2022 02:28:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001295 on epoch=232
03/19/2022 02:28:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.022108 on epoch=234
03/19/2022 02:29:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.004946 on epoch=237
03/19/2022 02:29:01 - INFO - __main__ - Global step 950 Train loss 0.009579 Classification-F1 0.7358630952380952 on epoch=237
03/19/2022 02:29:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001545 on epoch=239
03/19/2022 02:29:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.013488 on epoch=242
03/19/2022 02:29:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.005708 on epoch=244
03/19/2022 02:29:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.019630 on epoch=247
03/19/2022 02:29:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003798 on epoch=249
03/19/2022 02:29:27 - INFO - __main__ - Global step 1000 Train loss 0.008834 Classification-F1 0.6681818181818181 on epoch=249
03/19/2022 02:29:27 - INFO - __main__ - save last model!
03/19/2022 02:29:28 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:29:28 - INFO - __main__ - Printing 3 examples
03/19/2022 02:29:28 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 02:29:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:29:28 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:29:28 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:29:28 - INFO - __main__ - Printing 3 examples
03/19/2022 02:29:28 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:29:28 - INFO - __main__ - ['happy']
03/19/2022 02:29:28 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:29:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:29:28 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:29:34 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 02:29:35 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 02:29:35 - INFO - __main__ - Printing 3 examples
03/19/2022 02:29:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 02:29:35 - INFO - __main__ - ['others']
03/19/2022 02:29:35 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 02:29:35 - INFO - __main__ - ['others']
03/19/2022 02:29:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 02:29:35 - INFO - __main__ - ['others']
03/19/2022 02:29:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:29:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:29:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:29:40 - INFO - __main__ - Starting training!
03/19/2022 02:29:42 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:30:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_42_0.0003_8_predictions.txt
03/19/2022 02:30:26 - INFO - __main__ - Classification-F1 on test data: 0.4726
03/19/2022 02:30:26 - INFO - __main__ - prefix=emo_16_42, lr=0.0003, bsz=8, dev_performance=0.8135069863722615, test_performance=0.4725542496161442
03/19/2022 02:30:26 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0002, bsz=8 ...
03/19/2022 02:30:27 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:30:27 - INFO - __main__ - Printing 3 examples
03/19/2022 02:30:27 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:30:27 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:30:27 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:30:27 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:30:27 - INFO - __main__ - Printing 3 examples
03/19/2022 02:30:27 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:30:27 - INFO - __main__ - ['happy']
03/19/2022 02:30:27 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:30:27 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:30:27 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:30:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:30:40 - INFO - __main__ - Starting training!
03/19/2022 02:30:44 - INFO - __main__ - Step 10 Global step 10 Train loss 25.029291 on epoch=2
03/19/2022 02:30:49 - INFO - __main__ - Step 20 Global step 20 Train loss 22.199665 on epoch=4
03/19/2022 02:30:54 - INFO - __main__ - Step 30 Global step 30 Train loss 17.770668 on epoch=7
03/19/2022 02:30:59 - INFO - __main__ - Step 40 Global step 40 Train loss 17.006824 on epoch=9
03/19/2022 02:31:04 - INFO - __main__ - Step 50 Global step 50 Train loss 17.061916 on epoch=12
03/19/2022 02:31:23 - INFO - __main__ - Global step 50 Train loss 19.813671 Classification-F1 0.0 on epoch=12
03/19/2022 02:31:28 - INFO - __main__ - Step 60 Global step 60 Train loss 15.517138 on epoch=14
03/19/2022 02:31:33 - INFO - __main__ - Step 70 Global step 70 Train loss 15.595019 on epoch=17
03/19/2022 02:31:38 - INFO - __main__ - Step 80 Global step 80 Train loss 14.411427 on epoch=19
03/19/2022 02:31:43 - INFO - __main__ - Step 90 Global step 90 Train loss 14.228392 on epoch=22
03/19/2022 02:31:48 - INFO - __main__ - Step 100 Global step 100 Train loss 13.147575 on epoch=24
03/19/2022 02:32:01 - INFO - __main__ - Global step 100 Train loss 14.579911 Classification-F1 0.0 on epoch=24
03/19/2022 02:32:06 - INFO - __main__ - Step 110 Global step 110 Train loss 13.425349 on epoch=27
03/19/2022 02:32:11 - INFO - __main__ - Step 120 Global step 120 Train loss 12.214843 on epoch=29
03/19/2022 02:32:16 - INFO - __main__ - Step 130 Global step 130 Train loss 11.077520 on epoch=32
03/19/2022 02:32:21 - INFO - __main__ - Step 140 Global step 140 Train loss 10.340729 on epoch=34
03/19/2022 02:32:26 - INFO - __main__ - Step 150 Global step 150 Train loss 9.486046 on epoch=37
03/19/2022 02:32:36 - INFO - __main__ - Global step 150 Train loss 11.308898 Classification-F1 0.0 on epoch=37
03/19/2022 02:32:41 - INFO - __main__ - Step 160 Global step 160 Train loss 7.629289 on epoch=39
03/19/2022 02:32:46 - INFO - __main__ - Step 170 Global step 170 Train loss 5.019122 on epoch=42
03/19/2022 02:32:51 - INFO - __main__ - Step 180 Global step 180 Train loss 5.254771 on epoch=44
03/19/2022 02:32:56 - INFO - __main__ - Step 190 Global step 190 Train loss 3.564896 on epoch=47
03/19/2022 02:33:01 - INFO - __main__ - Step 200 Global step 200 Train loss 4.317950 on epoch=49
03/19/2022 02:33:02 - INFO - __main__ - Global step 200 Train loss 5.157205 Classification-F1 0.1736111111111111 on epoch=49
03/19/2022 02:33:07 - INFO - __main__ - Step 210 Global step 210 Train loss 3.783460 on epoch=52
03/19/2022 02:33:12 - INFO - __main__ - Step 220 Global step 220 Train loss 3.742535 on epoch=54
03/19/2022 02:33:17 - INFO - __main__ - Step 230 Global step 230 Train loss 3.336604 on epoch=57
03/19/2022 02:33:22 - INFO - __main__ - Step 240 Global step 240 Train loss 3.488275 on epoch=59
03/19/2022 02:33:27 - INFO - __main__ - Step 250 Global step 250 Train loss 3.436129 on epoch=62
03/19/2022 02:33:28 - INFO - __main__ - Global step 250 Train loss 3.557400 Classification-F1 0.1 on epoch=62
03/19/2022 02:33:33 - INFO - __main__ - Step 260 Global step 260 Train loss 3.558839 on epoch=64
03/19/2022 02:33:37 - INFO - __main__ - Step 270 Global step 270 Train loss 3.100219 on epoch=67
03/19/2022 02:33:42 - INFO - __main__ - Step 280 Global step 280 Train loss 3.218571 on epoch=69
03/19/2022 02:33:47 - INFO - __main__ - Step 290 Global step 290 Train loss 3.055552 on epoch=72
03/19/2022 02:33:52 - INFO - __main__ - Step 300 Global step 300 Train loss 2.645550 on epoch=74
03/19/2022 02:33:53 - INFO - __main__ - Global step 300 Train loss 3.115746 Classification-F1 0.1581196581196581 on epoch=74
03/19/2022 02:33:58 - INFO - __main__ - Step 310 Global step 310 Train loss 3.116968 on epoch=77
03/19/2022 02:34:03 - INFO - __main__ - Step 320 Global step 320 Train loss 2.216663 on epoch=79
03/19/2022 02:34:08 - INFO - __main__ - Step 330 Global step 330 Train loss 3.474608 on epoch=82
03/19/2022 02:34:13 - INFO - __main__ - Step 340 Global step 340 Train loss 2.939052 on epoch=84
03/19/2022 02:34:18 - INFO - __main__ - Step 350 Global step 350 Train loss 2.612847 on epoch=87
03/19/2022 02:34:18 - INFO - __main__ - Global step 350 Train loss 2.872027 Classification-F1 0.323953823953824 on epoch=87
03/19/2022 02:34:24 - INFO - __main__ - Step 360 Global step 360 Train loss 2.594085 on epoch=89
03/19/2022 02:34:29 - INFO - __main__ - Step 370 Global step 370 Train loss 2.720140 on epoch=92
03/19/2022 02:34:34 - INFO - __main__ - Step 380 Global step 380 Train loss 2.198556 on epoch=94
03/19/2022 02:34:39 - INFO - __main__ - Step 390 Global step 390 Train loss 2.303326 on epoch=97
03/19/2022 02:34:44 - INFO - __main__ - Step 400 Global step 400 Train loss 2.052214 on epoch=99
03/19/2022 02:34:44 - INFO - __main__ - Global step 400 Train loss 2.373664 Classification-F1 0.39069574954963726 on epoch=99
03/19/2022 02:34:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.934437 on epoch=102
03/19/2022 02:34:55 - INFO - __main__ - Step 420 Global step 420 Train loss 1.972368 on epoch=104
03/19/2022 02:35:00 - INFO - __main__ - Step 430 Global step 430 Train loss 1.830979 on epoch=107
03/19/2022 02:35:05 - INFO - __main__ - Step 440 Global step 440 Train loss 1.612159 on epoch=109
03/19/2022 02:35:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.251281 on epoch=112
03/19/2022 02:35:11 - INFO - __main__ - Global step 450 Train loss 1.720245 Classification-F1 0.23973037831733482 on epoch=112
03/19/2022 02:35:16 - INFO - __main__ - Step 460 Global step 460 Train loss 1.383728 on epoch=114
03/19/2022 02:35:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.943237 on epoch=117
03/19/2022 02:35:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.563672 on epoch=119
03/19/2022 02:35:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.411907 on epoch=122
03/19/2022 02:35:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.294417 on epoch=124
03/19/2022 02:35:36 - INFO - __main__ - Global step 500 Train loss 0.719392 Classification-F1 0.6350375939849624 on epoch=124
03/19/2022 02:35:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.205667 on epoch=127
03/19/2022 02:35:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.155181 on epoch=129
03/19/2022 02:35:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.099019 on epoch=132
03/19/2022 02:35:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.125983 on epoch=134
03/19/2022 02:36:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.080904 on epoch=137
03/19/2022 02:36:02 - INFO - __main__ - Global step 550 Train loss 0.133351 Classification-F1 0.7043010752688171 on epoch=137
03/19/2022 02:36:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.078369 on epoch=139
03/19/2022 02:36:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.032311 on epoch=142
03/19/2022 02:36:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.047715 on epoch=144
03/19/2022 02:36:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.100339 on epoch=147
03/19/2022 02:36:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.056569 on epoch=149
03/19/2022 02:36:28 - INFO - __main__ - Global step 600 Train loss 0.063061 Classification-F1 0.7011675824175824 on epoch=149
03/19/2022 02:36:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.014719 on epoch=152
03/19/2022 02:36:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.015198 on epoch=154
03/19/2022 02:36:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.051763 on epoch=157
03/19/2022 02:36:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.015534 on epoch=159
03/19/2022 02:36:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005057 on epoch=162
03/19/2022 02:36:54 - INFO - __main__ - Global step 650 Train loss 0.020454 Classification-F1 0.6991606432782903 on epoch=162
03/19/2022 02:36:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.003858 on epoch=164
03/19/2022 02:37:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022284 on epoch=167
03/19/2022 02:37:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.033231 on epoch=169
03/19/2022 02:37:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004566 on epoch=172
03/19/2022 02:37:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005482 on epoch=174
03/19/2022 02:37:19 - INFO - __main__ - Global step 700 Train loss 0.013884 Classification-F1 0.6978237448825684 on epoch=174
03/19/2022 02:37:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.025363 on epoch=177
03/19/2022 02:37:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.013158 on epoch=179
03/19/2022 02:37:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.009476 on epoch=182
03/19/2022 02:37:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.028335 on epoch=184
03/19/2022 02:37:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.019935 on epoch=187
03/19/2022 02:37:45 - INFO - __main__ - Global step 750 Train loss 0.019253 Classification-F1 0.7025974025974026 on epoch=187
03/19/2022 02:37:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006184 on epoch=189
03/19/2022 02:37:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007297 on epoch=192
03/19/2022 02:38:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.002300 on epoch=194
03/19/2022 02:38:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002908 on epoch=197
03/19/2022 02:38:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002041 on epoch=199
03/19/2022 02:38:11 - INFO - __main__ - Global step 800 Train loss 0.004146 Classification-F1 0.7293101111412306 on epoch=199
03/19/2022 02:38:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007730 on epoch=202
03/19/2022 02:38:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.055572 on epoch=204
03/19/2022 02:38:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.006957 on epoch=207
03/19/2022 02:38:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004481 on epoch=209
03/19/2022 02:38:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001911 on epoch=212
03/19/2022 02:38:38 - INFO - __main__ - Global step 850 Train loss 0.015330 Classification-F1 0.6999299719887955 on epoch=212
03/19/2022 02:38:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.008641 on epoch=214
03/19/2022 02:38:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001167 on epoch=217
03/19/2022 02:38:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000767 on epoch=219
03/19/2022 02:38:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001228 on epoch=222
03/19/2022 02:39:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.010582 on epoch=224
03/19/2022 02:39:04 - INFO - __main__ - Global step 900 Train loss 0.004477 Classification-F1 0.724449992198471 on epoch=224
03/19/2022 02:39:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001983 on epoch=227
03/19/2022 02:39:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000609 on epoch=229
03/19/2022 02:39:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.018075 on epoch=232
03/19/2022 02:39:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001089 on epoch=234
03/19/2022 02:39:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000298 on epoch=237
03/19/2022 02:39:30 - INFO - __main__ - Global step 950 Train loss 0.004411 Classification-F1 0.7347215860760322 on epoch=237
03/19/2022 02:39:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001957 on epoch=239
03/19/2022 02:39:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000218 on epoch=242
03/19/2022 02:39:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000177 on epoch=244
03/19/2022 02:39:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000068 on epoch=247
03/19/2022 02:39:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000068 on epoch=249
03/19/2022 02:39:57 - INFO - __main__ - Global step 1000 Train loss 0.000498 Classification-F1 0.7327380952380953 on epoch=249
03/19/2022 02:39:57 - INFO - __main__ - save last model!
03/19/2022 02:39:57 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:39:57 - INFO - __main__ - Printing 3 examples
03/19/2022 02:39:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 02:39:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:39:57 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:39:57 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:39:57 - INFO - __main__ - Printing 3 examples
03/19/2022 02:39:57 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:39:57 - INFO - __main__ - ['happy']
03/19/2022 02:39:57 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:39:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:39:58 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:40:04 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 02:40:04 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 02:40:04 - INFO - __main__ - Printing 3 examples
03/19/2022 02:40:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 02:40:04 - INFO - __main__ - ['others']
03/19/2022 02:40:04 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 02:40:04 - INFO - __main__ - ['others']
03/19/2022 02:40:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 02:40:04 - INFO - __main__ - ['others']
03/19/2022 02:40:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:40:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:40:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:40:10 - INFO - __main__ - Starting training!
03/19/2022 02:40:12 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:40:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_42_0.0002_8_predictions.txt
03/19/2022 02:40:54 - INFO - __main__ - Classification-F1 on test data: 0.4674
03/19/2022 02:40:55 - INFO - __main__ - prefix=emo_16_42, lr=0.0002, bsz=8, dev_performance=0.7347215860760322, test_performance=0.46737207166354977
03/19/2022 02:40:55 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0001, bsz=8 ...
03/19/2022 02:40:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:40:56 - INFO - __main__ - Printing 3 examples
03/19/2022 02:40:56 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:40:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:40:56 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:40:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:40:56 - INFO - __main__ - Printing 3 examples
03/19/2022 02:40:56 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/19/2022 02:40:56 - INFO - __main__ - ['happy']
03/19/2022 02:40:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:40:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:40:56 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:41:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:41:07 - INFO - __main__ - Starting training!
03/19/2022 02:41:11 - INFO - __main__ - Step 10 Global step 10 Train loss 26.200766 on epoch=2
03/19/2022 02:41:15 - INFO - __main__ - Step 20 Global step 20 Train loss 23.690378 on epoch=4
03/19/2022 02:41:20 - INFO - __main__ - Step 30 Global step 30 Train loss 20.417856 on epoch=7
03/19/2022 02:41:25 - INFO - __main__ - Step 40 Global step 40 Train loss 19.424389 on epoch=9
03/19/2022 02:41:30 - INFO - __main__ - Step 50 Global step 50 Train loss 17.996387 on epoch=12
03/19/2022 02:41:50 - INFO - __main__ - Global step 50 Train loss 21.545952 Classification-F1 0.0 on epoch=12
03/19/2022 02:41:55 - INFO - __main__ - Step 60 Global step 60 Train loss 17.914856 on epoch=14
03/19/2022 02:42:00 - INFO - __main__ - Step 70 Global step 70 Train loss 18.122175 on epoch=17
03/19/2022 02:42:05 - INFO - __main__ - Step 80 Global step 80 Train loss 17.030466 on epoch=19
03/19/2022 02:42:10 - INFO - __main__ - Step 90 Global step 90 Train loss 17.120077 on epoch=22
03/19/2022 02:42:15 - INFO - __main__ - Step 100 Global step 100 Train loss 16.880768 on epoch=24
03/19/2022 02:42:30 - INFO - __main__ - Global step 100 Train loss 17.413668 Classification-F1 0.0 on epoch=24
03/19/2022 02:42:35 - INFO - __main__ - Step 110 Global step 110 Train loss 16.883514 on epoch=27
03/19/2022 02:42:40 - INFO - __main__ - Step 120 Global step 120 Train loss 16.071707 on epoch=29
03/19/2022 02:42:45 - INFO - __main__ - Step 130 Global step 130 Train loss 15.803894 on epoch=32
03/19/2022 02:42:50 - INFO - __main__ - Step 140 Global step 140 Train loss 15.020459 on epoch=34
03/19/2022 02:42:55 - INFO - __main__ - Step 150 Global step 150 Train loss 14.842397 on epoch=37
03/19/2022 02:43:08 - INFO - __main__ - Global step 150 Train loss 15.724393 Classification-F1 0.0 on epoch=37
03/19/2022 02:43:13 - INFO - __main__ - Step 160 Global step 160 Train loss 14.346029 on epoch=39
03/19/2022 02:43:18 - INFO - __main__ - Step 170 Global step 170 Train loss 14.136154 on epoch=42
03/19/2022 02:43:23 - INFO - __main__ - Step 180 Global step 180 Train loss 13.799173 on epoch=44
03/19/2022 02:43:29 - INFO - __main__ - Step 190 Global step 190 Train loss 13.310626 on epoch=47
03/19/2022 02:43:34 - INFO - __main__ - Step 200 Global step 200 Train loss 13.278613 on epoch=49
03/19/2022 02:43:47 - INFO - __main__ - Global step 200 Train loss 13.774118 Classification-F1 0.0 on epoch=49
03/19/2022 02:43:52 - INFO - __main__ - Step 210 Global step 210 Train loss 13.672368 on epoch=52
03/19/2022 02:43:57 - INFO - __main__ - Step 220 Global step 220 Train loss 12.410103 on epoch=54
03/19/2022 02:44:02 - INFO - __main__ - Step 230 Global step 230 Train loss 12.091505 on epoch=57
03/19/2022 02:44:07 - INFO - __main__ - Step 240 Global step 240 Train loss 11.892293 on epoch=59
03/19/2022 02:44:12 - INFO - __main__ - Step 250 Global step 250 Train loss 11.006730 on epoch=62
03/19/2022 02:44:19 - INFO - __main__ - Global step 250 Train loss 12.214600 Classification-F1 0.0 on epoch=62
03/19/2022 02:44:24 - INFO - __main__ - Step 260 Global step 260 Train loss 11.905548 on epoch=64
03/19/2022 02:44:29 - INFO - __main__ - Step 270 Global step 270 Train loss 11.187881 on epoch=67
03/19/2022 02:44:34 - INFO - __main__ - Step 280 Global step 280 Train loss 11.151782 on epoch=69
03/19/2022 02:44:39 - INFO - __main__ - Step 290 Global step 290 Train loss 9.939037 on epoch=72
03/19/2022 02:44:44 - INFO - __main__ - Step 300 Global step 300 Train loss 10.306993 on epoch=74
03/19/2022 02:44:47 - INFO - __main__ - Global step 300 Train loss 10.898249 Classification-F1 0.0056022408963585435 on epoch=74
03/19/2022 02:44:53 - INFO - __main__ - Step 310 Global step 310 Train loss 9.226799 on epoch=77
03/19/2022 02:44:58 - INFO - __main__ - Step 320 Global step 320 Train loss 8.054008 on epoch=79
03/19/2022 02:45:03 - INFO - __main__ - Step 330 Global step 330 Train loss 7.556741 on epoch=82
03/19/2022 02:45:08 - INFO - __main__ - Step 340 Global step 340 Train loss 6.894530 on epoch=84
03/19/2022 02:45:13 - INFO - __main__ - Step 350 Global step 350 Train loss 6.060740 on epoch=87
03/19/2022 02:45:15 - INFO - __main__ - Global step 350 Train loss 7.558564 Classification-F1 0.0130718954248366 on epoch=87
03/19/2022 02:45:21 - INFO - __main__ - Step 360 Global step 360 Train loss 5.358500 on epoch=89
03/19/2022 02:45:26 - INFO - __main__ - Step 370 Global step 370 Train loss 5.136248 on epoch=92
03/19/2022 02:45:31 - INFO - __main__ - Step 380 Global step 380 Train loss 3.993527 on epoch=94
03/19/2022 02:45:36 - INFO - __main__ - Step 390 Global step 390 Train loss 3.582249 on epoch=97
03/19/2022 02:45:42 - INFO - __main__ - Step 400 Global step 400 Train loss 3.370828 on epoch=99
03/19/2022 02:45:42 - INFO - __main__ - Global step 400 Train loss 4.288270 Classification-F1 0.1581196581196581 on epoch=99
03/19/2022 02:45:48 - INFO - __main__ - Step 410 Global step 410 Train loss 4.082140 on epoch=102
03/19/2022 02:45:53 - INFO - __main__ - Step 420 Global step 420 Train loss 3.708764 on epoch=104
03/19/2022 02:45:58 - INFO - __main__ - Step 430 Global step 430 Train loss 3.400296 on epoch=107
03/19/2022 02:46:03 - INFO - __main__ - Step 440 Global step 440 Train loss 2.855365 on epoch=109
03/19/2022 02:46:09 - INFO - __main__ - Step 450 Global step 450 Train loss 3.746866 on epoch=112
03/19/2022 02:46:09 - INFO - __main__ - Global step 450 Train loss 3.558686 Classification-F1 0.1 on epoch=112
03/19/2022 02:46:14 - INFO - __main__ - Step 460 Global step 460 Train loss 4.174910 on epoch=114
03/19/2022 02:46:19 - INFO - __main__ - Step 470 Global step 470 Train loss 2.933691 on epoch=117
03/19/2022 02:46:24 - INFO - __main__ - Step 480 Global step 480 Train loss 3.700236 on epoch=119
03/19/2022 02:46:29 - INFO - __main__ - Step 490 Global step 490 Train loss 3.064820 on epoch=122
03/19/2022 02:46:34 - INFO - __main__ - Step 500 Global step 500 Train loss 3.424141 on epoch=124
03/19/2022 02:46:35 - INFO - __main__ - Global step 500 Train loss 3.459560 Classification-F1 0.22516726711271234 on epoch=124
03/19/2022 02:46:41 - INFO - __main__ - Step 510 Global step 510 Train loss 3.170893 on epoch=127
03/19/2022 02:46:46 - INFO - __main__ - Step 520 Global step 520 Train loss 3.208246 on epoch=129
03/19/2022 02:46:51 - INFO - __main__ - Step 530 Global step 530 Train loss 3.111680 on epoch=132
03/19/2022 02:46:56 - INFO - __main__ - Step 540 Global step 540 Train loss 2.272591 on epoch=134
03/19/2022 02:47:01 - INFO - __main__ - Step 550 Global step 550 Train loss 2.806599 on epoch=137
03/19/2022 02:47:02 - INFO - __main__ - Global step 550 Train loss 2.914002 Classification-F1 0.2934754819246306 on epoch=137
03/19/2022 02:47:08 - INFO - __main__ - Step 560 Global step 560 Train loss 2.363650 on epoch=139
03/19/2022 02:47:13 - INFO - __main__ - Step 570 Global step 570 Train loss 2.705709 on epoch=142
03/19/2022 02:47:18 - INFO - __main__ - Step 580 Global step 580 Train loss 3.441719 on epoch=144
03/19/2022 02:47:23 - INFO - __main__ - Step 590 Global step 590 Train loss 2.541183 on epoch=147
03/19/2022 02:47:28 - INFO - __main__ - Step 600 Global step 600 Train loss 3.001758 on epoch=149
03/19/2022 02:47:29 - INFO - __main__ - Global step 600 Train loss 2.810804 Classification-F1 0.27981859410430837 on epoch=149
03/19/2022 02:47:34 - INFO - __main__ - Step 610 Global step 610 Train loss 2.776262 on epoch=152
03/19/2022 02:47:39 - INFO - __main__ - Step 620 Global step 620 Train loss 2.570532 on epoch=154
03/19/2022 02:47:44 - INFO - __main__ - Step 630 Global step 630 Train loss 2.547357 on epoch=157
03/19/2022 02:47:49 - INFO - __main__ - Step 640 Global step 640 Train loss 2.585495 on epoch=159
03/19/2022 02:47:54 - INFO - __main__ - Step 650 Global step 650 Train loss 3.048464 on epoch=162
03/19/2022 02:47:55 - INFO - __main__ - Global step 650 Train loss 2.705622 Classification-F1 0.30995917428555 on epoch=162
03/19/2022 02:48:00 - INFO - __main__ - Step 660 Global step 660 Train loss 2.593002 on epoch=164
03/19/2022 02:48:06 - INFO - __main__ - Step 670 Global step 670 Train loss 2.332391 on epoch=167
03/19/2022 02:48:11 - INFO - __main__ - Step 680 Global step 680 Train loss 2.226017 on epoch=169
03/19/2022 02:48:16 - INFO - __main__ - Step 690 Global step 690 Train loss 2.093242 on epoch=172
03/19/2022 02:48:21 - INFO - __main__ - Step 700 Global step 700 Train loss 2.188333 on epoch=174
03/19/2022 02:48:21 - INFO - __main__ - Global step 700 Train loss 2.286597 Classification-F1 0.34775132275132276 on epoch=174
03/19/2022 02:48:27 - INFO - __main__ - Step 710 Global step 710 Train loss 2.362955 on epoch=177
03/19/2022 02:48:32 - INFO - __main__ - Step 720 Global step 720 Train loss 2.225859 on epoch=179
03/19/2022 02:48:37 - INFO - __main__ - Step 730 Global step 730 Train loss 1.920841 on epoch=182
03/19/2022 02:48:42 - INFO - __main__ - Step 740 Global step 740 Train loss 2.181576 on epoch=184
03/19/2022 02:48:48 - INFO - __main__ - Step 750 Global step 750 Train loss 2.091336 on epoch=187
03/19/2022 02:48:48 - INFO - __main__ - Global step 750 Train loss 2.156513 Classification-F1 0.2473389355742297 on epoch=187
03/19/2022 02:48:53 - INFO - __main__ - Step 760 Global step 760 Train loss 2.319889 on epoch=189
03/19/2022 02:48:58 - INFO - __main__ - Step 770 Global step 770 Train loss 1.925941 on epoch=192
03/19/2022 02:49:03 - INFO - __main__ - Step 780 Global step 780 Train loss 1.986803 on epoch=194
03/19/2022 02:49:08 - INFO - __main__ - Step 790 Global step 790 Train loss 2.145807 on epoch=197
03/19/2022 02:49:13 - INFO - __main__ - Step 800 Global step 800 Train loss 2.271679 on epoch=199
03/19/2022 02:49:14 - INFO - __main__ - Global step 800 Train loss 2.130023 Classification-F1 0.49910169970527674 on epoch=199
03/19/2022 02:49:20 - INFO - __main__ - Step 810 Global step 810 Train loss 1.791746 on epoch=202
03/19/2022 02:49:25 - INFO - __main__ - Step 820 Global step 820 Train loss 1.984223 on epoch=204
03/19/2022 02:49:30 - INFO - __main__ - Step 830 Global step 830 Train loss 2.027940 on epoch=207
03/19/2022 02:49:35 - INFO - __main__ - Step 840 Global step 840 Train loss 2.065705 on epoch=209
03/19/2022 02:49:40 - INFO - __main__ - Step 850 Global step 850 Train loss 1.803446 on epoch=212
03/19/2022 02:49:41 - INFO - __main__ - Global step 850 Train loss 1.934612 Classification-F1 0.41353046594982074 on epoch=212
03/19/2022 02:49:46 - INFO - __main__ - Step 860 Global step 860 Train loss 1.779927 on epoch=214
03/19/2022 02:49:51 - INFO - __main__ - Step 870 Global step 870 Train loss 1.462899 on epoch=217
03/19/2022 02:49:56 - INFO - __main__ - Step 880 Global step 880 Train loss 1.796136 on epoch=219
03/19/2022 02:50:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.570094 on epoch=222
03/19/2022 02:50:06 - INFO - __main__ - Step 900 Global step 900 Train loss 1.395219 on epoch=224
03/19/2022 02:50:07 - INFO - __main__ - Global step 900 Train loss 1.600855 Classification-F1 0.526100812375767 on epoch=224
03/19/2022 02:50:13 - INFO - __main__ - Step 910 Global step 910 Train loss 1.419919 on epoch=227
03/19/2022 02:50:18 - INFO - __main__ - Step 920 Global step 920 Train loss 1.777988 on epoch=229
03/19/2022 02:50:23 - INFO - __main__ - Step 930 Global step 930 Train loss 1.221790 on epoch=232
03/19/2022 02:50:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.909202 on epoch=234
03/19/2022 02:50:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.977573 on epoch=237
03/19/2022 02:50:34 - INFO - __main__ - Global step 950 Train loss 1.261294 Classification-F1 0.619001519102939 on epoch=237
03/19/2022 02:50:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.472532 on epoch=239
03/19/2022 02:50:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.380867 on epoch=242
03/19/2022 02:50:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.360995 on epoch=244
03/19/2022 02:50:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.268004 on epoch=247
03/19/2022 02:51:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.238021 on epoch=249
03/19/2022 02:51:00 - INFO - __main__ - Global step 1000 Train loss 0.344084 Classification-F1 0.6415825416839616 on epoch=249
03/19/2022 02:51:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:51:01 - INFO - __main__ - Printing 3 examples
03/19/2022 02:51:01 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 02:51:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:51:01 - INFO - __main__ - save last model!
03/19/2022 02:51:01 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:51:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:51:01 - INFO - __main__ - Printing 3 examples
03/19/2022 02:51:01 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 02:51:01 - INFO - __main__ - ['others']
03/19/2022 02:51:01 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:51:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:51:01 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:51:09 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 02:51:09 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 02:51:09 - INFO - __main__ - Printing 3 examples
03/19/2022 02:51:09 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 02:51:09 - INFO - __main__ - ['others']
03/19/2022 02:51:09 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 02:51:09 - INFO - __main__ - ['others']
03/19/2022 02:51:09 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 02:51:09 - INFO - __main__ - ['others']
03/19/2022 02:51:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:51:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:51:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:51:14 - INFO - __main__ - Starting training!
03/19/2022 02:51:17 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 02:52:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_42_0.0001_8_predictions.txt
03/19/2022 02:52:00 - INFO - __main__ - Classification-F1 on test data: 0.3943
03/19/2022 02:52:00 - INFO - __main__ - prefix=emo_16_42, lr=0.0001, bsz=8, dev_performance=0.6415825416839616, test_performance=0.3943103324543851
03/19/2022 02:52:00 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0005, bsz=8 ...
03/19/2022 02:52:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:52:01 - INFO - __main__ - Printing 3 examples
03/19/2022 02:52:01 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 02:52:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:52:01 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 02:52:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 02:52:01 - INFO - __main__ - Printing 3 examples
03/19/2022 02:52:01 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 02:52:01 - INFO - __main__ - ['others']
03/19/2022 02:52:01 - INFO - __main__ - Tokenizing Input ...
03/19/2022 02:52:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 02:52:01 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 02:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 02:52:14 - INFO - __main__ - Starting training!
03/19/2022 02:52:19 - INFO - __main__ - Step 10 Global step 10 Train loss 23.753534 on epoch=2
03/19/2022 02:52:24 - INFO - __main__ - Step 20 Global step 20 Train loss 19.346266 on epoch=4
03/19/2022 02:52:29 - INFO - __main__ - Step 30 Global step 30 Train loss 17.929567 on epoch=7
03/19/2022 02:52:34 - INFO - __main__ - Step 40 Global step 40 Train loss 16.527523 on epoch=9
03/19/2022 02:52:39 - INFO - __main__ - Step 50 Global step 50 Train loss 14.279844 on epoch=12
03/19/2022 02:52:40 - INFO - __main__ - Global step 50 Train loss 18.367346 Classification-F1 0.0 on epoch=12
03/19/2022 02:52:45 - INFO - __main__ - Step 60 Global step 60 Train loss 12.965352 on epoch=14
03/19/2022 02:52:50 - INFO - __main__ - Step 70 Global step 70 Train loss 11.142451 on epoch=17
03/19/2022 02:52:55 - INFO - __main__ - Step 80 Global step 80 Train loss 8.722147 on epoch=19
03/19/2022 02:53:00 - INFO - __main__ - Step 90 Global step 90 Train loss 6.292352 on epoch=22
03/19/2022 02:53:05 - INFO - __main__ - Step 100 Global step 100 Train loss 4.340140 on epoch=24
03/19/2022 02:53:06 - INFO - __main__ - Global step 100 Train loss 8.692489 Classification-F1 0.1 on epoch=24
03/19/2022 02:53:12 - INFO - __main__ - Step 110 Global step 110 Train loss 3.466342 on epoch=27
03/19/2022 02:53:17 - INFO - __main__ - Step 120 Global step 120 Train loss 3.293484 on epoch=29
03/19/2022 02:53:22 - INFO - __main__ - Step 130 Global step 130 Train loss 2.369266 on epoch=32
03/19/2022 02:53:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.574069 on epoch=34
03/19/2022 02:53:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.964467 on epoch=37
03/19/2022 02:53:33 - INFO - __main__ - Global step 150 Train loss 2.933525 Classification-F1 0.1 on epoch=37
03/19/2022 02:53:38 - INFO - __main__ - Step 160 Global step 160 Train loss 2.070506 on epoch=39
03/19/2022 02:53:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.890353 on epoch=42
03/19/2022 02:53:48 - INFO - __main__ - Step 180 Global step 180 Train loss 1.621951 on epoch=44
03/19/2022 02:53:54 - INFO - __main__ - Step 190 Global step 190 Train loss 1.420997 on epoch=47
03/19/2022 02:53:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.374581 on epoch=49
03/19/2022 02:53:59 - INFO - __main__ - Global step 200 Train loss 1.675678 Classification-F1 0.17142857142857143 on epoch=49
03/19/2022 02:54:05 - INFO - __main__ - Step 210 Global step 210 Train loss 1.135402 on epoch=52
03/19/2022 02:54:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.120131 on epoch=54
03/19/2022 02:54:15 - INFO - __main__ - Step 230 Global step 230 Train loss 2.965317 on epoch=57
03/19/2022 02:54:20 - INFO - __main__ - Step 240 Global step 240 Train loss 4.313698 on epoch=59
03/19/2022 02:54:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.987654 on epoch=62
03/19/2022 02:54:26 - INFO - __main__ - Global step 250 Train loss 2.104440 Classification-F1 0.16464237516869096 on epoch=62
03/19/2022 02:54:31 - INFO - __main__ - Step 260 Global step 260 Train loss 2.075303 on epoch=64
03/19/2022 02:54:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.771649 on epoch=67
03/19/2022 02:54:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.017241 on epoch=69
03/19/2022 02:54:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.977815 on epoch=72
03/19/2022 02:54:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.977132 on epoch=74
03/19/2022 02:54:52 - INFO - __main__ - Global step 300 Train loss 1.163828 Classification-F1 0.1 on epoch=74
03/19/2022 02:54:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.850678 on epoch=77
03/19/2022 02:55:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.909027 on epoch=79
03/19/2022 02:55:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.962602 on epoch=82
03/19/2022 02:55:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.894896 on epoch=84
03/19/2022 02:55:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.968698 on epoch=87
03/19/2022 02:55:17 - INFO - __main__ - Global step 350 Train loss 0.917180 Classification-F1 0.17809523809523808 on epoch=87
03/19/2022 02:55:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.890509 on epoch=89
03/19/2022 02:55:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.890846 on epoch=92
03/19/2022 02:55:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.771326 on epoch=94
03/19/2022 02:55:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.882566 on epoch=97
03/19/2022 02:55:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.848115 on epoch=99
03/19/2022 02:55:45 - INFO - __main__ - Global step 400 Train loss 0.856673 Classification-F1 0.1 on epoch=99
03/19/2022 02:55:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.940918 on epoch=102
03/19/2022 02:55:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.828838 on epoch=104
03/19/2022 02:56:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.797890 on epoch=107
03/19/2022 02:56:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.821407 on epoch=109
03/19/2022 02:56:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.895093 on epoch=112
03/19/2022 02:56:11 - INFO - __main__ - Global step 450 Train loss 0.856829 Classification-F1 0.1 on epoch=112
03/19/2022 02:56:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.855283 on epoch=114
03/19/2022 02:56:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.880697 on epoch=117
03/19/2022 02:56:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.808865 on epoch=119
03/19/2022 02:56:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.748624 on epoch=122
03/19/2022 02:56:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.784042 on epoch=124
03/19/2022 02:56:36 - INFO - __main__ - Global step 500 Train loss 0.815502 Classification-F1 0.1 on epoch=124
03/19/2022 02:56:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.801336 on epoch=127
03/19/2022 02:56:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.857787 on epoch=129
03/19/2022 02:56:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.784864 on epoch=132
03/19/2022 02:56:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.853652 on epoch=134
03/19/2022 02:57:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.825339 on epoch=137
03/19/2022 02:57:02 - INFO - __main__ - Global step 550 Train loss 0.824596 Classification-F1 0.13067758749069247 on epoch=137
03/19/2022 02:57:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.833404 on epoch=139
03/19/2022 02:57:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.781684 on epoch=142
03/19/2022 02:57:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.838180 on epoch=144
03/19/2022 02:57:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.806798 on epoch=147
03/19/2022 02:57:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.848124 on epoch=149
03/19/2022 02:57:27 - INFO - __main__ - Global step 600 Train loss 0.821638 Classification-F1 0.38345110771581364 on epoch=149
03/19/2022 02:57:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.721526 on epoch=152
03/19/2022 02:57:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.891545 on epoch=154
03/19/2022 02:57:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.788630 on epoch=157
03/19/2022 02:57:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.819200 on epoch=159
03/19/2022 02:57:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.775120 on epoch=162
03/19/2022 02:57:54 - INFO - __main__ - Global step 650 Train loss 0.799204 Classification-F1 0.13067758749069247 on epoch=162
03/19/2022 02:57:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.823111 on epoch=164
03/19/2022 02:58:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.727197 on epoch=167
03/19/2022 02:58:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.806256 on epoch=169
03/19/2022 02:58:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.732593 on epoch=172
03/19/2022 02:58:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.725173 on epoch=174
03/19/2022 02:58:20 - INFO - __main__ - Global step 700 Train loss 0.762866 Classification-F1 0.44606124357176247 on epoch=174
03/19/2022 02:58:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.716080 on epoch=177
03/19/2022 02:58:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.710449 on epoch=179
03/19/2022 02:58:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.628679 on epoch=182
03/19/2022 02:58:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.604709 on epoch=184
03/19/2022 02:58:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.600062 on epoch=187
03/19/2022 02:58:45 - INFO - __main__ - Global step 750 Train loss 0.651996 Classification-F1 0.4817460317460318 on epoch=187
03/19/2022 02:58:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.617208 on epoch=189
03/19/2022 02:58:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.714553 on epoch=192
03/19/2022 02:59:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.683788 on epoch=194
03/19/2022 02:59:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.602505 on epoch=197
03/19/2022 02:59:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.575494 on epoch=199
03/19/2022 02:59:11 - INFO - __main__ - Global step 800 Train loss 0.638709 Classification-F1 0.585575920358529 on epoch=199
03/19/2022 02:59:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.501494 on epoch=202
03/19/2022 02:59:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.521737 on epoch=204
03/19/2022 02:59:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.393654 on epoch=207
03/19/2022 02:59:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.381671 on epoch=209
03/19/2022 02:59:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.366621 on epoch=212
03/19/2022 02:59:38 - INFO - __main__ - Global step 850 Train loss 0.433035 Classification-F1 0.5346369825839768 on epoch=212
03/19/2022 02:59:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.314622 on epoch=214
03/19/2022 02:59:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.167454 on epoch=217
03/19/2022 02:59:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.217291 on epoch=219
03/19/2022 02:59:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.125186 on epoch=222
03/19/2022 03:00:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.119193 on epoch=224
03/19/2022 03:00:04 - INFO - __main__ - Global step 900 Train loss 0.188749 Classification-F1 0.6188574938574938 on epoch=224
03/19/2022 03:00:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.135289 on epoch=227
03/19/2022 03:00:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.050276 on epoch=229
03/19/2022 03:00:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.112943 on epoch=232
03/19/2022 03:00:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.010606 on epoch=234
03/19/2022 03:00:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.040366 on epoch=237
03/19/2022 03:00:30 - INFO - __main__ - Global step 950 Train loss 0.069896 Classification-F1 0.5837374376273806 on epoch=237
03/19/2022 03:00:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.028082 on epoch=239
03/19/2022 03:00:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.028628 on epoch=242
03/19/2022 03:00:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.023139 on epoch=244
03/19/2022 03:00:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.017136 on epoch=247
03/19/2022 03:00:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.034469 on epoch=249
03/19/2022 03:00:56 - INFO - __main__ - Global step 1000 Train loss 0.026291 Classification-F1 0.5760876550350235 on epoch=249
03/19/2022 03:00:56 - INFO - __main__ - save last model!
03/19/2022 03:00:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:00:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:00:56 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:00:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:00:56 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:00:56 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:00:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:00:56 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:00:56 - INFO - __main__ - ['others']
03/19/2022 03:00:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:00:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:00:56 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:01:02 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:01:03 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 03:01:03 - INFO - __main__ - Printing 3 examples
03/19/2022 03:01:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 03:01:03 - INFO - __main__ - ['others']
03/19/2022 03:01:03 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 03:01:03 - INFO - __main__ - ['others']
03/19/2022 03:01:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 03:01:03 - INFO - __main__ - ['others']
03/19/2022 03:01:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:01:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:01:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:01:07 - INFO - __main__ - Starting training!
03/19/2022 03:01:10 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 03:01:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_87_0.0005_8_predictions.txt
03/19/2022 03:01:54 - INFO - __main__ - Classification-F1 on test data: 0.3741
03/19/2022 03:01:54 - INFO - __main__ - prefix=emo_16_87, lr=0.0005, bsz=8, dev_performance=0.6188574938574938, test_performance=0.3740717780410034
03/19/2022 03:01:54 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0003, bsz=8 ...
03/19/2022 03:01:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:01:55 - INFO - __main__ - Printing 3 examples
03/19/2022 03:01:55 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:01:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:01:55 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:01:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:01:55 - INFO - __main__ - Printing 3 examples
03/19/2022 03:01:55 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:01:55 - INFO - __main__ - ['others']
03/19/2022 03:01:55 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:01:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:01:55 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:02:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:02:08 - INFO - __main__ - Starting training!
03/19/2022 03:02:12 - INFO - __main__ - Step 10 Global step 10 Train loss 23.923161 on epoch=2
03/19/2022 03:02:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.020388 on epoch=4
03/19/2022 03:02:22 - INFO - __main__ - Step 30 Global step 30 Train loss 17.723930 on epoch=7
03/19/2022 03:02:27 - INFO - __main__ - Step 40 Global step 40 Train loss 16.018490 on epoch=9
03/19/2022 03:02:32 - INFO - __main__ - Step 50 Global step 50 Train loss 14.903528 on epoch=12
03/19/2022 03:02:36 - INFO - __main__ - Global step 50 Train loss 18.117901 Classification-F1 0.0 on epoch=12
03/19/2022 03:02:41 - INFO - __main__ - Step 60 Global step 60 Train loss 14.207335 on epoch=14
03/19/2022 03:02:46 - INFO - __main__ - Step 70 Global step 70 Train loss 13.533381 on epoch=17
03/19/2022 03:02:51 - INFO - __main__ - Step 80 Global step 80 Train loss 13.657254 on epoch=19
03/19/2022 03:02:56 - INFO - __main__ - Step 90 Global step 90 Train loss 12.043974 on epoch=22
03/19/2022 03:03:01 - INFO - __main__ - Step 100 Global step 100 Train loss 10.652925 on epoch=24
03/19/2022 03:03:02 - INFO - __main__ - Global step 100 Train loss 12.818974 Classification-F1 0.0 on epoch=24
03/19/2022 03:03:07 - INFO - __main__ - Step 110 Global step 110 Train loss 9.368122 on epoch=27
03/19/2022 03:03:12 - INFO - __main__ - Step 120 Global step 120 Train loss 4.588294 on epoch=29
03/19/2022 03:03:17 - INFO - __main__ - Step 130 Global step 130 Train loss 1.723985 on epoch=32
03/19/2022 03:03:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.836634 on epoch=34
03/19/2022 03:03:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.684521 on epoch=37
03/19/2022 03:03:27 - INFO - __main__ - Global step 150 Train loss 3.440311 Classification-F1 0.56710615280595 on epoch=37
03/19/2022 03:03:33 - INFO - __main__ - Step 160 Global step 160 Train loss 1.576091 on epoch=39
03/19/2022 03:03:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.611379 on epoch=42
03/19/2022 03:03:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.469112 on epoch=44
03/19/2022 03:03:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.493866 on epoch=47
03/19/2022 03:03:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.788743 on epoch=49
03/19/2022 03:03:53 - INFO - __main__ - Global step 200 Train loss 0.787838 Classification-F1 0.6145932163187856 on epoch=49
03/19/2022 03:03:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.185328 on epoch=52
03/19/2022 03:04:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.400981 on epoch=54
03/19/2022 03:04:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.540036 on epoch=57
03/19/2022 03:04:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.702940 on epoch=59
03/19/2022 03:04:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.538850 on epoch=62
03/19/2022 03:04:19 - INFO - __main__ - Global step 250 Train loss 0.473627 Classification-F1 0.5527677957030355 on epoch=62
03/19/2022 03:04:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.713135 on epoch=64
03/19/2022 03:04:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.638166 on epoch=67
03/19/2022 03:04:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.609872 on epoch=69
03/19/2022 03:04:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.255665 on epoch=72
03/19/2022 03:04:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.314535 on epoch=74
03/19/2022 03:04:44 - INFO - __main__ - Global step 300 Train loss 0.506275 Classification-F1 0.4805711036019029 on epoch=74
03/19/2022 03:04:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381922 on epoch=77
03/19/2022 03:04:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.394667 on epoch=79
03/19/2022 03:04:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.458785 on epoch=82
03/19/2022 03:05:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.436938 on epoch=84
03/19/2022 03:05:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.491105 on epoch=87
03/19/2022 03:05:10 - INFO - __main__ - Global step 350 Train loss 0.432683 Classification-F1 0.6160387231815803 on epoch=87
03/19/2022 03:05:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.385759 on epoch=89
03/19/2022 03:05:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.341321 on epoch=92
03/19/2022 03:05:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.377331 on epoch=94
03/19/2022 03:05:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.381757 on epoch=97
03/19/2022 03:05:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.458708 on epoch=99
03/19/2022 03:05:36 - INFO - __main__ - Global step 400 Train loss 0.388975 Classification-F1 0.6511949485431784 on epoch=99
03/19/2022 03:05:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.375285 on epoch=102
03/19/2022 03:05:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.514334 on epoch=104
03/19/2022 03:05:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.375108 on epoch=107
03/19/2022 03:05:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.330192 on epoch=109
03/19/2022 03:06:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.287213 on epoch=112
03/19/2022 03:06:01 - INFO - __main__ - Global step 450 Train loss 0.376426 Classification-F1 0.6539981157628216 on epoch=112
03/19/2022 03:06:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.401929 on epoch=114
03/19/2022 03:06:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.215539 on epoch=117
03/19/2022 03:06:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.327107 on epoch=119
03/19/2022 03:06:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.256014 on epoch=122
03/19/2022 03:06:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.228609 on epoch=124
03/19/2022 03:06:27 - INFO - __main__ - Global step 500 Train loss 0.285840 Classification-F1 0.6681235431235432 on epoch=124
03/19/2022 03:06:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.133168 on epoch=127
03/19/2022 03:06:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.173189 on epoch=129
03/19/2022 03:06:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.163859 on epoch=132
03/19/2022 03:06:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.165150 on epoch=134
03/19/2022 03:06:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.162051 on epoch=137
03/19/2022 03:06:54 - INFO - __main__ - Global step 550 Train loss 0.159484 Classification-F1 0.6337877572841486 on epoch=137
03/19/2022 03:06:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.256602 on epoch=139
03/19/2022 03:07:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.255183 on epoch=142
03/19/2022 03:07:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.235212 on epoch=144
03/19/2022 03:07:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.204707 on epoch=147
03/19/2022 03:07:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.188175 on epoch=149
03/19/2022 03:07:19 - INFO - __main__ - Global step 600 Train loss 0.227976 Classification-F1 0.5634199134199134 on epoch=149
03/19/2022 03:07:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.237139 on epoch=152
03/19/2022 03:07:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.109609 on epoch=154
03/19/2022 03:07:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.108981 on epoch=157
03/19/2022 03:07:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.101527 on epoch=159
03/19/2022 03:07:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.158096 on epoch=162
03/19/2022 03:07:44 - INFO - __main__ - Global step 650 Train loss 0.143070 Classification-F1 0.6435544635544636 on epoch=162
03/19/2022 03:07:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.044296 on epoch=164
03/19/2022 03:07:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.111568 on epoch=167
03/19/2022 03:07:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.034615 on epoch=169
03/19/2022 03:08:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.153245 on epoch=172
03/19/2022 03:08:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.094605 on epoch=174
03/19/2022 03:08:10 - INFO - __main__ - Global step 700 Train loss 0.087666 Classification-F1 0.5892525431225122 on epoch=174
03/19/2022 03:08:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.048966 on epoch=177
03/19/2022 03:08:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.015329 on epoch=179
03/19/2022 03:08:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.059259 on epoch=182
03/19/2022 03:08:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.029423 on epoch=184
03/19/2022 03:08:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.062307 on epoch=187
03/19/2022 03:08:35 - INFO - __main__ - Global step 750 Train loss 0.043057 Classification-F1 0.6870552656546489 on epoch=187
03/19/2022 03:08:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.026486 on epoch=189
03/19/2022 03:08:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.027138 on epoch=192
03/19/2022 03:08:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.022418 on epoch=194
03/19/2022 03:08:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.063615 on epoch=197
03/19/2022 03:09:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.042927 on epoch=199
03/19/2022 03:09:01 - INFO - __main__ - Global step 800 Train loss 0.036517 Classification-F1 0.5306209526115113 on epoch=199
03/19/2022 03:09:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004362 on epoch=202
03/19/2022 03:09:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.064474 on epoch=204
03/19/2022 03:09:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.007489 on epoch=207
03/19/2022 03:09:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.038776 on epoch=209
03/19/2022 03:09:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.105135 on epoch=212
03/19/2022 03:09:27 - INFO - __main__ - Global step 850 Train loss 0.044047 Classification-F1 0.5462545252018937 on epoch=212
03/19/2022 03:09:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.037528 on epoch=214
03/19/2022 03:09:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.036203 on epoch=217
03/19/2022 03:09:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.015696 on epoch=219
03/19/2022 03:09:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.021371 on epoch=222
03/19/2022 03:09:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.010744 on epoch=224
03/19/2022 03:09:52 - INFO - __main__ - Global step 900 Train loss 0.024308 Classification-F1 0.5702157038363935 on epoch=224
03/19/2022 03:09:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.047538 on epoch=227
03/19/2022 03:10:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.037938 on epoch=229
03/19/2022 03:10:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.017717 on epoch=232
03/19/2022 03:10:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.020840 on epoch=234
03/19/2022 03:10:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.020291 on epoch=237
03/19/2022 03:10:18 - INFO - __main__ - Global step 950 Train loss 0.028865 Classification-F1 0.4762108262108262 on epoch=237
03/19/2022 03:10:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.043340 on epoch=239
03/19/2022 03:10:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.081232 on epoch=242
03/19/2022 03:10:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.047699 on epoch=244
03/19/2022 03:10:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004111 on epoch=247
03/19/2022 03:10:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003771 on epoch=249
03/19/2022 03:10:43 - INFO - __main__ - Global step 1000 Train loss 0.036031 Classification-F1 0.48892670547842965 on epoch=249
03/19/2022 03:10:43 - INFO - __main__ - save last model!
03/19/2022 03:10:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:10:44 - INFO - __main__ - Printing 3 examples
03/19/2022 03:10:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:10:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:10:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:10:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:10:44 - INFO - __main__ - Printing 3 examples
03/19/2022 03:10:44 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:10:44 - INFO - __main__ - ['others']
03/19/2022 03:10:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:10:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:10:44 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:10:50 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:10:51 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 03:10:51 - INFO - __main__ - Printing 3 examples
03/19/2022 03:10:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 03:10:51 - INFO - __main__ - ['others']
03/19/2022 03:10:51 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 03:10:51 - INFO - __main__ - ['others']
03/19/2022 03:10:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 03:10:51 - INFO - __main__ - ['others']
03/19/2022 03:10:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:10:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:10:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:10:57 - INFO - __main__ - Starting training!
03/19/2022 03:10:58 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 03:11:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_87_0.0003_8_predictions.txt
03/19/2022 03:11:41 - INFO - __main__ - Classification-F1 on test data: 0.0606
03/19/2022 03:11:41 - INFO - __main__ - prefix=emo_16_87, lr=0.0003, bsz=8, dev_performance=0.6870552656546489, test_performance=0.06059554245024011
03/19/2022 03:11:41 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0002, bsz=8 ...
03/19/2022 03:11:42 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:11:42 - INFO - __main__ - Printing 3 examples
03/19/2022 03:11:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:11:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:11:42 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:11:42 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:11:42 - INFO - __main__ - Printing 3 examples
03/19/2022 03:11:42 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:11:42 - INFO - __main__ - ['others']
03/19/2022 03:11:42 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:11:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:11:42 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:11:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:11:53 - INFO - __main__ - Starting training!
03/19/2022 03:11:57 - INFO - __main__ - Step 10 Global step 10 Train loss 24.431984 on epoch=2
03/19/2022 03:12:02 - INFO - __main__ - Step 20 Global step 20 Train loss 19.342587 on epoch=4
03/19/2022 03:12:07 - INFO - __main__ - Step 30 Global step 30 Train loss 18.010372 on epoch=7
03/19/2022 03:12:12 - INFO - __main__ - Step 40 Global step 40 Train loss 16.992674 on epoch=9
03/19/2022 03:12:17 - INFO - __main__ - Step 50 Global step 50 Train loss 16.086740 on epoch=12
03/19/2022 03:12:21 - INFO - __main__ - Global step 50 Train loss 18.972872 Classification-F1 0.0 on epoch=12
03/19/2022 03:12:26 - INFO - __main__ - Step 60 Global step 60 Train loss 15.177923 on epoch=14
03/19/2022 03:12:31 - INFO - __main__ - Step 70 Global step 70 Train loss 14.715815 on epoch=17
03/19/2022 03:12:36 - INFO - __main__ - Step 80 Global step 80 Train loss 13.662766 on epoch=19
03/19/2022 03:12:41 - INFO - __main__ - Step 90 Global step 90 Train loss 14.119548 on epoch=22
03/19/2022 03:12:46 - INFO - __main__ - Step 100 Global step 100 Train loss 14.019983 on epoch=24
03/19/2022 03:12:48 - INFO - __main__ - Global step 100 Train loss 14.339206 Classification-F1 0.0 on epoch=24
03/19/2022 03:12:53 - INFO - __main__ - Step 110 Global step 110 Train loss 12.318600 on epoch=27
03/19/2022 03:12:58 - INFO - __main__ - Step 120 Global step 120 Train loss 12.108232 on epoch=29
03/19/2022 03:13:03 - INFO - __main__ - Step 130 Global step 130 Train loss 11.116453 on epoch=32
03/19/2022 03:13:08 - INFO - __main__ - Step 140 Global step 140 Train loss 9.725831 on epoch=34
03/19/2022 03:13:13 - INFO - __main__ - Step 150 Global step 150 Train loss 10.202931 on epoch=37
03/19/2022 03:13:14 - INFO - __main__ - Global step 150 Train loss 11.094410 Classification-F1 0.0 on epoch=37
03/19/2022 03:13:19 - INFO - __main__ - Step 160 Global step 160 Train loss 7.618701 on epoch=39
03/19/2022 03:13:24 - INFO - __main__ - Step 170 Global step 170 Train loss 5.613939 on epoch=42
03/19/2022 03:13:29 - INFO - __main__ - Step 180 Global step 180 Train loss 4.014293 on epoch=44
03/19/2022 03:13:34 - INFO - __main__ - Step 190 Global step 190 Train loss 4.197855 on epoch=47
03/19/2022 03:13:39 - INFO - __main__ - Step 200 Global step 200 Train loss 4.175538 on epoch=49
03/19/2022 03:13:39 - INFO - __main__ - Global step 200 Train loss 5.124065 Classification-F1 0.1 on epoch=49
03/19/2022 03:13:46 - INFO - __main__ - Step 210 Global step 210 Train loss 3.350690 on epoch=52
03/19/2022 03:13:51 - INFO - __main__ - Step 220 Global step 220 Train loss 3.417560 on epoch=54
03/19/2022 03:13:56 - INFO - __main__ - Step 230 Global step 230 Train loss 3.260793 on epoch=57
03/19/2022 03:14:01 - INFO - __main__ - Step 240 Global step 240 Train loss 2.711625 on epoch=59
03/19/2022 03:14:06 - INFO - __main__ - Step 250 Global step 250 Train loss 3.432341 on epoch=62
03/19/2022 03:14:06 - INFO - __main__ - Global step 250 Train loss 3.234602 Classification-F1 0.17121848739495799 on epoch=62
03/19/2022 03:14:12 - INFO - __main__ - Step 260 Global step 260 Train loss 3.321180 on epoch=64
03/19/2022 03:14:17 - INFO - __main__ - Step 270 Global step 270 Train loss 2.367854 on epoch=67
03/19/2022 03:14:22 - INFO - __main__ - Step 280 Global step 280 Train loss 2.683561 on epoch=69
03/19/2022 03:14:27 - INFO - __main__ - Step 290 Global step 290 Train loss 2.287883 on epoch=72
03/19/2022 03:14:32 - INFO - __main__ - Step 300 Global step 300 Train loss 2.372099 on epoch=74
03/19/2022 03:14:42 - INFO - __main__ - Global step 300 Train loss 2.606515 Classification-F1 0.3099851558634339 on epoch=74
03/19/2022 03:14:48 - INFO - __main__ - Step 310 Global step 310 Train loss 1.008361 on epoch=77
03/19/2022 03:14:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.505682 on epoch=79
03/19/2022 03:14:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.386119 on epoch=82
03/19/2022 03:15:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.359060 on epoch=84
03/19/2022 03:15:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.184889 on epoch=87
03/19/2022 03:15:08 - INFO - __main__ - Global step 350 Train loss 0.488822 Classification-F1 0.7797181372549019 on epoch=87
03/19/2022 03:15:14 - INFO - __main__ - Step 360 Global step 360 Train loss 1.036299 on epoch=89
03/19/2022 03:15:19 - INFO - __main__ - Step 370 Global step 370 Train loss 1.196965 on epoch=92
03/19/2022 03:15:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.456629 on epoch=94
03/19/2022 03:15:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.207170 on epoch=97
03/19/2022 03:15:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.306813 on epoch=99
03/19/2022 03:15:34 - INFO - __main__ - Global step 400 Train loss 0.640775 Classification-F1 0.6752809274548405 on epoch=99
03/19/2022 03:15:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.300213 on epoch=102
03/19/2022 03:15:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.171471 on epoch=104
03/19/2022 03:15:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.140024 on epoch=107
03/19/2022 03:15:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.125543 on epoch=109
03/19/2022 03:15:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.087711 on epoch=112
03/19/2022 03:16:00 - INFO - __main__ - Global step 450 Train loss 0.164993 Classification-F1 0.7696167631651503 on epoch=112
03/19/2022 03:16:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.130952 on epoch=114
03/19/2022 03:16:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.131991 on epoch=117
03/19/2022 03:16:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.063320 on epoch=119
03/19/2022 03:16:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.042528 on epoch=122
03/19/2022 03:16:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.104565 on epoch=124
03/19/2022 03:16:25 - INFO - __main__ - Global step 500 Train loss 0.094671 Classification-F1 0.7579365079365079 on epoch=124
03/19/2022 03:16:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.098234 on epoch=127
03/19/2022 03:16:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.179721 on epoch=129
03/19/2022 03:16:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.153446 on epoch=132
03/19/2022 03:16:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.169038 on epoch=134
03/19/2022 03:16:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.163839 on epoch=137
03/19/2022 03:16:51 - INFO - __main__ - Global step 550 Train loss 0.152856 Classification-F1 0.7596198156682028 on epoch=137
03/19/2022 03:16:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.088202 on epoch=139
03/19/2022 03:17:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.035516 on epoch=142
03/19/2022 03:17:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.066592 on epoch=144
03/19/2022 03:17:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.038504 on epoch=147
03/19/2022 03:17:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.022588 on epoch=149
03/19/2022 03:17:16 - INFO - __main__ - Global step 600 Train loss 0.050280 Classification-F1 0.7584430231120999 on epoch=149
03/19/2022 03:17:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.015552 on epoch=152
03/19/2022 03:17:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.034832 on epoch=154
03/19/2022 03:17:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.020043 on epoch=157
03/19/2022 03:17:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.020870 on epoch=159
03/19/2022 03:17:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012408 on epoch=162
03/19/2022 03:17:42 - INFO - __main__ - Global step 650 Train loss 0.020741 Classification-F1 0.7502150537634409 on epoch=162
03/19/2022 03:17:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.071321 on epoch=164
03/19/2022 03:17:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.054720 on epoch=167
03/19/2022 03:17:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.012022 on epoch=169
03/19/2022 03:18:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002893 on epoch=172
03/19/2022 03:18:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.010719 on epoch=174
03/19/2022 03:18:07 - INFO - __main__ - Global step 700 Train loss 0.030335 Classification-F1 0.7763888888888888 on epoch=174
03/19/2022 03:18:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.058177 on epoch=177
03/19/2022 03:18:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.048803 on epoch=179
03/19/2022 03:18:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.008548 on epoch=182
03/19/2022 03:18:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.031358 on epoch=184
03/19/2022 03:18:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.080868 on epoch=187
03/19/2022 03:18:33 - INFO - __main__ - Global step 750 Train loss 0.045551 Classification-F1 0.7471611073390829 on epoch=187
03/19/2022 03:18:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.029443 on epoch=189
03/19/2022 03:18:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.005210 on epoch=192
03/19/2022 03:18:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.016946 on epoch=194
03/19/2022 03:18:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008728 on epoch=197
03/19/2022 03:18:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000585 on epoch=199
03/19/2022 03:18:58 - INFO - __main__ - Global step 800 Train loss 0.012182 Classification-F1 0.7773617511520736 on epoch=199
03/19/2022 03:19:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001396 on epoch=202
03/19/2022 03:19:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.005042 on epoch=204
03/19/2022 03:19:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.031679 on epoch=207
03/19/2022 03:19:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.005037 on epoch=209
03/19/2022 03:19:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001468 on epoch=212
03/19/2022 03:19:24 - INFO - __main__ - Global step 850 Train loss 0.008924 Classification-F1 0.7592128178335075 on epoch=212
03/19/2022 03:19:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.007249 on epoch=214
03/19/2022 03:19:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001420 on epoch=217
03/19/2022 03:19:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.004895 on epoch=219
03/19/2022 03:19:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.004336 on epoch=222
03/19/2022 03:19:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.004940 on epoch=224
03/19/2022 03:19:50 - INFO - __main__ - Global step 900 Train loss 0.004568 Classification-F1 0.7741129785247431 on epoch=224
03/19/2022 03:19:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000473 on epoch=227
03/19/2022 03:20:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000948 on epoch=229
03/19/2022 03:20:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000862 on epoch=232
03/19/2022 03:20:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007104 on epoch=234
03/19/2022 03:20:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001893 on epoch=237
03/19/2022 03:20:15 - INFO - __main__ - Global step 950 Train loss 0.002256 Classification-F1 0.7603896103896104 on epoch=237
03/19/2022 03:20:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.005994 on epoch=239
03/19/2022 03:20:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001354 on epoch=242
03/19/2022 03:20:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000308 on epoch=244
03/19/2022 03:20:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000331 on epoch=247
03/19/2022 03:20:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006740 on epoch=249
03/19/2022 03:20:41 - INFO - __main__ - Global step 1000 Train loss 0.002945 Classification-F1 0.7763888888888888 on epoch=249
03/19/2022 03:20:41 - INFO - __main__ - save last model!
03/19/2022 03:20:41 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:20:41 - INFO - __main__ - Printing 3 examples
03/19/2022 03:20:41 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:20:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:20:41 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:20:41 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:20:41 - INFO - __main__ - Printing 3 examples
03/19/2022 03:20:41 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:20:41 - INFO - __main__ - ['others']
03/19/2022 03:20:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:20:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:20:41 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:20:47 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:20:48 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 03:20:48 - INFO - __main__ - Printing 3 examples
03/19/2022 03:20:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 03:20:48 - INFO - __main__ - ['others']
03/19/2022 03:20:48 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 03:20:48 - INFO - __main__ - ['others']
03/19/2022 03:20:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 03:20:48 - INFO - __main__ - ['others']
03/19/2022 03:20:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:20:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:20:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:20:54 - INFO - __main__ - Starting training!
03/19/2022 03:20:55 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 03:21:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_87_0.0002_8_predictions.txt
03/19/2022 03:21:37 - INFO - __main__ - Classification-F1 on test data: 0.4801
03/19/2022 03:21:38 - INFO - __main__ - prefix=emo_16_87, lr=0.0002, bsz=8, dev_performance=0.7797181372549019, test_performance=0.48005355429212426
03/19/2022 03:21:38 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0001, bsz=8 ...
03/19/2022 03:21:39 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:21:39 - INFO - __main__ - Printing 3 examples
03/19/2022 03:21:39 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:21:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:21:39 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/19/2022 03:21:39 - INFO - __main__ - Start tokenizing ... 64 instances
03/19/2022 03:21:39 - INFO - __main__ - Printing 3 examples
03/19/2022 03:21:39 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/19/2022 03:21:39 - INFO - __main__ - ['others']
03/19/2022 03:21:39 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:21:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:21:39 - INFO - __main__ - Loaded 64 examples from dev data
03/19/2022 03:21:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:21:52 - INFO - __main__ - Starting training!
03/19/2022 03:21:56 - INFO - __main__ - Step 10 Global step 10 Train loss 25.323559 on epoch=2
03/19/2022 03:22:00 - INFO - __main__ - Step 20 Global step 20 Train loss 22.842718 on epoch=4
03/19/2022 03:22:05 - INFO - __main__ - Step 30 Global step 30 Train loss 19.669788 on epoch=7
03/19/2022 03:22:10 - INFO - __main__ - Step 40 Global step 40 Train loss 17.543211 on epoch=9
03/19/2022 03:22:15 - INFO - __main__ - Step 50 Global step 50 Train loss 18.233337 on epoch=12
03/19/2022 03:22:34 - INFO - __main__ - Global step 50 Train loss 20.722523 Classification-F1 0.0 on epoch=12
03/19/2022 03:22:40 - INFO - __main__ - Step 60 Global step 60 Train loss 17.626507 on epoch=14
03/19/2022 03:22:45 - INFO - __main__ - Step 70 Global step 70 Train loss 16.777578 on epoch=17
03/19/2022 03:22:50 - INFO - __main__ - Step 80 Global step 80 Train loss 16.701214 on epoch=19
03/19/2022 03:22:55 - INFO - __main__ - Step 90 Global step 90 Train loss 15.684796 on epoch=22
03/19/2022 03:23:00 - INFO - __main__ - Step 100 Global step 100 Train loss 15.748660 on epoch=24
03/19/2022 03:23:16 - INFO - __main__ - Global step 100 Train loss 16.507750 Classification-F1 0.0 on epoch=24
03/19/2022 03:23:21 - INFO - __main__ - Step 110 Global step 110 Train loss 15.835713 on epoch=27
03/19/2022 03:23:26 - INFO - __main__ - Step 120 Global step 120 Train loss 15.115257 on epoch=29
03/19/2022 03:23:31 - INFO - __main__ - Step 130 Global step 130 Train loss 15.205213 on epoch=32
03/19/2022 03:23:36 - INFO - __main__ - Step 140 Global step 140 Train loss 14.798630 on epoch=34
03/19/2022 03:23:41 - INFO - __main__ - Step 150 Global step 150 Train loss 13.663816 on epoch=37
03/19/2022 03:23:47 - INFO - __main__ - Global step 150 Train loss 14.923727 Classification-F1 0.0 on epoch=37
03/19/2022 03:23:52 - INFO - __main__ - Step 160 Global step 160 Train loss 13.832250 on epoch=39
03/19/2022 03:23:57 - INFO - __main__ - Step 170 Global step 170 Train loss 13.547648 on epoch=42
03/19/2022 03:24:02 - INFO - __main__ - Step 180 Global step 180 Train loss 12.941656 on epoch=44
03/19/2022 03:24:08 - INFO - __main__ - Step 190 Global step 190 Train loss 12.822092 on epoch=47
03/19/2022 03:24:12 - INFO - __main__ - Step 200 Global step 200 Train loss 12.709829 on epoch=49
03/19/2022 03:24:23 - INFO - __main__ - Global step 200 Train loss 13.170694 Classification-F1 0.0 on epoch=49
03/19/2022 03:24:28 - INFO - __main__ - Step 210 Global step 210 Train loss 12.366327 on epoch=52
03/19/2022 03:24:33 - INFO - __main__ - Step 220 Global step 220 Train loss 11.890821 on epoch=54
03/19/2022 03:24:38 - INFO - __main__ - Step 230 Global step 230 Train loss 12.003366 on epoch=57
03/19/2022 03:24:43 - INFO - __main__ - Step 240 Global step 240 Train loss 10.727298 on epoch=59
03/19/2022 03:24:48 - INFO - __main__ - Step 250 Global step 250 Train loss 10.970749 on epoch=62
03/19/2022 03:24:55 - INFO - __main__ - Global step 250 Train loss 11.591711 Classification-F1 0.0 on epoch=62
03/19/2022 03:25:00 - INFO - __main__ - Step 260 Global step 260 Train loss 10.424098 on epoch=64
03/19/2022 03:25:05 - INFO - __main__ - Step 270 Global step 270 Train loss 10.154593 on epoch=67
03/19/2022 03:25:10 - INFO - __main__ - Step 280 Global step 280 Train loss 9.149776 on epoch=69
03/19/2022 03:25:15 - INFO - __main__ - Step 290 Global step 290 Train loss 8.202662 on epoch=72
03/19/2022 03:25:20 - INFO - __main__ - Step 300 Global step 300 Train loss 8.157717 on epoch=74
03/19/2022 03:25:27 - INFO - __main__ - Global step 300 Train loss 9.217769 Classification-F1 0.0 on epoch=74
03/19/2022 03:25:32 - INFO - __main__ - Step 310 Global step 310 Train loss 7.233796 on epoch=77
03/19/2022 03:25:37 - INFO - __main__ - Step 320 Global step 320 Train loss 5.075781 on epoch=79
03/19/2022 03:25:42 - INFO - __main__ - Step 330 Global step 330 Train loss 4.653945 on epoch=82
03/19/2022 03:25:47 - INFO - __main__ - Step 340 Global step 340 Train loss 4.499469 on epoch=84
03/19/2022 03:25:52 - INFO - __main__ - Step 350 Global step 350 Train loss 4.705390 on epoch=87
03/19/2022 03:25:52 - INFO - __main__ - Global step 350 Train loss 5.233677 Classification-F1 0.39365721997300945 on epoch=87
03/19/2022 03:25:58 - INFO - __main__ - Step 360 Global step 360 Train loss 4.023269 on epoch=89
03/19/2022 03:26:03 - INFO - __main__ - Step 370 Global step 370 Train loss 3.226544 on epoch=92
03/19/2022 03:26:08 - INFO - __main__ - Step 380 Global step 380 Train loss 2.496551 on epoch=94
03/19/2022 03:26:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.582385 on epoch=97
03/19/2022 03:26:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.130316 on epoch=99
03/19/2022 03:26:19 - INFO - __main__ - Global step 400 Train loss 2.491813 Classification-F1 0.563115372999094 on epoch=99
03/19/2022 03:26:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.970116 on epoch=102
03/19/2022 03:26:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.921686 on epoch=104
03/19/2022 03:26:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.936340 on epoch=107
03/19/2022 03:26:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.785711 on epoch=109
03/19/2022 03:26:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.628637 on epoch=112
03/19/2022 03:26:46 - INFO - __main__ - Global step 450 Train loss 0.848498 Classification-F1 0.6432900432900432 on epoch=112
03/19/2022 03:26:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.715424 on epoch=114
03/19/2022 03:26:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.542296 on epoch=117
03/19/2022 03:27:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.581565 on epoch=119
03/19/2022 03:27:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.497747 on epoch=122
03/19/2022 03:27:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.531813 on epoch=124
03/19/2022 03:27:12 - INFO - __main__ - Global step 500 Train loss 0.573769 Classification-F1 0.6099121466768526 on epoch=124
03/19/2022 03:27:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.416385 on epoch=127
03/19/2022 03:27:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.337290 on epoch=129
03/19/2022 03:27:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.436355 on epoch=132
03/19/2022 03:27:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.318937 on epoch=134
03/19/2022 03:27:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.375919 on epoch=137
03/19/2022 03:27:38 - INFO - __main__ - Global step 550 Train loss 0.376977 Classification-F1 0.7300592128178336 on epoch=137
03/19/2022 03:27:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.285819 on epoch=139
03/19/2022 03:27:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.758895 on epoch=142
03/19/2022 03:27:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.795579 on epoch=144
03/19/2022 03:27:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.638209 on epoch=147
03/19/2022 03:28:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.489971 on epoch=149
03/19/2022 03:28:05 - INFO - __main__ - Global step 600 Train loss 0.593695 Classification-F1 0.6967948717948718 on epoch=149
03/19/2022 03:28:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.492414 on epoch=152
03/19/2022 03:28:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.424482 on epoch=154
03/19/2022 03:28:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.211206 on epoch=157
03/19/2022 03:28:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.485443 on epoch=159
03/19/2022 03:28:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.330039 on epoch=162
03/19/2022 03:28:31 - INFO - __main__ - Global step 650 Train loss 0.388717 Classification-F1 0.7720985691573927 on epoch=162
03/19/2022 03:28:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.448654 on epoch=164
03/19/2022 03:28:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.143504 on epoch=167
03/19/2022 03:28:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.331145 on epoch=169
03/19/2022 03:28:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.124806 on epoch=172
03/19/2022 03:28:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.155261 on epoch=174
03/19/2022 03:28:58 - INFO - __main__ - Global step 700 Train loss 0.240674 Classification-F1 0.6526616154565338 on epoch=174
03/19/2022 03:29:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.442112 on epoch=177
03/19/2022 03:29:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.251124 on epoch=179
03/19/2022 03:29:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.092405 on epoch=182
03/19/2022 03:29:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.321314 on epoch=184
03/19/2022 03:29:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.301540 on epoch=187
03/19/2022 03:29:23 - INFO - __main__ - Global step 750 Train loss 0.281699 Classification-F1 0.7296810779673683 on epoch=187
03/19/2022 03:29:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.427216 on epoch=189
03/19/2022 03:29:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.111224 on epoch=192
03/19/2022 03:29:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.119371 on epoch=194
03/19/2022 03:29:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.196562 on epoch=197
03/19/2022 03:29:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.165591 on epoch=199
03/19/2022 03:29:49 - INFO - __main__ - Global step 800 Train loss 0.203993 Classification-F1 0.7226636227650427 on epoch=199
03/19/2022 03:29:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.225318 on epoch=202
03/19/2022 03:29:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.363194 on epoch=204
03/19/2022 03:30:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.345152 on epoch=207
03/19/2022 03:30:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.287847 on epoch=209
03/19/2022 03:30:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.353195 on epoch=212
03/19/2022 03:30:15 - INFO - __main__ - Global step 850 Train loss 0.314941 Classification-F1 0.7557043650793651 on epoch=212
03/19/2022 03:30:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.269478 on epoch=214
03/19/2022 03:30:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.194811 on epoch=217
03/19/2022 03:30:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.373472 on epoch=219
03/19/2022 03:30:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.170582 on epoch=222
03/19/2022 03:30:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.102578 on epoch=224
03/19/2022 03:30:41 - INFO - __main__ - Global step 900 Train loss 0.222184 Classification-F1 0.7226636227650427 on epoch=224
03/19/2022 03:30:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.066062 on epoch=227
03/19/2022 03:30:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.091419 on epoch=229
03/19/2022 03:30:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.092673 on epoch=232
03/19/2022 03:31:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.067507 on epoch=234
03/19/2022 03:31:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.083285 on epoch=237
03/19/2022 03:31:07 - INFO - __main__ - Global step 950 Train loss 0.080189 Classification-F1 0.7503922784994892 on epoch=237
03/19/2022 03:31:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.256934 on epoch=239
03/19/2022 03:31:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.128838 on epoch=242
03/19/2022 03:31:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.347025 on epoch=244
03/19/2022 03:31:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.076538 on epoch=247
03/19/2022 03:31:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.027325 on epoch=249
03/19/2022 03:31:33 - INFO - __main__ - Global step 1000 Train loss 0.167332 Classification-F1 0.776320084485407 on epoch=249
03/19/2022 03:31:34 - INFO - __main__ - save last model!
03/19/2022 03:31:40 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:31:41 - INFO - __main__ - Start tokenizing ... 5509 instances
03/19/2022 03:31:41 - INFO - __main__ - Printing 3 examples
03/19/2022 03:31:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/19/2022 03:31:41 - INFO - __main__ - ['others']
03/19/2022 03:31:41 - INFO - __main__ -  [emo] what you like very little things ok
03/19/2022 03:31:41 - INFO - __main__ - ['others']
03/19/2022 03:31:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/19/2022 03:31:41 - INFO - __main__ - ['others']
03/19/2022 03:31:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:31:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:31:49 - INFO - __main__ - Loaded 5509 examples from test data
03/19/2022 03:32:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-emo/emo_16_87_0.0001_8_predictions.txt
03/19/2022 03:32:40 - INFO - __main__ - Classification-F1 on test data: 0.0393
03/19/2022 03:32:41 - INFO - __main__ - prefix=emo_16_87, lr=0.0001, bsz=8, dev_performance=0.776320084485407, test_performance=0.03925310072206633
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22199): No such process
Task: yelp_polarity, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 03:32:46 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 03:32:46 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-yelp_polarity
03/19/2022 03:32:46 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 03:32:46 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-yelp_polarity
03/19/2022 03:32:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 03:32:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 03:32:50 - INFO - __main__ - args.device: cuda:0
03/19/2022 03:32:50 - INFO - __main__ - Using 2 gpus
03/19/2022 03:32:50 - INFO - __main__ - args.device: cuda:1
03/19/2022 03:32:50 - INFO - __main__ - Using 2 gpus
03/19/2022 03:32:50 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/19/2022 03:32:50 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/19/2022 03:32:55 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0005, bsz=8 ...
03/19/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:32:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:32:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:32:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:32:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:32:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:32:56 - INFO - __main__ - Printing 3 examples
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:32:56 - INFO - __main__ - ['negative']
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:32:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:32:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:33:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:33:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:33:09 - INFO - __main__ - Starting training!
03/19/2022 03:33:09 - INFO - __main__ - Starting training!
03/19/2022 03:33:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.968962 on epoch=4
03/19/2022 03:33:21 - INFO - __main__ - Step 20 Global step 20 Train loss 16.910526 on epoch=9
03/19/2022 03:33:27 - INFO - __main__ - Step 30 Global step 30 Train loss 14.691599 on epoch=14
03/19/2022 03:33:33 - INFO - __main__ - Step 40 Global step 40 Train loss 12.035636 on epoch=19
03/19/2022 03:33:39 - INFO - __main__ - Step 50 Global step 50 Train loss 10.325590 on epoch=24
03/19/2022 03:33:40 - INFO - __main__ - Global step 50 Train loss 15.586464 Classification-F1 0.0 on epoch=24
03/19/2022 03:33:48 - INFO - __main__ - Step 60 Global step 60 Train loss 4.838543 on epoch=29
03/19/2022 03:33:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.461858 on epoch=34
03/19/2022 03:33:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.146874 on epoch=39
03/19/2022 03:34:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.027347 on epoch=44
03/19/2022 03:34:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.008077 on epoch=49
03/19/2022 03:34:12 - INFO - __main__ - Global step 100 Train loss 1.096540 Classification-F1 0.592741935483871 on epoch=49
03/19/2022 03:34:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.011242 on epoch=54
03/19/2022 03:34:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.005399 on epoch=59
03/19/2022 03:34:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.006691 on epoch=64
03/19/2022 03:34:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.011508 on epoch=69
03/19/2022 03:34:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.004086 on epoch=74
03/19/2022 03:34:45 - INFO - __main__ - Global step 150 Train loss 0.007785 Classification-F1 0.906158357771261 on epoch=74
03/19/2022 03:34:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.006911 on epoch=79
03/19/2022 03:34:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.000334 on epoch=84
03/19/2022 03:35:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.000466 on epoch=89
03/19/2022 03:35:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.000547 on epoch=94
03/19/2022 03:35:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000112 on epoch=99
03/19/2022 03:35:17 - INFO - __main__ - Global step 200 Train loss 0.001674 Classification-F1 0.9375 on epoch=99
03/19/2022 03:35:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000728 on epoch=104
03/19/2022 03:35:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000192 on epoch=109
03/19/2022 03:35:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000353 on epoch=114
03/19/2022 03:35:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000062 on epoch=119
03/19/2022 03:35:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.943909 on epoch=124
03/19/2022 03:35:48 - INFO - __main__ - Global step 250 Train loss 0.189049 Classification-F1 0.5134502923976608 on epoch=124
03/19/2022 03:35:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.603685 on epoch=129
03/19/2022 03:36:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.910416 on epoch=134
03/19/2022 03:36:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.231928 on epoch=139
03/19/2022 03:36:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.107916 on epoch=144
03/19/2022 03:36:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.506300 on epoch=149
03/19/2022 03:36:19 - INFO - __main__ - Global step 300 Train loss 0.672049 Classification-F1 0.8435972629521017 on epoch=149
03/19/2022 03:36:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.191934 on epoch=154
03/19/2022 03:36:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.083739 on epoch=159
03/19/2022 03:36:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.282345 on epoch=164
03/19/2022 03:36:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.170215 on epoch=169
03/19/2022 03:36:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004507 on epoch=174
03/19/2022 03:36:49 - INFO - __main__ - Global step 350 Train loss 0.146548 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 03:36:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006225 on epoch=179
03/19/2022 03:37:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001789 on epoch=184
03/19/2022 03:37:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002369 on epoch=189
03/19/2022 03:37:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.159406 on epoch=194
03/19/2022 03:37:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.059915 on epoch=199
03/19/2022 03:37:21 - INFO - __main__ - Global step 400 Train loss 0.045941 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 03:37:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.098521 on epoch=204
03/19/2022 03:37:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.167214 on epoch=209
03/19/2022 03:37:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.009450 on epoch=214
03/19/2022 03:37:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014378 on epoch=219
03/19/2022 03:37:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.079208 on epoch=224
03/19/2022 03:37:52 - INFO - __main__ - Global step 450 Train loss 0.073754 Classification-F1 0.8125 on epoch=224
03/19/2022 03:37:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.256863 on epoch=229
03/19/2022 03:38:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.182220 on epoch=234
03/19/2022 03:38:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.030807 on epoch=239
03/19/2022 03:38:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.420154 on epoch=244
03/19/2022 03:38:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.342867 on epoch=249
03/19/2022 03:38:24 - INFO - __main__ - Global step 500 Train loss 0.246582 Classification-F1 0.3992490613266583 on epoch=249
03/19/2022 03:38:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.385619 on epoch=254
03/19/2022 03:38:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.338765 on epoch=259
03/19/2022 03:38:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.300514 on epoch=264
03/19/2022 03:38:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.268324 on epoch=269
03/19/2022 03:38:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.246207 on epoch=274
03/19/2022 03:38:55 - INFO - __main__ - Global step 550 Train loss 0.307886 Classification-F1 0.6101882613510521 on epoch=274
03/19/2022 03:39:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.230289 on epoch=279
03/19/2022 03:39:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.189329 on epoch=284
03/19/2022 03:39:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211513 on epoch=289
03/19/2022 03:39:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.099602 on epoch=294
03/19/2022 03:39:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.081705 on epoch=299
03/19/2022 03:39:27 - INFO - __main__ - Global step 600 Train loss 0.162488 Classification-F1 0.6666666666666667 on epoch=299
03/19/2022 03:39:27 - INFO - __main__ - save last model!
03/19/2022 03:39:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:39:27 - INFO - __main__ - Printing 3 examples
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:39:27 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:39:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:39:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:39:27 - INFO - __main__ - Printing 3 examples
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:39:27 - INFO - __main__ - ['negative']
03/19/2022 03:39:27 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:39:27 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:39:27 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:39:33 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:39:34 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 03:39:34 - INFO - __main__ - Printing 3 examples
03/19/2022 03:39:34 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 03:39:34 - INFO - __main__ - ['negative']
03/19/2022 03:39:34 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 03:39:34 - INFO - __main__ - ['negative']
03/19/2022 03:39:34 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 03:39:34 - INFO - __main__ - ['negative']
03/19/2022 03:39:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:39:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:39:38 - INFO - __main__ - Starting training!
03/19/2022 03:39:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:39:48 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 03:42:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_100_0.0005_8_predictions.txt
03/19/2022 03:42:30 - INFO - __main__ - Classification-F1 on test data: 0.9431
03/19/2022 03:42:31 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.943148051948052
03/19/2022 03:42:31 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0003, bsz=8 ...
03/19/2022 03:42:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:42:32 - INFO - __main__ - Printing 3 examples
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:42:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:42:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:42:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:42:32 - INFO - __main__ - Printing 3 examples
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:42:32 - INFO - __main__ - ['negative']
03/19/2022 03:42:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:42:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:42:32 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:42:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:42:42 - INFO - __main__ - Starting training!
03/19/2022 03:42:48 - INFO - __main__ - Step 10 Global step 10 Train loss 24.261126 on epoch=4
03/19/2022 03:42:54 - INFO - __main__ - Step 20 Global step 20 Train loss 17.266132 on epoch=9
03/19/2022 03:43:00 - INFO - __main__ - Step 30 Global step 30 Train loss 15.970775 on epoch=14
03/19/2022 03:43:06 - INFO - __main__ - Step 40 Global step 40 Train loss 14.536761 on epoch=19
03/19/2022 03:43:12 - INFO - __main__ - Step 50 Global step 50 Train loss 13.269529 on epoch=24
03/19/2022 03:43:21 - INFO - __main__ - Global step 50 Train loss 17.060865 Classification-F1 0.0 on epoch=24
03/19/2022 03:43:28 - INFO - __main__ - Step 60 Global step 60 Train loss 11.930053 on epoch=29
03/19/2022 03:43:34 - INFO - __main__ - Step 70 Global step 70 Train loss 11.126322 on epoch=34
03/19/2022 03:43:40 - INFO - __main__ - Step 80 Global step 80 Train loss 8.261042 on epoch=39
03/19/2022 03:43:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.993138 on epoch=44
03/19/2022 03:43:51 - INFO - __main__ - Step 100 Global step 100 Train loss 2.446302 on epoch=49
03/19/2022 03:43:52 - INFO - __main__ - Global step 100 Train loss 8.151370 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 03:43:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.440965 on epoch=54
03/19/2022 03:44:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.223194 on epoch=59
03/19/2022 03:44:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.164206 on epoch=64
03/19/2022 03:44:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.110992 on epoch=69
03/19/2022 03:44:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.076290 on epoch=74
03/19/2022 03:44:24 - INFO - __main__ - Global step 150 Train loss 0.203129 Classification-F1 0.9375 on epoch=74
03/19/2022 03:44:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.026769 on epoch=79
03/19/2022 03:44:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.074274 on epoch=84
03/19/2022 03:44:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.059164 on epoch=89
03/19/2022 03:44:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.060352 on epoch=94
03/19/2022 03:44:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.101774 on epoch=99
03/19/2022 03:44:55 - INFO - __main__ - Global step 200 Train loss 0.064467 Classification-F1 0.9375 on epoch=99
03/19/2022 03:45:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.044061 on epoch=104
03/19/2022 03:45:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.054393 on epoch=109
03/19/2022 03:45:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.094860 on epoch=114
03/19/2022 03:45:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.102085 on epoch=119
03/19/2022 03:45:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.082013 on epoch=124
03/19/2022 03:45:27 - INFO - __main__ - Global step 250 Train loss 0.075482 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 03:45:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.073626 on epoch=129
03/19/2022 03:45:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.041751 on epoch=134
03/19/2022 03:45:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027332 on epoch=139
03/19/2022 03:45:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.083500 on epoch=144
03/19/2022 03:45:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.038344 on epoch=149
03/19/2022 03:45:58 - INFO - __main__ - Global step 300 Train loss 0.052910 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 03:46:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.041564 on epoch=154
03/19/2022 03:46:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.104230 on epoch=159
03/19/2022 03:46:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.122866 on epoch=164
03/19/2022 03:46:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.056257 on epoch=169
03/19/2022 03:46:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.506999 on epoch=174
03/19/2022 03:46:29 - INFO - __main__ - Global step 350 Train loss 0.166383 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 03:46:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.364859 on epoch=179
03/19/2022 03:46:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.106071 on epoch=184
03/19/2022 03:46:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034566 on epoch=189
03/19/2022 03:46:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.023333 on epoch=194
03/19/2022 03:47:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.061855 on epoch=199
03/19/2022 03:47:01 - INFO - __main__ - Global step 400 Train loss 0.118137 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 03:47:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.013749 on epoch=204
03/19/2022 03:47:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.014570 on epoch=209
03/19/2022 03:47:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.014100 on epoch=214
03/19/2022 03:47:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.019803 on epoch=219
03/19/2022 03:47:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.024073 on epoch=224
03/19/2022 03:47:32 - INFO - __main__ - Global step 450 Train loss 0.017259 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 03:47:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.033012 on epoch=229
03/19/2022 03:47:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.021048 on epoch=234
03/19/2022 03:47:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.020466 on epoch=239
03/19/2022 03:47:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.018218 on epoch=244
03/19/2022 03:48:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017488 on epoch=249
03/19/2022 03:48:03 - INFO - __main__ - Global step 500 Train loss 0.022046 Classification-F1 0.9375 on epoch=249
03/19/2022 03:48:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009813 on epoch=254
03/19/2022 03:48:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.009907 on epoch=259
03/19/2022 03:48:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008992 on epoch=264
03/19/2022 03:48:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018393 on epoch=269
03/19/2022 03:48:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022354 on epoch=274
03/19/2022 03:48:34 - INFO - __main__ - Global step 550 Train loss 0.013892 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 03:48:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017785 on epoch=279
03/19/2022 03:48:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011843 on epoch=284
03/19/2022 03:48:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.019444 on epoch=289
03/19/2022 03:48:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.009984 on epoch=294
03/19/2022 03:49:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.025546 on epoch=299
03/19/2022 03:49:05 - INFO - __main__ - Global step 600 Train loss 0.016920 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 03:49:05 - INFO - __main__ - save last model!
03/19/2022 03:49:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:49:06 - INFO - __main__ - Printing 3 examples
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:49:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:49:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:49:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:49:06 - INFO - __main__ - Printing 3 examples
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:49:06 - INFO - __main__ - ['negative']
03/19/2022 03:49:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:49:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:49:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:49:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:49:13 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 03:49:13 - INFO - __main__ - Printing 3 examples
03/19/2022 03:49:13 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 03:49:13 - INFO - __main__ - ['negative']
03/19/2022 03:49:13 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 03:49:13 - INFO - __main__ - ['negative']
03/19/2022 03:49:13 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 03:49:13 - INFO - __main__ - ['negative']
03/19/2022 03:49:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:49:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:49:17 - INFO - __main__ - Starting training!
03/19/2022 03:49:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:49:27 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 03:52:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_100_0.0003_8_predictions.txt
03/19/2022 03:52:13 - INFO - __main__ - Classification-F1 on test data: 0.9404
03/19/2022 03:52:13 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.940364546393717
03/19/2022 03:52:14 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0002, bsz=8 ...
03/19/2022 03:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:52:14 - INFO - __main__ - Printing 3 examples
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:52:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:52:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:52:14 - INFO - __main__ - Printing 3 examples
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:52:14 - INFO - __main__ - ['negative']
03/19/2022 03:52:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:52:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:52:15 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:52:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:52:27 - INFO - __main__ - Starting training!
03/19/2022 03:52:33 - INFO - __main__ - Step 10 Global step 10 Train loss 23.632687 on epoch=4
03/19/2022 03:52:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.946789 on epoch=9
03/19/2022 03:52:45 - INFO - __main__ - Step 30 Global step 30 Train loss 15.606997 on epoch=14
03/19/2022 03:52:51 - INFO - __main__ - Step 40 Global step 40 Train loss 16.291832 on epoch=19
03/19/2022 03:52:57 - INFO - __main__ - Step 50 Global step 50 Train loss 15.110929 on epoch=24
03/19/2022 03:53:08 - INFO - __main__ - Global step 50 Train loss 17.917845 Classification-F1 0.0 on epoch=24
03/19/2022 03:53:15 - INFO - __main__ - Step 60 Global step 60 Train loss 14.055799 on epoch=29
03/19/2022 03:53:21 - INFO - __main__ - Step 70 Global step 70 Train loss 12.795126 on epoch=34
03/19/2022 03:53:27 - INFO - __main__ - Step 80 Global step 80 Train loss 12.584316 on epoch=39
03/19/2022 03:53:33 - INFO - __main__ - Step 90 Global step 90 Train loss 11.800443 on epoch=44
03/19/2022 03:53:39 - INFO - __main__ - Step 100 Global step 100 Train loss 10.121588 on epoch=49
03/19/2022 03:53:50 - INFO - __main__ - Global step 100 Train loss 12.271454 Classification-F1 0.0 on epoch=49
03/19/2022 03:53:56 - INFO - __main__ - Step 110 Global step 110 Train loss 9.426647 on epoch=54
03/19/2022 03:54:02 - INFO - __main__ - Step 120 Global step 120 Train loss 5.908662 on epoch=59
03/19/2022 03:54:08 - INFO - __main__ - Step 130 Global step 130 Train loss 5.295270 on epoch=64
03/19/2022 03:54:14 - INFO - __main__ - Step 140 Global step 140 Train loss 2.504963 on epoch=69
03/19/2022 03:54:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.572093 on epoch=74
03/19/2022 03:54:21 - INFO - __main__ - Global step 150 Train loss 4.741527 Classification-F1 0.9375 on epoch=74
03/19/2022 03:54:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.458854 on epoch=79
03/19/2022 03:54:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.175801 on epoch=84
03/19/2022 03:54:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.090706 on epoch=89
03/19/2022 03:54:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.057047 on epoch=94
03/19/2022 03:54:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.038197 on epoch=99
03/19/2022 03:54:52 - INFO - __main__ - Global step 200 Train loss 0.164121 Classification-F1 0.9375 on epoch=99
03/19/2022 03:54:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.030395 on epoch=104
03/19/2022 03:55:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.026871 on epoch=109
03/19/2022 03:55:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.642668 on epoch=114
03/19/2022 03:55:17 - INFO - __main__ - Step 240 Global step 240 Train loss 1.261088 on epoch=119
03/19/2022 03:55:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.895182 on epoch=124
03/19/2022 03:55:24 - INFO - __main__ - Global step 250 Train loss 0.571241 Classification-F1 0.6000000000000001 on epoch=124
03/19/2022 03:55:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.482419 on epoch=129
03/19/2022 03:55:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.209947 on epoch=134
03/19/2022 03:55:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.133534 on epoch=139
03/19/2022 03:55:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.137819 on epoch=144
03/19/2022 03:55:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.091114 on epoch=149
03/19/2022 03:55:55 - INFO - __main__ - Global step 300 Train loss 0.210966 Classification-F1 0.873015873015873 on epoch=149
03/19/2022 03:56:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.108442 on epoch=154
03/19/2022 03:56:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.115441 on epoch=159
03/19/2022 03:56:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.059920 on epoch=164
03/19/2022 03:56:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072601 on epoch=169
03/19/2022 03:56:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.029541 on epoch=174
03/19/2022 03:56:27 - INFO - __main__ - Global step 350 Train loss 0.077189 Classification-F1 0.8398398398398398 on epoch=174
03/19/2022 03:56:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.026113 on epoch=179
03/19/2022 03:56:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.051927 on epoch=184
03/19/2022 03:56:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.022134 on epoch=189
03/19/2022 03:56:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025867 on epoch=194
03/19/2022 03:56:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009961 on epoch=199
03/19/2022 03:56:58 - INFO - __main__ - Global step 400 Train loss 0.027200 Classification-F1 0.873015873015873 on epoch=199
03/19/2022 03:57:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.024108 on epoch=204
03/19/2022 03:57:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042109 on epoch=209
03/19/2022 03:57:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.027744 on epoch=214
03/19/2022 03:57:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011467 on epoch=219
03/19/2022 03:57:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009683 on epoch=224
03/19/2022 03:57:29 - INFO - __main__ - Global step 450 Train loss 0.023022 Classification-F1 0.8423645320197044 on epoch=224
03/19/2022 03:57:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022041 on epoch=229
03/19/2022 03:57:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.028282 on epoch=234
03/19/2022 03:57:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004260 on epoch=239
03/19/2022 03:57:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.021549 on epoch=244
03/19/2022 03:58:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013992 on epoch=249
03/19/2022 03:58:01 - INFO - __main__ - Global step 500 Train loss 0.018025 Classification-F1 0.8398398398398398 on epoch=249
03/19/2022 03:58:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002464 on epoch=254
03/19/2022 03:58:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004162 on epoch=259
03/19/2022 03:58:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004177 on epoch=264
03/19/2022 03:58:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003591 on epoch=269
03/19/2022 03:58:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002948 on epoch=274
03/19/2022 03:58:32 - INFO - __main__ - Global step 550 Train loss 0.003468 Classification-F1 0.805668016194332 on epoch=274
03/19/2022 03:58:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.029553 on epoch=279
03/19/2022 03:58:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006874 on epoch=284
03/19/2022 03:58:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021443 on epoch=289
03/19/2022 03:58:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001936 on epoch=294
03/19/2022 03:59:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001120 on epoch=299
03/19/2022 03:59:04 - INFO - __main__ - Global step 600 Train loss 0.012185 Classification-F1 0.8423645320197044 on epoch=299
03/19/2022 03:59:04 - INFO - __main__ - save last model!
03/19/2022 03:59:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:59:04 - INFO - __main__ - Printing 3 examples
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 03:59:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:59:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 03:59:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 03:59:04 - INFO - __main__ - Printing 3 examples
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 03:59:04 - INFO - __main__ - ['negative']
03/19/2022 03:59:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 03:59:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:59:05 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 03:59:11 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 03:59:11 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 03:59:11 - INFO - __main__ - Printing 3 examples
03/19/2022 03:59:11 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 03:59:11 - INFO - __main__ - ['negative']
03/19/2022 03:59:11 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 03:59:11 - INFO - __main__ - ['negative']
03/19/2022 03:59:11 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 03:59:11 - INFO - __main__ - ['negative']
03/19/2022 03:59:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 03:59:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 03:59:15 - INFO - __main__ - Starting training!
03/19/2022 03:59:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 03:59:25 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:01:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_100_0.0002_8_predictions.txt
03/19/2022 04:01:57 - INFO - __main__ - Classification-F1 on test data: 0.8498
03/19/2022 04:01:57 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.8497829848095886
03/19/2022 04:01:57 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0001, bsz=8 ...
03/19/2022 04:01:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:01:58 - INFO - __main__ - Printing 3 examples
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:01:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:01:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:01:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:01:58 - INFO - __main__ - Printing 3 examples
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/19/2022 04:01:58 - INFO - __main__ - ['negative']
03/19/2022 04:01:58 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:01:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:01:58 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:02:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:02:11 - INFO - __main__ - Starting training!
03/19/2022 04:02:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.560270 on epoch=4
03/19/2022 04:02:22 - INFO - __main__ - Step 20 Global step 20 Train loss 19.783438 on epoch=9
03/19/2022 04:02:29 - INFO - __main__ - Step 30 Global step 30 Train loss 18.575932 on epoch=14
03/19/2022 04:02:35 - INFO - __main__ - Step 40 Global step 40 Train loss 16.623589 on epoch=19
03/19/2022 04:02:41 - INFO - __main__ - Step 50 Global step 50 Train loss 16.288242 on epoch=24
03/19/2022 04:02:53 - INFO - __main__ - Global step 50 Train loss 18.966295 Classification-F1 0.0 on epoch=24
03/19/2022 04:02:59 - INFO - __main__ - Step 60 Global step 60 Train loss 16.290216 on epoch=29
03/19/2022 04:03:06 - INFO - __main__ - Step 70 Global step 70 Train loss 15.263115 on epoch=34
03/19/2022 04:03:12 - INFO - __main__ - Step 80 Global step 80 Train loss 14.820120 on epoch=39
03/19/2022 04:03:18 - INFO - __main__ - Step 90 Global step 90 Train loss 14.903806 on epoch=44
03/19/2022 04:03:24 - INFO - __main__ - Step 100 Global step 100 Train loss 14.430057 on epoch=49
03/19/2022 04:03:36 - INFO - __main__ - Global step 100 Train loss 15.141461 Classification-F1 0.0 on epoch=49
03/19/2022 04:03:42 - INFO - __main__ - Step 110 Global step 110 Train loss 13.496310 on epoch=54
03/19/2022 04:03:48 - INFO - __main__ - Step 120 Global step 120 Train loss 13.852350 on epoch=59
03/19/2022 04:03:54 - INFO - __main__ - Step 130 Global step 130 Train loss 12.616032 on epoch=64
03/19/2022 04:04:00 - INFO - __main__ - Step 140 Global step 140 Train loss 12.478420 on epoch=69
03/19/2022 04:04:06 - INFO - __main__ - Step 150 Global step 150 Train loss 12.291012 on epoch=74
03/19/2022 04:04:18 - INFO - __main__ - Global step 150 Train loss 12.946824 Classification-F1 0.0 on epoch=74
03/19/2022 04:04:24 - INFO - __main__ - Step 160 Global step 160 Train loss 11.081976 on epoch=79
03/19/2022 04:04:30 - INFO - __main__ - Step 170 Global step 170 Train loss 11.349352 on epoch=84
03/19/2022 04:04:36 - INFO - __main__ - Step 180 Global step 180 Train loss 11.377690 on epoch=89
03/19/2022 04:04:42 - INFO - __main__ - Step 190 Global step 190 Train loss 10.119073 on epoch=94
03/19/2022 04:04:48 - INFO - __main__ - Step 200 Global step 200 Train loss 8.298039 on epoch=99
03/19/2022 04:04:51 - INFO - __main__ - Global step 200 Train loss 10.445226 Classification-F1 0.03838383838383838 on epoch=99
03/19/2022 04:04:58 - INFO - __main__ - Step 210 Global step 210 Train loss 5.617471 on epoch=104
03/19/2022 04:05:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.699453 on epoch=109
03/19/2022 04:05:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.542090 on epoch=114
03/19/2022 04:05:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.329769 on epoch=119
03/19/2022 04:05:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.384263 on epoch=124
03/19/2022 04:05:23 - INFO - __main__ - Global step 250 Train loss 1.714610 Classification-F1 0.9372549019607843 on epoch=124
03/19/2022 04:05:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.211281 on epoch=129
03/19/2022 04:05:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.245012 on epoch=134
03/19/2022 04:05:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.182942 on epoch=139
03/19/2022 04:05:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.126855 on epoch=144
03/19/2022 04:05:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.156607 on epoch=149
03/19/2022 04:05:56 - INFO - __main__ - Global step 300 Train loss 0.184539 Classification-F1 1.0 on epoch=149
03/19/2022 04:06:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.099229 on epoch=154
03/19/2022 04:06:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090187 on epoch=159
03/19/2022 04:06:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.020904 on epoch=164
03/19/2022 04:06:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022281 on epoch=169
03/19/2022 04:06:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.025196 on epoch=174
03/19/2022 04:06:28 - INFO - __main__ - Global step 350 Train loss 0.051559 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 04:06:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.159810 on epoch=179
03/19/2022 04:06:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.026505 on epoch=184
03/19/2022 04:06:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.035999 on epoch=189
03/19/2022 04:06:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.046825 on epoch=194
03/19/2022 04:06:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.037797 on epoch=199
03/19/2022 04:07:00 - INFO - __main__ - Global step 400 Train loss 0.061387 Classification-F1 0.6559139784946236 on epoch=199
03/19/2022 04:07:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.009588 on epoch=204
03/19/2022 04:07:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.065752 on epoch=209
03/19/2022 04:07:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.005915 on epoch=214
03/19/2022 04:07:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012312 on epoch=219
03/19/2022 04:07:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011535 on epoch=224
03/19/2022 04:07:32 - INFO - __main__ - Global step 450 Train loss 0.021021 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 04:07:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001134 on epoch=229
03/19/2022 04:07:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002736 on epoch=234
03/19/2022 04:07:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002482 on epoch=239
03/19/2022 04:07:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005638 on epoch=244
03/19/2022 04:08:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003600 on epoch=249
03/19/2022 04:08:03 - INFO - __main__ - Global step 500 Train loss 0.003118 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 04:08:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009992 on epoch=254
03/19/2022 04:08:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.048879 on epoch=259
03/19/2022 04:08:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.105264 on epoch=264
03/19/2022 04:08:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.033484 on epoch=269
03/19/2022 04:08:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.036968 on epoch=274
03/19/2022 04:08:35 - INFO - __main__ - Global step 550 Train loss 0.046918 Classification-F1 1.0 on epoch=274
03/19/2022 04:08:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007640 on epoch=279
03/19/2022 04:08:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000505 on epoch=284
03/19/2022 04:08:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017296 on epoch=289
03/19/2022 04:08:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.047724 on epoch=294
03/19/2022 04:09:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000585 on epoch=299
03/19/2022 04:09:06 - INFO - __main__ - Global step 600 Train loss 0.014750 Classification-F1 1.0 on epoch=299
03/19/2022 04:09:06 - INFO - __main__ - save last model!
03/19/2022 04:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:09:07 - INFO - __main__ - Printing 3 examples
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:09:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:09:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:09:07 - INFO - __main__ - Printing 3 examples
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:09:07 - INFO - __main__ - ['negative']
03/19/2022 04:09:07 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:09:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:09:07 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:09:13 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:09:14 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:09:14 - INFO - __main__ - Printing 3 examples
03/19/2022 04:09:14 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:09:14 - INFO - __main__ - ['negative']
03/19/2022 04:09:14 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:09:14 - INFO - __main__ - ['negative']
03/19/2022 04:09:14 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:09:14 - INFO - __main__ - ['negative']
03/19/2022 04:09:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:09:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:09:20 - INFO - __main__ - Starting training!
03/19/2022 04:09:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:09:28 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:12:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_100_0.0001_8_predictions.txt
03/19/2022 04:12:12 - INFO - __main__ - Classification-F1 on test data: 0.6191
03/19/2022 04:12:13 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.619147631393219
03/19/2022 04:12:13 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0005, bsz=8 ...
03/19/2022 04:12:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:12:14 - INFO - __main__ - Printing 3 examples
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:12:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:12:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:12:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:12:14 - INFO - __main__ - Printing 3 examples
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:12:14 - INFO - __main__ - ['negative']
03/19/2022 04:12:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:12:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:12:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:12:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:12:26 - INFO - __main__ - Starting training!
03/19/2022 04:12:32 - INFO - __main__ - Step 10 Global step 10 Train loss 22.382744 on epoch=4
03/19/2022 04:12:38 - INFO - __main__ - Step 20 Global step 20 Train loss 16.538494 on epoch=9
03/19/2022 04:12:44 - INFO - __main__ - Step 30 Global step 30 Train loss 14.636528 on epoch=14
03/19/2022 04:12:50 - INFO - __main__ - Step 40 Global step 40 Train loss 12.750735 on epoch=19
03/19/2022 04:12:56 - INFO - __main__ - Step 50 Global step 50 Train loss 9.305380 on epoch=24
03/19/2022 04:12:57 - INFO - __main__ - Global step 50 Train loss 15.122777 Classification-F1 0.4444444444444445 on epoch=24
03/19/2022 04:13:03 - INFO - __main__ - Step 60 Global step 60 Train loss 6.174947 on epoch=29
03/19/2022 04:13:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.066854 on epoch=34
03/19/2022 04:13:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.263325 on epoch=39
03/19/2022 04:13:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.073649 on epoch=44
03/19/2022 04:13:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.124679 on epoch=49
03/19/2022 04:13:28 - INFO - __main__ - Global step 100 Train loss 1.540691 Classification-F1 0.8435972629521017 on epoch=49
03/19/2022 04:13:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.053786 on epoch=54
03/19/2022 04:13:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.024040 on epoch=59
03/19/2022 04:13:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.004504 on epoch=64
03/19/2022 04:13:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.036797 on epoch=69
03/19/2022 04:13:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.055166 on epoch=74
03/19/2022 04:14:00 - INFO - __main__ - Global step 150 Train loss 0.034859 Classification-F1 0.875 on epoch=74
03/19/2022 04:14:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005479 on epoch=79
03/19/2022 04:14:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003312 on epoch=84
03/19/2022 04:14:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.001025 on epoch=89
03/19/2022 04:14:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.048177 on epoch=94
03/19/2022 04:14:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003020 on epoch=99
03/19/2022 04:14:32 - INFO - __main__ - Global step 200 Train loss 0.012203 Classification-F1 0.8435972629521017 on epoch=99
03/19/2022 04:14:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.008643 on epoch=104
03/19/2022 04:14:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002821 on epoch=109
03/19/2022 04:14:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001414 on epoch=114
03/19/2022 04:14:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.012336 on epoch=119
03/19/2022 04:15:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000583 on epoch=124
03/19/2022 04:15:04 - INFO - __main__ - Global step 250 Train loss 0.005159 Classification-F1 0.906158357771261 on epoch=124
03/19/2022 04:15:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000209 on epoch=129
03/19/2022 04:15:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001872 on epoch=134
03/19/2022 04:15:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.034431 on epoch=139
03/19/2022 04:15:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001012 on epoch=144
03/19/2022 04:15:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.024053 on epoch=149
03/19/2022 04:15:35 - INFO - __main__ - Global step 300 Train loss 0.012315 Classification-F1 0.875 on epoch=149
03/19/2022 04:15:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000448 on epoch=154
03/19/2022 04:15:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000142 on epoch=159
03/19/2022 04:15:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029003 on epoch=164
03/19/2022 04:16:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.000219 on epoch=169
03/19/2022 04:16:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.234845 on epoch=174
03/19/2022 04:16:07 - INFO - __main__ - Global step 350 Train loss 0.252932 Classification-F1 0.8423645320197044 on epoch=174
03/19/2022 04:16:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.032690 on epoch=179
03/19/2022 04:16:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.010749 on epoch=184
03/19/2022 04:16:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.010111 on epoch=189
03/19/2022 04:16:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.005042 on epoch=194
03/19/2022 04:16:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.004210 on epoch=199
03/19/2022 04:16:38 - INFO - __main__ - Global step 400 Train loss 0.012560 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 04:16:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010083 on epoch=204
03/19/2022 04:16:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000849 on epoch=209
03/19/2022 04:16:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001332 on epoch=214
03/19/2022 04:17:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001932 on epoch=219
03/19/2022 04:17:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000419 on epoch=224
03/19/2022 04:17:10 - INFO - __main__ - Global step 450 Train loss 0.002923 Classification-F1 0.8745098039215686 on epoch=224
03/19/2022 04:17:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.205609 on epoch=229
03/19/2022 04:17:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.024593 on epoch=234
03/19/2022 04:17:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.007156 on epoch=239
03/19/2022 04:17:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.078926 on epoch=244
03/19/2022 04:17:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.072305 on epoch=249
03/19/2022 04:17:41 - INFO - __main__ - Global step 500 Train loss 0.077718 Classification-F1 0.5933528836754642 on epoch=249
03/19/2022 04:17:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011511 on epoch=254
03/19/2022 04:17:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005843 on epoch=259
03/19/2022 04:17:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.019279 on epoch=264
03/19/2022 04:18:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002464 on epoch=269
03/19/2022 04:18:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000924 on epoch=274
03/19/2022 04:18:12 - INFO - __main__ - Global step 550 Train loss 0.008004 Classification-F1 0.6235294117647059 on epoch=274
03/19/2022 04:18:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001584 on epoch=279
03/19/2022 04:18:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000746 on epoch=284
03/19/2022 04:18:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001464 on epoch=289
03/19/2022 04:18:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000653 on epoch=294
03/19/2022 04:18:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000452 on epoch=299
03/19/2022 04:18:43 - INFO - __main__ - Global step 600 Train loss 0.000980 Classification-F1 0.7184750733137829 on epoch=299
03/19/2022 04:18:43 - INFO - __main__ - save last model!
03/19/2022 04:18:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:18:44 - INFO - __main__ - Printing 3 examples
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:18:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:18:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:18:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:18:44 - INFO - __main__ - Printing 3 examples
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:18:44 - INFO - __main__ - ['negative']
03/19/2022 04:18:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:18:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:18:44 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:18:50 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:18:50 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:18:50 - INFO - __main__ - Printing 3 examples
03/19/2022 04:18:50 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:18:50 - INFO - __main__ - ['negative']
03/19/2022 04:18:50 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:18:50 - INFO - __main__ - ['negative']
03/19/2022 04:18:50 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:18:50 - INFO - __main__ - ['negative']
03/19/2022 04:18:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:18:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:18:55 - INFO - __main__ - Starting training!
03/19/2022 04:18:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:19:04 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:21:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_13_0.0005_8_predictions.txt
03/19/2022 04:21:50 - INFO - __main__ - Classification-F1 on test data: 0.9523
03/19/2022 04:21:51 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.906158357771261, test_performance=0.9523467691933987
03/19/2022 04:21:51 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0003, bsz=8 ...
03/19/2022 04:21:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:21:52 - INFO - __main__ - Printing 3 examples
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:21:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:21:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:21:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:21:52 - INFO - __main__ - Printing 3 examples
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:21:52 - INFO - __main__ - ['negative']
03/19/2022 04:21:52 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:21:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:21:52 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:22:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:22:03 - INFO - __main__ - Starting training!
03/19/2022 04:22:08 - INFO - __main__ - Step 10 Global step 10 Train loss 23.003292 on epoch=4
03/19/2022 04:22:14 - INFO - __main__ - Step 20 Global step 20 Train loss 17.055460 on epoch=9
03/19/2022 04:22:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.067924 on epoch=14
03/19/2022 04:22:26 - INFO - __main__ - Step 40 Global step 40 Train loss 14.814878 on epoch=19
03/19/2022 04:22:32 - INFO - __main__ - Step 50 Global step 50 Train loss 12.896375 on epoch=24
03/19/2022 04:22:38 - INFO - __main__ - Global step 50 Train loss 16.767586 Classification-F1 0.0 on epoch=24
03/19/2022 04:22:45 - INFO - __main__ - Step 60 Global step 60 Train loss 12.329859 on epoch=29
03/19/2022 04:22:51 - INFO - __main__ - Step 70 Global step 70 Train loss 10.732346 on epoch=34
03/19/2022 04:22:57 - INFO - __main__ - Step 80 Global step 80 Train loss 6.827376 on epoch=39
03/19/2022 04:23:03 - INFO - __main__ - Step 90 Global step 90 Train loss 2.021800 on epoch=44
03/19/2022 04:23:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.521758 on epoch=49
03/19/2022 04:23:10 - INFO - __main__ - Global step 100 Train loss 6.686627 Classification-F1 0.4385964912280702 on epoch=49
03/19/2022 04:23:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.307936 on epoch=54
03/19/2022 04:23:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.658474 on epoch=59
03/19/2022 04:23:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.102511 on epoch=64
03/19/2022 04:23:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.041991 on epoch=69
03/19/2022 04:23:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.039061 on epoch=74
03/19/2022 04:23:41 - INFO - __main__ - Global step 150 Train loss 0.229994 Classification-F1 0.906158357771261 on epoch=74
03/19/2022 04:23:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.177732 on epoch=79
03/19/2022 04:23:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.053045 on epoch=84
03/19/2022 04:23:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.225033 on epoch=89
03/19/2022 04:24:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.271661 on epoch=94
03/19/2022 04:24:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.201018 on epoch=99
03/19/2022 04:24:12 - INFO - __main__ - Global step 200 Train loss 0.385698 Classification-F1 0.746031746031746 on epoch=99
03/19/2022 04:24:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.143150 on epoch=104
03/19/2022 04:24:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.015035 on epoch=109
03/19/2022 04:24:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.038709 on epoch=114
03/19/2022 04:24:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.022895 on epoch=119
03/19/2022 04:24:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.011317 on epoch=124
03/19/2022 04:24:43 - INFO - __main__ - Global step 250 Train loss 0.046221 Classification-F1 0.906158357771261 on epoch=124
03/19/2022 04:24:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.007492 on epoch=129
03/19/2022 04:24:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.028516 on epoch=134
03/19/2022 04:25:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.010072 on epoch=139
03/19/2022 04:25:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.062604 on epoch=144
03/19/2022 04:25:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.010515 on epoch=149
03/19/2022 04:25:14 - INFO - __main__ - Global step 300 Train loss 0.023840 Classification-F1 0.875 on epoch=149
03/19/2022 04:25:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.056142 on epoch=154
03/19/2022 04:25:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070694 on epoch=159
03/19/2022 04:25:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.035310 on epoch=164
03/19/2022 04:25:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.096757 on epoch=169
03/19/2022 04:25:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.113974 on epoch=174
03/19/2022 04:25:44 - INFO - __main__ - Global step 350 Train loss 0.074575 Classification-F1 0.7117117117117117 on epoch=174
03/19/2022 04:25:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.055965 on epoch=179
03/19/2022 04:25:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.100078 on epoch=184
03/19/2022 04:26:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.430361 on epoch=189
03/19/2022 04:26:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.416842 on epoch=194
03/19/2022 04:26:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.337403 on epoch=199
03/19/2022 04:26:15 - INFO - __main__ - Global step 400 Train loss 0.268130 Classification-F1 0.4666666666666667 on epoch=199
03/19/2022 04:26:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.352920 on epoch=204
03/19/2022 04:26:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.336898 on epoch=209
03/19/2022 04:26:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.286562 on epoch=214
03/19/2022 04:26:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.318389 on epoch=219
03/19/2022 04:26:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.329061 on epoch=224
03/19/2022 04:26:46 - INFO - __main__ - Global step 450 Train loss 0.324766 Classification-F1 0.625 on epoch=224
03/19/2022 04:26:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.313347 on epoch=229
03/19/2022 04:26:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.266033 on epoch=234
03/19/2022 04:27:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.331404 on epoch=239
03/19/2022 04:27:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.281807 on epoch=244
03/19/2022 04:27:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.324985 on epoch=249
03/19/2022 04:27:17 - INFO - __main__ - Global step 500 Train loss 0.303515 Classification-F1 0.5733333333333335 on epoch=249
03/19/2022 04:27:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.276905 on epoch=254
03/19/2022 04:27:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.244166 on epoch=259
03/19/2022 04:27:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.270270 on epoch=264
03/19/2022 04:27:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.305467 on epoch=269
03/19/2022 04:27:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.293565 on epoch=274
03/19/2022 04:27:48 - INFO - __main__ - Global step 550 Train loss 0.278075 Classification-F1 0.5733333333333335 on epoch=274
03/19/2022 04:27:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.282097 on epoch=279
03/19/2022 04:28:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.262035 on epoch=284
03/19/2022 04:28:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.269392 on epoch=289
03/19/2022 04:28:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.295022 on epoch=294
03/19/2022 04:28:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.285817 on epoch=299
03/19/2022 04:28:18 - INFO - __main__ - Global step 600 Train loss 0.278873 Classification-F1 0.39756367663344405 on epoch=299
03/19/2022 04:28:18 - INFO - __main__ - save last model!
03/19/2022 04:28:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:28:20 - INFO - __main__ - Printing 3 examples
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:28:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:28:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:28:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:28:20 - INFO - __main__ - Printing 3 examples
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:28:20 - INFO - __main__ - ['negative']
03/19/2022 04:28:20 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:28:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:28:20 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:28:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:28:26 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:28:26 - INFO - __main__ - Printing 3 examples
03/19/2022 04:28:26 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:28:26 - INFO - __main__ - ['negative']
03/19/2022 04:28:26 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:28:26 - INFO - __main__ - ['negative']
03/19/2022 04:28:26 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:28:26 - INFO - __main__ - ['negative']
03/19/2022 04:28:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:28:31 - INFO - __main__ - Starting training!
03/19/2022 04:28:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:28:40 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:31:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_13_0.0003_8_predictions.txt
03/19/2022 04:31:24 - INFO - __main__ - Classification-F1 on test data: 0.6420
03/19/2022 04:31:24 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.906158357771261, test_performance=0.6419698468630979
03/19/2022 04:31:24 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0002, bsz=8 ...
03/19/2022 04:31:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:31:25 - INFO - __main__ - Printing 3 examples
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:31:25 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:31:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:31:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:31:25 - INFO - __main__ - Printing 3 examples
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:31:25 - INFO - __main__ - ['negative']
03/19/2022 04:31:25 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:31:25 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:31:25 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:31:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:31:36 - INFO - __main__ - Starting training!
03/19/2022 04:31:41 - INFO - __main__ - Step 10 Global step 10 Train loss 23.551332 on epoch=4
03/19/2022 04:31:47 - INFO - __main__ - Step 20 Global step 20 Train loss 18.678638 on epoch=9
03/19/2022 04:31:53 - INFO - __main__ - Step 30 Global step 30 Train loss 16.796072 on epoch=14
03/19/2022 04:32:00 - INFO - __main__ - Step 40 Global step 40 Train loss 15.274791 on epoch=19
03/19/2022 04:32:06 - INFO - __main__ - Step 50 Global step 50 Train loss 14.622679 on epoch=24
03/19/2022 04:32:14 - INFO - __main__ - Global step 50 Train loss 17.784700 Classification-F1 0.0 on epoch=24
03/19/2022 04:32:21 - INFO - __main__ - Step 60 Global step 60 Train loss 14.866760 on epoch=29
03/19/2022 04:32:27 - INFO - __main__ - Step 70 Global step 70 Train loss 13.279032 on epoch=34
03/19/2022 04:32:33 - INFO - __main__ - Step 80 Global step 80 Train loss 12.454967 on epoch=39
03/19/2022 04:32:39 - INFO - __main__ - Step 90 Global step 90 Train loss 12.091654 on epoch=44
03/19/2022 04:32:45 - INFO - __main__ - Step 100 Global step 100 Train loss 10.794875 on epoch=49
03/19/2022 04:32:47 - INFO - __main__ - Global step 100 Train loss 12.697457 Classification-F1 0.04 on epoch=49
03/19/2022 04:32:54 - INFO - __main__ - Step 110 Global step 110 Train loss 9.972924 on epoch=54
03/19/2022 04:33:00 - INFO - __main__ - Step 120 Global step 120 Train loss 5.253943 on epoch=59
03/19/2022 04:33:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.787070 on epoch=64
03/19/2022 04:33:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.967089 on epoch=69
03/19/2022 04:33:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.249817 on epoch=74
03/19/2022 04:33:19 - INFO - __main__ - Global step 150 Train loss 3.446169 Classification-F1 0.8435972629521017 on epoch=74
03/19/2022 04:33:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.073021 on epoch=79
03/19/2022 04:33:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.041467 on epoch=84
03/19/2022 04:33:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.007622 on epoch=89
03/19/2022 04:33:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.034744 on epoch=94
03/19/2022 04:33:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.004608 on epoch=99
03/19/2022 04:33:51 - INFO - __main__ - Global step 200 Train loss 0.032293 Classification-F1 0.8745098039215686 on epoch=99
03/19/2022 04:33:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.004202 on epoch=104
03/19/2022 04:34:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.013301 on epoch=109
03/19/2022 04:34:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.022681 on epoch=114
03/19/2022 04:34:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001157 on epoch=119
03/19/2022 04:34:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.002207 on epoch=124
03/19/2022 04:34:23 - INFO - __main__ - Global step 250 Train loss 0.008710 Classification-F1 0.8745098039215686 on epoch=124
03/19/2022 04:34:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.002354 on epoch=129
03/19/2022 04:34:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.003403 on epoch=134
03/19/2022 04:34:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000576 on epoch=139
03/19/2022 04:34:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000625 on epoch=144
03/19/2022 04:34:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.008367 on epoch=149
03/19/2022 04:34:55 - INFO - __main__ - Global step 300 Train loss 0.003065 Classification-F1 0.8745098039215686 on epoch=149
03/19/2022 04:35:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001965 on epoch=154
03/19/2022 04:35:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000821 on epoch=159
03/19/2022 04:35:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000815 on epoch=164
03/19/2022 04:35:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001072 on epoch=169
03/19/2022 04:35:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000148 on epoch=174
03/19/2022 04:35:26 - INFO - __main__ - Global step 350 Train loss 0.000964 Classification-F1 0.8745098039215686 on epoch=174
03/19/2022 04:35:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001131 on epoch=179
03/19/2022 04:35:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000156 on epoch=184
03/19/2022 04:35:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000180 on epoch=189
03/19/2022 04:35:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.016059 on epoch=194
03/19/2022 04:35:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000494 on epoch=199
03/19/2022 04:35:57 - INFO - __main__ - Global step 400 Train loss 0.003604 Classification-F1 0.8745098039215686 on epoch=199
03/19/2022 04:36:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000151 on epoch=204
03/19/2022 04:36:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000595 on epoch=209
03/19/2022 04:36:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000461 on epoch=214
03/19/2022 04:36:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000608 on epoch=219
03/19/2022 04:36:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002073 on epoch=224
03/19/2022 04:36:27 - INFO - __main__ - Global step 450 Train loss 0.000778 Classification-F1 0.8745098039215686 on epoch=224
03/19/2022 04:36:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000225 on epoch=229
03/19/2022 04:36:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000645 on epoch=234
03/19/2022 04:36:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000186 on epoch=239
03/19/2022 04:36:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000130 on epoch=244
03/19/2022 04:36:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005428 on epoch=249
03/19/2022 04:36:58 - INFO - __main__ - Global step 500 Train loss 0.001323 Classification-F1 0.8095238095238095 on epoch=249
03/19/2022 04:37:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.044987 on epoch=254
03/19/2022 04:37:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001348 on epoch=259
03/19/2022 04:37:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000236 on epoch=264
03/19/2022 04:37:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000673 on epoch=269
03/19/2022 04:37:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000131 on epoch=274
03/19/2022 04:37:29 - INFO - __main__ - Global step 550 Train loss 0.009475 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 04:37:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000342 on epoch=279
03/19/2022 04:37:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000191 on epoch=284
03/19/2022 04:37:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000171 on epoch=289
03/19/2022 04:37:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000149 on epoch=294
03/19/2022 04:38:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000115 on epoch=299
03/19/2022 04:38:01 - INFO - __main__ - Global step 600 Train loss 0.000193 Classification-F1 0.906158357771261 on epoch=299
03/19/2022 04:38:01 - INFO - __main__ - save last model!
03/19/2022 04:38:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:38:02 - INFO - __main__ - Printing 3 examples
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:38:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:38:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:38:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:38:02 - INFO - __main__ - Printing 3 examples
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:38:02 - INFO - __main__ - ['negative']
03/19/2022 04:38:02 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:38:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:38:02 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:38:09 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:38:09 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:38:09 - INFO - __main__ - Printing 3 examples
03/19/2022 04:38:09 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:38:09 - INFO - __main__ - ['negative']
03/19/2022 04:38:09 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:38:09 - INFO - __main__ - ['negative']
03/19/2022 04:38:09 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:38:09 - INFO - __main__ - ['negative']
03/19/2022 04:38:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:38:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:38:12 - INFO - __main__ - Starting training!
03/19/2022 04:38:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:38:23 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:41:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_13_0.0002_8_predictions.txt
03/19/2022 04:41:06 - INFO - __main__ - Classification-F1 on test data: 0.9622
03/19/2022 04:41:06 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0002, bsz=8, dev_performance=0.906158357771261, test_performance=0.9622367891478518
03/19/2022 04:41:06 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0001, bsz=8 ...
03/19/2022 04:41:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:41:07 - INFO - __main__ - Printing 3 examples
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:41:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:41:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:41:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:41:07 - INFO - __main__ - Printing 3 examples
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] good store
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/19/2022 04:41:07 - INFO - __main__ - ['negative']
03/19/2022 04:41:07 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:41:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:41:07 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:41:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:41:18 - INFO - __main__ - Starting training!
03/19/2022 04:41:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.254255 on epoch=4
03/19/2022 04:41:31 - INFO - __main__ - Step 20 Global step 20 Train loss 19.929539 on epoch=9
03/19/2022 04:41:37 - INFO - __main__ - Step 30 Global step 30 Train loss 18.473780 on epoch=14
03/19/2022 04:41:44 - INFO - __main__ - Step 40 Global step 40 Train loss 17.295391 on epoch=19
03/19/2022 04:41:50 - INFO - __main__ - Step 50 Global step 50 Train loss 16.851187 on epoch=24
03/19/2022 04:42:03 - INFO - __main__ - Global step 50 Train loss 19.160830 Classification-F1 0.0 on epoch=24
03/19/2022 04:42:10 - INFO - __main__ - Step 60 Global step 60 Train loss 15.960180 on epoch=29
03/19/2022 04:42:16 - INFO - __main__ - Step 70 Global step 70 Train loss 15.738312 on epoch=34
03/19/2022 04:42:22 - INFO - __main__ - Step 80 Global step 80 Train loss 15.407425 on epoch=39
03/19/2022 04:42:28 - INFO - __main__ - Step 90 Global step 90 Train loss 14.857478 on epoch=44
03/19/2022 04:42:34 - INFO - __main__ - Step 100 Global step 100 Train loss 14.150952 on epoch=49
03/19/2022 04:42:46 - INFO - __main__ - Global step 100 Train loss 15.222870 Classification-F1 0.0 on epoch=49
03/19/2022 04:42:52 - INFO - __main__ - Step 110 Global step 110 Train loss 13.657030 on epoch=54
03/19/2022 04:42:59 - INFO - __main__ - Step 120 Global step 120 Train loss 13.809735 on epoch=59
03/19/2022 04:43:05 - INFO - __main__ - Step 130 Global step 130 Train loss 13.430605 on epoch=64
03/19/2022 04:43:11 - INFO - __main__ - Step 140 Global step 140 Train loss 13.300647 on epoch=69
03/19/2022 04:43:17 - INFO - __main__ - Step 150 Global step 150 Train loss 12.681955 on epoch=74
03/19/2022 04:43:25 - INFO - __main__ - Global step 150 Train loss 13.375995 Classification-F1 0.0 on epoch=74
03/19/2022 04:43:31 - INFO - __main__ - Step 160 Global step 160 Train loss 12.211585 on epoch=79
03/19/2022 04:43:37 - INFO - __main__ - Step 170 Global step 170 Train loss 11.486865 on epoch=84
03/19/2022 04:43:43 - INFO - __main__ - Step 180 Global step 180 Train loss 11.224695 on epoch=89
03/19/2022 04:43:49 - INFO - __main__ - Step 190 Global step 190 Train loss 10.584871 on epoch=94
03/19/2022 04:43:55 - INFO - __main__ - Step 200 Global step 200 Train loss 10.033457 on epoch=99
03/19/2022 04:44:01 - INFO - __main__ - Global step 200 Train loss 11.108294 Classification-F1 0.016993464052287584 on epoch=99
03/19/2022 04:44:08 - INFO - __main__ - Step 210 Global step 210 Train loss 9.455308 on epoch=104
03/19/2022 04:44:14 - INFO - __main__ - Step 220 Global step 220 Train loss 6.511902 on epoch=109
03/19/2022 04:44:20 - INFO - __main__ - Step 230 Global step 230 Train loss 4.429627 on epoch=114
03/19/2022 04:44:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.357264 on epoch=119
03/19/2022 04:44:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.835521 on epoch=124
03/19/2022 04:44:33 - INFO - __main__ - Global step 250 Train loss 4.517924 Classification-F1 0.5717171717171717 on epoch=124
03/19/2022 04:44:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.311919 on epoch=129
03/19/2022 04:44:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.058982 on epoch=134
03/19/2022 04:44:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.044676 on epoch=139
03/19/2022 04:44:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.041231 on epoch=144
03/19/2022 04:45:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.245558 on epoch=149
03/19/2022 04:45:05 - INFO - __main__ - Global step 300 Train loss 0.140473 Classification-F1 0.906158357771261 on epoch=149
03/19/2022 04:45:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.048225 on epoch=154
03/19/2022 04:45:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.121016 on epoch=159
03/19/2022 04:45:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.135319 on epoch=164
03/19/2022 04:45:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.019736 on epoch=169
03/19/2022 04:45:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.010061 on epoch=174
03/19/2022 04:45:37 - INFO - __main__ - Global step 350 Train loss 0.066871 Classification-F1 0.8745098039215686 on epoch=174
03/19/2022 04:45:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.016854 on epoch=179
03/19/2022 04:45:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.008774 on epoch=184
03/19/2022 04:45:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.005926 on epoch=189
03/19/2022 04:46:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004854 on epoch=194
03/19/2022 04:46:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003798 on epoch=199
03/19/2022 04:46:09 - INFO - __main__ - Global step 400 Train loss 0.008041 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 04:46:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016000 on epoch=204
03/19/2022 04:46:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.078779 on epoch=209
03/19/2022 04:46:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000786 on epoch=214
03/19/2022 04:46:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.009436 on epoch=219
03/19/2022 04:46:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.025596 on epoch=224
03/19/2022 04:46:40 - INFO - __main__ - Global step 450 Train loss 0.026119 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 04:46:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172121 on epoch=229
03/19/2022 04:46:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000375 on epoch=234
03/19/2022 04:46:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004835 on epoch=239
03/19/2022 04:47:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095745 on epoch=244
03/19/2022 04:47:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001964 on epoch=249
03/19/2022 04:47:12 - INFO - __main__ - Global step 500 Train loss 0.055008 Classification-F1 0.9375 on epoch=249
03/19/2022 04:47:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001633 on epoch=254
03/19/2022 04:47:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000539 on epoch=259
03/19/2022 04:47:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000257 on epoch=264
03/19/2022 04:47:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000300 on epoch=269
03/19/2022 04:47:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000266 on epoch=274
03/19/2022 04:47:43 - INFO - __main__ - Global step 550 Train loss 0.000599 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 04:47:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000918 on epoch=279
03/19/2022 04:47:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000252 on epoch=284
03/19/2022 04:48:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.009660 on epoch=289
03/19/2022 04:48:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000158 on epoch=294
03/19/2022 04:48:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000296 on epoch=299
03/19/2022 04:48:14 - INFO - __main__ - Global step 600 Train loss 0.002257 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 04:48:14 - INFO - __main__ - save last model!
03/19/2022 04:48:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:48:15 - INFO - __main__ - Printing 3 examples
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:48:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:48:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:48:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:48:15 - INFO - __main__ - Printing 3 examples
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 04:48:15 - INFO - __main__ - ['positive']
03/19/2022 04:48:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:48:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:48:15 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:48:21 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:48:22 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:48:22 - INFO - __main__ - Printing 3 examples
03/19/2022 04:48:22 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:48:22 - INFO - __main__ - ['negative']
03/19/2022 04:48:22 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:48:22 - INFO - __main__ - ['negative']
03/19/2022 04:48:22 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:48:22 - INFO - __main__ - ['negative']
03/19/2022 04:48:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:48:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:48:26 - INFO - __main__ - Starting training!
03/19/2022 04:48:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:48:35 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 04:51:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_13_0.0001_8_predictions.txt
03/19/2022 04:51:17 - INFO - __main__ - Classification-F1 on test data: 0.9651
03/19/2022 04:51:17 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9651227382619615
03/19/2022 04:51:17 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0005, bsz=8 ...
03/19/2022 04:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:51:18 - INFO - __main__ - Printing 3 examples
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:51:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:51:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:51:18 - INFO - __main__ - Printing 3 examples
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 04:51:18 - INFO - __main__ - ['positive']
03/19/2022 04:51:18 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:51:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:51:19 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:51:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:51:29 - INFO - __main__ - Starting training!
03/19/2022 04:51:35 - INFO - __main__ - Step 10 Global step 10 Train loss 24.371519 on epoch=4
03/19/2022 04:51:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.296124 on epoch=9
03/19/2022 04:51:47 - INFO - __main__ - Step 30 Global step 30 Train loss 15.277510 on epoch=14
03/19/2022 04:51:54 - INFO - __main__ - Step 40 Global step 40 Train loss 14.108066 on epoch=19
03/19/2022 04:52:00 - INFO - __main__ - Step 50 Global step 50 Train loss 11.527250 on epoch=24
03/19/2022 04:52:02 - INFO - __main__ - Global step 50 Train loss 16.516092 Classification-F1 0.0 on epoch=24
03/19/2022 04:52:09 - INFO - __main__ - Step 60 Global step 60 Train loss 8.205577 on epoch=29
03/19/2022 04:52:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.541016 on epoch=34
03/19/2022 04:52:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.633138 on epoch=39
03/19/2022 04:52:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.314885 on epoch=44
03/19/2022 04:52:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.427242 on epoch=49
03/19/2022 04:52:35 - INFO - __main__ - Global step 100 Train loss 2.224371 Classification-F1 0.3992490613266583 on epoch=49
03/19/2022 04:52:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.354840 on epoch=54
03/19/2022 04:52:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.179669 on epoch=59
03/19/2022 04:52:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.187012 on epoch=64
03/19/2022 04:53:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.059341 on epoch=69
03/19/2022 04:53:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.401351 on epoch=74
03/19/2022 04:53:08 - INFO - __main__ - Global step 150 Train loss 0.236443 Classification-F1 0.6101882613510521 on epoch=74
03/19/2022 04:53:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.664137 on epoch=79
03/19/2022 04:53:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.493109 on epoch=84
03/19/2022 04:53:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.411941 on epoch=89
03/19/2022 04:53:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.381129 on epoch=94
03/19/2022 04:53:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.378953 on epoch=99
03/19/2022 04:53:41 - INFO - __main__ - Global step 200 Train loss 0.465854 Classification-F1 0.7490196078431373 on epoch=99
03/19/2022 04:53:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.432096 on epoch=104
03/19/2022 04:53:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.364073 on epoch=109
03/19/2022 04:54:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.325204 on epoch=114
03/19/2022 04:54:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.337307 on epoch=119
03/19/2022 04:54:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.338802 on epoch=124
03/19/2022 04:54:15 - INFO - __main__ - Global step 250 Train loss 0.359496 Classification-F1 0.36374269005847953 on epoch=124
03/19/2022 04:54:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.313740 on epoch=129
03/19/2022 04:54:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.325456 on epoch=134
03/19/2022 04:54:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.294900 on epoch=139
03/19/2022 04:54:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280768 on epoch=144
03/19/2022 04:54:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.285009 on epoch=149
03/19/2022 04:54:48 - INFO - __main__ - Global step 300 Train loss 0.299974 Classification-F1 0.6101882613510521 on epoch=149
03/19/2022 04:54:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.308810 on epoch=154
03/19/2022 04:55:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.278281 on epoch=159
03/19/2022 04:55:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.329841 on epoch=164
03/19/2022 04:55:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.295394 on epoch=169
03/19/2022 04:55:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.654743 on epoch=174
03/19/2022 04:55:20 - INFO - __main__ - Global step 350 Train loss 0.373414 Classification-F1 0.6267232237539766 on epoch=174
03/19/2022 04:55:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.334807 on epoch=179
03/19/2022 04:55:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.255704 on epoch=184
03/19/2022 04:55:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.314526 on epoch=189
03/19/2022 04:55:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.205354 on epoch=194
03/19/2022 04:55:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.262121 on epoch=199
03/19/2022 04:55:53 - INFO - __main__ - Global step 400 Train loss 0.274502 Classification-F1 0.5588547189819725 on epoch=199
03/19/2022 04:55:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.263637 on epoch=204
03/19/2022 04:56:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.184777 on epoch=209
03/19/2022 04:56:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.148170 on epoch=214
03/19/2022 04:56:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.269794 on epoch=219
03/19/2022 04:56:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.152107 on epoch=224
03/19/2022 04:56:25 - INFO - __main__ - Global step 450 Train loss 0.203697 Classification-F1 0.6825396825396826 on epoch=224
03/19/2022 04:56:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.058914 on epoch=229
03/19/2022 04:56:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.118121 on epoch=234
03/19/2022 04:56:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.036586 on epoch=239
03/19/2022 04:56:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.063515 on epoch=244
03/19/2022 04:56:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.101461 on epoch=249
03/19/2022 04:56:58 - INFO - __main__ - Global step 500 Train loss 0.075719 Classification-F1 0.5465587044534412 on epoch=249
03/19/2022 04:57:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027339 on epoch=254
03/19/2022 04:57:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.007637 on epoch=259
03/19/2022 04:57:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006960 on epoch=264
03/19/2022 04:57:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.021724 on epoch=269
03/19/2022 04:57:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.008599 on epoch=274
03/19/2022 04:57:31 - INFO - __main__ - Global step 550 Train loss 0.014452 Classification-F1 0.5733333333333335 on epoch=274
03/19/2022 04:57:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.031514 on epoch=279
03/19/2022 04:57:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.142808 on epoch=284
03/19/2022 04:57:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.092332 on epoch=289
03/19/2022 04:57:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.029609 on epoch=294
03/19/2022 04:58:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008488 on epoch=299
03/19/2022 04:58:04 - INFO - __main__ - Global step 600 Train loss 0.060950 Classification-F1 0.7793103448275862 on epoch=299
03/19/2022 04:58:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:58:04 - INFO - __main__ - Printing 3 examples
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 04:58:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:58:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 04:58:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 04:58:04 - INFO - __main__ - Printing 3 examples
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 04:58:04 - INFO - __main__ - ['positive']
03/19/2022 04:58:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 04:58:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:58:04 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 04:58:04 - INFO - __main__ - save last model!
03/19/2022 04:58:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 04:58:12 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 04:58:12 - INFO - __main__ - Printing 3 examples
03/19/2022 04:58:12 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 04:58:12 - INFO - __main__ - ['negative']
03/19/2022 04:58:12 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 04:58:12 - INFO - __main__ - ['negative']
03/19/2022 04:58:12 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 04:58:12 - INFO - __main__ - ['negative']
03/19/2022 04:58:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 04:58:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 04:58:15 - INFO - __main__ - Starting training!
03/19/2022 04:58:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 04:58:26 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:01:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_21_0.0005_8_predictions.txt
03/19/2022 05:01:12 - INFO - __main__ - Classification-F1 on test data: 0.4841
03/19/2022 05:01:12 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.7793103448275862, test_performance=0.4841488981683786
03/19/2022 05:01:12 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0003, bsz=8 ...
03/19/2022 05:01:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:01:13 - INFO - __main__ - Printing 3 examples
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:01:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:01:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:01:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:01:13 - INFO - __main__ - Printing 3 examples
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 05:01:13 - INFO - __main__ - ['positive']
03/19/2022 05:01:13 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:01:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:01:13 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:01:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:01:26 - INFO - __main__ - Starting training!
03/19/2022 05:01:31 - INFO - __main__ - Step 10 Global step 10 Train loss 22.474869 on epoch=4
03/19/2022 05:01:38 - INFO - __main__ - Step 20 Global step 20 Train loss 18.658798 on epoch=9
03/19/2022 05:01:44 - INFO - __main__ - Step 30 Global step 30 Train loss 16.231548 on epoch=14
03/19/2022 05:01:51 - INFO - __main__ - Step 40 Global step 40 Train loss 15.064084 on epoch=19
03/19/2022 05:01:57 - INFO - __main__ - Step 50 Global step 50 Train loss 13.640371 on epoch=24
03/19/2022 05:02:05 - INFO - __main__ - Global step 50 Train loss 17.213934 Classification-F1 0.0 on epoch=24
03/19/2022 05:02:12 - INFO - __main__ - Step 60 Global step 60 Train loss 12.603197 on epoch=29
03/19/2022 05:02:18 - INFO - __main__ - Step 70 Global step 70 Train loss 10.809740 on epoch=34
03/19/2022 05:02:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.696496 on epoch=39
03/19/2022 05:02:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.522555 on epoch=44
03/19/2022 05:02:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.197831 on epoch=49
03/19/2022 05:02:38 - INFO - __main__ - Global step 100 Train loss 5.165964 Classification-F1 0.8745098039215686 on epoch=49
03/19/2022 05:02:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.126959 on epoch=54
03/19/2022 05:02:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.043081 on epoch=59
03/19/2022 05:02:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.011491 on epoch=64
03/19/2022 05:03:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.007960 on epoch=69
03/19/2022 05:03:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.002865 on epoch=74
03/19/2022 05:03:11 - INFO - __main__ - Global step 150 Train loss 0.038471 Classification-F1 0.9372549019607843 on epoch=74
03/19/2022 05:03:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.003832 on epoch=79
03/19/2022 05:03:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003042 on epoch=84
03/19/2022 05:03:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.000955 on epoch=89
03/19/2022 05:03:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001115 on epoch=94
03/19/2022 05:03:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000563 on epoch=99
03/19/2022 05:03:44 - INFO - __main__ - Global step 200 Train loss 0.001902 Classification-F1 0.9372549019607843 on epoch=99
03/19/2022 05:03:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000438 on epoch=104
03/19/2022 05:03:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.064431 on epoch=109
03/19/2022 05:04:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.185217 on epoch=114
03/19/2022 05:04:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000922 on epoch=119
03/19/2022 05:04:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001507 on epoch=124
03/19/2022 05:04:17 - INFO - __main__ - Global step 250 Train loss 0.050503 Classification-F1 0.9372549019607843 on epoch=124
03/19/2022 05:04:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000497 on epoch=129
03/19/2022 05:04:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000487 on epoch=134
03/19/2022 05:04:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.291807 on epoch=139
03/19/2022 05:04:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000217 on epoch=144
03/19/2022 05:04:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000217 on epoch=149
03/19/2022 05:04:50 - INFO - __main__ - Global step 300 Train loss 0.058645 Classification-F1 0.9372549019607843 on epoch=149
03/19/2022 05:04:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000163 on epoch=154
03/19/2022 05:05:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000472 on epoch=159
03/19/2022 05:05:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000142 on epoch=164
03/19/2022 05:05:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000244 on epoch=169
03/19/2022 05:05:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000164 on epoch=174
03/19/2022 05:05:22 - INFO - __main__ - Global step 350 Train loss 0.000237 Classification-F1 0.9372549019607843 on epoch=174
03/19/2022 05:05:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000114 on epoch=179
03/19/2022 05:05:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000074 on epoch=184
03/19/2022 05:05:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000243 on epoch=189
03/19/2022 05:05:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000347 on epoch=194
03/19/2022 05:05:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000068 on epoch=199
03/19/2022 05:05:55 - INFO - __main__ - Global step 400 Train loss 0.000169 Classification-F1 0.9372549019607843 on epoch=199
03/19/2022 05:06:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000041 on epoch=204
03/19/2022 05:06:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.388907 on epoch=209
03/19/2022 05:06:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.629377 on epoch=214
03/19/2022 05:06:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.214791 on epoch=219
03/19/2022 05:06:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.185003 on epoch=224
03/19/2022 05:06:27 - INFO - __main__ - Global step 450 Train loss 0.283624 Classification-F1 0.9372549019607843 on epoch=224
03/19/2022 05:06:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.201840 on epoch=229
03/19/2022 05:06:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000029 on epoch=234
03/19/2022 05:06:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000221 on epoch=239
03/19/2022 05:06:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.006660 on epoch=244
03/19/2022 05:06:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000073 on epoch=249
03/19/2022 05:07:00 - INFO - __main__ - Global step 500 Train loss 0.041765 Classification-F1 0.9372549019607843 on epoch=249
03/19/2022 05:07:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.181901 on epoch=254
03/19/2022 05:07:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000025 on epoch=259
03/19/2022 05:07:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000013 on epoch=264
03/19/2022 05:07:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000024 on epoch=269
03/19/2022 05:07:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004477 on epoch=274
03/19/2022 05:07:32 - INFO - __main__ - Global step 550 Train loss 0.037288 Classification-F1 0.9372549019607843 on epoch=274
03/19/2022 05:07:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.105165 on epoch=279
03/19/2022 05:07:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004607 on epoch=284
03/19/2022 05:07:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.176096 on epoch=289
03/19/2022 05:07:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000056 on epoch=294
03/19/2022 05:08:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000038 on epoch=299
03/19/2022 05:08:04 - INFO - __main__ - Global step 600 Train loss 0.057192 Classification-F1 0.9372549019607843 on epoch=299
03/19/2022 05:08:04 - INFO - __main__ - save last model!
03/19/2022 05:08:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:08:05 - INFO - __main__ - Printing 3 examples
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:08:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:08:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:08:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:08:05 - INFO - __main__ - Printing 3 examples
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 05:08:05 - INFO - __main__ - ['positive']
03/19/2022 05:08:05 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:08:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:08:05 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:08:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:08:12 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:08:12 - INFO - __main__ - Printing 3 examples
03/19/2022 05:08:12 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:08:12 - INFO - __main__ - ['negative']
03/19/2022 05:08:12 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:08:12 - INFO - __main__ - ['negative']
03/19/2022 05:08:12 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:08:12 - INFO - __main__ - ['negative']
03/19/2022 05:08:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:08:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:08:18 - INFO - __main__ - Starting training!
03/19/2022 05:08:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:08:26 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:11:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_21_0.0003_8_predictions.txt
03/19/2022 05:11:10 - INFO - __main__ - Classification-F1 on test data: 0.9564
03/19/2022 05:11:11 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9564455579256333
03/19/2022 05:11:11 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0002, bsz=8 ...
03/19/2022 05:11:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:11:12 - INFO - __main__ - Printing 3 examples
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:11:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:11:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:11:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:11:12 - INFO - __main__ - Printing 3 examples
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 05:11:12 - INFO - __main__ - ['positive']
03/19/2022 05:11:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:11:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:11:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:11:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:11:25 - INFO - __main__ - Starting training!
03/19/2022 05:11:30 - INFO - __main__ - Step 10 Global step 10 Train loss 23.065794 on epoch=4
03/19/2022 05:11:37 - INFO - __main__ - Step 20 Global step 20 Train loss 19.218761 on epoch=9
03/19/2022 05:11:43 - INFO - __main__ - Step 30 Global step 30 Train loss 17.221134 on epoch=14
03/19/2022 05:11:49 - INFO - __main__ - Step 40 Global step 40 Train loss 16.066298 on epoch=19
03/19/2022 05:11:56 - INFO - __main__ - Step 50 Global step 50 Train loss 15.178558 on epoch=24
03/19/2022 05:12:08 - INFO - __main__ - Global step 50 Train loss 18.150110 Classification-F1 0.0 on epoch=24
03/19/2022 05:12:15 - INFO - __main__ - Step 60 Global step 60 Train loss 14.408873 on epoch=29
03/19/2022 05:12:21 - INFO - __main__ - Step 70 Global step 70 Train loss 13.802834 on epoch=34
03/19/2022 05:12:27 - INFO - __main__ - Step 80 Global step 80 Train loss 13.176405 on epoch=39
03/19/2022 05:12:34 - INFO - __main__ - Step 90 Global step 90 Train loss 13.298639 on epoch=44
03/19/2022 05:12:40 - INFO - __main__ - Step 100 Global step 100 Train loss 10.923406 on epoch=49
03/19/2022 05:12:54 - INFO - __main__ - Global step 100 Train loss 13.122031 Classification-F1 0.004273504273504273 on epoch=49
03/19/2022 05:13:01 - INFO - __main__ - Step 110 Global step 110 Train loss 3.816837 on epoch=54
03/19/2022 05:13:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.987559 on epoch=59
03/19/2022 05:13:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.425747 on epoch=64
03/19/2022 05:13:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.502011 on epoch=69
03/19/2022 05:13:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.409381 on epoch=74
03/19/2022 05:13:27 - INFO - __main__ - Global step 150 Train loss 1.228307 Classification-F1 0.3816425120772947 on epoch=74
03/19/2022 05:13:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.428491 on epoch=79
03/19/2022 05:13:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.386345 on epoch=84
03/19/2022 05:13:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.399416 on epoch=89
03/19/2022 05:13:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.424160 on epoch=94
03/19/2022 05:14:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.365107 on epoch=99
03/19/2022 05:14:06 - INFO - __main__ - Global step 200 Train loss 0.400704 Classification-F1 0.05755693581780538 on epoch=99
03/19/2022 05:14:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.343035 on epoch=104
03/19/2022 05:14:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.357678 on epoch=109
03/19/2022 05:14:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.390222 on epoch=114
03/19/2022 05:14:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.397128 on epoch=119
03/19/2022 05:14:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.371456 on epoch=124
03/19/2022 05:14:40 - INFO - __main__ - Global step 250 Train loss 0.371904 Classification-F1 0.19516483516483513 on epoch=124
03/19/2022 05:14:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.359969 on epoch=129
03/19/2022 05:14:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.342906 on epoch=134
03/19/2022 05:14:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.677620 on epoch=139
03/19/2022 05:15:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.297737 on epoch=144
03/19/2022 05:15:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.350678 on epoch=149
03/19/2022 05:15:13 - INFO - __main__ - Global step 300 Train loss 0.405782 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 05:15:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.350534 on epoch=154
03/19/2022 05:15:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.344348 on epoch=159
03/19/2022 05:15:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.359070 on epoch=164
03/19/2022 05:15:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.295741 on epoch=169
03/19/2022 05:15:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.289007 on epoch=174
03/19/2022 05:15:46 - INFO - __main__ - Global step 350 Train loss 0.327740 Classification-F1 0.25720720720720724 on epoch=174
03/19/2022 05:15:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.548631 on epoch=179
03/19/2022 05:15:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.252363 on epoch=184
03/19/2022 05:16:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.294716 on epoch=189
03/19/2022 05:16:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.268930 on epoch=194
03/19/2022 05:16:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.254723 on epoch=199
03/19/2022 05:16:19 - INFO - __main__ - Global step 400 Train loss 0.323872 Classification-F1 0.40641711229946526 on epoch=199
03/19/2022 05:16:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.267049 on epoch=204
03/19/2022 05:16:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.286660 on epoch=209
03/19/2022 05:16:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.198220 on epoch=214
03/19/2022 05:16:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.183169 on epoch=219
03/19/2022 05:16:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.213252 on epoch=224
03/19/2022 05:16:52 - INFO - __main__ - Global step 450 Train loss 0.229670 Classification-F1 0.6235294117647059 on epoch=224
03/19/2022 05:16:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.227982 on epoch=229
03/19/2022 05:17:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.215262 on epoch=234
03/19/2022 05:17:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.153144 on epoch=239
03/19/2022 05:17:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.188372 on epoch=244
03/19/2022 05:17:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.179377 on epoch=249
03/19/2022 05:17:25 - INFO - __main__ - Global step 500 Train loss 0.192827 Classification-F1 0.716256157635468 on epoch=249
03/19/2022 05:17:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.097229 on epoch=254
03/19/2022 05:17:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.098593 on epoch=259
03/19/2022 05:17:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086727 on epoch=264
03/19/2022 05:17:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.164738 on epoch=269
03/19/2022 05:17:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.227955 on epoch=274
03/19/2022 05:17:58 - INFO - __main__ - Global step 550 Train loss 0.135048 Classification-F1 0.3992490613266583 on epoch=274
03/19/2022 05:18:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.177193 on epoch=279
03/19/2022 05:18:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.114149 on epoch=284
03/19/2022 05:18:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.197320 on epoch=289
03/19/2022 05:18:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.082613 on epoch=294
03/19/2022 05:18:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.046920 on epoch=299
03/19/2022 05:18:31 - INFO - __main__ - Global step 600 Train loss 0.123639 Classification-F1 0.7793103448275862 on epoch=299
03/19/2022 05:18:31 - INFO - __main__ - save last model!
03/19/2022 05:18:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:18:31 - INFO - __main__ - Printing 3 examples
03/19/2022 05:18:31 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 05:18:31 - INFO - __main__ - ['positive']
03/19/2022 05:18:31 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 05:18:31 - INFO - __main__ - ['positive']
03/19/2022 05:18:31 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 05:18:31 - INFO - __main__ - ['positive']
03/19/2022 05:18:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:18:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:18:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:18:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:18:32 - INFO - __main__ - Printing 3 examples
03/19/2022 05:18:32 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 05:18:32 - INFO - __main__ - ['positive']
03/19/2022 05:18:32 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 05:18:32 - INFO - __main__ - ['positive']
03/19/2022 05:18:32 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 05:18:32 - INFO - __main__ - ['positive']
03/19/2022 05:18:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:18:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:18:32 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:18:38 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:18:39 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:18:39 - INFO - __main__ - Printing 3 examples
03/19/2022 05:18:39 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:18:39 - INFO - __main__ - ['negative']
03/19/2022 05:18:39 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:18:39 - INFO - __main__ - ['negative']
03/19/2022 05:18:39 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:18:39 - INFO - __main__ - ['negative']
03/19/2022 05:18:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:18:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:18:43 - INFO - __main__ - Starting training!
03/19/2022 05:18:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:18:52 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:21:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_21_0.0002_8_predictions.txt
03/19/2022 05:21:37 - INFO - __main__ - Classification-F1 on test data: 0.8352
03/19/2022 05:21:37 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.7793103448275862, test_performance=0.8352164159584325
03/19/2022 05:21:37 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0001, bsz=8 ...
03/19/2022 05:21:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:21:38 - INFO - __main__ - Printing 3 examples
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:21:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:21:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:21:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:21:38 - INFO - __main__ - Printing 3 examples
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/19/2022 05:21:38 - INFO - __main__ - ['positive']
03/19/2022 05:21:38 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:21:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:21:38 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:21:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:21:51 - INFO - __main__ - Starting training!
03/19/2022 05:21:57 - INFO - __main__ - Step 10 Global step 10 Train loss 23.817799 on epoch=4
03/19/2022 05:22:03 - INFO - __main__ - Step 20 Global step 20 Train loss 18.130354 on epoch=9
03/19/2022 05:22:09 - INFO - __main__ - Step 30 Global step 30 Train loss 18.526516 on epoch=14
03/19/2022 05:22:16 - INFO - __main__ - Step 40 Global step 40 Train loss 17.000025 on epoch=19
03/19/2022 05:22:22 - INFO - __main__ - Step 50 Global step 50 Train loss 17.070560 on epoch=24
03/19/2022 05:22:33 - INFO - __main__ - Global step 50 Train loss 18.909050 Classification-F1 0.0 on epoch=24
03/19/2022 05:22:40 - INFO - __main__ - Step 60 Global step 60 Train loss 15.906360 on epoch=29
03/19/2022 05:22:46 - INFO - __main__ - Step 70 Global step 70 Train loss 15.670329 on epoch=34
03/19/2022 05:22:53 - INFO - __main__ - Step 80 Global step 80 Train loss 15.152122 on epoch=39
03/19/2022 05:22:59 - INFO - __main__ - Step 90 Global step 90 Train loss 14.957959 on epoch=44
03/19/2022 05:23:05 - INFO - __main__ - Step 100 Global step 100 Train loss 13.790210 on epoch=49
03/19/2022 05:23:14 - INFO - __main__ - Global step 100 Train loss 15.095397 Classification-F1 0.0 on epoch=49
03/19/2022 05:23:21 - INFO - __main__ - Step 110 Global step 110 Train loss 13.858406 on epoch=54
03/19/2022 05:23:27 - INFO - __main__ - Step 120 Global step 120 Train loss 13.883451 on epoch=59
03/19/2022 05:23:33 - INFO - __main__ - Step 130 Global step 130 Train loss 13.656683 on epoch=64
03/19/2022 05:23:40 - INFO - __main__ - Step 140 Global step 140 Train loss 13.813522 on epoch=69
03/19/2022 05:23:46 - INFO - __main__ - Step 150 Global step 150 Train loss 12.843862 on epoch=74
03/19/2022 05:23:56 - INFO - __main__ - Global step 150 Train loss 13.611184 Classification-F1 0.0 on epoch=74
03/19/2022 05:24:03 - INFO - __main__ - Step 160 Global step 160 Train loss 12.109190 on epoch=79
03/19/2022 05:24:09 - INFO - __main__ - Step 170 Global step 170 Train loss 11.190024 on epoch=84
03/19/2022 05:24:15 - INFO - __main__ - Step 180 Global step 180 Train loss 10.864467 on epoch=89
03/19/2022 05:24:22 - INFO - __main__ - Step 190 Global step 190 Train loss 10.565975 on epoch=94
03/19/2022 05:24:28 - INFO - __main__ - Step 200 Global step 200 Train loss 8.967246 on epoch=99
03/19/2022 05:24:37 - INFO - __main__ - Global step 200 Train loss 10.739381 Classification-F1 0.0056022408963585435 on epoch=99
03/19/2022 05:24:44 - INFO - __main__ - Step 210 Global step 210 Train loss 7.249264 on epoch=104
03/19/2022 05:24:50 - INFO - __main__ - Step 220 Global step 220 Train loss 3.388799 on epoch=109
03/19/2022 05:24:56 - INFO - __main__ - Step 230 Global step 230 Train loss 2.606259 on epoch=114
03/19/2022 05:25:03 - INFO - __main__ - Step 240 Global step 240 Train loss 2.450350 on epoch=119
03/19/2022 05:25:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.754191 on epoch=124
03/19/2022 05:25:14 - INFO - __main__ - Global step 250 Train loss 3.489773 Classification-F1 0.04132231404958678 on epoch=124
03/19/2022 05:25:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.565219 on epoch=129
03/19/2022 05:25:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.525344 on epoch=134
03/19/2022 05:25:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.309514 on epoch=139
03/19/2022 05:25:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.365461 on epoch=144
03/19/2022 05:25:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.346980 on epoch=149
03/19/2022 05:25:47 - INFO - __main__ - Global step 300 Train loss 0.622503 Classification-F1 0.8745098039215686 on epoch=149
03/19/2022 05:25:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.288018 on epoch=154
03/19/2022 05:26:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.261406 on epoch=159
03/19/2022 05:26:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.326772 on epoch=164
03/19/2022 05:26:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.349700 on epoch=169
03/19/2022 05:26:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.259599 on epoch=174
03/19/2022 05:26:20 - INFO - __main__ - Global step 350 Train loss 0.297099 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 05:26:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.448156 on epoch=179
03/19/2022 05:26:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.271685 on epoch=184
03/19/2022 05:26:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.276448 on epoch=189
03/19/2022 05:26:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.234398 on epoch=194
03/19/2022 05:26:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.269994 on epoch=199
03/19/2022 05:26:54 - INFO - __main__ - Global step 400 Train loss 0.300136 Classification-F1 0.5717171717171717 on epoch=199
03/19/2022 05:27:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.278955 on epoch=204
03/19/2022 05:27:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.227080 on epoch=209
03/19/2022 05:27:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.283788 on epoch=214
03/19/2022 05:27:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.250931 on epoch=219
03/19/2022 05:27:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.225762 on epoch=224
03/19/2022 05:27:26 - INFO - __main__ - Global step 450 Train loss 0.253303 Classification-F1 0.906158357771261 on epoch=224
03/19/2022 05:27:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.233558 on epoch=229
03/19/2022 05:27:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.255782 on epoch=234
03/19/2022 05:27:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.294722 on epoch=239
03/19/2022 05:27:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.363795 on epoch=244
03/19/2022 05:27:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.236295 on epoch=249
03/19/2022 05:27:59 - INFO - __main__ - Global step 500 Train loss 0.276830 Classification-F1 0.9375 on epoch=249
03/19/2022 05:28:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.302736 on epoch=254
03/19/2022 05:28:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.243894 on epoch=259
03/19/2022 05:28:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.250094 on epoch=264
03/19/2022 05:28:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.247496 on epoch=269
03/19/2022 05:28:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.226141 on epoch=274
03/19/2022 05:28:32 - INFO - __main__ - Global step 550 Train loss 0.254072 Classification-F1 0.9372549019607843 on epoch=274
03/19/2022 05:28:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.221883 on epoch=279
03/19/2022 05:28:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.290785 on epoch=284
03/19/2022 05:28:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.286917 on epoch=289
03/19/2022 05:28:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.243669 on epoch=294
03/19/2022 05:29:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.270371 on epoch=299
03/19/2022 05:29:04 - INFO - __main__ - Global step 600 Train loss 0.262725 Classification-F1 0.9372549019607843 on epoch=299
03/19/2022 05:29:04 - INFO - __main__ - save last model!
03/19/2022 05:29:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:29:05 - INFO - __main__ - Printing 3 examples
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:29:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:29:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:29:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:29:05 - INFO - __main__ - Printing 3 examples
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:29:05 - INFO - __main__ - ['positive']
03/19/2022 05:29:05 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:29:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:29:05 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:29:11 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:29:12 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:29:12 - INFO - __main__ - Printing 3 examples
03/19/2022 05:29:12 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:29:12 - INFO - __main__ - ['negative']
03/19/2022 05:29:12 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:29:12 - INFO - __main__ - ['negative']
03/19/2022 05:29:12 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:29:12 - INFO - __main__ - ['negative']
03/19/2022 05:29:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:29:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:29:16 - INFO - __main__ - Starting training!
03/19/2022 05:29:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:29:26 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:32:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_21_0.0001_8_predictions.txt
03/19/2022 05:32:13 - INFO - __main__ - Classification-F1 on test data: 0.4627
03/19/2022 05:32:14 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.4626828265074024
03/19/2022 05:32:14 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0005, bsz=8 ...
03/19/2022 05:32:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:32:15 - INFO - __main__ - Printing 3 examples
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:32:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:32:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:32:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:32:15 - INFO - __main__ - Printing 3 examples
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:32:15 - INFO - __main__ - ['positive']
03/19/2022 05:32:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:32:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:32:15 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:32:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:32:26 - INFO - __main__ - Starting training!
03/19/2022 05:32:31 - INFO - __main__ - Step 10 Global step 10 Train loss 22.229599 on epoch=4
03/19/2022 05:32:37 - INFO - __main__ - Step 20 Global step 20 Train loss 18.160662 on epoch=9
03/19/2022 05:32:43 - INFO - __main__ - Step 30 Global step 30 Train loss 15.638596 on epoch=14
03/19/2022 05:32:49 - INFO - __main__ - Step 40 Global step 40 Train loss 13.741450 on epoch=19
03/19/2022 05:32:55 - INFO - __main__ - Step 50 Global step 50 Train loss 11.271267 on epoch=24
03/19/2022 05:33:01 - INFO - __main__ - Global step 50 Train loss 16.208315 Classification-F1 0.018518518518518517 on epoch=24
03/19/2022 05:33:08 - INFO - __main__ - Step 60 Global step 60 Train loss 7.130945 on epoch=29
03/19/2022 05:33:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.321344 on epoch=34
03/19/2022 05:33:20 - INFO - __main__ - Step 80 Global step 80 Train loss 2.641981 on epoch=39
03/19/2022 05:33:26 - INFO - __main__ - Step 90 Global step 90 Train loss 1.071441 on epoch=44
03/19/2022 05:33:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.848478 on epoch=49
03/19/2022 05:33:33 - INFO - __main__ - Global step 100 Train loss 2.802838 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 05:33:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.559268 on epoch=54
03/19/2022 05:33:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.142888 on epoch=59
03/19/2022 05:33:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.064708 on epoch=64
03/19/2022 05:33:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.011673 on epoch=69
03/19/2022 05:34:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.018310 on epoch=74
03/19/2022 05:34:05 - INFO - __main__ - Global step 150 Train loss 0.159369 Classification-F1 0.9375 on epoch=74
03/19/2022 05:34:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.026255 on epoch=79
03/19/2022 05:34:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.025186 on epoch=84
03/19/2022 05:34:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.003098 on epoch=89
03/19/2022 05:34:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.004243 on epoch=94
03/19/2022 05:34:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.207537 on epoch=99
03/19/2022 05:34:36 - INFO - __main__ - Global step 200 Train loss 0.053264 Classification-F1 0.9372549019607843 on epoch=99
03/19/2022 05:34:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.006690 on epoch=104
03/19/2022 05:34:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.012671 on epoch=109
03/19/2022 05:34:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.010461 on epoch=114
03/19/2022 05:35:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001258 on epoch=119
03/19/2022 05:35:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001273 on epoch=124
03/19/2022 05:35:08 - INFO - __main__ - Global step 250 Train loss 0.006471 Classification-F1 0.873015873015873 on epoch=124
03/19/2022 05:35:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003974 on epoch=129
03/19/2022 05:35:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000873 on epoch=134
03/19/2022 05:35:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002177 on epoch=139
03/19/2022 05:35:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000456 on epoch=144
03/19/2022 05:35:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000333 on epoch=149
03/19/2022 05:35:39 - INFO - __main__ - Global step 300 Train loss 0.001563 Classification-F1 0.9375 on epoch=149
03/19/2022 05:35:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000674 on epoch=154
03/19/2022 05:35:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000295 on epoch=159
03/19/2022 05:35:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000297 on epoch=164
03/19/2022 05:36:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000233 on epoch=169
03/19/2022 05:36:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004930 on epoch=174
03/19/2022 05:36:10 - INFO - __main__ - Global step 350 Train loss 0.001286 Classification-F1 0.9375 on epoch=174
03/19/2022 05:36:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001664 on epoch=179
03/19/2022 05:36:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000910 on epoch=184
03/19/2022 05:36:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000181 on epoch=189
03/19/2022 05:36:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000453 on epoch=194
03/19/2022 05:36:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333401 on epoch=199
03/19/2022 05:36:41 - INFO - __main__ - Global step 400 Train loss 0.067322 Classification-F1 0.9054187192118226 on epoch=199
03/19/2022 05:36:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.030384 on epoch=204
03/19/2022 05:36:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003007 on epoch=209
03/19/2022 05:36:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000551 on epoch=214
03/19/2022 05:37:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000326 on epoch=219
03/19/2022 05:37:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000167 on epoch=224
03/19/2022 05:37:12 - INFO - __main__ - Global step 450 Train loss 0.006887 Classification-F1 0.9375 on epoch=224
03/19/2022 05:37:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000239 on epoch=229
03/19/2022 05:37:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000134 on epoch=234
03/19/2022 05:37:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000187 on epoch=239
03/19/2022 05:37:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001615 on epoch=244
03/19/2022 05:37:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000307 on epoch=249
03/19/2022 05:37:44 - INFO - __main__ - Global step 500 Train loss 0.000496 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 05:37:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000091 on epoch=254
03/19/2022 05:37:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000070 on epoch=259
03/19/2022 05:38:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000075 on epoch=264
03/19/2022 05:38:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000091 on epoch=269
03/19/2022 05:38:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000087 on epoch=274
03/19/2022 05:38:15 - INFO - __main__ - Global step 550 Train loss 0.000083 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 05:38:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000112 on epoch=279
03/19/2022 05:38:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000168 on epoch=284
03/19/2022 05:38:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000071 on epoch=289
03/19/2022 05:38:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000041 on epoch=294
03/19/2022 05:38:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000064 on epoch=299
03/19/2022 05:38:47 - INFO - __main__ - Global step 600 Train loss 0.000091 Classification-F1 0.9375 on epoch=299
03/19/2022 05:38:47 - INFO - __main__ - save last model!
03/19/2022 05:38:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:38:48 - INFO - __main__ - Printing 3 examples
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:38:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:38:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:38:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:38:48 - INFO - __main__ - Printing 3 examples
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:38:48 - INFO - __main__ - ['positive']
03/19/2022 05:38:48 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:38:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:38:48 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:38:54 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:38:55 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:38:55 - INFO - __main__ - Printing 3 examples
03/19/2022 05:38:55 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:38:55 - INFO - __main__ - ['negative']
03/19/2022 05:38:55 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:38:55 - INFO - __main__ - ['negative']
03/19/2022 05:38:55 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:38:55 - INFO - __main__ - ['negative']
03/19/2022 05:38:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:39:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:39:00 - INFO - __main__ - Starting training!
03/19/2022 05:39:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:39:08 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:41:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_42_0.0005_8_predictions.txt
03/19/2022 05:41:50 - INFO - __main__ - Classification-F1 on test data: 0.9146
03/19/2022 05:41:51 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.9145757843302209
03/19/2022 05:41:51 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0003, bsz=8 ...
03/19/2022 05:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:41:52 - INFO - __main__ - Printing 3 examples
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:41:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:41:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:41:52 - INFO - __main__ - Printing 3 examples
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:41:52 - INFO - __main__ - ['positive']
03/19/2022 05:41:52 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:41:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:41:52 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:42:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:42:02 - INFO - __main__ - Starting training!
03/19/2022 05:42:08 - INFO - __main__ - Step 10 Global step 10 Train loss 23.460339 on epoch=4
03/19/2022 05:42:14 - INFO - __main__ - Step 20 Global step 20 Train loss 18.575138 on epoch=9
03/19/2022 05:42:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.741501 on epoch=14
03/19/2022 05:42:26 - INFO - __main__ - Step 40 Global step 40 Train loss 15.415495 on epoch=19
03/19/2022 05:42:32 - INFO - __main__ - Step 50 Global step 50 Train loss 15.550125 on epoch=24
03/19/2022 05:42:43 - INFO - __main__ - Global step 50 Train loss 17.948519 Classification-F1 0.0 on epoch=24
03/19/2022 05:42:49 - INFO - __main__ - Step 60 Global step 60 Train loss 14.438855 on epoch=29
03/19/2022 05:42:56 - INFO - __main__ - Step 70 Global step 70 Train loss 12.517298 on epoch=34
03/19/2022 05:43:02 - INFO - __main__ - Step 80 Global step 80 Train loss 11.292316 on epoch=39
03/19/2022 05:43:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.568943 on epoch=44
03/19/2022 05:43:14 - INFO - __main__ - Step 100 Global step 100 Train loss 5.313498 on epoch=49
03/19/2022 05:43:15 - INFO - __main__ - Global step 100 Train loss 10.426182 Classification-F1 0.13333333333333336 on epoch=49
03/19/2022 05:43:22 - INFO - __main__ - Step 110 Global step 110 Train loss 3.468857 on epoch=54
03/19/2022 05:43:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.719550 on epoch=59
03/19/2022 05:43:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.181697 on epoch=64
03/19/2022 05:43:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.103067 on epoch=69
03/19/2022 05:43:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.054019 on epoch=74
03/19/2022 05:43:48 - INFO - __main__ - Global step 150 Train loss 0.905438 Classification-F1 1.0 on epoch=74
03/19/2022 05:43:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.012641 on epoch=79
03/19/2022 05:44:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.814242 on epoch=84
03/19/2022 05:44:07 - INFO - __main__ - Step 180 Global step 180 Train loss 1.860721 on epoch=89
03/19/2022 05:44:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.133708 on epoch=94
03/19/2022 05:44:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.016315 on epoch=99
03/19/2022 05:44:20 - INFO - __main__ - Global step 200 Train loss 0.567525 Classification-F1 0.9687194525904204 on epoch=99
03/19/2022 05:44:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.020844 on epoch=104
03/19/2022 05:44:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004771 on epoch=109
03/19/2022 05:44:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003700 on epoch=114
03/19/2022 05:44:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001647 on epoch=119
03/19/2022 05:44:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.005064 on epoch=124
03/19/2022 05:44:52 - INFO - __main__ - Global step 250 Train loss 0.007205 Classification-F1 0.9372549019607843 on epoch=124
03/19/2022 05:44:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.041387 on epoch=129
03/19/2022 05:45:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007989 on epoch=134
03/19/2022 05:45:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001646 on epoch=139
03/19/2022 05:45:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001585 on epoch=144
03/19/2022 05:45:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001564 on epoch=149
03/19/2022 05:45:24 - INFO - __main__ - Global step 300 Train loss 0.010834 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 05:45:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003889 on epoch=154
03/19/2022 05:45:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000710 on epoch=159
03/19/2022 05:45:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004972 on epoch=164
03/19/2022 05:45:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.004188 on epoch=169
03/19/2022 05:45:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.005358 on epoch=174
03/19/2022 05:45:56 - INFO - __main__ - Global step 350 Train loss 0.003823 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 05:46:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000448 on epoch=179
03/19/2022 05:46:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.026867 on epoch=184
03/19/2022 05:46:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000778 on epoch=189
03/19/2022 05:46:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002042 on epoch=194
03/19/2022 05:46:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000688 on epoch=199
03/19/2022 05:46:28 - INFO - __main__ - Global step 400 Train loss 0.006165 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 05:46:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000684 on epoch=204
03/19/2022 05:46:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000202 on epoch=209
03/19/2022 05:46:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000430 on epoch=214
03/19/2022 05:46:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012532 on epoch=219
03/19/2022 05:46:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000459 on epoch=224
03/19/2022 05:47:00 - INFO - __main__ - Global step 450 Train loss 0.002862 Classification-F1 1.0 on epoch=224
03/19/2022 05:47:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000262 on epoch=229
03/19/2022 05:47:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000278 on epoch=234
03/19/2022 05:47:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000206 on epoch=239
03/19/2022 05:47:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000321 on epoch=244
03/19/2022 05:47:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000168 on epoch=249
03/19/2022 05:47:32 - INFO - __main__ - Global step 500 Train loss 0.000247 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 05:47:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000604 on epoch=254
03/19/2022 05:47:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000700 on epoch=259
03/19/2022 05:47:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000375 on epoch=264
03/19/2022 05:47:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000585 on epoch=269
03/19/2022 05:48:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000494 on epoch=274
03/19/2022 05:48:04 - INFO - __main__ - Global step 550 Train loss 0.000552 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 05:48:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001098 on epoch=279
03/19/2022 05:48:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000091 on epoch=284
03/19/2022 05:48:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.013197 on epoch=289
03/19/2022 05:48:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000377 on epoch=294
03/19/2022 05:48:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000804 on epoch=299
03/19/2022 05:48:36 - INFO - __main__ - Global step 600 Train loss 0.003113 Classification-F1 1.0 on epoch=299
03/19/2022 05:48:36 - INFO - __main__ - save last model!
03/19/2022 05:48:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:48:36 - INFO - __main__ - Printing 3 examples
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:48:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:48:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:48:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:48:36 - INFO - __main__ - Printing 3 examples
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:48:36 - INFO - __main__ - ['positive']
03/19/2022 05:48:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:48:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:48:36 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:48:42 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:48:43 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:48:43 - INFO - __main__ - Printing 3 examples
03/19/2022 05:48:43 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:48:43 - INFO - __main__ - ['negative']
03/19/2022 05:48:43 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:48:43 - INFO - __main__ - ['negative']
03/19/2022 05:48:43 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:48:43 - INFO - __main__ - ['negative']
03/19/2022 05:48:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:48:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:48:47 - INFO - __main__ - Starting training!
03/19/2022 05:48:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:48:57 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 05:51:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_42_0.0003_8_predictions.txt
03/19/2022 05:51:43 - INFO - __main__ - Classification-F1 on test data: 0.9553
03/19/2022 05:51:44 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.9552557180733445
03/19/2022 05:51:44 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0002, bsz=8 ...
03/19/2022 05:51:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:51:45 - INFO - __main__ - Printing 3 examples
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:51:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:51:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:51:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:51:45 - INFO - __main__ - Printing 3 examples
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:51:45 - INFO - __main__ - ['positive']
03/19/2022 05:51:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:51:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:51:45 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:51:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:51:58 - INFO - __main__ - Starting training!
03/19/2022 05:52:03 - INFO - __main__ - Step 10 Global step 10 Train loss 25.701365 on epoch=4
03/19/2022 05:52:09 - INFO - __main__ - Step 20 Global step 20 Train loss 19.662073 on epoch=9
03/19/2022 05:52:15 - INFO - __main__ - Step 30 Global step 30 Train loss 17.521709 on epoch=14
03/19/2022 05:52:21 - INFO - __main__ - Step 40 Global step 40 Train loss 15.774341 on epoch=19
03/19/2022 05:52:28 - INFO - __main__ - Step 50 Global step 50 Train loss 14.892940 on epoch=24
03/19/2022 05:52:39 - INFO - __main__ - Global step 50 Train loss 18.710485 Classification-F1 0.0 on epoch=24
03/19/2022 05:52:46 - INFO - __main__ - Step 60 Global step 60 Train loss 13.834857 on epoch=29
03/19/2022 05:52:52 - INFO - __main__ - Step 70 Global step 70 Train loss 13.019053 on epoch=34
03/19/2022 05:52:58 - INFO - __main__ - Step 80 Global step 80 Train loss 12.782738 on epoch=39
03/19/2022 05:53:04 - INFO - __main__ - Step 90 Global step 90 Train loss 11.789937 on epoch=44
03/19/2022 05:53:10 - INFO - __main__ - Step 100 Global step 100 Train loss 10.576070 on epoch=49
03/19/2022 05:53:15 - INFO - __main__ - Global step 100 Train loss 12.400531 Classification-F1 0.014705882352941176 on epoch=49
03/19/2022 05:53:21 - INFO - __main__ - Step 110 Global step 110 Train loss 7.197553 on epoch=54
03/19/2022 05:53:27 - INFO - __main__ - Step 120 Global step 120 Train loss 3.571910 on epoch=59
03/19/2022 05:53:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.436023 on epoch=64
03/19/2022 05:53:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.271191 on epoch=69
03/19/2022 05:53:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.471671 on epoch=74
03/19/2022 05:53:47 - INFO - __main__ - Global step 150 Train loss 2.389669 Classification-F1 0.9372549019607843 on epoch=74
03/19/2022 05:53:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.474816 on epoch=79
03/19/2022 05:53:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.414002 on epoch=84
03/19/2022 05:54:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.363756 on epoch=89
03/19/2022 05:54:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.137610 on epoch=94
03/19/2022 05:54:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.158878 on epoch=99
03/19/2022 05:54:18 - INFO - __main__ - Global step 200 Train loss 0.509812 Classification-F1 0.9687194525904204 on epoch=99
03/19/2022 05:54:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.083180 on epoch=104
03/19/2022 05:54:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.073873 on epoch=109
03/19/2022 05:54:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.091711 on epoch=114
03/19/2022 05:54:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.011840 on epoch=119
03/19/2022 05:54:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.058690 on epoch=124
03/19/2022 05:54:50 - INFO - __main__ - Global step 250 Train loss 0.063859 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 05:54:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.018476 on epoch=129
03/19/2022 05:55:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.017720 on epoch=134
03/19/2022 05:55:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003980 on epoch=139
03/19/2022 05:55:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002086 on epoch=144
03/19/2022 05:55:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.007650 on epoch=149
03/19/2022 05:55:22 - INFO - __main__ - Global step 300 Train loss 0.009982 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 05:55:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001469 on epoch=154
03/19/2022 05:55:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.031579 on epoch=159
03/19/2022 05:55:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.057977 on epoch=164
03/19/2022 05:55:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.019036 on epoch=169
03/19/2022 05:55:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000442 on epoch=174
03/19/2022 05:55:53 - INFO - __main__ - Global step 350 Train loss 0.022100 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 05:56:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001920 on epoch=179
03/19/2022 05:56:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.005985 on epoch=184
03/19/2022 05:56:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001553 on epoch=189
03/19/2022 05:56:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001011 on epoch=194
03/19/2022 05:56:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007872 on epoch=199
03/19/2022 05:56:25 - INFO - __main__ - Global step 400 Train loss 0.003668 Classification-F1 1.0 on epoch=199
03/19/2022 05:56:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000537 on epoch=204
03/19/2022 05:56:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000604 on epoch=209
03/19/2022 05:56:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000302 on epoch=214
03/19/2022 05:56:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001071 on epoch=219
03/19/2022 05:56:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000192 on epoch=224
03/19/2022 05:56:57 - INFO - __main__ - Global step 450 Train loss 0.000541 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 05:57:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020579 on epoch=229
03/19/2022 05:57:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000161 on epoch=234
03/19/2022 05:57:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001425 on epoch=239
03/19/2022 05:57:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001023 on epoch=244
03/19/2022 05:57:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000186 on epoch=249
03/19/2022 05:57:29 - INFO - __main__ - Global step 500 Train loss 0.004675 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 05:57:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000210 on epoch=254
03/19/2022 05:57:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000143 on epoch=259
03/19/2022 05:57:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000202 on epoch=264
03/19/2022 05:57:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000109 on epoch=269
03/19/2022 05:58:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000370 on epoch=274
03/19/2022 05:58:00 - INFO - __main__ - Global step 550 Train loss 0.000207 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 05:58:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000441 on epoch=279
03/19/2022 05:58:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000072 on epoch=284
03/19/2022 05:58:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.023125 on epoch=289
03/19/2022 05:58:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.037307 on epoch=294
03/19/2022 05:58:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000189 on epoch=299
03/19/2022 05:58:32 - INFO - __main__ - Global step 600 Train loss 0.012227 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 05:58:32 - INFO - __main__ - save last model!
03/19/2022 05:58:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:58:32 - INFO - __main__ - Printing 3 examples
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 05:58:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:58:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 05:58:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 05:58:32 - INFO - __main__ - Printing 3 examples
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 05:58:32 - INFO - __main__ - ['positive']
03/19/2022 05:58:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 05:58:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:58:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 05:58:39 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 05:58:40 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 05:58:40 - INFO - __main__ - Printing 3 examples
03/19/2022 05:58:40 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 05:58:40 - INFO - __main__ - ['negative']
03/19/2022 05:58:40 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 05:58:40 - INFO - __main__ - ['negative']
03/19/2022 05:58:40 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 05:58:40 - INFO - __main__ - ['negative']
03/19/2022 05:58:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 05:58:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 05:58:43 - INFO - __main__ - Starting training!
03/19/2022 05:58:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 05:58:54 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:01:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_42_0.0002_8_predictions.txt
03/19/2022 06:01:38 - INFO - __main__ - Classification-F1 on test data: 0.9548
03/19/2022 06:01:38 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9548302261612871
03/19/2022 06:01:38 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0001, bsz=8 ...
03/19/2022 06:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:01:39 - INFO - __main__ - Printing 3 examples
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:01:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:01:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:01:39 - INFO - __main__ - Printing 3 examples
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/19/2022 06:01:39 - INFO - __main__ - ['positive']
03/19/2022 06:01:39 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:01:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:01:39 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:01:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:01:50 - INFO - __main__ - Starting training!
03/19/2022 06:01:56 - INFO - __main__ - Step 10 Global step 10 Train loss 23.431227 on epoch=4
03/19/2022 06:02:02 - INFO - __main__ - Step 20 Global step 20 Train loss 20.214491 on epoch=9
03/19/2022 06:02:08 - INFO - __main__ - Step 30 Global step 30 Train loss 17.705877 on epoch=14
03/19/2022 06:02:14 - INFO - __main__ - Step 40 Global step 40 Train loss 17.225437 on epoch=19
03/19/2022 06:02:21 - INFO - __main__ - Step 50 Global step 50 Train loss 16.804588 on epoch=24
03/19/2022 06:02:34 - INFO - __main__ - Global step 50 Train loss 19.076326 Classification-F1 0.0 on epoch=24
03/19/2022 06:02:40 - INFO - __main__ - Step 60 Global step 60 Train loss 15.959699 on epoch=29
03/19/2022 06:02:46 - INFO - __main__ - Step 70 Global step 70 Train loss 15.410213 on epoch=34
03/19/2022 06:02:52 - INFO - __main__ - Step 80 Global step 80 Train loss 15.124631 on epoch=39
03/19/2022 06:02:59 - INFO - __main__ - Step 90 Global step 90 Train loss 14.461634 on epoch=44
03/19/2022 06:03:05 - INFO - __main__ - Step 100 Global step 100 Train loss 14.340212 on epoch=49
03/19/2022 06:03:17 - INFO - __main__ - Global step 100 Train loss 15.059278 Classification-F1 0.0 on epoch=49
03/19/2022 06:03:24 - INFO - __main__ - Step 110 Global step 110 Train loss 14.286154 on epoch=54
03/19/2022 06:03:30 - INFO - __main__ - Step 120 Global step 120 Train loss 13.550273 on epoch=59
03/19/2022 06:03:36 - INFO - __main__ - Step 130 Global step 130 Train loss 13.442592 on epoch=64
03/19/2022 06:03:42 - INFO - __main__ - Step 140 Global step 140 Train loss 13.253566 on epoch=69
03/19/2022 06:03:48 - INFO - __main__ - Step 150 Global step 150 Train loss 12.282158 on epoch=74
03/19/2022 06:04:01 - INFO - __main__ - Global step 150 Train loss 13.362949 Classification-F1 0.0 on epoch=74
03/19/2022 06:04:07 - INFO - __main__ - Step 160 Global step 160 Train loss 12.457760 on epoch=79
03/19/2022 06:04:13 - INFO - __main__ - Step 170 Global step 170 Train loss 11.771916 on epoch=84
03/19/2022 06:04:19 - INFO - __main__ - Step 180 Global step 180 Train loss 10.347776 on epoch=89
03/19/2022 06:04:25 - INFO - __main__ - Step 190 Global step 190 Train loss 9.284361 on epoch=94
03/19/2022 06:04:31 - INFO - __main__ - Step 200 Global step 200 Train loss 8.395842 on epoch=99
03/19/2022 06:04:43 - INFO - __main__ - Global step 200 Train loss 10.451530 Classification-F1 0.14814814814814814 on epoch=99
03/19/2022 06:04:50 - INFO - __main__ - Step 210 Global step 210 Train loss 4.602535 on epoch=104
03/19/2022 06:04:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.439025 on epoch=109
03/19/2022 06:05:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.568153 on epoch=114
03/19/2022 06:05:08 - INFO - __main__ - Step 240 Global step 240 Train loss 1.590576 on epoch=119
03/19/2022 06:05:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.093961 on epoch=124
03/19/2022 06:05:15 - INFO - __main__ - Global step 250 Train loss 2.258850 Classification-F1 0.9375 on epoch=124
03/19/2022 06:05:22 - INFO - __main__ - Step 260 Global step 260 Train loss 1.799839 on epoch=129
03/19/2022 06:05:28 - INFO - __main__ - Step 270 Global step 270 Train loss 1.683654 on epoch=134
03/19/2022 06:05:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.435587 on epoch=139
03/19/2022 06:05:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.197330 on epoch=144
03/19/2022 06:05:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.213099 on epoch=149
03/19/2022 06:05:47 - INFO - __main__ - Global step 300 Train loss 0.865902 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 06:05:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.144122 on epoch=154
03/19/2022 06:06:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.430988 on epoch=159
03/19/2022 06:06:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.475881 on epoch=164
03/19/2022 06:06:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.468017 on epoch=169
03/19/2022 06:06:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.653699 on epoch=174
03/19/2022 06:06:19 - INFO - __main__ - Global step 350 Train loss 0.434541 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 06:06:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.268224 on epoch=179
03/19/2022 06:06:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.190900 on epoch=184
03/19/2022 06:06:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.284659 on epoch=189
03/19/2022 06:06:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.446053 on epoch=194
03/19/2022 06:06:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.479809 on epoch=199
03/19/2022 06:06:50 - INFO - __main__ - Global step 400 Train loss 0.333929 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 06:06:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.315216 on epoch=204
03/19/2022 06:07:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.145984 on epoch=209
03/19/2022 06:07:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.276338 on epoch=214
03/19/2022 06:07:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.293144 on epoch=219
03/19/2022 06:07:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.099834 on epoch=224
03/19/2022 06:07:21 - INFO - __main__ - Global step 450 Train loss 0.226103 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 06:07:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.283262 on epoch=229
03/19/2022 06:07:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.109148 on epoch=234
03/19/2022 06:07:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.117989 on epoch=239
03/19/2022 06:07:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.040671 on epoch=244
03/19/2022 06:07:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053729 on epoch=249
03/19/2022 06:07:53 - INFO - __main__ - Global step 500 Train loss 0.120960 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 06:07:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.039128 on epoch=254
03/19/2022 06:08:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.051134 on epoch=259
03/19/2022 06:08:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.057514 on epoch=264
03/19/2022 06:08:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.034752 on epoch=269
03/19/2022 06:08:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.017384 on epoch=274
03/19/2022 06:08:25 - INFO - __main__ - Global step 550 Train loss 0.039982 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 06:08:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.053778 on epoch=279
03/19/2022 06:08:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.028958 on epoch=284
03/19/2022 06:08:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018075 on epoch=289
03/19/2022 06:08:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068642 on epoch=294
03/19/2022 06:08:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.042410 on epoch=299
03/19/2022 06:08:56 - INFO - __main__ - Global step 600 Train loss 0.042373 Classification-F1 0.9375 on epoch=299
03/19/2022 06:08:56 - INFO - __main__ - save last model!
03/19/2022 06:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:08:57 - INFO - __main__ - Printing 3 examples
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 06:08:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:08:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:08:57 - INFO - __main__ - Printing 3 examples
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:08:57 - INFO - __main__ - ['positive']
03/19/2022 06:08:57 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:08:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:08:57 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:09:03 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:09:04 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 06:09:04 - INFO - __main__ - Printing 3 examples
03/19/2022 06:09:04 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 06:09:04 - INFO - __main__ - ['negative']
03/19/2022 06:09:04 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 06:09:04 - INFO - __main__ - ['negative']
03/19/2022 06:09:04 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 06:09:04 - INFO - __main__ - ['negative']
03/19/2022 06:09:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:09:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:09:09 - INFO - __main__ - Starting training!
03/19/2022 06:09:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:09:18 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:12:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_42_0.0001_8_predictions.txt
03/19/2022 06:12:01 - INFO - __main__ - Classification-F1 on test data: 0.3177
03/19/2022 06:12:02 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.317672489853538
03/19/2022 06:12:02 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0005, bsz=8 ...
03/19/2022 06:12:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:12:03 - INFO - __main__ - Printing 3 examples
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:12:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:12:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:12:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:12:03 - INFO - __main__ - Printing 3 examples
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:12:03 - INFO - __main__ - ['positive']
03/19/2022 06:12:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:12:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:12:03 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:12:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:12:16 - INFO - __main__ - Starting training!
03/19/2022 06:12:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.295023 on epoch=4
03/19/2022 06:12:31 - INFO - __main__ - Step 20 Global step 20 Train loss 16.832111 on epoch=9
03/19/2022 06:12:37 - INFO - __main__ - Step 30 Global step 30 Train loss 14.708758 on epoch=14
03/19/2022 06:12:43 - INFO - __main__ - Step 40 Global step 40 Train loss 12.034437 on epoch=19
03/19/2022 06:12:49 - INFO - __main__ - Step 50 Global step 50 Train loss 9.616117 on epoch=24
03/19/2022 06:13:05 - INFO - __main__ - Global step 50 Train loss 15.297291 Classification-F1 0.0 on epoch=24
03/19/2022 06:13:11 - INFO - __main__ - Step 60 Global step 60 Train loss 3.935359 on epoch=29
03/19/2022 06:13:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.474516 on epoch=34
03/19/2022 06:13:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.342183 on epoch=39
03/19/2022 06:13:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.266720 on epoch=44
03/19/2022 06:13:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.224508 on epoch=49
03/19/2022 06:13:37 - INFO - __main__ - Global step 100 Train loss 1.048657 Classification-F1 0.8435972629521017 on epoch=49
03/19/2022 06:13:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.073764 on epoch=54
03/19/2022 06:13:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.148695 on epoch=59
03/19/2022 06:13:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.058448 on epoch=64
03/19/2022 06:14:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.004281 on epoch=69
03/19/2022 06:14:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.076984 on epoch=74
03/19/2022 06:14:09 - INFO - __main__ - Global step 150 Train loss 0.072434 Classification-F1 0.8745098039215686 on epoch=74
03/19/2022 06:14:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005640 on epoch=79
03/19/2022 06:14:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.002549 on epoch=84
03/19/2022 06:14:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.002183 on epoch=89
03/19/2022 06:14:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.002312 on epoch=94
03/19/2022 06:14:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001024 on epoch=99
03/19/2022 06:14:41 - INFO - __main__ - Global step 200 Train loss 0.002742 Classification-F1 0.906158357771261 on epoch=99
03/19/2022 06:14:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000933 on epoch=104
03/19/2022 06:14:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000449 on epoch=109
03/19/2022 06:15:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001638 on epoch=114
03/19/2022 06:15:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000586 on epoch=119
03/19/2022 06:15:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000298 on epoch=124
03/19/2022 06:15:13 - INFO - __main__ - Global step 250 Train loss 0.000781 Classification-F1 0.9375 on epoch=124
03/19/2022 06:15:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000607 on epoch=129
03/19/2022 06:15:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.021749 on epoch=134
03/19/2022 06:15:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.019590 on epoch=139
03/19/2022 06:15:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001474 on epoch=144
03/19/2022 06:15:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000544 on epoch=149
03/19/2022 06:15:45 - INFO - __main__ - Global step 300 Train loss 0.008793 Classification-F1 0.906158357771261 on epoch=149
03/19/2022 06:15:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000469 on epoch=154
03/19/2022 06:15:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000674 on epoch=159
03/19/2022 06:16:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000396 on epoch=164
03/19/2022 06:16:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000523 on epoch=169
03/19/2022 06:16:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000595 on epoch=174
03/19/2022 06:16:17 - INFO - __main__ - Global step 350 Train loss 0.000531 Classification-F1 0.8745098039215686 on epoch=174
03/19/2022 06:16:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.069719 on epoch=179
03/19/2022 06:16:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.232150 on epoch=184
03/19/2022 06:16:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.087419 on epoch=189
03/19/2022 06:16:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.017179 on epoch=194
03/19/2022 06:16:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.012645 on epoch=199
03/19/2022 06:16:49 - INFO - __main__ - Global step 400 Train loss 0.083822 Classification-F1 0.8125 on epoch=199
03/19/2022 06:16:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.064269 on epoch=204
03/19/2022 06:17:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.131253 on epoch=209
03/19/2022 06:17:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.010810 on epoch=214
03/19/2022 06:17:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004281 on epoch=219
03/19/2022 06:17:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001442 on epoch=224
03/19/2022 06:17:21 - INFO - __main__ - Global step 450 Train loss 0.042411 Classification-F1 0.8435972629521017 on epoch=224
03/19/2022 06:17:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000952 on epoch=229
03/19/2022 06:17:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000562 on epoch=234
03/19/2022 06:17:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001324 on epoch=239
03/19/2022 06:17:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.007596 on epoch=244
03/19/2022 06:17:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.064255 on epoch=249
03/19/2022 06:17:52 - INFO - __main__ - Global step 500 Train loss 0.014938 Classification-F1 0.7793103448275862 on epoch=249
03/19/2022 06:17:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011445 on epoch=254
03/19/2022 06:18:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000477 on epoch=259
03/19/2022 06:18:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001354 on epoch=264
03/19/2022 06:18:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001503 on epoch=269
03/19/2022 06:18:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001277 on epoch=274
03/19/2022 06:18:24 - INFO - __main__ - Global step 550 Train loss 0.003211 Classification-F1 0.8125 on epoch=274
03/19/2022 06:18:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000294 on epoch=279
03/19/2022 06:18:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000140 on epoch=284
03/19/2022 06:18:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004711 on epoch=289
03/19/2022 06:18:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000270 on epoch=294
03/19/2022 06:18:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006865 on epoch=299
03/19/2022 06:18:56 - INFO - __main__ - Global step 600 Train loss 0.002456 Classification-F1 0.8435972629521017 on epoch=299
03/19/2022 06:18:56 - INFO - __main__ - save last model!
03/19/2022 06:18:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:18:56 - INFO - __main__ - Printing 3 examples
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 06:18:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:18:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:18:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:18:56 - INFO - __main__ - Printing 3 examples
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:18:56 - INFO - __main__ - ['positive']
03/19/2022 06:18:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:18:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:18:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:19:02 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:19:03 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 06:19:03 - INFO - __main__ - Printing 3 examples
03/19/2022 06:19:03 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 06:19:03 - INFO - __main__ - ['negative']
03/19/2022 06:19:03 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 06:19:03 - INFO - __main__ - ['negative']
03/19/2022 06:19:03 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 06:19:03 - INFO - __main__ - ['negative']
03/19/2022 06:19:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:19:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:19:07 - INFO - __main__ - Starting training!
03/19/2022 06:19:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:19:17 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:22:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_87_0.0005_8_predictions.txt
03/19/2022 06:22:04 - INFO - __main__ - Classification-F1 on test data: 0.9389
03/19/2022 06:22:05 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.9389435629644507
03/19/2022 06:22:05 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0003, bsz=8 ...
03/19/2022 06:22:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:22:06 - INFO - __main__ - Printing 3 examples
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:22:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:22:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:22:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:22:06 - INFO - __main__ - Printing 3 examples
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:22:06 - INFO - __main__ - ['positive']
03/19/2022 06:22:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:22:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:22:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:22:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:22:19 - INFO - __main__ - Starting training!
03/19/2022 06:22:26 - INFO - __main__ - Step 10 Global step 10 Train loss 22.898594 on epoch=4
03/19/2022 06:22:32 - INFO - __main__ - Step 20 Global step 20 Train loss 17.528791 on epoch=9
03/19/2022 06:22:38 - INFO - __main__ - Step 30 Global step 30 Train loss 16.368593 on epoch=14
03/19/2022 06:22:45 - INFO - __main__ - Step 40 Global step 40 Train loss 14.821452 on epoch=19
03/19/2022 06:22:51 - INFO - __main__ - Step 50 Global step 50 Train loss 13.642047 on epoch=24
03/19/2022 06:22:55 - INFO - __main__ - Global step 50 Train loss 17.051895 Classification-F1 0.0 on epoch=24
03/19/2022 06:23:02 - INFO - __main__ - Step 60 Global step 60 Train loss 12.665302 on epoch=29
03/19/2022 06:23:08 - INFO - __main__ - Step 70 Global step 70 Train loss 11.179861 on epoch=34
03/19/2022 06:23:14 - INFO - __main__ - Step 80 Global step 80 Train loss 4.953536 on epoch=39
03/19/2022 06:23:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.517180 on epoch=44
03/19/2022 06:23:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.053532 on epoch=49
03/19/2022 06:23:27 - INFO - __main__ - Global step 100 Train loss 5.873882 Classification-F1 0.6125760649087221 on epoch=49
03/19/2022 06:23:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.121848 on epoch=54
03/19/2022 06:23:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.028342 on epoch=59
03/19/2022 06:23:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.003386 on epoch=64
03/19/2022 06:23:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.006123 on epoch=69
03/19/2022 06:23:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.002296 on epoch=74
03/19/2022 06:23:59 - INFO - __main__ - Global step 150 Train loss 0.032399 Classification-F1 0.6559139784946236 on epoch=74
03/19/2022 06:24:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.000942 on epoch=79
03/19/2022 06:24:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.019883 on epoch=84
03/19/2022 06:24:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.034680 on epoch=89
03/19/2022 06:24:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001322 on epoch=94
03/19/2022 06:24:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001987 on epoch=99
03/19/2022 06:24:31 - INFO - __main__ - Global step 200 Train loss 0.011763 Classification-F1 1.0 on epoch=99
03/19/2022 06:24:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001188 on epoch=104
03/19/2022 06:24:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000969 on epoch=109
03/19/2022 06:24:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001502 on epoch=114
03/19/2022 06:24:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.005788 on epoch=119
03/19/2022 06:25:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000245 on epoch=124
03/19/2022 06:25:02 - INFO - __main__ - Global step 250 Train loss 0.001938 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 06:25:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000171 on epoch=129
03/19/2022 06:25:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.099141 on epoch=134
03/19/2022 06:25:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002450 on epoch=139
03/19/2022 06:25:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.008471 on epoch=144
03/19/2022 06:25:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000639 on epoch=149
03/19/2022 06:25:33 - INFO - __main__ - Global step 300 Train loss 0.022174 Classification-F1 0.9372549019607843 on epoch=149
03/19/2022 06:25:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000286 on epoch=154
03/19/2022 06:25:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000437 on epoch=159
03/19/2022 06:25:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000354 on epoch=164
03/19/2022 06:25:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000503 on epoch=169
03/19/2022 06:26:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000197 on epoch=174
03/19/2022 06:26:04 - INFO - __main__ - Global step 350 Train loss 0.000356 Classification-F1 0.9372549019607843 on epoch=174
03/19/2022 06:26:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000132 on epoch=179
03/19/2022 06:26:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000123 on epoch=184
03/19/2022 06:26:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000085 on epoch=189
03/19/2022 06:26:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000096 on epoch=194
03/19/2022 06:26:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000157 on epoch=199
03/19/2022 06:26:35 - INFO - __main__ - Global step 400 Train loss 0.000119 Classification-F1 0.9372549019607843 on epoch=199
03/19/2022 06:26:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000131 on epoch=204
03/19/2022 06:26:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000099 on epoch=209
03/19/2022 06:26:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000052 on epoch=214
03/19/2022 06:26:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000093 on epoch=219
03/19/2022 06:27:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000825 on epoch=224
03/19/2022 06:27:06 - INFO - __main__ - Global step 450 Train loss 0.000240 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 06:27:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000350 on epoch=229
03/19/2022 06:27:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000115 on epoch=234
03/19/2022 06:27:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000036 on epoch=239
03/19/2022 06:27:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000068 on epoch=244
03/19/2022 06:27:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000993 on epoch=249
03/19/2022 06:27:37 - INFO - __main__ - Global step 500 Train loss 0.000312 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 06:27:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000190 on epoch=254
03/19/2022 06:27:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000045 on epoch=259
03/19/2022 06:27:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000033 on epoch=264
03/19/2022 06:28:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000040 on epoch=269
03/19/2022 06:28:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000016 on epoch=274
03/19/2022 06:28:08 - INFO - __main__ - Global step 550 Train loss 0.000065 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 06:28:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000034 on epoch=279
03/19/2022 06:28:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000011 on epoch=284
03/19/2022 06:28:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000023 on epoch=289
03/19/2022 06:28:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000018 on epoch=294
03/19/2022 06:28:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000035 on epoch=299
03/19/2022 06:28:39 - INFO - __main__ - Global step 600 Train loss 0.000024 Classification-F1 0.9372549019607843 on epoch=299
03/19/2022 06:28:39 - INFO - __main__ - save last model!
03/19/2022 06:28:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:28:40 - INFO - __main__ - Printing 3 examples
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 06:28:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:28:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:28:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:28:40 - INFO - __main__ - Printing 3 examples
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:28:40 - INFO - __main__ - ['positive']
03/19/2022 06:28:40 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:28:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:28:40 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:28:46 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:28:47 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 06:28:47 - INFO - __main__ - Printing 3 examples
03/19/2022 06:28:47 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 06:28:47 - INFO - __main__ - ['negative']
03/19/2022 06:28:47 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 06:28:47 - INFO - __main__ - ['negative']
03/19/2022 06:28:47 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 06:28:47 - INFO - __main__ - ['negative']
03/19/2022 06:28:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:28:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:28:51 - INFO - __main__ - Starting training!
03/19/2022 06:28:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:29:01 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:32:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_87_0.0003_8_predictions.txt
03/19/2022 06:32:15 - INFO - __main__ - Classification-F1 on test data: 0.1598
03/19/2022 06:32:17 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.15977575154846732
03/19/2022 06:32:17 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0002, bsz=8 ...
03/19/2022 06:32:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:32:18 - INFO - __main__ - Printing 3 examples
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:32:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:32:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:32:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:32:18 - INFO - __main__ - Printing 3 examples
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:32:18 - INFO - __main__ - ['positive']
03/19/2022 06:32:18 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:32:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:32:18 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:32:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:32:29 - INFO - __main__ - Starting training!
03/19/2022 06:32:34 - INFO - __main__ - Step 10 Global step 10 Train loss 23.768965 on epoch=4
03/19/2022 06:32:40 - INFO - __main__ - Step 20 Global step 20 Train loss 17.935154 on epoch=9
03/19/2022 06:32:46 - INFO - __main__ - Step 30 Global step 30 Train loss 16.290628 on epoch=14
03/19/2022 06:32:53 - INFO - __main__ - Step 40 Global step 40 Train loss 15.966875 on epoch=19
03/19/2022 06:32:59 - INFO - __main__ - Step 50 Global step 50 Train loss 15.522962 on epoch=24
03/19/2022 06:33:11 - INFO - __main__ - Global step 50 Train loss 17.896917 Classification-F1 0.0 on epoch=24
03/19/2022 06:33:17 - INFO - __main__ - Step 60 Global step 60 Train loss 14.730677 on epoch=29
03/19/2022 06:33:24 - INFO - __main__ - Step 70 Global step 70 Train loss 13.437895 on epoch=34
03/19/2022 06:33:30 - INFO - __main__ - Step 80 Global step 80 Train loss 13.135287 on epoch=39
03/19/2022 06:33:36 - INFO - __main__ - Step 90 Global step 90 Train loss 12.045431 on epoch=44
03/19/2022 06:33:42 - INFO - __main__ - Step 100 Global step 100 Train loss 10.799685 on epoch=49
03/19/2022 06:33:42 - INFO - __main__ - Global step 100 Train loss 12.829795 Classification-F1 0.0 on epoch=49
03/19/2022 06:33:48 - INFO - __main__ - Step 110 Global step 110 Train loss 9.320612 on epoch=54
03/19/2022 06:33:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.911997 on epoch=59
03/19/2022 06:34:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.636729 on epoch=64
03/19/2022 06:34:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.274779 on epoch=69
03/19/2022 06:34:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.020491 on epoch=74
03/19/2022 06:34:13 - INFO - __main__ - Global step 150 Train loss 2.432922 Classification-F1 0.6559139784946236 on epoch=74
03/19/2022 06:34:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.037456 on epoch=79
03/19/2022 06:34:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003737 on epoch=84
03/19/2022 06:34:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.011395 on epoch=89
03/19/2022 06:34:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.006590 on epoch=94
03/19/2022 06:34:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003071 on epoch=99
03/19/2022 06:34:45 - INFO - __main__ - Global step 200 Train loss 0.012450 Classification-F1 0.6559139784946236 on epoch=99
03/19/2022 06:34:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.004042 on epoch=104
03/19/2022 06:34:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002968 on epoch=109
03/19/2022 06:35:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.670138 on epoch=114
03/19/2022 06:35:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.509821 on epoch=119
03/19/2022 06:35:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.005272 on epoch=124
03/19/2022 06:35:15 - INFO - __main__ - Global step 250 Train loss 0.238448 Classification-F1 0.6559139784946236 on epoch=124
03/19/2022 06:35:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001428 on epoch=129
03/19/2022 06:35:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.036839 on epoch=134
03/19/2022 06:35:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.007065 on epoch=139
03/19/2022 06:35:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.081840 on epoch=144
03/19/2022 06:35:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.131198 on epoch=149
03/19/2022 06:35:46 - INFO - __main__ - Global step 300 Train loss 0.051674 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 06:35:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035061 on epoch=154
03/19/2022 06:35:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.032271 on epoch=159
03/19/2022 06:36:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.054988 on epoch=164
03/19/2022 06:36:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.096124 on epoch=169
03/19/2022 06:36:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.052626 on epoch=174
03/19/2022 06:36:17 - INFO - __main__ - Global step 350 Train loss 0.054214 Classification-F1 1.0 on epoch=174
03/19/2022 06:36:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.063624 on epoch=179
03/19/2022 06:36:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.276331 on epoch=184
03/19/2022 06:36:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.113349 on epoch=189
03/19/2022 06:36:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.094030 on epoch=194
03/19/2022 06:36:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.027402 on epoch=199
03/19/2022 06:36:48 - INFO - __main__ - Global step 400 Train loss 0.114947 Classification-F1 1.0 on epoch=199
03/19/2022 06:36:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.330847 on epoch=204
03/19/2022 06:37:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.077157 on epoch=209
03/19/2022 06:37:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.149402 on epoch=214
03/19/2022 06:37:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.094314 on epoch=219
03/19/2022 06:37:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066862 on epoch=224
03/19/2022 06:37:19 - INFO - __main__ - Global step 450 Train loss 0.143716 Classification-F1 0.9054187192118226 on epoch=224
03/19/2022 06:37:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.067808 on epoch=229
03/19/2022 06:37:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.046220 on epoch=234
03/19/2022 06:37:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.060586 on epoch=239
03/19/2022 06:37:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.051064 on epoch=244
03/19/2022 06:37:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.028208 on epoch=249
03/19/2022 06:37:50 - INFO - __main__ - Global step 500 Train loss 0.050777 Classification-F1 0.9054187192118226 on epoch=249
03/19/2022 06:37:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.056714 on epoch=254
03/19/2022 06:38:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.078501 on epoch=259
03/19/2022 06:38:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.061049 on epoch=264
03/19/2022 06:38:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.213245 on epoch=269
03/19/2022 06:38:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.020388 on epoch=274
03/19/2022 06:38:21 - INFO - __main__ - Global step 550 Train loss 0.085979 Classification-F1 1.0 on epoch=274
03/19/2022 06:38:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.034676 on epoch=279
03/19/2022 06:38:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017368 on epoch=284
03/19/2022 06:38:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.043022 on epoch=289
03/19/2022 06:38:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.033389 on epoch=294
03/19/2022 06:38:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.010686 on epoch=299
03/19/2022 06:38:52 - INFO - __main__ - Global step 600 Train loss 0.027828 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 06:38:52 - INFO - __main__ - save last model!
03/19/2022 06:38:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:38:52 - INFO - __main__ - Printing 3 examples
03/19/2022 06:38:52 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:38:52 - INFO - __main__ - ['positive']
03/19/2022 06:38:52 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:38:52 - INFO - __main__ - ['positive']
03/19/2022 06:38:52 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:38:52 - INFO - __main__ - ['positive']
03/19/2022 06:38:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 06:38:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:38:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:38:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:38:53 - INFO - __main__ - Printing 3 examples
03/19/2022 06:38:53 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:38:53 - INFO - __main__ - ['positive']
03/19/2022 06:38:53 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:38:53 - INFO - __main__ - ['positive']
03/19/2022 06:38:53 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:38:53 - INFO - __main__ - ['positive']
03/19/2022 06:38:53 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:38:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:38:53 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:38:59 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:38:59 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 06:38:59 - INFO - __main__ - Printing 3 examples
03/19/2022 06:38:59 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 06:38:59 - INFO - __main__ - ['negative']
03/19/2022 06:38:59 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 06:38:59 - INFO - __main__ - ['negative']
03/19/2022 06:38:59 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 06:38:59 - INFO - __main__ - ['negative']
03/19/2022 06:38:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:39:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:39:03 - INFO - __main__ - Starting training!
03/19/2022 06:39:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:39:13 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:41:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_87_0.0002_8_predictions.txt
03/19/2022 06:41:58 - INFO - __main__ - Classification-F1 on test data: 0.6446
03/19/2022 06:41:59 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.6446456747108301
03/19/2022 06:41:59 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0001, bsz=8 ...
03/19/2022 06:42:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:42:00 - INFO - __main__ - Printing 3 examples
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:42:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:42:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:42:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:42:00 - INFO - __main__ - Printing 3 examples
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/19/2022 06:42:00 - INFO - __main__ - ['positive']
03/19/2022 06:42:00 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:42:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:42:00 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:42:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:42:13 - INFO - __main__ - Starting training!
03/19/2022 06:42:19 - INFO - __main__ - Step 10 Global step 10 Train loss 22.673130 on epoch=4
03/19/2022 06:42:25 - INFO - __main__ - Step 20 Global step 20 Train loss 19.229366 on epoch=9
03/19/2022 06:42:31 - INFO - __main__ - Step 30 Global step 30 Train loss 18.238201 on epoch=14
03/19/2022 06:42:37 - INFO - __main__ - Step 40 Global step 40 Train loss 17.850704 on epoch=19
03/19/2022 06:42:43 - INFO - __main__ - Step 50 Global step 50 Train loss 17.097223 on epoch=24
03/19/2022 06:42:54 - INFO - __main__ - Global step 50 Train loss 19.017725 Classification-F1 0.0 on epoch=24
03/19/2022 06:43:01 - INFO - __main__ - Step 60 Global step 60 Train loss 16.261385 on epoch=29
03/19/2022 06:43:07 - INFO - __main__ - Step 70 Global step 70 Train loss 16.010738 on epoch=34
03/19/2022 06:43:13 - INFO - __main__ - Step 80 Global step 80 Train loss 15.384074 on epoch=39
03/19/2022 06:43:19 - INFO - __main__ - Step 90 Global step 90 Train loss 14.531021 on epoch=44
03/19/2022 06:43:25 - INFO - __main__ - Step 100 Global step 100 Train loss 14.855609 on epoch=49
03/19/2022 06:43:34 - INFO - __main__ - Global step 100 Train loss 15.408566 Classification-F1 0.0 on epoch=49
03/19/2022 06:43:40 - INFO - __main__ - Step 110 Global step 110 Train loss 14.344086 on epoch=54
03/19/2022 06:43:46 - INFO - __main__ - Step 120 Global step 120 Train loss 14.536428 on epoch=59
03/19/2022 06:43:52 - INFO - __main__ - Step 130 Global step 130 Train loss 13.442311 on epoch=64
03/19/2022 06:43:58 - INFO - __main__ - Step 140 Global step 140 Train loss 13.196417 on epoch=69
03/19/2022 06:44:04 - INFO - __main__ - Step 150 Global step 150 Train loss 13.339305 on epoch=74
03/19/2022 06:44:05 - INFO - __main__ - Global step 150 Train loss 13.771709 Classification-F1 0.0 on epoch=74
03/19/2022 06:44:11 - INFO - __main__ - Step 160 Global step 160 Train loss 12.978668 on epoch=79
03/19/2022 06:44:17 - INFO - __main__ - Step 170 Global step 170 Train loss 12.024139 on epoch=84
03/19/2022 06:44:23 - INFO - __main__ - Step 180 Global step 180 Train loss 11.096582 on epoch=89
03/19/2022 06:44:30 - INFO - __main__ - Step 190 Global step 190 Train loss 10.757922 on epoch=94
03/19/2022 06:44:36 - INFO - __main__ - Step 200 Global step 200 Train loss 10.044832 on epoch=99
03/19/2022 06:44:37 - INFO - __main__ - Global step 200 Train loss 11.380428 Classification-F1 0.0 on epoch=99
03/19/2022 06:44:44 - INFO - __main__ - Step 210 Global step 210 Train loss 9.767197 on epoch=104
03/19/2022 06:44:50 - INFO - __main__ - Step 220 Global step 220 Train loss 8.036098 on epoch=109
03/19/2022 06:44:56 - INFO - __main__ - Step 230 Global step 230 Train loss 5.522786 on epoch=114
03/19/2022 06:45:02 - INFO - __main__ - Step 240 Global step 240 Train loss 1.533545 on epoch=119
03/19/2022 06:45:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.841084 on epoch=124
03/19/2022 06:45:09 - INFO - __main__ - Global step 250 Train loss 5.140142 Classification-F1 0.6336917562724015 on epoch=124
03/19/2022 06:45:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.303476 on epoch=129
03/19/2022 06:45:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.416484 on epoch=134
03/19/2022 06:45:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.340092 on epoch=139
03/19/2022 06:45:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.092488 on epoch=144
03/19/2022 06:45:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.135896 on epoch=149
03/19/2022 06:45:40 - INFO - __main__ - Global step 300 Train loss 0.257687 Classification-F1 0.6343434343434343 on epoch=149
03/19/2022 06:45:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.085828 on epoch=154
03/19/2022 06:45:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003258 on epoch=159
03/19/2022 06:45:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004380 on epoch=164
03/19/2022 06:46:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.008710 on epoch=169
03/19/2022 06:46:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.047777 on epoch=174
03/19/2022 06:46:11 - INFO - __main__ - Global step 350 Train loss 0.029991 Classification-F1 0.6559139784946236 on epoch=174
03/19/2022 06:46:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.011713 on epoch=179
03/19/2022 06:46:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.002006 on epoch=184
03/19/2022 06:46:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.006033 on epoch=189
03/19/2022 06:46:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002767 on epoch=194
03/19/2022 06:46:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.019397 on epoch=199
03/19/2022 06:46:43 - INFO - __main__ - Global step 400 Train loss 0.008383 Classification-F1 0.6559139784946236 on epoch=199
03/19/2022 06:46:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.038094 on epoch=204
03/19/2022 06:46:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.107252 on epoch=209
03/19/2022 06:47:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.129561 on epoch=214
03/19/2022 06:47:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.090663 on epoch=219
03/19/2022 06:47:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001039 on epoch=224
03/19/2022 06:47:14 - INFO - __main__ - Global step 450 Train loss 0.073322 Classification-F1 0.6559139784946236 on epoch=224
03/19/2022 06:47:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.049762 on epoch=229
03/19/2022 06:47:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.038506 on epoch=234
03/19/2022 06:47:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.068782 on epoch=239
03/19/2022 06:47:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000618 on epoch=244
03/19/2022 06:47:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002030 on epoch=249
03/19/2022 06:47:45 - INFO - __main__ - Global step 500 Train loss 0.031940 Classification-F1 0.6559139784946236 on epoch=249
03/19/2022 06:47:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001042 on epoch=254
03/19/2022 06:47:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004043 on epoch=259
03/19/2022 06:48:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001366 on epoch=264
03/19/2022 06:48:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000477 on epoch=269
03/19/2022 06:48:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.064216 on epoch=274
03/19/2022 06:48:16 - INFO - __main__ - Global step 550 Train loss 0.014229 Classification-F1 0.6559139784946236 on epoch=274
03/19/2022 06:48:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.047061 on epoch=279
03/19/2022 06:48:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000380 on epoch=284
03/19/2022 06:48:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000411 on epoch=289
03/19/2022 06:48:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000657 on epoch=294
03/19/2022 06:48:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000282 on epoch=299
03/19/2022 06:48:46 - INFO - __main__ - Global step 600 Train loss 0.009758 Classification-F1 0.6343434343434343 on epoch=299
03/19/2022 06:48:46 - INFO - __main__ - save last model!
03/19/2022 06:48:54 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:48:54 - INFO - __main__ - Start tokenizing ... 7600 instances
03/19/2022 06:48:54 - INFO - __main__ - Printing 3 examples
03/19/2022 06:48:54 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/19/2022 06:48:54 - INFO - __main__ - ['negative']
03/19/2022 06:48:54 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/19/2022 06:48:54 - INFO - __main__ - ['negative']
03/19/2022 06:48:54 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/19/2022 06:48:54 - INFO - __main__ - ['negative']
03/19/2022 06:48:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:49:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:49:08 - INFO - __main__ - Loaded 7600 examples from test data
03/19/2022 06:51:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-yelp_polarity/yelp_polarity_16_87_0.0001_8_predictions.txt
03/19/2022 06:51:53 - INFO - __main__ - Classification-F1 on test data: 0.2388
03/19/2022 06:51:53 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.6559139784946236, test_performance=0.23883116731794998
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22252): No such process
Task: ethos-religion, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 06:51:59 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 06:51:59 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-ethos-religion
03/19/2022 06:51:59 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 06:51:59 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-ethos-religion
03/19/2022 06:52:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 06:52:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 06:52:00 - INFO - __main__ - args.device: cuda:0
03/19/2022 06:52:00 - INFO - __main__ - Using 2 gpus
03/19/2022 06:52:00 - INFO - __main__ - args.device: cuda:1
03/19/2022 06:52:00 - INFO - __main__ - Using 2 gpus
03/19/2022 06:52:00 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/19/2022 06:52:00 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/19/2022 06:52:05 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.0005, bsz=8 ...
03/19/2022 06:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:52:05 - INFO - __main__ - Printing 3 examples
03/19/2022 06:52:05 - INFO - __main__ - Printing 3 examples
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 06:52:05 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ - ['false']
03/19/2022 06:52:05 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:52:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:52:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:52:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:52:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:52:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:52:06 - INFO - __main__ - Printing 3 examples
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:52:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:52:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:52:06 - INFO - __main__ - Printing 3 examples
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 06:52:06 - INFO - __main__ - ['false']
03/19/2022 06:52:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:52:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:52:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:52:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:52:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:52:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:52:20 - INFO - __main__ - Starting training!
03/19/2022 06:52:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:52:20 - INFO - __main__ - Starting training!
03/19/2022 06:52:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.644018 on epoch=4
03/19/2022 06:52:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.532934 on epoch=9
03/19/2022 06:52:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.665661 on epoch=14
03/19/2022 06:52:40 - INFO - __main__ - Step 40 Global step 40 Train loss 14.218988 on epoch=19
03/19/2022 06:52:45 - INFO - __main__ - Step 50 Global step 50 Train loss 12.537258 on epoch=24
03/19/2022 06:52:56 - INFO - __main__ - Global step 50 Train loss 17.319773 Classification-F1 0.0 on epoch=24
03/19/2022 06:53:03 - INFO - __main__ - Step 60 Global step 60 Train loss 10.419994 on epoch=29
03/19/2022 06:53:08 - INFO - __main__ - Step 70 Global step 70 Train loss 6.205606 on epoch=34
03/19/2022 06:53:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.497198 on epoch=39
03/19/2022 06:53:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.417966 on epoch=44
03/19/2022 06:53:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.418897 on epoch=49
03/19/2022 06:53:23 - INFO - __main__ - Global step 100 Train loss 3.791932 Classification-F1 0.4589371980676329 on epoch=49
03/19/2022 06:53:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.269634 on epoch=54
03/19/2022 06:53:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.296758 on epoch=59
03/19/2022 06:53:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.471320 on epoch=64
03/19/2022 06:53:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.566514 on epoch=69
03/19/2022 06:53:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.372723 on epoch=74
03/19/2022 06:53:51 - INFO - __main__ - Global step 150 Train loss 0.395390 Classification-F1 0.6532019704433498 on epoch=74
03/19/2022 06:53:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.324135 on epoch=79
03/19/2022 06:54:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.321980 on epoch=84
03/19/2022 06:54:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.332974 on epoch=89
03/19/2022 06:54:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.344723 on epoch=94
03/19/2022 06:54:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.230937 on epoch=99
03/19/2022 06:54:17 - INFO - __main__ - Global step 200 Train loss 0.310950 Classification-F1 0.5844155844155844 on epoch=99
03/19/2022 06:54:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.371983 on epoch=104
03/19/2022 06:54:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.377785 on epoch=109
03/19/2022 06:54:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.234595 on epoch=114
03/19/2022 06:54:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.190496 on epoch=119
03/19/2022 06:54:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.290340 on epoch=124
03/19/2022 06:54:43 - INFO - __main__ - Global step 250 Train loss 0.293040 Classification-F1 0.805668016194332 on epoch=124
03/19/2022 06:54:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.248054 on epoch=129
03/19/2022 06:54:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.170423 on epoch=134
03/19/2022 06:54:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.237462 on epoch=139
03/19/2022 06:55:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.157733 on epoch=144
03/19/2022 06:55:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.124277 on epoch=149
03/19/2022 06:55:09 - INFO - __main__ - Global step 300 Train loss 0.187590 Classification-F1 0.7046153846153846 on epoch=149
03/19/2022 06:55:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.207011 on epoch=154
03/19/2022 06:55:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.184634 on epoch=159
03/19/2022 06:55:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.068423 on epoch=164
03/19/2022 06:55:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.207560 on epoch=169
03/19/2022 06:55:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.064392 on epoch=174
03/19/2022 06:55:35 - INFO - __main__ - Global step 350 Train loss 0.146404 Classification-F1 0.8398398398398398 on epoch=174
03/19/2022 06:55:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.037157 on epoch=179
03/19/2022 06:55:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.077800 on epoch=184
03/19/2022 06:55:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.116916 on epoch=189
03/19/2022 06:55:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.045165 on epoch=194
03/19/2022 06:56:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.025399 on epoch=199
03/19/2022 06:56:01 - INFO - __main__ - Global step 400 Train loss 0.060488 Classification-F1 0.7702564102564102 on epoch=199
03/19/2022 06:56:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.100349 on epoch=204
03/19/2022 06:56:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.048460 on epoch=209
03/19/2022 06:56:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.056263 on epoch=214
03/19/2022 06:56:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.018108 on epoch=219
03/19/2022 06:56:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.100809 on epoch=224
03/19/2022 06:56:27 - INFO - __main__ - Global step 450 Train loss 0.064798 Classification-F1 0.805668016194332 on epoch=224
03/19/2022 06:56:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.021797 on epoch=229
03/19/2022 06:56:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003487 on epoch=234
03/19/2022 06:56:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005437 on epoch=239
03/19/2022 06:56:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.010920 on epoch=244
03/19/2022 06:56:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013747 on epoch=249
03/19/2022 06:56:52 - INFO - __main__ - Global step 500 Train loss 0.011078 Classification-F1 0.7702564102564102 on epoch=249
03/19/2022 06:56:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.012093 on epoch=254
03/19/2022 06:57:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003971 on epoch=259
03/19/2022 06:57:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003028 on epoch=264
03/19/2022 06:57:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.017667 on epoch=269
03/19/2022 06:57:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001466 on epoch=274
03/19/2022 06:57:18 - INFO - __main__ - Global step 550 Train loss 0.007645 Classification-F1 0.8398398398398398 on epoch=274
03/19/2022 06:57:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.128028 on epoch=279
03/19/2022 06:57:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.009707 on epoch=284
03/19/2022 06:57:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005571 on epoch=289
03/19/2022 06:57:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000778 on epoch=294
03/19/2022 06:57:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.017038 on epoch=299
03/19/2022 06:57:44 - INFO - __main__ - Global step 600 Train loss 0.032225 Classification-F1 0.873015873015873 on epoch=299
03/19/2022 06:57:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:57:45 - INFO - __main__ - Printing 3 examples
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 06:57:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:57:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:57:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:57:45 - INFO - __main__ - Printing 3 examples
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 06:57:45 - INFO - __main__ - ['false']
03/19/2022 06:57:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:57:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:57:45 - INFO - __main__ - save last model!
03/19/2022 06:57:45 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:57:52 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 06:57:52 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 06:57:52 - INFO - __main__ - Printing 3 examples
03/19/2022 06:57:52 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 06:57:52 - INFO - __main__ - ['false']
03/19/2022 06:57:52 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 06:57:52 - INFO - __main__ - ['false']
03/19/2022 06:57:52 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 06:57:52 - INFO - __main__ - ['true']
03/19/2022 06:57:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:57:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:57:52 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 06:57:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_100_0.0005_8_predictions.txt
03/19/2022 06:57:54 - INFO - __main__ - Classification-F1 on test data: 0.7155
03/19/2022 06:57:55 - INFO - __main__ - prefix=ethos-religion_16_100, lr=0.0005, bsz=8, dev_performance=0.873015873015873, test_performance=0.7155172413793103
03/19/2022 06:57:55 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.0003, bsz=8 ...
03/19/2022 06:57:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:57:55 - INFO - __main__ - Starting training!
03/19/2022 06:57:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:57:55 - INFO - __main__ - Printing 3 examples
03/19/2022 06:57:55 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 06:57:55 - INFO - __main__ - ['false']
03/19/2022 06:57:55 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 06:57:55 - INFO - __main__ - ['false']
03/19/2022 06:57:55 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 06:57:55 - INFO - __main__ - ['false']
03/19/2022 06:57:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 06:57:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:57:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 06:57:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 06:57:56 - INFO - __main__ - Printing 3 examples
03/19/2022 06:57:56 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 06:57:56 - INFO - __main__ - ['false']
03/19/2022 06:57:56 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 06:57:56 - INFO - __main__ - ['false']
03/19/2022 06:57:56 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 06:57:56 - INFO - __main__ - ['false']
03/19/2022 06:57:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 06:57:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 06:57:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 06:58:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 06:58:08 - INFO - __main__ - Starting training!
03/19/2022 06:58:12 - INFO - __main__ - Step 10 Global step 10 Train loss 24.130690 on epoch=4
03/19/2022 06:58:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.818913 on epoch=9
03/19/2022 06:58:22 - INFO - __main__ - Step 30 Global step 30 Train loss 17.321766 on epoch=14
03/19/2022 06:58:27 - INFO - __main__ - Step 40 Global step 40 Train loss 16.249968 on epoch=19
03/19/2022 06:58:32 - INFO - __main__ - Step 50 Global step 50 Train loss 15.568344 on epoch=24
03/19/2022 06:58:33 - INFO - __main__ - Global step 50 Train loss 18.417936 Classification-F1 0.0 on epoch=24
03/19/2022 06:58:38 - INFO - __main__ - Step 60 Global step 60 Train loss 14.628596 on epoch=29
03/19/2022 06:58:43 - INFO - __main__ - Step 70 Global step 70 Train loss 13.690394 on epoch=34
03/19/2022 06:58:48 - INFO - __main__ - Step 80 Global step 80 Train loss 12.469644 on epoch=39
03/19/2022 06:58:53 - INFO - __main__ - Step 90 Global step 90 Train loss 10.983066 on epoch=44
03/19/2022 06:58:59 - INFO - __main__ - Step 100 Global step 100 Train loss 7.763310 on epoch=49
03/19/2022 06:58:59 - INFO - __main__ - Global step 100 Train loss 11.907003 Classification-F1 0.0 on epoch=49
03/19/2022 06:59:04 - INFO - __main__ - Step 110 Global step 110 Train loss 4.649158 on epoch=54
03/19/2022 06:59:09 - INFO - __main__ - Step 120 Global step 120 Train loss 2.102724 on epoch=59
03/19/2022 06:59:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.872189 on epoch=64
03/19/2022 06:59:19 - INFO - __main__ - Step 140 Global step 140 Train loss 1.365895 on epoch=69
03/19/2022 06:59:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.387635 on epoch=74
03/19/2022 06:59:25 - INFO - __main__ - Global step 150 Train loss 2.075520 Classification-F1 0.2238095238095238 on epoch=74
03/19/2022 06:59:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.740863 on epoch=79
03/19/2022 06:59:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.924181 on epoch=84
03/19/2022 06:59:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.421483 on epoch=89
03/19/2022 06:59:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.428723 on epoch=94
03/19/2022 06:59:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.437755 on epoch=99
03/19/2022 06:59:52 - INFO - __main__ - Global step 200 Train loss 0.590601 Classification-F1 0.37662337662337664 on epoch=99
03/19/2022 06:59:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.551433 on epoch=104
03/19/2022 07:00:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.408461 on epoch=109
03/19/2022 07:00:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.433109 on epoch=114
03/19/2022 07:00:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.429472 on epoch=119
03/19/2022 07:00:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.429450 on epoch=124
03/19/2022 07:00:18 - INFO - __main__ - Global step 250 Train loss 0.450385 Classification-F1 0.5076923076923077 on epoch=124
03/19/2022 07:00:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.353532 on epoch=129
03/19/2022 07:00:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.394773 on epoch=134
03/19/2022 07:00:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.375513 on epoch=139
03/19/2022 07:00:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.391758 on epoch=144
03/19/2022 07:00:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351155 on epoch=149
03/19/2022 07:00:45 - INFO - __main__ - Global step 300 Train loss 0.373346 Classification-F1 0.5076923076923077 on epoch=149
03/19/2022 07:00:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.399002 on epoch=154
03/19/2022 07:00:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.394329 on epoch=159
03/19/2022 07:01:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.340790 on epoch=164
03/19/2022 07:01:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.389017 on epoch=169
03/19/2022 07:01:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.392420 on epoch=174
03/19/2022 07:01:11 - INFO - __main__ - Global step 350 Train loss 0.383112 Classification-F1 0.4181818181818182 on epoch=174
03/19/2022 07:01:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.353120 on epoch=179
03/19/2022 07:01:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.403023 on epoch=184
03/19/2022 07:01:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.380754 on epoch=189
03/19/2022 07:01:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.318850 on epoch=194
03/19/2022 07:01:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.325784 on epoch=199
03/19/2022 07:01:37 - INFO - __main__ - Global step 400 Train loss 0.356306 Classification-F1 0.5076923076923077 on epoch=199
03/19/2022 07:01:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.341809 on epoch=204
03/19/2022 07:01:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.352731 on epoch=209
03/19/2022 07:01:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.349012 on epoch=214
03/19/2022 07:01:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.332016 on epoch=219
03/19/2022 07:02:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.328155 on epoch=224
03/19/2022 07:02:03 - INFO - __main__ - Global step 450 Train loss 0.340745 Classification-F1 0.6825396825396826 on epoch=224
03/19/2022 07:02:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.323803 on epoch=229
03/19/2022 07:02:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.423950 on epoch=234
03/19/2022 07:02:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.298515 on epoch=239
03/19/2022 07:02:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373796 on epoch=244
03/19/2022 07:02:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.249482 on epoch=249
03/19/2022 07:02:30 - INFO - __main__ - Global step 500 Train loss 0.333909 Classification-F1 0.7810361681329424 on epoch=249
03/19/2022 07:02:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.289224 on epoch=254
03/19/2022 07:02:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.304684 on epoch=259
03/19/2022 07:02:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.307918 on epoch=264
03/19/2022 07:02:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.218635 on epoch=269
03/19/2022 07:02:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.373556 on epoch=274
03/19/2022 07:02:56 - INFO - __main__ - Global step 550 Train loss 0.298803 Classification-F1 0.6945917285259808 on epoch=274
03/19/2022 07:03:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.213430 on epoch=279
03/19/2022 07:03:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.212328 on epoch=284
03/19/2022 07:03:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.145807 on epoch=289
03/19/2022 07:03:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.120450 on epoch=294
03/19/2022 07:03:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.251700 on epoch=299
03/19/2022 07:03:22 - INFO - __main__ - Global step 600 Train loss 0.188743 Classification-F1 0.8435972629521017 on epoch=299
03/19/2022 07:03:23 - INFO - __main__ - save last model!
03/19/2022 07:03:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:03:23 - INFO - __main__ - Printing 3 examples
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:03:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:03:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:03:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:03:23 - INFO - __main__ - Printing 3 examples
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 07:03:23 - INFO - __main__ - ['false']
03/19/2022 07:03:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:03:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:03:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:03:29 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:03:30 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:03:30 - INFO - __main__ - Printing 3 examples
03/19/2022 07:03:30 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:03:30 - INFO - __main__ - ['false']
03/19/2022 07:03:30 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:03:30 - INFO - __main__ - ['false']
03/19/2022 07:03:30 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:03:30 - INFO - __main__ - ['true']
03/19/2022 07:03:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:03:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:03:30 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:03:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_100_0.0003_8_predictions.txt
03/19/2022 07:03:31 - INFO - __main__ - Classification-F1 on test data: 0.5850
03/19/2022 07:03:32 - INFO - __main__ - prefix=ethos-religion_16_100, lr=0.0003, bsz=8, dev_performance=0.8435972629521017, test_performance=0.585016835016835
03/19/2022 07:03:32 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.0002, bsz=8 ...
03/19/2022 07:03:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:03:33 - INFO - __main__ - Printing 3 examples
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:03:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:03:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:03:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:03:33 - INFO - __main__ - Printing 3 examples
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 07:03:33 - INFO - __main__ - ['false']
03/19/2022 07:03:33 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:03:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:03:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:03:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:03:36 - INFO - __main__ - Starting training!
03/19/2022 07:03:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:03:46 - INFO - __main__ - Starting training!
03/19/2022 07:03:51 - INFO - __main__ - Step 10 Global step 10 Train loss 24.890932 on epoch=4
03/19/2022 07:03:56 - INFO - __main__ - Step 20 Global step 20 Train loss 20.853657 on epoch=9
03/19/2022 07:04:00 - INFO - __main__ - Step 30 Global step 30 Train loss 18.889225 on epoch=14
03/19/2022 07:04:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.043602 on epoch=19
03/19/2022 07:04:11 - INFO - __main__ - Step 50 Global step 50 Train loss 16.431980 on epoch=24
03/19/2022 07:04:22 - INFO - __main__ - Global step 50 Train loss 19.621881 Classification-F1 0.0 on epoch=24
03/19/2022 07:04:27 - INFO - __main__ - Step 60 Global step 60 Train loss 16.937763 on epoch=29
03/19/2022 07:04:32 - INFO - __main__ - Step 70 Global step 70 Train loss 14.681523 on epoch=34
03/19/2022 07:04:37 - INFO - __main__ - Step 80 Global step 80 Train loss 15.465128 on epoch=39
03/19/2022 07:04:42 - INFO - __main__ - Step 90 Global step 90 Train loss 13.877376 on epoch=44
03/19/2022 07:04:47 - INFO - __main__ - Step 100 Global step 100 Train loss 12.876279 on epoch=49
03/19/2022 07:04:55 - INFO - __main__ - Global step 100 Train loss 14.767613 Classification-F1 0.0 on epoch=49
03/19/2022 07:05:00 - INFO - __main__ - Step 110 Global step 110 Train loss 13.114386 on epoch=54
03/19/2022 07:05:05 - INFO - __main__ - Step 120 Global step 120 Train loss 11.738667 on epoch=59
03/19/2022 07:05:10 - INFO - __main__ - Step 130 Global step 130 Train loss 10.195073 on epoch=64
03/19/2022 07:05:15 - INFO - __main__ - Step 140 Global step 140 Train loss 7.576356 on epoch=69
03/19/2022 07:05:20 - INFO - __main__ - Step 150 Global step 150 Train loss 4.353635 on epoch=74
03/19/2022 07:05:20 - INFO - __main__ - Global step 150 Train loss 9.395622 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 07:05:26 - INFO - __main__ - Step 160 Global step 160 Train loss 3.911797 on epoch=79
03/19/2022 07:05:31 - INFO - __main__ - Step 170 Global step 170 Train loss 2.299452 on epoch=84
03/19/2022 07:05:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.565004 on epoch=89
03/19/2022 07:05:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.484078 on epoch=94
03/19/2022 07:05:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.477066 on epoch=99
03/19/2022 07:05:46 - INFO - __main__ - Global step 200 Train loss 1.547480 Classification-F1 0.5151515151515151 on epoch=99
03/19/2022 07:05:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.452000 on epoch=104
03/19/2022 07:05:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.458475 on epoch=109
03/19/2022 07:06:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.347975 on epoch=114
03/19/2022 07:06:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.408330 on epoch=119
03/19/2022 07:06:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.294625 on epoch=124
03/19/2022 07:06:12 - INFO - __main__ - Global step 250 Train loss 0.392281 Classification-F1 0.6761133603238867 on epoch=124
03/19/2022 07:06:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.240263 on epoch=129
03/19/2022 07:06:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.169036 on epoch=134
03/19/2022 07:06:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.171296 on epoch=139
03/19/2022 07:06:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.162940 on epoch=144
03/19/2022 07:06:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.187585 on epoch=149
03/19/2022 07:06:38 - INFO - __main__ - Global step 300 Train loss 0.186224 Classification-F1 0.7702564102564102 on epoch=149
03/19/2022 07:06:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.214812 on epoch=154
03/19/2022 07:06:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.114457 on epoch=159
03/19/2022 07:06:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.114727 on epoch=164
03/19/2022 07:06:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.092136 on epoch=169
03/19/2022 07:07:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.115155 on epoch=174
03/19/2022 07:07:04 - INFO - __main__ - Global step 350 Train loss 0.130258 Classification-F1 0.805668016194332 on epoch=174
03/19/2022 07:07:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.059940 on epoch=179
03/19/2022 07:07:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.114132 on epoch=184
03/19/2022 07:07:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.118514 on epoch=189
03/19/2022 07:07:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.060757 on epoch=194
03/19/2022 07:07:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.111529 on epoch=199
03/19/2022 07:07:29 - INFO - __main__ - Global step 400 Train loss 0.092974 Classification-F1 0.7702564102564102 on epoch=199
03/19/2022 07:07:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.124918 on epoch=204
03/19/2022 07:07:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.057219 on epoch=209
03/19/2022 07:07:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.075642 on epoch=214
03/19/2022 07:07:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.085770 on epoch=219
03/19/2022 07:07:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.065683 on epoch=224
03/19/2022 07:07:55 - INFO - __main__ - Global step 450 Train loss 0.081846 Classification-F1 0.8095238095238095 on epoch=224
03/19/2022 07:08:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.058292 on epoch=229
03/19/2022 07:08:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.021927 on epoch=234
03/19/2022 07:08:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.036080 on epoch=239
03/19/2022 07:08:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.058338 on epoch=244
03/19/2022 07:08:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.024260 on epoch=249
03/19/2022 07:08:20 - INFO - __main__ - Global step 500 Train loss 0.039779 Classification-F1 0.8745098039215686 on epoch=249
03/19/2022 07:08:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006576 on epoch=254
03/19/2022 07:08:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.044947 on epoch=259
03/19/2022 07:08:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.022331 on epoch=264
03/19/2022 07:08:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.037395 on epoch=269
03/19/2022 07:08:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.032077 on epoch=274
03/19/2022 07:08:46 - INFO - __main__ - Global step 550 Train loss 0.028665 Classification-F1 0.805668016194332 on epoch=274
03/19/2022 07:08:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.115445 on epoch=279
03/19/2022 07:08:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.041294 on epoch=284
03/19/2022 07:09:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.048581 on epoch=289
03/19/2022 07:09:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.073130 on epoch=294
03/19/2022 07:09:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009543 on epoch=299
03/19/2022 07:09:11 - INFO - __main__ - Global step 600 Train loss 0.057599 Classification-F1 0.8423645320197044 on epoch=299
03/19/2022 07:09:11 - INFO - __main__ - save last model!
03/19/2022 07:09:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:09:12 - INFO - __main__ - Printing 3 examples
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:09:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:09:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:09:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:09:12 - INFO - __main__ - Printing 3 examples
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 07:09:12 - INFO - __main__ - ['false']
03/19/2022 07:09:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:09:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:09:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:09:17 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:09:18 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:09:18 - INFO - __main__ - Printing 3 examples
03/19/2022 07:09:18 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:09:18 - INFO - __main__ - ['false']
03/19/2022 07:09:18 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:09:18 - INFO - __main__ - ['false']
03/19/2022 07:09:18 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:09:18 - INFO - __main__ - ['true']
03/19/2022 07:09:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:09:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:09:18 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:09:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_100_0.0002_8_predictions.txt
03/19/2022 07:09:21 - INFO - __main__ - Classification-F1 on test data: 0.8919
03/19/2022 07:09:21 - INFO - __main__ - prefix=ethos-religion_16_100, lr=0.0002, bsz=8, dev_performance=0.8745098039215686, test_performance=0.8918870939108823
03/19/2022 07:09:21 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.0001, bsz=8 ...
03/19/2022 07:09:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:09:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:09:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:09:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:09:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:09:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 07:09:22 - INFO - __main__ - ['false']
03/19/2022 07:09:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:09:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:09:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:09:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:09:25 - INFO - __main__ - Starting training!
03/19/2022 07:09:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:09:35 - INFO - __main__ - Starting training!
03/19/2022 07:09:39 - INFO - __main__ - Step 10 Global step 10 Train loss 22.815174 on epoch=4
03/19/2022 07:09:44 - INFO - __main__ - Step 20 Global step 20 Train loss 22.272003 on epoch=9
03/19/2022 07:09:49 - INFO - __main__ - Step 30 Global step 30 Train loss 19.922861 on epoch=14
03/19/2022 07:09:54 - INFO - __main__ - Step 40 Global step 40 Train loss 18.817230 on epoch=19
03/19/2022 07:09:59 - INFO - __main__ - Step 50 Global step 50 Train loss 18.012266 on epoch=24
03/19/2022 07:10:08 - INFO - __main__ - Global step 50 Train loss 20.367908 Classification-F1 0.0 on epoch=24
03/19/2022 07:10:14 - INFO - __main__ - Step 60 Global step 60 Train loss 17.406391 on epoch=29
03/19/2022 07:10:19 - INFO - __main__ - Step 70 Global step 70 Train loss 16.694735 on epoch=34
03/19/2022 07:10:23 - INFO - __main__ - Step 80 Global step 80 Train loss 17.435572 on epoch=39
03/19/2022 07:10:28 - INFO - __main__ - Step 90 Global step 90 Train loss 15.733124 on epoch=44
03/19/2022 07:10:33 - INFO - __main__ - Step 100 Global step 100 Train loss 15.728546 on epoch=49
03/19/2022 07:10:41 - INFO - __main__ - Global step 100 Train loss 16.599672 Classification-F1 0.0 on epoch=49
03/19/2022 07:10:46 - INFO - __main__ - Step 110 Global step 110 Train loss 15.178128 on epoch=54
03/19/2022 07:10:51 - INFO - __main__ - Step 120 Global step 120 Train loss 14.597986 on epoch=59
03/19/2022 07:10:56 - INFO - __main__ - Step 130 Global step 130 Train loss 15.177542 on epoch=64
03/19/2022 07:11:01 - INFO - __main__ - Step 140 Global step 140 Train loss 14.592900 on epoch=69
03/19/2022 07:11:06 - INFO - __main__ - Step 150 Global step 150 Train loss 13.967061 on epoch=74
03/19/2022 07:11:12 - INFO - __main__ - Global step 150 Train loss 14.702723 Classification-F1 0.0 on epoch=74
03/19/2022 07:11:17 - INFO - __main__ - Step 160 Global step 160 Train loss 13.877321 on epoch=79
03/19/2022 07:11:22 - INFO - __main__ - Step 170 Global step 170 Train loss 12.940119 on epoch=84
03/19/2022 07:11:27 - INFO - __main__ - Step 180 Global step 180 Train loss 12.656996 on epoch=89
03/19/2022 07:11:32 - INFO - __main__ - Step 190 Global step 190 Train loss 12.230856 on epoch=94
03/19/2022 07:11:37 - INFO - __main__ - Step 200 Global step 200 Train loss 11.738142 on epoch=99
03/19/2022 07:11:42 - INFO - __main__ - Global step 200 Train loss 12.688686 Classification-F1 0.0 on epoch=99
03/19/2022 07:11:47 - INFO - __main__ - Step 210 Global step 210 Train loss 10.167408 on epoch=104
03/19/2022 07:11:52 - INFO - __main__ - Step 220 Global step 220 Train loss 10.316961 on epoch=109
03/19/2022 07:11:57 - INFO - __main__ - Step 230 Global step 230 Train loss 8.015049 on epoch=114
03/19/2022 07:12:02 - INFO - __main__ - Step 240 Global step 240 Train loss 6.294802 on epoch=119
03/19/2022 07:12:07 - INFO - __main__ - Step 250 Global step 250 Train loss 5.547758 on epoch=124
03/19/2022 07:12:07 - INFO - __main__ - Global step 250 Train loss 8.068396 Classification-F1 0.0 on epoch=124
03/19/2022 07:12:12 - INFO - __main__ - Step 260 Global step 260 Train loss 2.629258 on epoch=129
03/19/2022 07:12:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.441125 on epoch=134
03/19/2022 07:12:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.995904 on epoch=139
03/19/2022 07:12:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.746446 on epoch=144
03/19/2022 07:12:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.668994 on epoch=149
03/19/2022 07:12:32 - INFO - __main__ - Global step 300 Train loss 1.296345 Classification-F1 0.6559139784946237 on epoch=149
03/19/2022 07:12:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.961002 on epoch=154
03/19/2022 07:12:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.654921 on epoch=159
03/19/2022 07:12:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.582864 on epoch=164
03/19/2022 07:12:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.743750 on epoch=169
03/19/2022 07:12:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.347819 on epoch=174
03/19/2022 07:12:58 - INFO - __main__ - Global step 350 Train loss 0.658071 Classification-F1 0.7408906882591093 on epoch=174
03/19/2022 07:13:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.581915 on epoch=179
03/19/2022 07:13:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.263230 on epoch=184
03/19/2022 07:13:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.376813 on epoch=189
03/19/2022 07:13:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.127526 on epoch=194
03/19/2022 07:13:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.179236 on epoch=199
03/19/2022 07:13:24 - INFO - __main__ - Global step 400 Train loss 0.305744 Classification-F1 0.875 on epoch=199
03/19/2022 07:13:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.131066 on epoch=204
03/19/2022 07:13:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.129754 on epoch=209
03/19/2022 07:13:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.133854 on epoch=214
03/19/2022 07:13:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.152546 on epoch=219
03/19/2022 07:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.139156 on epoch=224
03/19/2022 07:13:51 - INFO - __main__ - Global step 450 Train loss 0.137275 Classification-F1 0.8398398398398398 on epoch=224
03/19/2022 07:13:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.141408 on epoch=229
03/19/2022 07:14:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.150827 on epoch=234
03/19/2022 07:14:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.118728 on epoch=239
03/19/2022 07:14:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.094941 on epoch=244
03/19/2022 07:14:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.101180 on epoch=249
03/19/2022 07:14:16 - INFO - __main__ - Global step 500 Train loss 0.121417 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 07:14:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.117838 on epoch=254
03/19/2022 07:14:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.064861 on epoch=259
03/19/2022 07:14:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.094113 on epoch=264
03/19/2022 07:14:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.125257 on epoch=269
03/19/2022 07:14:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.252636 on epoch=274
03/19/2022 07:14:42 - INFO - __main__ - Global step 550 Train loss 0.130941 Classification-F1 0.8398398398398398 on epoch=274
03/19/2022 07:14:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.203687 on epoch=279
03/19/2022 07:14:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.188775 on epoch=284
03/19/2022 07:14:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.051891 on epoch=289
03/19/2022 07:15:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.105677 on epoch=294
03/19/2022 07:15:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.181666 on epoch=299
03/19/2022 07:15:07 - INFO - __main__ - Global step 600 Train loss 0.146339 Classification-F1 0.9054187192118226 on epoch=299
03/19/2022 07:15:07 - INFO - __main__ - save last model!
03/19/2022 07:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:15:08 - INFO - __main__ - Printing 3 examples
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:15:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:15:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:15:08 - INFO - __main__ - Printing 3 examples
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:15:08 - INFO - __main__ - ['false']
03/19/2022 07:15:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:15:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:15:08 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:15:14 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:15:15 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:15:15 - INFO - __main__ - Printing 3 examples
03/19/2022 07:15:15 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:15:15 - INFO - __main__ - ['false']
03/19/2022 07:15:15 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:15:15 - INFO - __main__ - ['false']
03/19/2022 07:15:15 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:15:15 - INFO - __main__ - ['true']
03/19/2022 07:15:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:15:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:15:15 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:15:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:15:19 - INFO - __main__ - Starting training!
03/19/2022 07:15:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_100_0.0001_8_predictions.txt
03/19/2022 07:15:21 - INFO - __main__ - Classification-F1 on test data: 0.8818
03/19/2022 07:15:21 - INFO - __main__ - prefix=ethos-religion_16_100, lr=0.0001, bsz=8, dev_performance=0.906158357771261, test_performance=0.8817934782608696
03/19/2022 07:15:21 - INFO - __main__ - Running ... prefix=ethos-religion_16_13, lr=0.0005, bsz=8 ...
03/19/2022 07:15:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:15:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:15:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:15:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:15:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:15:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:15:22 - INFO - __main__ - ['false']
03/19/2022 07:15:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:15:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:15:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:15:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:15:35 - INFO - __main__ - Starting training!
03/19/2022 07:15:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.981070 on epoch=4
03/19/2022 07:15:44 - INFO - __main__ - Step 20 Global step 20 Train loss 19.616611 on epoch=9
03/19/2022 07:15:48 - INFO - __main__ - Step 30 Global step 30 Train loss 16.105564 on epoch=14
03/19/2022 07:15:53 - INFO - __main__ - Step 40 Global step 40 Train loss 15.302455 on epoch=19
03/19/2022 07:15:58 - INFO - __main__ - Step 50 Global step 50 Train loss 13.291519 on epoch=24
03/19/2022 07:15:59 - INFO - __main__ - Global step 50 Train loss 17.859444 Classification-F1 0.0 on epoch=24
03/19/2022 07:16:04 - INFO - __main__ - Step 60 Global step 60 Train loss 10.666317 on epoch=29
03/19/2022 07:16:09 - INFO - __main__ - Step 70 Global step 70 Train loss 5.719255 on epoch=34
03/19/2022 07:16:14 - INFO - __main__ - Step 80 Global step 80 Train loss 2.363006 on epoch=39
03/19/2022 07:16:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.920892 on epoch=44
03/19/2022 07:16:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.940793 on epoch=49
03/19/2022 07:16:24 - INFO - __main__ - Global step 100 Train loss 4.122053 Classification-F1 0.6101882613510521 on epoch=49
03/19/2022 07:16:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.523058 on epoch=54
03/19/2022 07:16:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.314965 on epoch=59
03/19/2022 07:16:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.207391 on epoch=64
03/19/2022 07:16:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.174541 on epoch=69
03/19/2022 07:16:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.174308 on epoch=74
03/19/2022 07:16:51 - INFO - __main__ - Global step 150 Train loss 0.278853 Classification-F1 0.7793103448275862 on epoch=74
03/19/2022 07:16:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.259250 on epoch=79
03/19/2022 07:17:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.343131 on epoch=84
03/19/2022 07:17:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.153649 on epoch=89
03/19/2022 07:17:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.272291 on epoch=94
03/19/2022 07:17:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.103455 on epoch=99
03/19/2022 07:17:17 - INFO - __main__ - Global step 200 Train loss 0.226355 Classification-F1 0.8117647058823529 on epoch=99
03/19/2022 07:17:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.163658 on epoch=104
03/19/2022 07:17:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.108506 on epoch=109
03/19/2022 07:17:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.116364 on epoch=114
03/19/2022 07:17:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.183197 on epoch=119
03/19/2022 07:17:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.300278 on epoch=124
03/19/2022 07:17:43 - INFO - __main__ - Global step 250 Train loss 0.174400 Classification-F1 0.7793103448275862 on epoch=124
03/19/2022 07:17:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.165491 on epoch=129
03/19/2022 07:17:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.170206 on epoch=134
03/19/2022 07:17:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.187553 on epoch=139
03/19/2022 07:18:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.146741 on epoch=144
03/19/2022 07:18:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.192542 on epoch=149
03/19/2022 07:18:09 - INFO - __main__ - Global step 300 Train loss 0.172506 Classification-F1 0.8095238095238095 on epoch=149
03/19/2022 07:18:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.179997 on epoch=154
03/19/2022 07:18:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.128189 on epoch=159
03/19/2022 07:18:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.193833 on epoch=164
03/19/2022 07:18:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.275899 on epoch=169
03/19/2022 07:18:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.150718 on epoch=174
03/19/2022 07:18:34 - INFO - __main__ - Global step 350 Train loss 0.185727 Classification-F1 0.7046153846153846 on epoch=174
03/19/2022 07:18:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.139717 on epoch=179
03/19/2022 07:18:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.246282 on epoch=184
03/19/2022 07:18:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.168438 on epoch=189
03/19/2022 07:18:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.109647 on epoch=194
03/19/2022 07:19:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.151681 on epoch=199
03/19/2022 07:19:00 - INFO - __main__ - Global step 400 Train loss 0.163153 Classification-F1 0.8435972629521017 on epoch=199
03/19/2022 07:19:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.050909 on epoch=204
03/19/2022 07:19:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.043027 on epoch=209
03/19/2022 07:19:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.024418 on epoch=214
03/19/2022 07:19:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.029179 on epoch=219
03/19/2022 07:19:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.032717 on epoch=224
03/19/2022 07:19:26 - INFO - __main__ - Global step 450 Train loss 0.036050 Classification-F1 0.7408906882591093 on epoch=224
03/19/2022 07:19:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.037924 on epoch=229
03/19/2022 07:19:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.070220 on epoch=234
03/19/2022 07:19:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.049647 on epoch=239
03/19/2022 07:19:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.045829 on epoch=244
03/19/2022 07:19:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006362 on epoch=249
03/19/2022 07:19:52 - INFO - __main__ - Global step 500 Train loss 0.041996 Classification-F1 0.8745098039215686 on epoch=249
03/19/2022 07:19:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006113 on epoch=254
03/19/2022 07:20:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012645 on epoch=259
03/19/2022 07:20:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001183 on epoch=264
03/19/2022 07:20:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.438942 on epoch=269
03/19/2022 07:20:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.152077 on epoch=274
03/19/2022 07:20:19 - INFO - __main__ - Global step 550 Train loss 0.122192 Classification-F1 0.7333333333333334 on epoch=274
03/19/2022 07:20:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.326571 on epoch=279
03/19/2022 07:20:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.151947 on epoch=284
03/19/2022 07:20:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.191410 on epoch=289
03/19/2022 07:20:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.111447 on epoch=294
03/19/2022 07:20:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.138532 on epoch=299
03/19/2022 07:20:44 - INFO - __main__ - Global step 600 Train loss 0.183981 Classification-F1 0.5636363636363637 on epoch=299
03/19/2022 07:20:44 - INFO - __main__ - save last model!
03/19/2022 07:20:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:20:45 - INFO - __main__ - Printing 3 examples
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:20:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:20:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:20:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:20:45 - INFO - __main__ - Printing 3 examples
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:20:45 - INFO - __main__ - ['false']
03/19/2022 07:20:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:20:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:20:45 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:20:51 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:20:52 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:20:52 - INFO - __main__ - Printing 3 examples
03/19/2022 07:20:52 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:20:52 - INFO - __main__ - ['false']
03/19/2022 07:20:52 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:20:52 - INFO - __main__ - ['false']
03/19/2022 07:20:52 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:20:52 - INFO - __main__ - ['true']
03/19/2022 07:20:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:20:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:20:52 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:20:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_13_0.0005_8_predictions.txt
03/19/2022 07:20:53 - INFO - __main__ - Classification-F1 on test data: 0.7642
03/19/2022 07:20:54 - INFO - __main__ - prefix=ethos-religion_16_13, lr=0.0005, bsz=8, dev_performance=0.8745098039215686, test_performance=0.7642276422764227
03/19/2022 07:20:54 - INFO - __main__ - Running ... prefix=ethos-religion_16_13, lr=0.0003, bsz=8 ...
03/19/2022 07:20:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:20:54 - INFO - __main__ - Printing 3 examples
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:20:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:20:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:20:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:20:54 - INFO - __main__ - Printing 3 examples
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:20:54 - INFO - __main__ - ['false']
03/19/2022 07:20:54 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:20:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:20:54 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:20:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:20:58 - INFO - __main__ - Starting training!
03/19/2022 07:21:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:21:05 - INFO - __main__ - Starting training!
03/19/2022 07:21:10 - INFO - __main__ - Step 10 Global step 10 Train loss 23.975391 on epoch=4
03/19/2022 07:21:15 - INFO - __main__ - Step 20 Global step 20 Train loss 18.835880 on epoch=9
03/19/2022 07:21:20 - INFO - __main__ - Step 30 Global step 30 Train loss 17.399765 on epoch=14
03/19/2022 07:21:25 - INFO - __main__ - Step 40 Global step 40 Train loss 16.582157 on epoch=19
03/19/2022 07:21:30 - INFO - __main__ - Step 50 Global step 50 Train loss 14.927317 on epoch=24
03/19/2022 07:21:37 - INFO - __main__ - Global step 50 Train loss 18.344101 Classification-F1 0.0 on epoch=24
03/19/2022 07:21:43 - INFO - __main__ - Step 60 Global step 60 Train loss 13.595535 on epoch=29
03/19/2022 07:21:48 - INFO - __main__ - Step 70 Global step 70 Train loss 13.035082 on epoch=34
03/19/2022 07:21:53 - INFO - __main__ - Step 80 Global step 80 Train loss 12.233762 on epoch=39
03/19/2022 07:21:58 - INFO - __main__ - Step 90 Global step 90 Train loss 10.262708 on epoch=44
03/19/2022 07:22:03 - INFO - __main__ - Step 100 Global step 100 Train loss 5.668421 on epoch=49
03/19/2022 07:22:03 - INFO - __main__ - Global step 100 Train loss 10.959102 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 07:22:09 - INFO - __main__ - Step 110 Global step 110 Train loss 2.946848 on epoch=54
03/19/2022 07:22:14 - INFO - __main__ - Step 120 Global step 120 Train loss 2.901572 on epoch=59
03/19/2022 07:22:19 - INFO - __main__ - Step 130 Global step 130 Train loss 2.021463 on epoch=64
03/19/2022 07:22:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.789998 on epoch=69
03/19/2022 07:22:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.747075 on epoch=74
03/19/2022 07:22:29 - INFO - __main__ - Global step 150 Train loss 1.881391 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 07:22:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.530950 on epoch=79
03/19/2022 07:22:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.407991 on epoch=84
03/19/2022 07:22:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.408307 on epoch=89
03/19/2022 07:22:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.444908 on epoch=94
03/19/2022 07:22:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.421570 on epoch=99
03/19/2022 07:22:55 - INFO - __main__ - Global step 200 Train loss 0.442745 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 07:23:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.343168 on epoch=104
03/19/2022 07:23:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.454460 on epoch=109
03/19/2022 07:23:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.392213 on epoch=114
03/19/2022 07:23:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.365421 on epoch=119
03/19/2022 07:23:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.370670 on epoch=124
03/19/2022 07:23:20 - INFO - __main__ - Global step 250 Train loss 0.385187 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 07:23:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.387467 on epoch=129
03/19/2022 07:23:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.342061 on epoch=134
03/19/2022 07:23:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.331344 on epoch=139
03/19/2022 07:23:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.441404 on epoch=144
03/19/2022 07:23:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.401877 on epoch=149
03/19/2022 07:23:46 - INFO - __main__ - Global step 300 Train loss 0.380831 Classification-F1 0.3191489361702127 on epoch=149
03/19/2022 07:23:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.351101 on epoch=154
03/19/2022 07:23:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.392528 on epoch=159
03/19/2022 07:24:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.416554 on epoch=164
03/19/2022 07:24:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360282 on epoch=169
03/19/2022 07:24:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.388262 on epoch=174
03/19/2022 07:24:11 - INFO - __main__ - Global step 350 Train loss 0.381745 Classification-F1 0.4666666666666667 on epoch=174
03/19/2022 07:24:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.335989 on epoch=179
03/19/2022 07:24:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.364512 on epoch=184
03/19/2022 07:24:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342212 on epoch=189
03/19/2022 07:24:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.365343 on epoch=194
03/19/2022 07:24:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.382587 on epoch=199
03/19/2022 07:24:38 - INFO - __main__ - Global step 400 Train loss 0.358128 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 07:24:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.340820 on epoch=204
03/19/2022 07:24:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.380914 on epoch=209
03/19/2022 07:24:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.360170 on epoch=214
03/19/2022 07:24:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.326698 on epoch=219
03/19/2022 07:25:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.347555 on epoch=224
03/19/2022 07:25:04 - INFO - __main__ - Global step 450 Train loss 0.351231 Classification-F1 0.3816425120772947 on epoch=224
03/19/2022 07:25:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.297562 on epoch=229
03/19/2022 07:25:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.305780 on epoch=234
03/19/2022 07:25:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.259820 on epoch=239
03/19/2022 07:25:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.282053 on epoch=244
03/19/2022 07:25:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.252169 on epoch=249
03/19/2022 07:25:30 - INFO - __main__ - Global step 500 Train loss 0.279477 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 07:25:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.258699 on epoch=254
03/19/2022 07:25:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.276601 on epoch=259
03/19/2022 07:25:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.230323 on epoch=264
03/19/2022 07:25:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.270630 on epoch=269
03/19/2022 07:25:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.235247 on epoch=274
03/19/2022 07:25:57 - INFO - __main__ - Global step 550 Train loss 0.254300 Classification-F1 0.46843853820598 on epoch=274
03/19/2022 07:26:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.215433 on epoch=279
03/19/2022 07:26:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.228709 on epoch=284
03/19/2022 07:26:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.146540 on epoch=289
03/19/2022 07:26:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.110009 on epoch=294
03/19/2022 07:26:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.141145 on epoch=299
03/19/2022 07:26:23 - INFO - __main__ - Global step 600 Train loss 0.168367 Classification-F1 0.746031746031746 on epoch=299
03/19/2022 07:26:24 - INFO - __main__ - save last model!
03/19/2022 07:26:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:26:24 - INFO - __main__ - Printing 3 examples
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:26:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:26:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:26:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:26:24 - INFO - __main__ - Printing 3 examples
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:26:24 - INFO - __main__ - ['false']
03/19/2022 07:26:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:26:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:26:24 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:26:31 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:26:32 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:26:32 - INFO - __main__ - Printing 3 examples
03/19/2022 07:26:32 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:26:32 - INFO - __main__ - ['false']
03/19/2022 07:26:32 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:26:32 - INFO - __main__ - ['false']
03/19/2022 07:26:32 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:26:32 - INFO - __main__ - ['true']
03/19/2022 07:26:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:26:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:26:32 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:26:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_13_0.0003_8_predictions.txt
03/19/2022 07:26:33 - INFO - __main__ - Classification-F1 on test data: 0.7367
03/19/2022 07:26:34 - INFO - __main__ - prefix=ethos-religion_16_13, lr=0.0003, bsz=8, dev_performance=0.746031746031746, test_performance=0.736682808716707
03/19/2022 07:26:34 - INFO - __main__ - Running ... prefix=ethos-religion_16_13, lr=0.0002, bsz=8 ...
03/19/2022 07:26:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:26:35 - INFO - __main__ - Printing 3 examples
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:26:35 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:26:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:26:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:26:35 - INFO - __main__ - Printing 3 examples
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:26:35 - INFO - __main__ - ['false']
03/19/2022 07:26:35 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:26:35 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:26:35 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:26:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:26:37 - INFO - __main__ - Starting training!
03/19/2022 07:26:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:26:47 - INFO - __main__ - Starting training!
03/19/2022 07:26:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.897320 on epoch=4
03/19/2022 07:26:57 - INFO - __main__ - Step 20 Global step 20 Train loss 20.108633 on epoch=9
03/19/2022 07:27:03 - INFO - __main__ - Step 30 Global step 30 Train loss 18.132189 on epoch=14
03/19/2022 07:27:08 - INFO - __main__ - Step 40 Global step 40 Train loss 16.760403 on epoch=19
03/19/2022 07:27:13 - INFO - __main__ - Step 50 Global step 50 Train loss 16.144112 on epoch=24
03/19/2022 07:27:21 - INFO - __main__ - Global step 50 Train loss 18.808531 Classification-F1 0.0 on epoch=24
03/19/2022 07:27:27 - INFO - __main__ - Step 60 Global step 60 Train loss 15.232454 on epoch=29
03/19/2022 07:27:32 - INFO - __main__ - Step 70 Global step 70 Train loss 14.912374 on epoch=34
03/19/2022 07:27:37 - INFO - __main__ - Step 80 Global step 80 Train loss 13.536819 on epoch=39
03/19/2022 07:27:42 - INFO - __main__ - Step 90 Global step 90 Train loss 13.407529 on epoch=44
03/19/2022 07:27:47 - INFO - __main__ - Step 100 Global step 100 Train loss 12.377304 on epoch=49
03/19/2022 07:27:52 - INFO - __main__ - Global step 100 Train loss 13.893296 Classification-F1 0.0 on epoch=49
03/19/2022 07:27:57 - INFO - __main__ - Step 110 Global step 110 Train loss 11.198702 on epoch=54
03/19/2022 07:28:02 - INFO - __main__ - Step 120 Global step 120 Train loss 8.504575 on epoch=59
03/19/2022 07:28:07 - INFO - __main__ - Step 130 Global step 130 Train loss 4.569965 on epoch=64
03/19/2022 07:28:13 - INFO - __main__ - Step 140 Global step 140 Train loss 3.956327 on epoch=69
03/19/2022 07:28:18 - INFO - __main__ - Step 150 Global step 150 Train loss 3.995567 on epoch=74
03/19/2022 07:28:18 - INFO - __main__ - Global step 150 Train loss 6.445027 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 07:28:24 - INFO - __main__ - Step 160 Global step 160 Train loss 2.880246 on epoch=79
03/19/2022 07:28:29 - INFO - __main__ - Step 170 Global step 170 Train loss 2.337012 on epoch=84
03/19/2022 07:28:34 - INFO - __main__ - Step 180 Global step 180 Train loss 2.713237 on epoch=89
03/19/2022 07:28:39 - INFO - __main__ - Step 190 Global step 190 Train loss 2.595165 on epoch=94
03/19/2022 07:28:44 - INFO - __main__ - Step 200 Global step 200 Train loss 2.343862 on epoch=99
03/19/2022 07:28:44 - INFO - __main__ - Global step 200 Train loss 2.573905 Classification-F1 0.3655913978494623 on epoch=99
03/19/2022 07:28:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.850893 on epoch=104
03/19/2022 07:28:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.709119 on epoch=109
03/19/2022 07:29:00 - INFO - __main__ - Step 230 Global step 230 Train loss 1.034878 on epoch=114
03/19/2022 07:29:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.645163 on epoch=119
03/19/2022 07:29:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.493933 on epoch=124
03/19/2022 07:29:21 - INFO - __main__ - Global step 250 Train loss 0.746797 Classification-F1 0.23504273504273504 on epoch=124
03/19/2022 07:29:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.594798 on epoch=129
03/19/2022 07:29:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.477194 on epoch=134
03/19/2022 07:29:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.520422 on epoch=139
03/19/2022 07:29:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.513303 on epoch=144
03/19/2022 07:29:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.400915 on epoch=149
03/19/2022 07:29:46 - INFO - __main__ - Global step 300 Train loss 0.501326 Classification-F1 0.5333333333333333 on epoch=149
03/19/2022 07:29:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.429782 on epoch=154
03/19/2022 07:29:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.405555 on epoch=159
03/19/2022 07:30:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.368451 on epoch=164
03/19/2022 07:30:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.401695 on epoch=169
03/19/2022 07:30:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.439830 on epoch=174
03/19/2022 07:30:13 - INFO - __main__ - Global step 350 Train loss 0.409063 Classification-F1 0.5733333333333335 on epoch=174
03/19/2022 07:30:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.384549 on epoch=179
03/19/2022 07:30:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.362740 on epoch=184
03/19/2022 07:30:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.359074 on epoch=189
03/19/2022 07:30:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.320503 on epoch=194
03/19/2022 07:30:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.275470 on epoch=199
03/19/2022 07:30:39 - INFO - __main__ - Global step 400 Train loss 0.340467 Classification-F1 0.5076923076923077 on epoch=199
03/19/2022 07:30:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.351569 on epoch=204
03/19/2022 07:30:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.317009 on epoch=209
03/19/2022 07:30:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.312592 on epoch=214
03/19/2022 07:30:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.360220 on epoch=219
03/19/2022 07:31:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.354122 on epoch=224
03/19/2022 07:31:05 - INFO - __main__ - Global step 450 Train loss 0.339102 Classification-F1 0.5307917888563051 on epoch=224
03/19/2022 07:31:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.295405 on epoch=229
03/19/2022 07:31:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.380154 on epoch=234
03/19/2022 07:31:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.308167 on epoch=239
03/19/2022 07:31:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.342056 on epoch=244
03/19/2022 07:31:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276616 on epoch=249
03/19/2022 07:31:31 - INFO - __main__ - Global step 500 Train loss 0.320480 Classification-F1 0.5588547189819725 on epoch=249
03/19/2022 07:31:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.230542 on epoch=254
03/19/2022 07:31:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.210594 on epoch=259
03/19/2022 07:31:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.235457 on epoch=264
03/19/2022 07:31:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.283336 on epoch=269
03/19/2022 07:31:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.259508 on epoch=274
03/19/2022 07:31:57 - INFO - __main__ - Global step 550 Train loss 0.243887 Classification-F1 0.4589371980676329 on epoch=274
03/19/2022 07:32:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.151441 on epoch=279
03/19/2022 07:32:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.149227 on epoch=284
03/19/2022 07:32:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.112594 on epoch=289
03/19/2022 07:32:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.101694 on epoch=294
03/19/2022 07:32:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.080987 on epoch=299
03/19/2022 07:32:23 - INFO - __main__ - Global step 600 Train loss 0.119189 Classification-F1 0.873015873015873 on epoch=299
03/19/2022 07:32:23 - INFO - __main__ - save last model!
03/19/2022 07:32:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:32:23 - INFO - __main__ - Printing 3 examples
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:32:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:32:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:32:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:32:23 - INFO - __main__ - Printing 3 examples
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:32:23 - INFO - __main__ - ['false']
03/19/2022 07:32:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:32:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:32:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:32:30 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:32:31 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:32:31 - INFO - __main__ - Printing 3 examples
03/19/2022 07:32:31 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:32:31 - INFO - __main__ - ['false']
03/19/2022 07:32:31 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:32:31 - INFO - __main__ - ['false']
03/19/2022 07:32:31 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:32:31 - INFO - __main__ - ['true']
03/19/2022 07:32:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:32:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:32:31 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:32:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_13_0.0002_8_predictions.txt
03/19/2022 07:32:33 - INFO - __main__ - Classification-F1 on test data: 0.7414
03/19/2022 07:32:33 - INFO - __main__ - prefix=ethos-religion_16_13, lr=0.0002, bsz=8, dev_performance=0.873015873015873, test_performance=0.7413793103448276
03/19/2022 07:32:33 - INFO - __main__ - Running ... prefix=ethos-religion_16_13, lr=0.0001, bsz=8 ...
03/19/2022 07:32:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:32:34 - INFO - __main__ - Printing 3 examples
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:32:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:32:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:32:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:32:34 - INFO - __main__ - Printing 3 examples
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] Arrest that fat ass bitch!
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ -  [ethos-religion] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 07:32:34 - INFO - __main__ - ['false']
03/19/2022 07:32:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:32:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:32:34 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:32:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:32:34 - INFO - __main__ - Starting training!
03/19/2022 07:32:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:32:47 - INFO - __main__ - Starting training!
03/19/2022 07:32:51 - INFO - __main__ - Step 10 Global step 10 Train loss 22.804924 on epoch=4
03/19/2022 07:32:56 - INFO - __main__ - Step 20 Global step 20 Train loss 20.870062 on epoch=9
03/19/2022 07:33:01 - INFO - __main__ - Step 30 Global step 30 Train loss 18.212357 on epoch=14
03/19/2022 07:33:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.833309 on epoch=19
03/19/2022 07:33:11 - INFO - __main__ - Step 50 Global step 50 Train loss 18.036083 on epoch=24
03/19/2022 07:33:22 - INFO - __main__ - Global step 50 Train loss 19.551346 Classification-F1 0.0 on epoch=24
03/19/2022 07:33:27 - INFO - __main__ - Step 60 Global step 60 Train loss 17.030647 on epoch=29
03/19/2022 07:33:32 - INFO - __main__ - Step 70 Global step 70 Train loss 16.975010 on epoch=34
03/19/2022 07:33:38 - INFO - __main__ - Step 80 Global step 80 Train loss 16.176056 on epoch=39
03/19/2022 07:33:43 - INFO - __main__ - Step 90 Global step 90 Train loss 16.348454 on epoch=44
03/19/2022 07:33:48 - INFO - __main__ - Step 100 Global step 100 Train loss 15.461415 on epoch=49
03/19/2022 07:33:54 - INFO - __main__ - Global step 100 Train loss 16.398315 Classification-F1 0.0 on epoch=49
03/19/2022 07:33:59 - INFO - __main__ - Step 110 Global step 110 Train loss 15.243536 on epoch=54
03/19/2022 07:34:04 - INFO - __main__ - Step 120 Global step 120 Train loss 14.830203 on epoch=59
03/19/2022 07:34:09 - INFO - __main__ - Step 130 Global step 130 Train loss 14.409841 on epoch=64
03/19/2022 07:34:14 - INFO - __main__ - Step 140 Global step 140 Train loss 13.849707 on epoch=69
03/19/2022 07:34:19 - INFO - __main__ - Step 150 Global step 150 Train loss 13.414017 on epoch=74
03/19/2022 07:34:21 - INFO - __main__ - Global step 150 Train loss 14.349461 Classification-F1 0.0 on epoch=74
03/19/2022 07:34:26 - INFO - __main__ - Step 160 Global step 160 Train loss 13.662371 on epoch=79
03/19/2022 07:34:31 - INFO - __main__ - Step 170 Global step 170 Train loss 13.212619 on epoch=84
03/19/2022 07:34:36 - INFO - __main__ - Step 180 Global step 180 Train loss 12.181996 on epoch=89
03/19/2022 07:34:41 - INFO - __main__ - Step 190 Global step 190 Train loss 11.766690 on epoch=94
03/19/2022 07:34:46 - INFO - __main__ - Step 200 Global step 200 Train loss 10.402598 on epoch=99
03/19/2022 07:34:49 - INFO - __main__ - Global step 200 Train loss 12.245255 Classification-F1 0.0 on epoch=99
03/19/2022 07:34:54 - INFO - __main__ - Step 210 Global step 210 Train loss 9.368660 on epoch=104
03/19/2022 07:34:59 - INFO - __main__ - Step 220 Global step 220 Train loss 6.253477 on epoch=109
03/19/2022 07:35:04 - INFO - __main__ - Step 230 Global step 230 Train loss 5.551943 on epoch=114
03/19/2022 07:35:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.408500 on epoch=119
03/19/2022 07:35:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.465934 on epoch=124
03/19/2022 07:35:14 - INFO - __main__ - Global step 250 Train loss 4.809703 Classification-F1 0.4980392156862745 on epoch=124
03/19/2022 07:35:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.564287 on epoch=129
03/19/2022 07:35:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.497306 on epoch=134
03/19/2022 07:35:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.433818 on epoch=139
03/19/2022 07:35:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.373339 on epoch=144
03/19/2022 07:35:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.349001 on epoch=149
03/19/2022 07:35:40 - INFO - __main__ - Global step 300 Train loss 0.443550 Classification-F1 0.6761133603238867 on epoch=149
03/19/2022 07:35:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.362245 on epoch=154
03/19/2022 07:35:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.387123 on epoch=159
03/19/2022 07:35:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.398901 on epoch=164
03/19/2022 07:36:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.188460 on epoch=169
03/19/2022 07:36:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.269157 on epoch=174
03/19/2022 07:36:06 - INFO - __main__ - Global step 350 Train loss 0.321177 Classification-F1 0.6101882613510521 on epoch=174
03/19/2022 07:36:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.235097 on epoch=179
03/19/2022 07:36:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.221082 on epoch=184
03/19/2022 07:36:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.142603 on epoch=189
03/19/2022 07:36:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.193415 on epoch=194
03/19/2022 07:36:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.140664 on epoch=199
03/19/2022 07:36:32 - INFO - __main__ - Global step 400 Train loss 0.186572 Classification-F1 0.8435972629521017 on epoch=199
03/19/2022 07:36:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.177286 on epoch=204
03/19/2022 07:36:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.141608 on epoch=209
03/19/2022 07:36:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.106759 on epoch=214
03/19/2022 07:36:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.131724 on epoch=219
03/19/2022 07:36:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.135156 on epoch=224
03/19/2022 07:36:58 - INFO - __main__ - Global step 450 Train loss 0.138507 Classification-F1 0.8117647058823529 on epoch=224
03/19/2022 07:37:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.096788 on epoch=229
03/19/2022 07:37:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.064413 on epoch=234
03/19/2022 07:37:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.122083 on epoch=239
03/19/2022 07:37:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.081033 on epoch=244
03/19/2022 07:37:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112909 on epoch=249
03/19/2022 07:37:23 - INFO - __main__ - Global step 500 Train loss 0.095445 Classification-F1 0.8423645320197044 on epoch=249
03/19/2022 07:37:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.064787 on epoch=254
03/19/2022 07:37:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.074057 on epoch=259
03/19/2022 07:37:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.063232 on epoch=264
03/19/2022 07:37:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.047348 on epoch=269
03/19/2022 07:37:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.049788 on epoch=274
03/19/2022 07:37:49 - INFO - __main__ - Global step 550 Train loss 0.059842 Classification-F1 0.8745098039215686 on epoch=274
03/19/2022 07:37:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.080695 on epoch=279
03/19/2022 07:38:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.076899 on epoch=284
03/19/2022 07:38:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.051627 on epoch=289
03/19/2022 07:38:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.066353 on epoch=294
03/19/2022 07:38:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.050469 on epoch=299
03/19/2022 07:38:16 - INFO - __main__ - Global step 600 Train loss 0.065209 Classification-F1 0.8745098039215686 on epoch=299
03/19/2022 07:38:16 - INFO - __main__ - save last model!
03/19/2022 07:38:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:38:16 - INFO - __main__ - Printing 3 examples
03/19/2022 07:38:16 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:38:16 - INFO - __main__ - ['false']
03/19/2022 07:38:16 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:38:16 - INFO - __main__ - ['false']
03/19/2022 07:38:16 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:38:16 - INFO - __main__ - ['false']
03/19/2022 07:38:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:38:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:38:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:38:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:38:17 - INFO - __main__ - Printing 3 examples
03/19/2022 07:38:17 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:38:17 - INFO - __main__ - ['false']
03/19/2022 07:38:17 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:38:17 - INFO - __main__ - ['false']
03/19/2022 07:38:17 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:38:17 - INFO - __main__ - ['false']
03/19/2022 07:38:17 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:38:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:38:17 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:38:22 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:38:23 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:38:23 - INFO - __main__ - Printing 3 examples
03/19/2022 07:38:23 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:38:23 - INFO - __main__ - ['false']
03/19/2022 07:38:23 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:38:23 - INFO - __main__ - ['false']
03/19/2022 07:38:23 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:38:23 - INFO - __main__ - ['true']
03/19/2022 07:38:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:38:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:38:23 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:38:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_13_0.0001_8_predictions.txt
03/19/2022 07:38:25 - INFO - __main__ - Classification-F1 on test data: 0.8522
03/19/2022 07:38:25 - INFO - __main__ - prefix=ethos-religion_16_13, lr=0.0001, bsz=8, dev_performance=0.8745098039215686, test_performance=0.8522418478260869
03/19/2022 07:38:25 - INFO - __main__ - Running ... prefix=ethos-religion_16_21, lr=0.0005, bsz=8 ...
03/19/2022 07:38:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:38:26 - INFO - __main__ - Printing 3 examples
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:38:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:38:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:38:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:38:26 - INFO - __main__ - Printing 3 examples
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:38:26 - INFO - __main__ - ['false']
03/19/2022 07:38:26 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:38:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:38:26 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:38:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:38:29 - INFO - __main__ - Starting training!
03/19/2022 07:38:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:38:38 - INFO - __main__ - Starting training!
03/19/2022 07:38:42 - INFO - __main__ - Step 10 Global step 10 Train loss 24.006954 on epoch=4
03/19/2022 07:38:47 - INFO - __main__ - Step 20 Global step 20 Train loss 18.746990 on epoch=9
03/19/2022 07:38:52 - INFO - __main__ - Step 30 Global step 30 Train loss 15.420778 on epoch=14
03/19/2022 07:38:57 - INFO - __main__ - Step 40 Global step 40 Train loss 14.246088 on epoch=19
03/19/2022 07:39:02 - INFO - __main__ - Step 50 Global step 50 Train loss 12.298617 on epoch=24
03/19/2022 07:39:09 - INFO - __main__ - Global step 50 Train loss 16.943886 Classification-F1 0.0 on epoch=24
03/19/2022 07:39:15 - INFO - __main__ - Step 60 Global step 60 Train loss 6.973811 on epoch=29
03/19/2022 07:39:19 - INFO - __main__ - Step 70 Global step 70 Train loss 2.624721 on epoch=34
03/19/2022 07:39:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.201296 on epoch=39
03/19/2022 07:39:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.543954 on epoch=44
03/19/2022 07:39:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.381966 on epoch=49
03/19/2022 07:39:35 - INFO - __main__ - Global step 100 Train loss 2.345150 Classification-F1 0.3191489361702127 on epoch=49
03/19/2022 07:39:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.424633 on epoch=54
03/19/2022 07:39:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.448309 on epoch=59
03/19/2022 07:39:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.386918 on epoch=64
03/19/2022 07:39:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.392656 on epoch=69
03/19/2022 07:40:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.382782 on epoch=74
03/19/2022 07:40:01 - INFO - __main__ - Global step 150 Train loss 0.407059 Classification-F1 0.6559139784946237 on epoch=74
03/19/2022 07:40:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.350425 on epoch=79
03/19/2022 07:40:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.421305 on epoch=84
03/19/2022 07:40:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.359091 on epoch=89
03/19/2022 07:40:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.296497 on epoch=94
03/19/2022 07:40:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.288049 on epoch=99
03/19/2022 07:40:27 - INFO - __main__ - Global step 200 Train loss 0.343073 Classification-F1 0.8117647058823529 on epoch=99
03/19/2022 07:40:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.194465 on epoch=104
03/19/2022 07:40:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.237508 on epoch=109
03/19/2022 07:40:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.200141 on epoch=114
03/19/2022 07:40:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.137832 on epoch=119
03/19/2022 07:40:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.098901 on epoch=124
03/19/2022 07:40:53 - INFO - __main__ - Global step 250 Train loss 0.173769 Classification-F1 0.7408906882591093 on epoch=124
03/19/2022 07:40:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.054473 on epoch=129
03/19/2022 07:41:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.038127 on epoch=134
03/19/2022 07:41:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.371049 on epoch=139
03/19/2022 07:41:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.074664 on epoch=144
03/19/2022 07:41:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.030086 on epoch=149
03/19/2022 07:41:19 - INFO - __main__ - Global step 300 Train loss 0.113680 Classification-F1 0.906158357771261 on epoch=149
03/19/2022 07:41:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.018279 on epoch=154
03/19/2022 07:41:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040066 on epoch=159
03/19/2022 07:41:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.031959 on epoch=164
03/19/2022 07:41:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.115628 on epoch=169
03/19/2022 07:41:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009925 on epoch=174
03/19/2022 07:41:45 - INFO - __main__ - Global step 350 Train loss 0.043172 Classification-F1 0.9375 on epoch=174
03/19/2022 07:41:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.022277 on epoch=179
03/19/2022 07:41:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.012948 on epoch=184
03/19/2022 07:42:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.126543 on epoch=189
03/19/2022 07:42:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.017869 on epoch=194
03/19/2022 07:42:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002811 on epoch=199
03/19/2022 07:42:12 - INFO - __main__ - Global step 400 Train loss 0.036490 Classification-F1 0.9375 on epoch=199
03/19/2022 07:42:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001322 on epoch=204
03/19/2022 07:42:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000769 on epoch=209
03/19/2022 07:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001514 on epoch=214
03/19/2022 07:42:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.525684 on epoch=219
03/19/2022 07:42:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008899 on epoch=224
03/19/2022 07:42:38 - INFO - __main__ - Global step 450 Train loss 0.107637 Classification-F1 0.8423645320197044 on epoch=224
03/19/2022 07:42:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002754 on epoch=229
03/19/2022 07:42:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001031 on epoch=234
03/19/2022 07:42:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.021228 on epoch=239
03/19/2022 07:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.160843 on epoch=244
03/19/2022 07:43:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.121999 on epoch=249
03/19/2022 07:43:03 - INFO - __main__ - Global step 500 Train loss 0.061571 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 07:43:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006582 on epoch=254
03/19/2022 07:43:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.006620 on epoch=259
03/19/2022 07:43:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000908 on epoch=264
03/19/2022 07:43:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000234 on epoch=269
03/19/2022 07:43:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001109 on epoch=274
03/19/2022 07:43:29 - INFO - __main__ - Global step 550 Train loss 0.003091 Classification-F1 0.9375 on epoch=274
03/19/2022 07:43:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002061 on epoch=279
03/19/2022 07:43:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000688 on epoch=284
03/19/2022 07:43:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.187780 on epoch=289
03/19/2022 07:43:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006950 on epoch=294
03/19/2022 07:43:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004750 on epoch=299
03/19/2022 07:43:55 - INFO - __main__ - Global step 600 Train loss 0.040446 Classification-F1 0.9375 on epoch=299
03/19/2022 07:43:55 - INFO - __main__ - save last model!
03/19/2022 07:43:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:43:56 - INFO - __main__ - Printing 3 examples
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:43:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:43:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:43:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:43:56 - INFO - __main__ - Printing 3 examples
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:43:56 - INFO - __main__ - ['false']
03/19/2022 07:43:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:43:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:43:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:44:02 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:44:03 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:44:03 - INFO - __main__ - Printing 3 examples
03/19/2022 07:44:03 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:44:03 - INFO - __main__ - ['false']
03/19/2022 07:44:03 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:44:03 - INFO - __main__ - ['false']
03/19/2022 07:44:03 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:44:03 - INFO - __main__ - ['true']
03/19/2022 07:44:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:44:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:44:03 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:44:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_21_0.0005_8_predictions.txt
03/19/2022 07:44:04 - INFO - __main__ - Classification-F1 on test data: 0.7752
03/19/2022 07:44:05 - INFO - __main__ - prefix=ethos-religion_16_21, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.7751937984496124
03/19/2022 07:44:05 - INFO - __main__ - Running ... prefix=ethos-religion_16_21, lr=0.0003, bsz=8 ...
03/19/2022 07:44:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:44:06 - INFO - __main__ - Printing 3 examples
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:44:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:44:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:44:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:44:06 - INFO - __main__ - Printing 3 examples
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:44:06 - INFO - __main__ - ['false']
03/19/2022 07:44:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:44:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:44:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:44:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:44:07 - INFO - __main__ - Starting training!
03/19/2022 07:44:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:44:18 - INFO - __main__ - Starting training!
03/19/2022 07:44:24 - INFO - __main__ - Step 10 Global step 10 Train loss 24.660791 on epoch=4
03/19/2022 07:44:28 - INFO - __main__ - Step 20 Global step 20 Train loss 20.799137 on epoch=9
03/19/2022 07:44:33 - INFO - __main__ - Step 30 Global step 30 Train loss 17.472868 on epoch=14
03/19/2022 07:44:38 - INFO - __main__ - Step 40 Global step 40 Train loss 15.770872 on epoch=19
03/19/2022 07:44:43 - INFO - __main__ - Step 50 Global step 50 Train loss 14.897646 on epoch=24
03/19/2022 07:44:52 - INFO - __main__ - Global step 50 Train loss 18.720263 Classification-F1 0.0 on epoch=24
03/19/2022 07:44:58 - INFO - __main__ - Step 60 Global step 60 Train loss 14.006322 on epoch=29
03/19/2022 07:45:03 - INFO - __main__ - Step 70 Global step 70 Train loss 13.730101 on epoch=34
03/19/2022 07:45:08 - INFO - __main__ - Step 80 Global step 80 Train loss 11.427354 on epoch=39
03/19/2022 07:45:13 - INFO - __main__ - Step 90 Global step 90 Train loss 8.970209 on epoch=44
03/19/2022 07:45:18 - INFO - __main__ - Step 100 Global step 100 Train loss 5.567015 on epoch=49
03/19/2022 07:45:18 - INFO - __main__ - Global step 100 Train loss 10.740200 Classification-F1 0.2868421052631579 on epoch=49
03/19/2022 07:45:24 - INFO - __main__ - Step 110 Global step 110 Train loss 1.393227 on epoch=54
03/19/2022 07:45:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.681055 on epoch=59
03/19/2022 07:45:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.432976 on epoch=64
03/19/2022 07:45:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.415387 on epoch=69
03/19/2022 07:45:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.369711 on epoch=74
03/19/2022 07:45:44 - INFO - __main__ - Global step 150 Train loss 0.658471 Classification-F1 0.4589371980676329 on epoch=74
03/19/2022 07:45:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.344090 on epoch=79
03/19/2022 07:45:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.272703 on epoch=84
03/19/2022 07:46:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.167531 on epoch=89
03/19/2022 07:46:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.283180 on epoch=94
03/19/2022 07:46:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.369109 on epoch=99
03/19/2022 07:46:11 - INFO - __main__ - Global step 200 Train loss 0.287323 Classification-F1 0.8745098039215686 on epoch=99
03/19/2022 07:46:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.193727 on epoch=104
03/19/2022 07:46:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.274596 on epoch=109
03/19/2022 07:46:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.197511 on epoch=114
03/19/2022 07:46:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.112312 on epoch=119
03/19/2022 07:46:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.122257 on epoch=124
03/19/2022 07:46:37 - INFO - __main__ - Global step 250 Train loss 0.180081 Classification-F1 0.6267232237539766 on epoch=124
03/19/2022 07:46:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.103319 on epoch=129
03/19/2022 07:46:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.103779 on epoch=134
03/19/2022 07:46:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.110697 on epoch=139
03/19/2022 07:46:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.102185 on epoch=144
03/19/2022 07:47:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.099462 on epoch=149
03/19/2022 07:47:02 - INFO - __main__ - Global step 300 Train loss 0.103889 Classification-F1 0.7702564102564102 on epoch=149
03/19/2022 07:47:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.021014 on epoch=154
03/19/2022 07:47:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.128707 on epoch=159
03/19/2022 07:47:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.032111 on epoch=164
03/19/2022 07:47:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022491 on epoch=169
03/19/2022 07:47:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.026616 on epoch=174
03/19/2022 07:47:28 - INFO - __main__ - Global step 350 Train loss 0.046188 Classification-F1 0.9372549019607843 on epoch=174
03/19/2022 07:47:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.014826 on epoch=179
03/19/2022 07:47:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.007210 on epoch=184
03/19/2022 07:47:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002342 on epoch=189
03/19/2022 07:47:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001153 on epoch=194
03/19/2022 07:47:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.023880 on epoch=199
03/19/2022 07:47:54 - INFO - __main__ - Global step 400 Train loss 0.009882 Classification-F1 0.9054187192118226 on epoch=199
03/19/2022 07:47:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002240 on epoch=204
03/19/2022 07:48:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006035 on epoch=209
03/19/2022 07:48:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001390 on epoch=214
03/19/2022 07:48:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001477 on epoch=219
03/19/2022 07:48:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003582 on epoch=224
03/19/2022 07:48:19 - INFO - __main__ - Global step 450 Train loss 0.002945 Classification-F1 0.6343434343434343 on epoch=224
03/19/2022 07:48:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002460 on epoch=229
03/19/2022 07:48:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.041734 on epoch=234
03/19/2022 07:48:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000595 on epoch=239
03/19/2022 07:48:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.045914 on epoch=244
03/19/2022 07:48:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.125558 on epoch=249
03/19/2022 07:48:45 - INFO - __main__ - Global step 500 Train loss 0.043252 Classification-F1 0.42223502304147464 on epoch=249
03/19/2022 07:48:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005580 on epoch=254
03/19/2022 07:48:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003116 on epoch=259
03/19/2022 07:49:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000714 on epoch=264
03/19/2022 07:49:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002943 on epoch=269
03/19/2022 07:49:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.006211 on epoch=274
03/19/2022 07:49:10 - INFO - __main__ - Global step 550 Train loss 0.003713 Classification-F1 0.5904761904761905 on epoch=274
03/19/2022 07:49:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000485 on epoch=279
03/19/2022 07:49:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001001 on epoch=284
03/19/2022 07:49:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000938 on epoch=289
03/19/2022 07:49:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000605 on epoch=294
03/19/2022 07:49:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000204 on epoch=299
03/19/2022 07:49:35 - INFO - __main__ - Global step 600 Train loss 0.000647 Classification-F1 0.6343434343434343 on epoch=299
03/19/2022 07:49:35 - INFO - __main__ - save last model!
03/19/2022 07:49:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:49:36 - INFO - __main__ - Printing 3 examples
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:49:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:49:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:49:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:49:36 - INFO - __main__ - Printing 3 examples
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:49:36 - INFO - __main__ - ['false']
03/19/2022 07:49:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:49:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:49:36 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:49:42 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:49:43 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:49:43 - INFO - __main__ - Printing 3 examples
03/19/2022 07:49:43 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:49:43 - INFO - __main__ - ['false']
03/19/2022 07:49:43 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:49:43 - INFO - __main__ - ['false']
03/19/2022 07:49:43 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:49:43 - INFO - __main__ - ['true']
03/19/2022 07:49:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:49:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:49:43 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:49:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_21_0.0003_8_predictions.txt
03/19/2022 07:49:44 - INFO - __main__ - Classification-F1 on test data: 0.7046
03/19/2022 07:49:45 - INFO - __main__ - prefix=ethos-religion_16_21, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.7046296296296297
03/19/2022 07:49:45 - INFO - __main__ - Running ... prefix=ethos-religion_16_21, lr=0.0002, bsz=8 ...
03/19/2022 07:49:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:49:45 - INFO - __main__ - Printing 3 examples
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:49:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:49:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:49:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:49:45 - INFO - __main__ - Printing 3 examples
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:49:45 - INFO - __main__ - ['false']
03/19/2022 07:49:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:49:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:49:46 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:49:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:49:49 - INFO - __main__ - Starting training!
03/19/2022 07:49:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:49:58 - INFO - __main__ - Starting training!
03/19/2022 07:50:02 - INFO - __main__ - Step 10 Global step 10 Train loss 24.794445 on epoch=4
03/19/2022 07:50:07 - INFO - __main__ - Step 20 Global step 20 Train loss 20.085972 on epoch=9
03/19/2022 07:50:12 - INFO - __main__ - Step 30 Global step 30 Train loss 20.382557 on epoch=14
03/19/2022 07:50:17 - INFO - __main__ - Step 40 Global step 40 Train loss 18.360619 on epoch=19
03/19/2022 07:50:22 - INFO - __main__ - Step 50 Global step 50 Train loss 16.942404 on epoch=24
03/19/2022 07:50:28 - INFO - __main__ - Global step 50 Train loss 20.113199 Classification-F1 0.0 on epoch=24
03/19/2022 07:50:34 - INFO - __main__ - Step 60 Global step 60 Train loss 16.880259 on epoch=29
03/19/2022 07:50:39 - INFO - __main__ - Step 70 Global step 70 Train loss 16.015577 on epoch=34
03/19/2022 07:50:44 - INFO - __main__ - Step 80 Global step 80 Train loss 14.649057 on epoch=39
03/19/2022 07:50:49 - INFO - __main__ - Step 90 Global step 90 Train loss 15.246210 on epoch=44
03/19/2022 07:50:54 - INFO - __main__ - Step 100 Global step 100 Train loss 14.629018 on epoch=49
03/19/2022 07:50:54 - INFO - __main__ - Global step 100 Train loss 15.484023 Classification-F1 0.0 on epoch=49
03/19/2022 07:50:59 - INFO - __main__ - Step 110 Global step 110 Train loss 12.841985 on epoch=54
03/19/2022 07:51:04 - INFO - __main__ - Step 120 Global step 120 Train loss 11.950996 on epoch=59
03/19/2022 07:51:09 - INFO - __main__ - Step 130 Global step 130 Train loss 10.384797 on epoch=64
03/19/2022 07:51:14 - INFO - __main__ - Step 140 Global step 140 Train loss 9.471958 on epoch=69
03/19/2022 07:51:19 - INFO - __main__ - Step 150 Global step 150 Train loss 8.169808 on epoch=74
03/19/2022 07:51:20 - INFO - __main__ - Global step 150 Train loss 10.563909 Classification-F1 0.0 on epoch=74
03/19/2022 07:51:25 - INFO - __main__ - Step 160 Global step 160 Train loss 6.784745 on epoch=79
03/19/2022 07:51:30 - INFO - __main__ - Step 170 Global step 170 Train loss 4.295318 on epoch=84
03/19/2022 07:51:35 - INFO - __main__ - Step 180 Global step 180 Train loss 3.144919 on epoch=89
03/19/2022 07:51:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.798432 on epoch=94
03/19/2022 07:51:45 - INFO - __main__ - Step 200 Global step 200 Train loss 2.269408 on epoch=99
03/19/2022 07:51:45 - INFO - __main__ - Global step 200 Train loss 3.858564 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 07:51:51 - INFO - __main__ - Step 210 Global step 210 Train loss 2.482375 on epoch=104
03/19/2022 07:51:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.468252 on epoch=109
03/19/2022 07:52:01 - INFO - __main__ - Step 230 Global step 230 Train loss 2.676090 on epoch=114
03/19/2022 07:52:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.634623 on epoch=119
03/19/2022 07:52:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.705514 on epoch=124
03/19/2022 07:52:11 - INFO - __main__ - Global step 250 Train loss 2.393371 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 07:52:16 - INFO - __main__ - Step 260 Global step 260 Train loss 2.319625 on epoch=129
03/19/2022 07:52:21 - INFO - __main__ - Step 270 Global step 270 Train loss 3.022086 on epoch=134
03/19/2022 07:52:26 - INFO - __main__ - Step 280 Global step 280 Train loss 2.559911 on epoch=139
03/19/2022 07:52:31 - INFO - __main__ - Step 290 Global step 290 Train loss 2.493531 on epoch=144
03/19/2022 07:52:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.755988 on epoch=149
03/19/2022 07:52:37 - INFO - __main__ - Global step 300 Train loss 2.430228 Classification-F1 0.5636363636363637 on epoch=149
03/19/2022 07:52:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.920195 on epoch=154
03/19/2022 07:52:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.441525 on epoch=159
03/19/2022 07:52:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.402454 on epoch=164
03/19/2022 07:52:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.686108 on epoch=169
03/19/2022 07:53:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.247337 on epoch=174
03/19/2022 07:53:02 - INFO - __main__ - Global step 350 Train loss 1.539524 Classification-F1 0.21276595744680848 on epoch=174
03/19/2022 07:53:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.968167 on epoch=179
03/19/2022 07:53:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.218308 on epoch=184
03/19/2022 07:53:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.883356 on epoch=189
03/19/2022 07:53:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.869453 on epoch=194
03/19/2022 07:53:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.414115 on epoch=199
03/19/2022 07:53:27 - INFO - __main__ - Global step 400 Train loss 0.870680 Classification-F1 0.49090909090909085 on epoch=199
03/19/2022 07:53:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.547967 on epoch=204
03/19/2022 07:53:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.673266 on epoch=209
03/19/2022 07:53:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.533630 on epoch=214
03/19/2022 07:53:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.428002 on epoch=219
03/19/2022 07:53:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.373396 on epoch=224
03/19/2022 07:53:52 - INFO - __main__ - Global step 450 Train loss 0.511252 Classification-F1 0.6190476190476191 on epoch=224
03/19/2022 07:53:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.479988 on epoch=229
03/19/2022 07:54:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.449601 on epoch=234
03/19/2022 07:54:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.442841 on epoch=239
03/19/2022 07:54:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.357986 on epoch=244
03/19/2022 07:54:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.473575 on epoch=249
03/19/2022 07:54:18 - INFO - __main__ - Global step 500 Train loss 0.440798 Classification-F1 0.5270935960591133 on epoch=249
03/19/2022 07:54:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.395347 on epoch=254
03/19/2022 07:54:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.470547 on epoch=259
03/19/2022 07:54:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.411846 on epoch=264
03/19/2022 07:54:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.344562 on epoch=269
03/19/2022 07:54:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.356489 on epoch=274
03/19/2022 07:54:43 - INFO - __main__ - Global step 550 Train loss 0.395758 Classification-F1 0.746031746031746 on epoch=274
03/19/2022 07:54:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.264555 on epoch=279
03/19/2022 07:54:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.310175 on epoch=284
03/19/2022 07:54:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.276589 on epoch=289
03/19/2022 07:55:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.280010 on epoch=294
03/19/2022 07:55:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.221258 on epoch=299
03/19/2022 07:55:09 - INFO - __main__ - Global step 600 Train loss 0.270517 Classification-F1 0.8095238095238095 on epoch=299
03/19/2022 07:55:10 - INFO - __main__ - save last model!
03/19/2022 07:55:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:55:10 - INFO - __main__ - Printing 3 examples
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 07:55:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:55:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:55:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:55:10 - INFO - __main__ - Printing 3 examples
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:55:10 - INFO - __main__ - ['false']
03/19/2022 07:55:10 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:55:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:55:10 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:55:17 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 07:55:17 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 07:55:17 - INFO - __main__ - Printing 3 examples
03/19/2022 07:55:17 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 07:55:17 - INFO - __main__ - ['false']
03/19/2022 07:55:17 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 07:55:17 - INFO - __main__ - ['false']
03/19/2022 07:55:17 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 07:55:17 - INFO - __main__ - ['true']
03/19/2022 07:55:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:55:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:55:17 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 07:55:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_21_0.0002_8_predictions.txt
03/19/2022 07:55:20 - INFO - __main__ - Classification-F1 on test data: 0.6778
03/19/2022 07:55:21 - INFO - __main__ - prefix=ethos-religion_16_21, lr=0.0002, bsz=8, dev_performance=0.8095238095238095, test_performance=0.6777777777777778
03/19/2022 07:55:21 - INFO - __main__ - Running ... prefix=ethos-religion_16_21, lr=0.0001, bsz=8 ...
03/19/2022 07:55:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:55:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 07:55:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:55:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 07:55:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 07:55:22 - INFO - __main__ - Printing 3 examples
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] The older Hispanic woman shouldn't have been on this episode. Her view on everything was so shallow and 'ass-kissy'. The story about her brother in law being killed by an illegal alien Im truly sorry for their lost and I hate to sound insensitive but ANYBODY could've killed him. INCLUDING A LEGAL CITIZEN OF THE UNITED STATES THEYRE ARE TWISTED PEOPLE EVERYWHERE don't pull the illegal alien card on violence bc that's an everyday part of life in the US with or WITHOUT people coming here illegally
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] What about a global massacre of men? Pussiezzz only girls
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ -  [ethos-religion] Too bad women dont know how to kill themselves
03/19/2022 07:55:22 - INFO - __main__ - ['false']
03/19/2022 07:55:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 07:55:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 07:55:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 07:55:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:55:23 - INFO - __main__ - Starting training!
03/19/2022 07:55:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 07:55:32 - INFO - __main__ - Starting training!
03/19/2022 07:55:37 - INFO - __main__ - Step 10 Global step 10 Train loss 24.400219 on epoch=4
03/19/2022 07:55:41 - INFO - __main__ - Step 20 Global step 20 Train loss 20.429060 on epoch=9
03/19/2022 07:55:46 - INFO - __main__ - Step 30 Global step 30 Train loss 18.603909 on epoch=14
03/19/2022 07:55:52 - INFO - __main__ - Step 40 Global step 40 Train loss 18.220575 on epoch=19
03/19/2022 07:55:56 - INFO - __main__ - Step 50 Global step 50 Train loss 16.729183 on epoch=24
03/19/2022 07:56:06 - INFO - __main__ - Global step 50 Train loss 19.676590 Classification-F1 0.0 on epoch=24
03/19/2022 07:56:12 - INFO - __main__ - Step 60 Global step 60 Train loss 16.855152 on epoch=29
03/19/2022 07:56:17 - INFO - __main__ - Step 70 Global step 70 Train loss 16.870005 on epoch=34
03/19/2022 07:56:22 - INFO - __main__ - Step 80 Global step 80 Train loss 16.228340 on epoch=39
03/19/2022 07:56:27 - INFO - __main__ - Step 90 Global step 90 Train loss 15.531535 on epoch=44
03/19/2022 07:56:32 - INFO - __main__ - Step 100 Global step 100 Train loss 14.956495 on epoch=49
03/19/2022 07:56:41 - INFO - __main__ - Global step 100 Train loss 16.088305 Classification-F1 0.0 on epoch=49
03/19/2022 07:56:46 - INFO - __main__ - Step 110 Global step 110 Train loss 15.391673 on epoch=54
03/19/2022 07:56:51 - INFO - __main__ - Step 120 Global step 120 Train loss 14.792241 on epoch=59
03/19/2022 07:56:56 - INFO - __main__ - Step 130 Global step 130 Train loss 14.209727 on epoch=64
03/19/2022 07:57:01 - INFO - __main__ - Step 140 Global step 140 Train loss 13.821121 on epoch=69
03/19/2022 07:57:06 - INFO - __main__ - Step 150 Global step 150 Train loss 13.165163 on epoch=74
03/19/2022 07:57:14 - INFO - __main__ - Global step 150 Train loss 14.275985 Classification-F1 0.0 on epoch=74
03/19/2022 07:57:19 - INFO - __main__ - Step 160 Global step 160 Train loss 13.579355 on epoch=79
03/19/2022 07:57:24 - INFO - __main__ - Step 170 Global step 170 Train loss 12.846354 on epoch=84
03/19/2022 07:57:29 - INFO - __main__ - Step 180 Global step 180 Train loss 12.406942 on epoch=89
03/19/2022 07:57:34 - INFO - __main__ - Step 190 Global step 190 Train loss 11.642858 on epoch=94
03/19/2022 07:57:39 - INFO - __main__ - Step 200 Global step 200 Train loss 11.291466 on epoch=99
03/19/2022 07:57:46 - INFO - __main__ - Global step 200 Train loss 12.353395 Classification-F1 0.0 on epoch=99
03/19/2022 07:57:51 - INFO - __main__ - Step 210 Global step 210 Train loss 10.254527 on epoch=104
03/19/2022 07:57:56 - INFO - __main__ - Step 220 Global step 220 Train loss 9.204946 on epoch=109
03/19/2022 07:58:01 - INFO - __main__ - Step 230 Global step 230 Train loss 7.883056 on epoch=114
03/19/2022 07:58:07 - INFO - __main__ - Step 240 Global step 240 Train loss 7.145253 on epoch=119
03/19/2022 07:58:12 - INFO - __main__ - Step 250 Global step 250 Train loss 6.748442 on epoch=124
03/19/2022 07:58:12 - INFO - __main__ - Global step 250 Train loss 8.247244 Classification-F1 0.0 on epoch=124
03/19/2022 07:58:17 - INFO - __main__ - Step 260 Global step 260 Train loss 4.019830 on epoch=129
03/19/2022 07:58:22 - INFO - __main__ - Step 270 Global step 270 Train loss 4.339154 on epoch=134
03/19/2022 07:58:27 - INFO - __main__ - Step 280 Global step 280 Train loss 3.215129 on epoch=139
03/19/2022 07:58:32 - INFO - __main__ - Step 290 Global step 290 Train loss 3.083959 on epoch=144
03/19/2022 07:58:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.903487 on epoch=149
03/19/2022 07:58:38 - INFO - __main__ - Global step 300 Train loss 3.512312 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 07:58:44 - INFO - __main__ - Step 310 Global step 310 Train loss 2.831995 on epoch=154
03/19/2022 07:58:49 - INFO - __main__ - Step 320 Global step 320 Train loss 2.975431 on epoch=159
03/19/2022 07:58:54 - INFO - __main__ - Step 330 Global step 330 Train loss 3.754717 on epoch=164
03/19/2022 07:58:59 - INFO - __main__ - Step 340 Global step 340 Train loss 2.437142 on epoch=169
03/19/2022 07:59:04 - INFO - __main__ - Step 350 Global step 350 Train loss 3.087043 on epoch=174
03/19/2022 07:59:04 - INFO - __main__ - Global step 350 Train loss 3.017266 Classification-F1 0.5134502923976608 on epoch=174
03/19/2022 07:59:10 - INFO - __main__ - Step 360 Global step 360 Train loss 3.013728 on epoch=179
03/19/2022 07:59:15 - INFO - __main__ - Step 370 Global step 370 Train loss 2.600321 on epoch=184
03/19/2022 07:59:20 - INFO - __main__ - Step 380 Global step 380 Train loss 2.705239 on epoch=189
03/19/2022 07:59:25 - INFO - __main__ - Step 390 Global step 390 Train loss 2.510804 on epoch=194
03/19/2022 07:59:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.108114 on epoch=199
03/19/2022 07:59:30 - INFO - __main__ - Global step 400 Train loss 2.387641 Classification-F1 0.8423645320197044 on epoch=199
03/19/2022 07:59:36 - INFO - __main__ - Step 410 Global step 410 Train loss 1.324502 on epoch=204
03/19/2022 07:59:41 - INFO - __main__ - Step 420 Global step 420 Train loss 1.713885 on epoch=209
03/19/2022 07:59:46 - INFO - __main__ - Step 430 Global step 430 Train loss 1.148317 on epoch=214
03/19/2022 07:59:51 - INFO - __main__ - Step 440 Global step 440 Train loss 1.744152 on epoch=219
03/19/2022 07:59:56 - INFO - __main__ - Step 450 Global step 450 Train loss 1.792233 on epoch=224
03/19/2022 07:59:57 - INFO - __main__ - Global step 450 Train loss 1.544618 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 08:00:02 - INFO - __main__ - Step 460 Global step 460 Train loss 1.230074 on epoch=229
03/19/2022 08:00:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.794725 on epoch=234
03/19/2022 08:00:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.502917 on epoch=239
03/19/2022 08:00:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.340906 on epoch=244
03/19/2022 08:00:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.300595 on epoch=249
03/19/2022 08:00:22 - INFO - __main__ - Global step 500 Train loss 0.633843 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 08:00:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.394180 on epoch=254
03/19/2022 08:00:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.591365 on epoch=259
03/19/2022 08:00:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.315368 on epoch=264
03/19/2022 08:00:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.449597 on epoch=269
03/19/2022 08:00:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.385450 on epoch=274
03/19/2022 08:00:48 - INFO - __main__ - Global step 550 Train loss 0.427192 Classification-F1 0.805668016194332 on epoch=274
03/19/2022 08:00:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.430864 on epoch=279
03/19/2022 08:00:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.426504 on epoch=284
03/19/2022 08:01:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.309521 on epoch=289
03/19/2022 08:01:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.290943 on epoch=294
03/19/2022 08:01:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.286542 on epoch=299
03/19/2022 08:01:13 - INFO - __main__ - Global step 600 Train loss 0.348875 Classification-F1 0.8117647058823529 on epoch=299
03/19/2022 08:01:13 - INFO - __main__ - save last model!
03/19/2022 08:01:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:01:14 - INFO - __main__ - Printing 3 examples
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:01:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:01:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:01:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:01:14 - INFO - __main__ - Printing 3 examples
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:01:14 - INFO - __main__ - ['false']
03/19/2022 08:01:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:01:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:01:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:01:20 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:01:21 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:01:21 - INFO - __main__ - Printing 3 examples
03/19/2022 08:01:21 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:01:21 - INFO - __main__ - ['false']
03/19/2022 08:01:21 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:01:21 - INFO - __main__ - ['false']
03/19/2022 08:01:21 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:01:21 - INFO - __main__ - ['true']
03/19/2022 08:01:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:01:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:01:21 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:01:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_21_0.0001_8_predictions.txt
03/19/2022 08:01:22 - INFO - __main__ - Classification-F1 on test data: 0.6538
03/19/2022 08:01:23 - INFO - __main__ - prefix=ethos-religion_16_21, lr=0.0001, bsz=8, dev_performance=0.8423645320197044, test_performance=0.65379513633014
03/19/2022 08:01:23 - INFO - __main__ - Running ... prefix=ethos-religion_16_42, lr=0.0005, bsz=8 ...
03/19/2022 08:01:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:01:24 - INFO - __main__ - Printing 3 examples
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:01:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:01:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:01:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:01:24 - INFO - __main__ - Printing 3 examples
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:01:24 - INFO - __main__ - ['false']
03/19/2022 08:01:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:01:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:01:24 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:01:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:01:27 - INFO - __main__ - Starting training!
03/19/2022 08:01:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:01:34 - INFO - __main__ - Starting training!
03/19/2022 08:01:38 - INFO - __main__ - Step 10 Global step 10 Train loss 24.359684 on epoch=4
03/19/2022 08:01:43 - INFO - __main__ - Step 20 Global step 20 Train loss 19.139299 on epoch=9
03/19/2022 08:01:48 - INFO - __main__ - Step 30 Global step 30 Train loss 17.601139 on epoch=14
03/19/2022 08:01:53 - INFO - __main__ - Step 40 Global step 40 Train loss 14.343073 on epoch=19
03/19/2022 08:01:59 - INFO - __main__ - Step 50 Global step 50 Train loss 13.488016 on epoch=24
03/19/2022 08:02:01 - INFO - __main__ - Global step 50 Train loss 17.786242 Classification-F1 0.0 on epoch=24
03/19/2022 08:02:07 - INFO - __main__ - Step 60 Global step 60 Train loss 10.281817 on epoch=29
03/19/2022 08:02:12 - INFO - __main__ - Step 70 Global step 70 Train loss 3.859375 on epoch=34
03/19/2022 08:02:17 - INFO - __main__ - Step 80 Global step 80 Train loss 3.516631 on epoch=39
03/19/2022 08:02:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.877976 on epoch=44
03/19/2022 08:02:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.539879 on epoch=49
03/19/2022 08:02:27 - INFO - __main__ - Global step 100 Train loss 4.215136 Classification-F1 0.7793103448275862 on epoch=49
03/19/2022 08:02:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.734089 on epoch=54
03/19/2022 08:02:38 - INFO - __main__ - Step 120 Global step 120 Train loss 1.253920 on epoch=59
03/19/2022 08:02:43 - INFO - __main__ - Step 130 Global step 130 Train loss 1.165664 on epoch=64
03/19/2022 08:02:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.699859 on epoch=69
03/19/2022 08:02:53 - INFO - __main__ - Step 150 Global step 150 Train loss 1.028576 on epoch=74
03/19/2022 08:02:54 - INFO - __main__ - Global step 150 Train loss 0.976422 Classification-F1 0.906158357771261 on epoch=74
03/19/2022 08:03:00 - INFO - __main__ - Step 160 Global step 160 Train loss 1.056710 on epoch=79
03/19/2022 08:03:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.805683 on epoch=84
03/19/2022 08:03:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.850943 on epoch=89
03/19/2022 08:03:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.555606 on epoch=94
03/19/2022 08:03:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.582217 on epoch=99
03/19/2022 08:03:20 - INFO - __main__ - Global step 200 Train loss 0.770232 Classification-F1 0.906158357771261 on epoch=99
03/19/2022 08:03:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.793252 on epoch=104
03/19/2022 08:03:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.466771 on epoch=109
03/19/2022 08:03:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.727138 on epoch=114
03/19/2022 08:03:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.769840 on epoch=119
03/19/2022 08:03:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.337775 on epoch=124
03/19/2022 08:03:45 - INFO - __main__ - Global step 250 Train loss 0.618955 Classification-F1 0.7117117117117117 on epoch=124
03/19/2022 08:03:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.783959 on epoch=129
03/19/2022 08:03:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.692139 on epoch=134
03/19/2022 08:04:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.515451 on epoch=139
03/19/2022 08:04:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.434010 on epoch=144
03/19/2022 08:04:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.414653 on epoch=149
03/19/2022 08:04:10 - INFO - __main__ - Global step 300 Train loss 0.568042 Classification-F1 0.6666666666666667 on epoch=149
03/19/2022 08:04:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.472657 on epoch=154
03/19/2022 08:04:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.615192 on epoch=159
03/19/2022 08:04:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.261361 on epoch=164
03/19/2022 08:04:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.381280 on epoch=169
03/19/2022 08:04:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.425228 on epoch=174
03/19/2022 08:04:36 - INFO - __main__ - Global step 350 Train loss 0.431144 Classification-F1 0.6945917285259808 on epoch=174
03/19/2022 08:04:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.502581 on epoch=179
03/19/2022 08:04:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.361352 on epoch=184
03/19/2022 08:04:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.230001 on epoch=189
03/19/2022 08:04:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.415437 on epoch=194
03/19/2022 08:05:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009676 on epoch=199
03/19/2022 08:05:01 - INFO - __main__ - Global step 400 Train loss 0.303810 Classification-F1 0.8095238095238095 on epoch=199
03/19/2022 08:05:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.004903 on epoch=204
03/19/2022 08:05:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000939 on epoch=209
03/19/2022 08:05:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001457 on epoch=214
03/19/2022 08:05:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000390 on epoch=219
03/19/2022 08:05:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001059 on epoch=224
03/19/2022 08:05:26 - INFO - __main__ - Global step 450 Train loss 0.001750 Classification-F1 0.8095238095238095 on epoch=224
03/19/2022 08:05:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000433 on epoch=229
03/19/2022 08:05:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.048200 on epoch=234
03/19/2022 08:05:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000486 on epoch=239
03/19/2022 08:05:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000388 on epoch=244
03/19/2022 08:05:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000273 on epoch=249
03/19/2022 08:05:52 - INFO - __main__ - Global step 500 Train loss 0.009956 Classification-F1 0.7757757757757757 on epoch=249
03/19/2022 08:05:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000277 on epoch=254
03/19/2022 08:06:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000205 on epoch=259
03/19/2022 08:06:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000411 on epoch=264
03/19/2022 08:06:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000647 on epoch=269
03/19/2022 08:06:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000132 on epoch=274
03/19/2022 08:06:17 - INFO - __main__ - Global step 550 Train loss 0.000334 Classification-F1 0.7757757757757757 on epoch=274
03/19/2022 08:06:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000185 on epoch=279
03/19/2022 08:06:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000105 on epoch=284
03/19/2022 08:06:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000154 on epoch=289
03/19/2022 08:06:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000513 on epoch=294
03/19/2022 08:06:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000039 on epoch=299
03/19/2022 08:06:42 - INFO - __main__ - Global step 600 Train loss 0.000199 Classification-F1 0.805668016194332 on epoch=299
03/19/2022 08:06:42 - INFO - __main__ - save last model!
03/19/2022 08:06:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:06:43 - INFO - __main__ - Printing 3 examples
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:06:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:06:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:06:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:06:43 - INFO - __main__ - Printing 3 examples
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:06:43 - INFO - __main__ - ['false']
03/19/2022 08:06:43 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:06:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:06:43 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:06:49 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:06:50 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:06:50 - INFO - __main__ - Printing 3 examples
03/19/2022 08:06:50 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:06:50 - INFO - __main__ - ['false']
03/19/2022 08:06:50 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:06:50 - INFO - __main__ - ['false']
03/19/2022 08:06:50 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:06:50 - INFO - __main__ - ['true']
03/19/2022 08:06:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:06:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:06:50 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:06:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_42_0.0005_8_predictions.txt
03/19/2022 08:06:52 - INFO - __main__ - Classification-F1 on test data: 0.9521
03/19/2022 08:06:52 - INFO - __main__ - prefix=ethos-religion_16_42, lr=0.0005, bsz=8, dev_performance=0.906158357771261, test_performance=0.9521364386576197
03/19/2022 08:06:52 - INFO - __main__ - Running ... prefix=ethos-religion_16_42, lr=0.0003, bsz=8 ...
03/19/2022 08:06:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:06:53 - INFO - __main__ - Printing 3 examples
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:06:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:06:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:06:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:06:53 - INFO - __main__ - Printing 3 examples
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:06:53 - INFO - __main__ - ['false']
03/19/2022 08:06:53 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:06:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:06:53 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:06:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:06:56 - INFO - __main__ - Starting training!
03/19/2022 08:07:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:07:05 - INFO - __main__ - Starting training!
03/19/2022 08:07:10 - INFO - __main__ - Step 10 Global step 10 Train loss 24.664087 on epoch=4
03/19/2022 08:07:15 - INFO - __main__ - Step 20 Global step 20 Train loss 20.767033 on epoch=9
03/19/2022 08:07:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.919903 on epoch=14
03/19/2022 08:07:24 - INFO - __main__ - Step 40 Global step 40 Train loss 16.801311 on epoch=19
03/19/2022 08:07:29 - INFO - __main__ - Step 50 Global step 50 Train loss 15.024196 on epoch=24
03/19/2022 08:07:36 - INFO - __main__ - Global step 50 Train loss 18.835304 Classification-F1 0.0 on epoch=24
03/19/2022 08:07:42 - INFO - __main__ - Step 60 Global step 60 Train loss 16.466187 on epoch=29
03/19/2022 08:07:47 - INFO - __main__ - Step 70 Global step 70 Train loss 14.322639 on epoch=34
03/19/2022 08:07:52 - INFO - __main__ - Step 80 Global step 80 Train loss 11.711376 on epoch=39
03/19/2022 08:07:57 - INFO - __main__ - Step 90 Global step 90 Train loss 10.863085 on epoch=44
03/19/2022 08:08:02 - INFO - __main__ - Step 100 Global step 100 Train loss 9.015821 on epoch=49
03/19/2022 08:08:06 - INFO - __main__ - Global step 100 Train loss 12.475821 Classification-F1 0.0 on epoch=49
03/19/2022 08:08:11 - INFO - __main__ - Step 110 Global step 110 Train loss 6.398313 on epoch=54
03/19/2022 08:08:16 - INFO - __main__ - Step 120 Global step 120 Train loss 4.089792 on epoch=59
03/19/2022 08:08:21 - INFO - __main__ - Step 130 Global step 130 Train loss 3.000004 on epoch=64
03/19/2022 08:08:26 - INFO - __main__ - Step 140 Global step 140 Train loss 3.373523 on epoch=69
03/19/2022 08:08:31 - INFO - __main__ - Step 150 Global step 150 Train loss 2.506219 on epoch=74
03/19/2022 08:08:31 - INFO - __main__ - Global step 150 Train loss 3.873570 Classification-F1 0.4589371980676329 on epoch=74
03/19/2022 08:08:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.762636 on epoch=79
03/19/2022 08:08:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.334109 on epoch=84
03/19/2022 08:08:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.607504 on epoch=89
03/19/2022 08:08:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.326559 on epoch=94
03/19/2022 08:08:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.255115 on epoch=99
03/19/2022 08:08:57 - INFO - __main__ - Global step 200 Train loss 1.057185 Classification-F1 0.4589371980676329 on epoch=99
03/19/2022 08:09:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.208663 on epoch=104
03/19/2022 08:09:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.154544 on epoch=109
03/19/2022 08:09:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.069040 on epoch=114
03/19/2022 08:09:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.115420 on epoch=119
03/19/2022 08:09:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.081454 on epoch=124
03/19/2022 08:09:23 - INFO - __main__ - Global step 250 Train loss 0.125824 Classification-F1 0.8423645320197044 on epoch=124
03/19/2022 08:09:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.054723 on epoch=129
03/19/2022 08:09:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.042239 on epoch=134
03/19/2022 08:09:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.031894 on epoch=139
03/19/2022 08:09:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.038234 on epoch=144
03/19/2022 08:09:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.135989 on epoch=149
03/19/2022 08:09:49 - INFO - __main__ - Global step 300 Train loss 0.060616 Classification-F1 0.8095238095238095 on epoch=149
03/19/2022 08:09:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.097501 on epoch=154
03/19/2022 08:09:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.053794 on epoch=159
03/19/2022 08:10:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.083812 on epoch=164
03/19/2022 08:10:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.032360 on epoch=169
03/19/2022 08:10:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.110718 on epoch=174
03/19/2022 08:10:14 - INFO - __main__ - Global step 350 Train loss 0.075637 Classification-F1 0.873015873015873 on epoch=174
03/19/2022 08:10:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.024673 on epoch=179
03/19/2022 08:10:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.050362 on epoch=184
03/19/2022 08:10:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.015593 on epoch=189
03/19/2022 08:10:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.047400 on epoch=194
03/19/2022 08:10:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.034935 on epoch=199
03/19/2022 08:10:40 - INFO - __main__ - Global step 400 Train loss 0.034593 Classification-F1 0.9372549019607843 on epoch=199
03/19/2022 08:10:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.017170 on epoch=204
03/19/2022 08:10:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.016043 on epoch=209
03/19/2022 08:10:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.020032 on epoch=214
03/19/2022 08:11:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.043665 on epoch=219
03/19/2022 08:11:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007429 on epoch=224
03/19/2022 08:11:06 - INFO - __main__ - Global step 450 Train loss 0.020868 Classification-F1 0.9054187192118226 on epoch=224
03/19/2022 08:11:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.074751 on epoch=229
03/19/2022 08:11:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.013761 on epoch=234
03/19/2022 08:11:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.007648 on epoch=239
03/19/2022 08:11:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.016705 on epoch=244
03/19/2022 08:11:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.008943 on epoch=249
03/19/2022 08:11:32 - INFO - __main__ - Global step 500 Train loss 0.024362 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 08:11:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.043587 on epoch=254
03/19/2022 08:11:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001023 on epoch=259
03/19/2022 08:11:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.061142 on epoch=264
03/19/2022 08:11:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.016803 on epoch=269
03/19/2022 08:11:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002862 on epoch=274
03/19/2022 08:11:58 - INFO - __main__ - Global step 550 Train loss 0.025083 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 08:12:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000373 on epoch=279
03/19/2022 08:12:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.010596 on epoch=284
03/19/2022 08:12:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016884 on epoch=289
03/19/2022 08:12:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004532 on epoch=294
03/19/2022 08:12:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000136 on epoch=299
03/19/2022 08:12:23 - INFO - __main__ - Global step 600 Train loss 0.006504 Classification-F1 0.906158357771261 on epoch=299
03/19/2022 08:12:23 - INFO - __main__ - save last model!
03/19/2022 08:12:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:12:24 - INFO - __main__ - Printing 3 examples
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:12:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:12:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:12:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:12:24 - INFO - __main__ - Printing 3 examples
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:12:24 - INFO - __main__ - ['false']
03/19/2022 08:12:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:12:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:12:24 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:12:30 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:12:31 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:12:31 - INFO - __main__ - Printing 3 examples
03/19/2022 08:12:31 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:12:31 - INFO - __main__ - ['false']
03/19/2022 08:12:31 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:12:31 - INFO - __main__ - ['false']
03/19/2022 08:12:31 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:12:31 - INFO - __main__ - ['true']
03/19/2022 08:12:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:12:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:12:31 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:12:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_42_0.0003_8_predictions.txt
03/19/2022 08:12:32 - INFO - __main__ - Classification-F1 on test data: 0.8818
03/19/2022 08:12:33 - INFO - __main__ - prefix=ethos-religion_16_42, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.8817934782608696
03/19/2022 08:12:33 - INFO - __main__ - Running ... prefix=ethos-religion_16_42, lr=0.0002, bsz=8 ...
03/19/2022 08:12:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:12:34 - INFO - __main__ - Printing 3 examples
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:12:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:12:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:12:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:12:34 - INFO - __main__ - Printing 3 examples
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:12:34 - INFO - __main__ - ['false']
03/19/2022 08:12:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:12:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:12:34 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:12:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:12:35 - INFO - __main__ - Starting training!
03/19/2022 08:12:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:12:46 - INFO - __main__ - Starting training!
03/19/2022 08:12:52 - INFO - __main__ - Step 10 Global step 10 Train loss 22.928547 on epoch=4
03/19/2022 08:12:57 - INFO - __main__ - Step 20 Global step 20 Train loss 19.111048 on epoch=9
03/19/2022 08:13:02 - INFO - __main__ - Step 30 Global step 30 Train loss 17.908049 on epoch=14
03/19/2022 08:13:07 - INFO - __main__ - Step 40 Global step 40 Train loss 17.207167 on epoch=19
03/19/2022 08:13:12 - INFO - __main__ - Step 50 Global step 50 Train loss 16.186838 on epoch=24
03/19/2022 08:13:24 - INFO - __main__ - Global step 50 Train loss 18.668329 Classification-F1 0.0 on epoch=24
03/19/2022 08:13:30 - INFO - __main__ - Step 60 Global step 60 Train loss 15.361920 on epoch=29
03/19/2022 08:13:35 - INFO - __main__ - Step 70 Global step 70 Train loss 14.194064 on epoch=34
03/19/2022 08:13:40 - INFO - __main__ - Step 80 Global step 80 Train loss 14.273740 on epoch=39
03/19/2022 08:13:45 - INFO - __main__ - Step 90 Global step 90 Train loss 13.315616 on epoch=44
03/19/2022 08:13:50 - INFO - __main__ - Step 100 Global step 100 Train loss 11.850112 on epoch=49
03/19/2022 08:14:00 - INFO - __main__ - Global step 100 Train loss 13.799089 Classification-F1 0.0 on epoch=49
03/19/2022 08:14:05 - INFO - __main__ - Step 110 Global step 110 Train loss 10.836549 on epoch=54
03/19/2022 08:14:10 - INFO - __main__ - Step 120 Global step 120 Train loss 9.246242 on epoch=59
03/19/2022 08:14:15 - INFO - __main__ - Step 130 Global step 130 Train loss 6.477652 on epoch=64
03/19/2022 08:14:19 - INFO - __main__ - Step 140 Global step 140 Train loss 2.446822 on epoch=69
03/19/2022 08:14:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.191131 on epoch=74
03/19/2022 08:14:25 - INFO - __main__ - Global step 150 Train loss 6.039679 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 08:14:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.643635 on epoch=79
03/19/2022 08:14:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.411337 on epoch=84
03/19/2022 08:14:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.630962 on epoch=89
03/19/2022 08:14:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.685180 on epoch=94
03/19/2022 08:14:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.405588 on epoch=99
03/19/2022 08:14:51 - INFO - __main__ - Global step 200 Train loss 0.555340 Classification-F1 0.6101882613510521 on epoch=99
03/19/2022 08:14:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.320064 on epoch=104
03/19/2022 08:15:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.514031 on epoch=109
03/19/2022 08:15:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.456281 on epoch=114
03/19/2022 08:15:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.199346 on epoch=119
03/19/2022 08:15:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.180645 on epoch=124
03/19/2022 08:15:18 - INFO - __main__ - Global step 250 Train loss 0.334073 Classification-F1 0.7702564102564102 on epoch=124
03/19/2022 08:15:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.104914 on epoch=129
03/19/2022 08:15:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.114420 on epoch=134
03/19/2022 08:15:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.090516 on epoch=139
03/19/2022 08:15:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.077384 on epoch=144
03/19/2022 08:15:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.058714 on epoch=149
03/19/2022 08:15:44 - INFO - __main__ - Global step 300 Train loss 0.089189 Classification-F1 0.9375 on epoch=149
03/19/2022 08:15:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.048123 on epoch=154
03/19/2022 08:15:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.069987 on epoch=159
03/19/2022 08:16:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.073507 on epoch=164
03/19/2022 08:16:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.060222 on epoch=169
03/19/2022 08:16:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.086611 on epoch=174
03/19/2022 08:16:11 - INFO - __main__ - Global step 350 Train loss 0.067690 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 08:16:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.123538 on epoch=179
03/19/2022 08:16:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.078555 on epoch=184
03/19/2022 08:16:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.168714 on epoch=189
03/19/2022 08:16:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.103765 on epoch=194
03/19/2022 08:16:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.140147 on epoch=199
03/19/2022 08:16:37 - INFO - __main__ - Global step 400 Train loss 0.122944 Classification-F1 0.7333333333333334 on epoch=199
03/19/2022 08:16:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.138523 on epoch=204
03/19/2022 08:16:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.104173 on epoch=209
03/19/2022 08:16:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.094414 on epoch=214
03/19/2022 08:16:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.142488 on epoch=219
03/19/2022 08:17:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.180040 on epoch=224
03/19/2022 08:17:03 - INFO - __main__ - Global step 450 Train loss 0.131928 Classification-F1 0.906158357771261 on epoch=224
03/19/2022 08:17:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.091838 on epoch=229
03/19/2022 08:17:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.089441 on epoch=234
03/19/2022 08:17:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.071714 on epoch=239
03/19/2022 08:17:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.059010 on epoch=244
03/19/2022 08:17:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.094970 on epoch=249
03/19/2022 08:17:28 - INFO - __main__ - Global step 500 Train loss 0.081394 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 08:17:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.077650 on epoch=254
03/19/2022 08:17:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.065374 on epoch=259
03/19/2022 08:17:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.048961 on epoch=264
03/19/2022 08:17:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.061056 on epoch=269
03/19/2022 08:17:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.063395 on epoch=274
03/19/2022 08:17:54 - INFO - __main__ - Global step 550 Train loss 0.063287 Classification-F1 0.9372549019607843 on epoch=274
03/19/2022 08:17:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.034059 on epoch=279
03/19/2022 08:18:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.061158 on epoch=284
03/19/2022 08:18:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036815 on epoch=289
03/19/2022 08:18:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.050224 on epoch=294
03/19/2022 08:18:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.176869 on epoch=299
03/19/2022 08:18:20 - INFO - __main__ - Global step 600 Train loss 0.071825 Classification-F1 0.873015873015873 on epoch=299
03/19/2022 08:18:20 - INFO - __main__ - save last model!
03/19/2022 08:18:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:18:21 - INFO - __main__ - Printing 3 examples
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:18:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:18:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:18:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:18:21 - INFO - __main__ - Printing 3 examples
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:18:21 - INFO - __main__ - ['false']
03/19/2022 08:18:21 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:18:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:18:21 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:18:27 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:18:28 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:18:28 - INFO - __main__ - Printing 3 examples
03/19/2022 08:18:28 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:18:28 - INFO - __main__ - ['false']
03/19/2022 08:18:28 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:18:28 - INFO - __main__ - ['false']
03/19/2022 08:18:28 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:18:28 - INFO - __main__ - ['true']
03/19/2022 08:18:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:18:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:18:28 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:18:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_42_0.0002_8_predictions.txt
03/19/2022 08:18:30 - INFO - __main__ - Classification-F1 on test data: 0.7474
03/19/2022 08:18:30 - INFO - __main__ - prefix=ethos-religion_16_42, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.747440794499618
03/19/2022 08:18:30 - INFO - __main__ - Running ... prefix=ethos-religion_16_42, lr=0.0001, bsz=8 ...
03/19/2022 08:18:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:18:31 - INFO - __main__ - Printing 3 examples
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:18:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:18:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:18:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:18:31 - INFO - __main__ - Printing 3 examples
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] Racist ass white ass people
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/19/2022 08:18:31 - INFO - __main__ - ['false']
03/19/2022 08:18:31 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:18:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:18:31 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:18:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:18:32 - INFO - __main__ - Starting training!
03/19/2022 08:18:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:18:43 - INFO - __main__ - Starting training!
03/19/2022 08:18:47 - INFO - __main__ - Step 10 Global step 10 Train loss 24.386139 on epoch=4
03/19/2022 08:18:53 - INFO - __main__ - Step 20 Global step 20 Train loss 21.379032 on epoch=9
03/19/2022 08:18:58 - INFO - __main__ - Step 30 Global step 30 Train loss 19.147800 on epoch=14
03/19/2022 08:19:03 - INFO - __main__ - Step 40 Global step 40 Train loss 18.056053 on epoch=19
03/19/2022 08:19:08 - INFO - __main__ - Step 50 Global step 50 Train loss 18.069599 on epoch=24
03/19/2022 08:19:18 - INFO - __main__ - Global step 50 Train loss 20.207726 Classification-F1 0.0 on epoch=24
03/19/2022 08:19:24 - INFO - __main__ - Step 60 Global step 60 Train loss 17.917942 on epoch=29
03/19/2022 08:19:29 - INFO - __main__ - Step 70 Global step 70 Train loss 17.110060 on epoch=34
03/19/2022 08:19:34 - INFO - __main__ - Step 80 Global step 80 Train loss 16.776432 on epoch=39
03/19/2022 08:19:39 - INFO - __main__ - Step 90 Global step 90 Train loss 16.616257 on epoch=44
03/19/2022 08:19:44 - INFO - __main__ - Step 100 Global step 100 Train loss 15.784662 on epoch=49
03/19/2022 08:19:45 - INFO - __main__ - Global step 100 Train loss 16.841070 Classification-F1 0.0 on epoch=49
03/19/2022 08:19:50 - INFO - __main__ - Step 110 Global step 110 Train loss 14.875540 on epoch=54
03/19/2022 08:19:55 - INFO - __main__ - Step 120 Global step 120 Train loss 14.498990 on epoch=59
03/19/2022 08:20:00 - INFO - __main__ - Step 130 Global step 130 Train loss 15.157834 on epoch=64
03/19/2022 08:20:05 - INFO - __main__ - Step 140 Global step 140 Train loss 13.904132 on epoch=69
03/19/2022 08:20:10 - INFO - __main__ - Step 150 Global step 150 Train loss 13.815407 on epoch=74
03/19/2022 08:20:11 - INFO - __main__ - Global step 150 Train loss 14.450379 Classification-F1 0.0 on epoch=74
03/19/2022 08:20:16 - INFO - __main__ - Step 160 Global step 160 Train loss 12.980385 on epoch=79
03/19/2022 08:20:21 - INFO - __main__ - Step 170 Global step 170 Train loss 12.785601 on epoch=84
03/19/2022 08:20:26 - INFO - __main__ - Step 180 Global step 180 Train loss 12.532345 on epoch=89
03/19/2022 08:20:31 - INFO - __main__ - Step 190 Global step 190 Train loss 12.055375 on epoch=94
03/19/2022 08:20:36 - INFO - __main__ - Step 200 Global step 200 Train loss 10.904261 on epoch=99
03/19/2022 08:20:37 - INFO - __main__ - Global step 200 Train loss 12.251593 Classification-F1 0.0 on epoch=99
03/19/2022 08:20:42 - INFO - __main__ - Step 210 Global step 210 Train loss 10.085535 on epoch=104
03/19/2022 08:20:47 - INFO - __main__ - Step 220 Global step 220 Train loss 8.869413 on epoch=109
03/19/2022 08:20:52 - INFO - __main__ - Step 230 Global step 230 Train loss 6.156394 on epoch=114
03/19/2022 08:20:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.174269 on epoch=119
03/19/2022 08:21:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.042681 on epoch=124
03/19/2022 08:21:03 - INFO - __main__ - Global step 250 Train loss 5.465659 Classification-F1 0.3273273273273273 on epoch=124
03/19/2022 08:21:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.867244 on epoch=129
03/19/2022 08:21:13 - INFO - __main__ - Step 270 Global step 270 Train loss 1.127962 on epoch=134
03/19/2022 08:21:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.684492 on epoch=139
03/19/2022 08:21:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.803354 on epoch=144
03/19/2022 08:21:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.341077 on epoch=149
03/19/2022 08:21:29 - INFO - __main__ - Global step 300 Train loss 0.764826 Classification-F1 0.7046153846153846 on epoch=149
03/19/2022 08:21:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.446499 on epoch=154
03/19/2022 08:21:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.437879 on epoch=159
03/19/2022 08:21:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.380292 on epoch=164
03/19/2022 08:21:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.289062 on epoch=169
03/19/2022 08:21:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.305812 on epoch=174
03/19/2022 08:21:55 - INFO - __main__ - Global step 350 Train loss 0.371909 Classification-F1 0.6666666666666667 on epoch=174
03/19/2022 08:22:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.302910 on epoch=179
03/19/2022 08:22:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.279798 on epoch=184
03/19/2022 08:22:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.289347 on epoch=189
03/19/2022 08:22:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.406861 on epoch=194
03/19/2022 08:22:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.276832 on epoch=199
03/19/2022 08:22:21 - INFO - __main__ - Global step 400 Train loss 0.311150 Classification-F1 0.5844155844155844 on epoch=199
03/19/2022 08:22:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.332553 on epoch=204
03/19/2022 08:22:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.231191 on epoch=209
03/19/2022 08:22:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.336236 on epoch=214
03/19/2022 08:22:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324077 on epoch=219
03/19/2022 08:22:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.455656 on epoch=224
03/19/2022 08:22:47 - INFO - __main__ - Global step 450 Train loss 0.335943 Classification-F1 0.5844155844155844 on epoch=224
03/19/2022 08:22:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.209017 on epoch=229
03/19/2022 08:22:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.302103 on epoch=234
03/19/2022 08:23:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.420170 on epoch=239
03/19/2022 08:23:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.328126 on epoch=244
03/19/2022 08:23:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.234036 on epoch=249
03/19/2022 08:23:12 - INFO - __main__ - Global step 500 Train loss 0.298691 Classification-F1 0.6389743589743591 on epoch=249
03/19/2022 08:23:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.257708 on epoch=254
03/19/2022 08:23:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.215084 on epoch=259
03/19/2022 08:23:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.355446 on epoch=264
03/19/2022 08:23:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.189572 on epoch=269
03/19/2022 08:23:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.234255 on epoch=274
03/19/2022 08:23:38 - INFO - __main__ - Global step 550 Train loss 0.250413 Classification-F1 0.6000000000000001 on epoch=274
03/19/2022 08:23:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.241193 on epoch=279
03/19/2022 08:23:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.146664 on epoch=284
03/19/2022 08:23:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.182594 on epoch=289
03/19/2022 08:23:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.208794 on epoch=294
03/19/2022 08:24:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.235600 on epoch=299
03/19/2022 08:24:04 - INFO - __main__ - Global step 600 Train loss 0.202969 Classification-F1 0.746031746031746 on epoch=299
03/19/2022 08:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:24:05 - INFO - __main__ - Printing 3 examples
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:24:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:24:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:24:05 - INFO - __main__ - Printing 3 examples
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:24:05 - INFO - __main__ - ['false']
03/19/2022 08:24:05 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:24:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:24:05 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:24:05 - INFO - __main__ - save last model!
03/19/2022 08:24:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:24:13 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:24:13 - INFO - __main__ - Printing 3 examples
03/19/2022 08:24:13 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:24:13 - INFO - __main__ - ['false']
03/19/2022 08:24:13 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:24:13 - INFO - __main__ - ['false']
03/19/2022 08:24:13 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:24:13 - INFO - __main__ - ['true']
03/19/2022 08:24:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:24:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:24:13 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:24:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_42_0.0001_8_predictions.txt
03/19/2022 08:24:15 - INFO - __main__ - Classification-F1 on test data: 0.7155
03/19/2022 08:24:15 - INFO - __main__ - prefix=ethos-religion_16_42, lr=0.0001, bsz=8, dev_performance=0.746031746031746, test_performance=0.7155172413793103
03/19/2022 08:24:15 - INFO - __main__ - Running ... prefix=ethos-religion_16_87, lr=0.0005, bsz=8 ...
03/19/2022 08:24:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:24:16 - INFO - __main__ - Starting training!
03/19/2022 08:24:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:24:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:24:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:24:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:24:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:24:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:24:16 - INFO - __main__ - ['false']
03/19/2022 08:24:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:24:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:24:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:24:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:24:29 - INFO - __main__ - Starting training!
03/19/2022 08:24:33 - INFO - __main__ - Step 10 Global step 10 Train loss 23.518326 on epoch=4
03/19/2022 08:24:38 - INFO - __main__ - Step 20 Global step 20 Train loss 17.754881 on epoch=9
03/19/2022 08:24:43 - INFO - __main__ - Step 30 Global step 30 Train loss 15.743414 on epoch=14
03/19/2022 08:24:48 - INFO - __main__ - Step 40 Global step 40 Train loss 13.768117 on epoch=19
03/19/2022 08:24:53 - INFO - __main__ - Step 50 Global step 50 Train loss 11.072218 on epoch=24
03/19/2022 08:25:03 - INFO - __main__ - Global step 50 Train loss 16.371393 Classification-F1 0.03 on epoch=24
03/19/2022 08:25:08 - INFO - __main__ - Step 60 Global step 60 Train loss 5.338881 on epoch=29
03/19/2022 08:25:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.777212 on epoch=34
03/19/2022 08:25:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.446864 on epoch=39
03/19/2022 08:25:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.409374 on epoch=44
03/19/2022 08:25:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.349168 on epoch=49
03/19/2022 08:25:29 - INFO - __main__ - Global step 100 Train loss 1.464300 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 08:25:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.402211 on epoch=54
03/19/2022 08:25:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.348835 on epoch=59
03/19/2022 08:25:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.303729 on epoch=64
03/19/2022 08:25:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.400622 on epoch=69
03/19/2022 08:25:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.323813 on epoch=74
03/19/2022 08:25:56 - INFO - __main__ - Global step 150 Train loss 0.355842 Classification-F1 0.4285714285714286 on epoch=74
03/19/2022 08:26:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.416253 on epoch=79
03/19/2022 08:26:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.315595 on epoch=84
03/19/2022 08:26:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.317224 on epoch=89
03/19/2022 08:26:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.315330 on epoch=94
03/19/2022 08:26:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.329326 on epoch=99
03/19/2022 08:26:23 - INFO - __main__ - Global step 200 Train loss 0.338745 Classification-F1 0.39999999999999997 on epoch=99
03/19/2022 08:26:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.304103 on epoch=104
03/19/2022 08:26:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.328048 on epoch=109
03/19/2022 08:26:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.337252 on epoch=114
03/19/2022 08:26:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.291078 on epoch=119
03/19/2022 08:26:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.319833 on epoch=124
03/19/2022 08:26:48 - INFO - __main__ - Global step 250 Train loss 0.316063 Classification-F1 0.3764102564102564 on epoch=124
03/19/2022 08:26:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.331185 on epoch=129
03/19/2022 08:26:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.350896 on epoch=134
03/19/2022 08:27:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.326539 on epoch=139
03/19/2022 08:27:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.310470 on epoch=144
03/19/2022 08:27:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.304379 on epoch=149
03/19/2022 08:27:14 - INFO - __main__ - Global step 300 Train loss 0.324694 Classification-F1 0.3764102564102564 on epoch=149
03/19/2022 08:27:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.333268 on epoch=154
03/19/2022 08:27:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.355638 on epoch=159
03/19/2022 08:27:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.317085 on epoch=164
03/19/2022 08:27:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.304299 on epoch=169
03/19/2022 08:27:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.335443 on epoch=174
03/19/2022 08:27:40 - INFO - __main__ - Global step 350 Train loss 0.329147 Classification-F1 0.3764102564102564 on epoch=174
03/19/2022 08:27:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.412875 on epoch=179
03/19/2022 08:27:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.358244 on epoch=184
03/19/2022 08:27:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.307084 on epoch=189
03/19/2022 08:28:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.294868 on epoch=194
03/19/2022 08:28:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.315304 on epoch=199
03/19/2022 08:28:05 - INFO - __main__ - Global step 400 Train loss 0.337675 Classification-F1 0.33793103448275863 on epoch=199
03/19/2022 08:28:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.291842 on epoch=204
03/19/2022 08:28:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.254921 on epoch=209
03/19/2022 08:28:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.305295 on epoch=214
03/19/2022 08:28:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.227828 on epoch=219
03/19/2022 08:28:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.224870 on epoch=224
03/19/2022 08:28:31 - INFO - __main__ - Global step 450 Train loss 0.260951 Classification-F1 0.3764102564102564 on epoch=224
03/19/2022 08:28:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.364825 on epoch=229
03/19/2022 08:28:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.341618 on epoch=234
03/19/2022 08:28:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.217441 on epoch=239
03/19/2022 08:28:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.156794 on epoch=244
03/19/2022 08:28:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.307515 on epoch=249
03/19/2022 08:28:56 - INFO - __main__ - Global step 500 Train loss 0.277639 Classification-F1 0.5151515151515151 on epoch=249
03/19/2022 08:29:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.151319 on epoch=254
03/19/2022 08:29:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.123668 on epoch=259
03/19/2022 08:29:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.099527 on epoch=264
03/19/2022 08:29:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.194513 on epoch=269
03/19/2022 08:29:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.104721 on epoch=274
03/19/2022 08:29:23 - INFO - __main__ - Global step 550 Train loss 0.134749 Classification-F1 0.49090909090909085 on epoch=274
03/19/2022 08:29:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.040347 on epoch=279
03/19/2022 08:29:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.117649 on epoch=284
03/19/2022 08:29:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.034146 on epoch=289
03/19/2022 08:29:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.011032 on epoch=294
03/19/2022 08:29:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.047617 on epoch=299
03/19/2022 08:29:49 - INFO - __main__ - Global step 600 Train loss 0.050158 Classification-F1 0.5933528836754642 on epoch=299
03/19/2022 08:29:50 - INFO - __main__ - save last model!
03/19/2022 08:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:29:50 - INFO - __main__ - Printing 3 examples
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:29:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:29:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:29:50 - INFO - __main__ - Printing 3 examples
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:29:50 - INFO - __main__ - ['false']
03/19/2022 08:29:50 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:29:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:29:50 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:29:56 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:29:57 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:29:57 - INFO - __main__ - Printing 3 examples
03/19/2022 08:29:57 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:29:57 - INFO - __main__ - ['false']
03/19/2022 08:29:57 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:29:57 - INFO - __main__ - ['false']
03/19/2022 08:29:57 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:29:57 - INFO - __main__ - ['true']
03/19/2022 08:29:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:29:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:29:57 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:30:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_87_0.0005_8_predictions.txt
03/19/2022 08:30:00 - INFO - __main__ - Classification-F1 on test data: 0.5510
03/19/2022 08:30:00 - INFO - __main__ - prefix=ethos-religion_16_87, lr=0.0005, bsz=8, dev_performance=0.5933528836754642, test_performance=0.5509677419354839
03/19/2022 08:30:00 - INFO - __main__ - Running ... prefix=ethos-religion_16_87, lr=0.0003, bsz=8 ...
03/19/2022 08:30:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:30:01 - INFO - __main__ - Starting training!
03/19/2022 08:30:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:30:01 - INFO - __main__ - Printing 3 examples
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:30:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:30:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:30:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:30:01 - INFO - __main__ - Printing 3 examples
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:30:01 - INFO - __main__ - ['false']
03/19/2022 08:30:01 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:30:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:30:01 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:30:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:30:12 - INFO - __main__ - Starting training!
03/19/2022 08:30:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.676737 on epoch=4
03/19/2022 08:30:21 - INFO - __main__ - Step 20 Global step 20 Train loss 20.175327 on epoch=9
03/19/2022 08:30:26 - INFO - __main__ - Step 30 Global step 30 Train loss 17.664518 on epoch=14
03/19/2022 08:30:31 - INFO - __main__ - Step 40 Global step 40 Train loss 15.893625 on epoch=19
03/19/2022 08:30:36 - INFO - __main__ - Step 50 Global step 50 Train loss 14.850061 on epoch=24
03/19/2022 08:30:37 - INFO - __main__ - Global step 50 Train loss 18.452053 Classification-F1 0.0 on epoch=24
03/19/2022 08:30:42 - INFO - __main__ - Step 60 Global step 60 Train loss 13.332720 on epoch=29
03/19/2022 08:30:47 - INFO - __main__ - Step 70 Global step 70 Train loss 12.579629 on epoch=34
03/19/2022 08:30:52 - INFO - __main__ - Step 80 Global step 80 Train loss 12.074501 on epoch=39
03/19/2022 08:30:57 - INFO - __main__ - Step 90 Global step 90 Train loss 9.595285 on epoch=44
03/19/2022 08:31:02 - INFO - __main__ - Step 100 Global step 100 Train loss 5.546049 on epoch=49
03/19/2022 08:31:03 - INFO - __main__ - Global step 100 Train loss 10.625637 Classification-F1 0.19047619047619047 on epoch=49
03/19/2022 08:31:09 - INFO - __main__ - Step 110 Global step 110 Train loss 4.281763 on epoch=54
03/19/2022 08:31:14 - INFO - __main__ - Step 120 Global step 120 Train loss 2.487817 on epoch=59
03/19/2022 08:31:19 - INFO - __main__ - Step 130 Global step 130 Train loss 2.780763 on epoch=64
03/19/2022 08:31:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.331271 on epoch=69
03/19/2022 08:31:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.587744 on epoch=74
03/19/2022 08:31:29 - INFO - __main__ - Global step 150 Train loss 2.893872 Classification-F1 0.4458874458874459 on epoch=74
03/19/2022 08:31:35 - INFO - __main__ - Step 160 Global step 160 Train loss 2.406470 on epoch=79
03/19/2022 08:31:40 - INFO - __main__ - Step 170 Global step 170 Train loss 2.280865 on epoch=84
03/19/2022 08:31:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.781208 on epoch=89
03/19/2022 08:31:50 - INFO - __main__ - Step 190 Global step 190 Train loss 2.486344 on epoch=94
03/19/2022 08:31:55 - INFO - __main__ - Step 200 Global step 200 Train loss 2.147906 on epoch=99
03/19/2022 08:31:55 - INFO - __main__ - Global step 200 Train loss 2.220559 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 08:32:00 - INFO - __main__ - Step 210 Global step 210 Train loss 1.656521 on epoch=104
03/19/2022 08:32:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.559113 on epoch=109
03/19/2022 08:32:10 - INFO - __main__ - Step 230 Global step 230 Train loss 1.536395 on epoch=114
03/19/2022 08:32:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.235060 on epoch=119
03/19/2022 08:32:20 - INFO - __main__ - Step 250 Global step 250 Train loss 1.311476 on epoch=124
03/19/2022 08:32:21 - INFO - __main__ - Global step 250 Train loss 1.459713 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 08:32:26 - INFO - __main__ - Step 260 Global step 260 Train loss 1.213624 on epoch=129
03/19/2022 08:32:31 - INFO - __main__ - Step 270 Global step 270 Train loss 1.641566 on epoch=134
03/19/2022 08:32:36 - INFO - __main__ - Step 280 Global step 280 Train loss 1.187793 on epoch=139
03/19/2022 08:32:41 - INFO - __main__ - Step 290 Global step 290 Train loss 1.321790 on epoch=144
03/19/2022 08:32:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.848933 on epoch=149
03/19/2022 08:32:46 - INFO - __main__ - Global step 300 Train loss 1.242741 Classification-F1 0.4385964912280702 on epoch=149
03/19/2022 08:32:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.807679 on epoch=154
03/19/2022 08:32:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.474364 on epoch=159
03/19/2022 08:33:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.297246 on epoch=164
03/19/2022 08:33:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.333095 on epoch=169
03/19/2022 08:33:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.586930 on epoch=174
03/19/2022 08:33:12 - INFO - __main__ - Global step 350 Train loss 0.499863 Classification-F1 0.7757757757757757 on epoch=174
03/19/2022 08:33:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.352168 on epoch=179
03/19/2022 08:33:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.213586 on epoch=184
03/19/2022 08:33:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.189475 on epoch=189
03/19/2022 08:33:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.188170 on epoch=194
03/19/2022 08:33:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.104805 on epoch=199
03/19/2022 08:33:38 - INFO - __main__ - Global step 400 Train loss 0.209641 Classification-F1 0.7702564102564102 on epoch=199
03/19/2022 08:33:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.100524 on epoch=204
03/19/2022 08:33:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.048456 on epoch=209
03/19/2022 08:33:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.085153 on epoch=214
03/19/2022 08:33:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.132680 on epoch=219
03/19/2022 08:34:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.061802 on epoch=224
03/19/2022 08:34:03 - INFO - __main__ - Global step 450 Train loss 0.085723 Classification-F1 0.8095238095238095 on epoch=224
03/19/2022 08:34:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.062073 on epoch=229
03/19/2022 08:34:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.081532 on epoch=234
03/19/2022 08:34:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.050435 on epoch=239
03/19/2022 08:34:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.032098 on epoch=244
03/19/2022 08:34:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.049663 on epoch=249
03/19/2022 08:34:29 - INFO - __main__ - Global step 500 Train loss 0.055160 Classification-F1 0.805668016194332 on epoch=249
03/19/2022 08:34:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.042681 on epoch=254
03/19/2022 08:34:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.020949 on epoch=259
03/19/2022 08:34:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.013533 on epoch=264
03/19/2022 08:34:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.039924 on epoch=269
03/19/2022 08:34:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.089430 on epoch=274
03/19/2022 08:34:55 - INFO - __main__ - Global step 550 Train loss 0.041303 Classification-F1 0.805668016194332 on epoch=274
03/19/2022 08:35:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.031736 on epoch=279
03/19/2022 08:35:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.007264 on epoch=284
03/19/2022 08:35:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012995 on epoch=289
03/19/2022 08:35:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014698 on epoch=294
03/19/2022 08:35:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.013351 on epoch=299
03/19/2022 08:35:20 - INFO - __main__ - Global step 600 Train loss 0.016009 Classification-F1 0.805668016194332 on epoch=299
03/19/2022 08:35:20 - INFO - __main__ - save last model!
03/19/2022 08:35:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:35:22 - INFO - __main__ - Printing 3 examples
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:35:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:35:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:35:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:35:22 - INFO - __main__ - Printing 3 examples
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:35:22 - INFO - __main__ - ['false']
03/19/2022 08:35:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:35:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:35:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:35:27 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:35:28 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:35:28 - INFO - __main__ - Printing 3 examples
03/19/2022 08:35:28 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:35:28 - INFO - __main__ - ['false']
03/19/2022 08:35:28 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:35:28 - INFO - __main__ - ['false']
03/19/2022 08:35:28 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:35:28 - INFO - __main__ - ['true']
03/19/2022 08:35:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:35:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:35:28 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:35:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_87_0.0003_8_predictions.txt
03/19/2022 08:35:29 - INFO - __main__ - Classification-F1 on test data: 0.7532
03/19/2022 08:35:29 - INFO - __main__ - prefix=ethos-religion_16_87, lr=0.0003, bsz=8, dev_performance=0.8095238095238095, test_performance=0.7531525851197982
03/19/2022 08:35:30 - INFO - __main__ - Running ... prefix=ethos-religion_16_87, lr=0.0002, bsz=8 ...
03/19/2022 08:35:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:35:30 - INFO - __main__ - Printing 3 examples
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:35:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:35:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:35:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:35:30 - INFO - __main__ - Printing 3 examples
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:35:30 - INFO - __main__ - ['false']
03/19/2022 08:35:30 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:35:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:35:31 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:35:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:35:35 - INFO - __main__ - Starting training!
03/19/2022 08:35:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:35:41 - INFO - __main__ - Starting training!
03/19/2022 08:35:45 - INFO - __main__ - Step 10 Global step 10 Train loss 23.101568 on epoch=4
03/19/2022 08:35:50 - INFO - __main__ - Step 20 Global step 20 Train loss 19.243010 on epoch=9
03/19/2022 08:35:55 - INFO - __main__ - Step 30 Global step 30 Train loss 18.139040 on epoch=14
03/19/2022 08:36:00 - INFO - __main__ - Step 40 Global step 40 Train loss 16.826021 on epoch=19
03/19/2022 08:36:05 - INFO - __main__ - Step 50 Global step 50 Train loss 16.394657 on epoch=24
03/19/2022 08:36:15 - INFO - __main__ - Global step 50 Train loss 18.740860 Classification-F1 0.0 on epoch=24
03/19/2022 08:36:21 - INFO - __main__ - Step 60 Global step 60 Train loss 15.100418 on epoch=29
03/19/2022 08:36:26 - INFO - __main__ - Step 70 Global step 70 Train loss 14.519630 on epoch=34
03/19/2022 08:36:31 - INFO - __main__ - Step 80 Global step 80 Train loss 13.647142 on epoch=39
03/19/2022 08:36:36 - INFO - __main__ - Step 90 Global step 90 Train loss 13.438273 on epoch=44
03/19/2022 08:36:41 - INFO - __main__ - Step 100 Global step 100 Train loss 11.913847 on epoch=49
03/19/2022 08:36:46 - INFO - __main__ - Global step 100 Train loss 13.723863 Classification-F1 0.0 on epoch=49
03/19/2022 08:36:51 - INFO - __main__ - Step 110 Global step 110 Train loss 10.466757 on epoch=54
03/19/2022 08:36:56 - INFO - __main__ - Step 120 Global step 120 Train loss 9.700800 on epoch=59
03/19/2022 08:37:01 - INFO - __main__ - Step 130 Global step 130 Train loss 7.224308 on epoch=64
03/19/2022 08:37:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.879188 on epoch=69
03/19/2022 08:37:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.130874 on epoch=74
03/19/2022 08:37:12 - INFO - __main__ - Global step 150 Train loss 6.680385 Classification-F1 0.41700404858299595 on epoch=74
03/19/2022 08:37:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.567850 on epoch=79
03/19/2022 08:37:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.541018 on epoch=84
03/19/2022 08:37:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.347406 on epoch=89
03/19/2022 08:37:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.192152 on epoch=94
03/19/2022 08:37:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.246347 on epoch=99
03/19/2022 08:37:38 - INFO - __main__ - Global step 200 Train loss 0.378955 Classification-F1 0.7184750733137829 on epoch=99
03/19/2022 08:37:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.262409 on epoch=104
03/19/2022 08:37:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.571683 on epoch=109
03/19/2022 08:37:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.188698 on epoch=114
03/19/2022 08:37:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.257422 on epoch=119
03/19/2022 08:38:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.228942 on epoch=124
03/19/2022 08:38:05 - INFO - __main__ - Global step 250 Train loss 0.301831 Classification-F1 0.5333333333333333 on epoch=124
03/19/2022 08:38:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.118796 on epoch=129
03/19/2022 08:38:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.073489 on epoch=134
03/19/2022 08:38:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.089325 on epoch=139
03/19/2022 08:38:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.132522 on epoch=144
03/19/2022 08:38:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.060696 on epoch=149
03/19/2022 08:38:30 - INFO - __main__ - Global step 300 Train loss 0.094966 Classification-F1 0.5588547189819725 on epoch=149
03/19/2022 08:38:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.053411 on epoch=154
03/19/2022 08:38:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.065450 on epoch=159
03/19/2022 08:38:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.058601 on epoch=164
03/19/2022 08:38:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.056752 on epoch=169
03/19/2022 08:38:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.069128 on epoch=174
03/19/2022 08:38:56 - INFO - __main__ - Global step 350 Train loss 0.060668 Classification-F1 0.6862745098039216 on epoch=174
03/19/2022 08:39:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.049313 on epoch=179
03/19/2022 08:39:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.042930 on epoch=184
03/19/2022 08:39:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.032582 on epoch=189
03/19/2022 08:39:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.013598 on epoch=194
03/19/2022 08:39:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.011598 on epoch=199
03/19/2022 08:39:21 - INFO - __main__ - Global step 400 Train loss 0.030004 Classification-F1 0.5844155844155844 on epoch=199
03/19/2022 08:39:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.050336 on epoch=204
03/19/2022 08:39:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.021768 on epoch=209
03/19/2022 08:39:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046136 on epoch=214
03/19/2022 08:39:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.009950 on epoch=219
03/19/2022 08:39:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005779 on epoch=224
03/19/2022 08:39:47 - INFO - __main__ - Global step 450 Train loss 0.026794 Classification-F1 0.8117647058823529 on epoch=224
03/19/2022 08:39:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008832 on epoch=229
03/19/2022 08:39:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019494 on epoch=234
03/19/2022 08:40:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.027632 on epoch=239
03/19/2022 08:40:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026089 on epoch=244
03/19/2022 08:40:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.023191 on epoch=249
03/19/2022 08:40:13 - INFO - __main__ - Global step 500 Train loss 0.021048 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 08:40:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006631 on epoch=254
03/19/2022 08:40:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016500 on epoch=259
03/19/2022 08:40:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.014889 on epoch=264
03/19/2022 08:40:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.008586 on epoch=269
03/19/2022 08:40:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.010757 on epoch=274
03/19/2022 08:40:39 - INFO - __main__ - Global step 550 Train loss 0.011472 Classification-F1 0.7046153846153846 on epoch=274
03/19/2022 08:40:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005600 on epoch=279
03/19/2022 08:40:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.041521 on epoch=284
03/19/2022 08:40:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.022373 on epoch=289
03/19/2022 08:40:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.018447 on epoch=294
03/19/2022 08:41:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006325 on epoch=299
03/19/2022 08:41:05 - INFO - __main__ - Global step 600 Train loss 0.018853 Classification-F1 0.6666666666666667 on epoch=299
03/19/2022 08:41:05 - INFO - __main__ - save last model!
03/19/2022 08:41:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:41:06 - INFO - __main__ - Printing 3 examples
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:41:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:41:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:41:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:41:06 - INFO - __main__ - Printing 3 examples
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:41:06 - INFO - __main__ - ['false']
03/19/2022 08:41:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:41:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:41:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:41:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:41:13 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:41:13 - INFO - __main__ - Printing 3 examples
03/19/2022 08:41:13 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:41:13 - INFO - __main__ - ['false']
03/19/2022 08:41:13 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:41:13 - INFO - __main__ - ['false']
03/19/2022 08:41:13 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:41:13 - INFO - __main__ - ['true']
03/19/2022 08:41:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:41:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:41:13 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:41:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_87_0.0002_8_predictions.txt
03/19/2022 08:41:15 - INFO - __main__ - Classification-F1 on test data: 0.7869
03/19/2022 08:41:15 - INFO - __main__ - prefix=ethos-religion_16_87, lr=0.0002, bsz=8, dev_performance=0.8117647058823529, test_performance=0.7869387755102042
03/19/2022 08:41:15 - INFO - __main__ - Running ... prefix=ethos-religion_16_87, lr=0.0001, bsz=8 ...
03/19/2022 08:41:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:41:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:41:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:41:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:41:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:41:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] Send all these whiny black crybabies back to central Apefrica to live for just one month. They would be crying to come back to America where White people pamper and coddle them. Let's try the experiment one time.
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] Are you mentally handicapped or this is your normal form? Both ways kill yourself please.
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
03/19/2022 08:41:16 - INFO - __main__ - ['false']
03/19/2022 08:41:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:41:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:41:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:41:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:41:17 - INFO - __main__ - Starting training!
03/19/2022 08:41:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:41:29 - INFO - __main__ - Starting training!
03/19/2022 08:41:33 - INFO - __main__ - Step 10 Global step 10 Train loss 23.366766 on epoch=4
03/19/2022 08:41:38 - INFO - __main__ - Step 20 Global step 20 Train loss 21.251154 on epoch=9
03/19/2022 08:41:43 - INFO - __main__ - Step 30 Global step 30 Train loss 19.292974 on epoch=14
03/19/2022 08:41:48 - INFO - __main__ - Step 40 Global step 40 Train loss 18.710079 on epoch=19
03/19/2022 08:41:53 - INFO - __main__ - Step 50 Global step 50 Train loss 18.426403 on epoch=24
03/19/2022 08:42:02 - INFO - __main__ - Global step 50 Train loss 20.209476 Classification-F1 0.0 on epoch=24
03/19/2022 08:42:07 - INFO - __main__ - Step 60 Global step 60 Train loss 18.382229 on epoch=29
03/19/2022 08:42:12 - INFO - __main__ - Step 70 Global step 70 Train loss 16.959661 on epoch=34
03/19/2022 08:42:17 - INFO - __main__ - Step 80 Global step 80 Train loss 16.714336 on epoch=39
03/19/2022 08:42:22 - INFO - __main__ - Step 90 Global step 90 Train loss 15.463823 on epoch=44
03/19/2022 08:42:27 - INFO - __main__ - Step 100 Global step 100 Train loss 15.527766 on epoch=49
03/19/2022 08:42:31 - INFO - __main__ - Global step 100 Train loss 16.609564 Classification-F1 0.0 on epoch=49
03/19/2022 08:42:36 - INFO - __main__ - Step 110 Global step 110 Train loss 15.130682 on epoch=54
03/19/2022 08:42:41 - INFO - __main__ - Step 120 Global step 120 Train loss 14.684082 on epoch=59
03/19/2022 08:42:46 - INFO - __main__ - Step 130 Global step 130 Train loss 14.457194 on epoch=64
03/19/2022 08:42:51 - INFO - __main__ - Step 140 Global step 140 Train loss 14.280022 on epoch=69
03/19/2022 08:42:56 - INFO - __main__ - Step 150 Global step 150 Train loss 13.513318 on epoch=74
03/19/2022 08:42:59 - INFO - __main__ - Global step 150 Train loss 14.413059 Classification-F1 0.0 on epoch=74
03/19/2022 08:43:04 - INFO - __main__ - Step 160 Global step 160 Train loss 13.658867 on epoch=79
03/19/2022 08:43:09 - INFO - __main__ - Step 170 Global step 170 Train loss 12.617867 on epoch=84
03/19/2022 08:43:14 - INFO - __main__ - Step 180 Global step 180 Train loss 12.726255 on epoch=89
03/19/2022 08:43:19 - INFO - __main__ - Step 190 Global step 190 Train loss 12.277905 on epoch=94
03/19/2022 08:43:24 - INFO - __main__ - Step 200 Global step 200 Train loss 11.074466 on epoch=99
03/19/2022 08:43:27 - INFO - __main__ - Global step 200 Train loss 12.471072 Classification-F1 0.0 on epoch=99
03/19/2022 08:43:32 - INFO - __main__ - Step 210 Global step 210 Train loss 10.326313 on epoch=104
03/19/2022 08:43:37 - INFO - __main__ - Step 220 Global step 220 Train loss 10.537004 on epoch=109
03/19/2022 08:43:42 - INFO - __main__ - Step 230 Global step 230 Train loss 9.057393 on epoch=114
03/19/2022 08:43:47 - INFO - __main__ - Step 240 Global step 240 Train loss 7.593614 on epoch=119
03/19/2022 08:43:52 - INFO - __main__ - Step 250 Global step 250 Train loss 6.412171 on epoch=124
03/19/2022 08:43:56 - INFO - __main__ - Global step 250 Train loss 8.785300 Classification-F1 0.0 on epoch=124
03/19/2022 08:44:01 - INFO - __main__ - Step 260 Global step 260 Train loss 4.125845 on epoch=129
03/19/2022 08:44:06 - INFO - __main__ - Step 270 Global step 270 Train loss 3.471007 on epoch=134
03/19/2022 08:44:11 - INFO - __main__ - Step 280 Global step 280 Train loss 3.610442 on epoch=139
03/19/2022 08:44:16 - INFO - __main__ - Step 290 Global step 290 Train loss 3.056116 on epoch=144
03/19/2022 08:44:21 - INFO - __main__ - Step 300 Global step 300 Train loss 3.226810 on epoch=149
03/19/2022 08:44:21 - INFO - __main__ - Global step 300 Train loss 3.498044 Classification-F1 0.4181818181818182 on epoch=149
03/19/2022 08:44:27 - INFO - __main__ - Step 310 Global step 310 Train loss 2.915145 on epoch=154
03/19/2022 08:44:32 - INFO - __main__ - Step 320 Global step 320 Train loss 3.768729 on epoch=159
03/19/2022 08:44:37 - INFO - __main__ - Step 330 Global step 330 Train loss 3.081087 on epoch=164
03/19/2022 08:44:42 - INFO - __main__ - Step 340 Global step 340 Train loss 2.971378 on epoch=169
03/19/2022 08:44:47 - INFO - __main__ - Step 350 Global step 350 Train loss 2.640514 on epoch=174
03/19/2022 08:44:47 - INFO - __main__ - Global step 350 Train loss 3.075371 Classification-F1 0.4385964912280702 on epoch=174
03/19/2022 08:44:53 - INFO - __main__ - Step 360 Global step 360 Train loss 3.138330 on epoch=179
03/19/2022 08:44:58 - INFO - __main__ - Step 370 Global step 370 Train loss 2.168939 on epoch=184
03/19/2022 08:45:02 - INFO - __main__ - Step 380 Global step 380 Train loss 3.232046 on epoch=189
03/19/2022 08:45:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.862999 on epoch=194
03/19/2022 08:45:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.657883 on epoch=199
03/19/2022 08:45:13 - INFO - __main__ - Global step 400 Train loss 2.012039 Classification-F1 0.4920634920634921 on epoch=199
03/19/2022 08:45:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.499065 on epoch=204
03/19/2022 08:45:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.462200 on epoch=209
03/19/2022 08:45:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.414862 on epoch=214
03/19/2022 08:45:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.265331 on epoch=219
03/19/2022 08:45:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.265839 on epoch=224
03/19/2022 08:45:38 - INFO - __main__ - Global step 450 Train loss 0.381459 Classification-F1 0.7046153846153846 on epoch=224
03/19/2022 08:45:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.175199 on epoch=229
03/19/2022 08:45:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.187996 on epoch=234
03/19/2022 08:45:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.121197 on epoch=239
03/19/2022 08:45:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.147619 on epoch=244
03/19/2022 08:46:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.123696 on epoch=249
03/19/2022 08:46:04 - INFO - __main__ - Global step 500 Train loss 0.151141 Classification-F1 0.7333333333333334 on epoch=249
03/19/2022 08:46:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.155812 on epoch=254
03/19/2022 08:46:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.076557 on epoch=259
03/19/2022 08:46:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.100531 on epoch=264
03/19/2022 08:46:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.097476 on epoch=269
03/19/2022 08:46:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.082696 on epoch=274
03/19/2022 08:46:30 - INFO - __main__ - Global step 550 Train loss 0.102614 Classification-F1 0.6945917285259808 on epoch=274
03/19/2022 08:46:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.070206 on epoch=279
03/19/2022 08:46:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.127346 on epoch=284
03/19/2022 08:46:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.047397 on epoch=289
03/19/2022 08:46:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.073590 on epoch=294
03/19/2022 08:46:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.047436 on epoch=299
03/19/2022 08:46:54 - INFO - __main__ - Global step 600 Train loss 0.073195 Classification-F1 0.6945917285259808 on epoch=299
03/19/2022 08:46:54 - INFO - __main__ - save last model!
03/19/2022 08:47:01 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:47:02 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 08:47:02 - INFO - __main__ - Printing 3 examples
03/19/2022 08:47:02 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 08:47:02 - INFO - __main__ - ['false']
03/19/2022 08:47:02 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/19/2022 08:47:02 - INFO - __main__ - ['false']
03/19/2022 08:47:02 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/19/2022 08:47:02 - INFO - __main__ - ['true']
03/19/2022 08:47:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:47:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:47:02 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 08:47:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-religion/ethos-religion_16_87_0.0001_8_predictions.txt
03/19/2022 08:47:04 - INFO - __main__ - Classification-F1 on test data: 0.5328
03/19/2022 08:47:05 - INFO - __main__ - prefix=ethos-religion_16_87, lr=0.0001, bsz=8, dev_performance=0.7333333333333334, test_performance=0.5327604726100966
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22278): No such process
Task: amazon_polarity, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 08:47:10 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 08:47:10 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-amazon_polarity
03/19/2022 08:47:10 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 08:47:10 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-amazon_polarity
03/19/2022 08:47:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 08:47:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 08:47:11 - INFO - __main__ - args.device: cuda:0
03/19/2022 08:47:11 - INFO - __main__ - Using 2 gpus
03/19/2022 08:47:11 - INFO - __main__ - args.device: cuda:1
03/19/2022 08:47:11 - INFO - __main__ - Using 2 gpus
03/19/2022 08:47:11 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/19/2022 08:47:11 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/19/2022 08:47:15 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0005, bsz=8 ...
03/19/2022 08:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:47:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:47:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:47:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:47:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:47:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:47:16 - INFO - __main__ - Printing 3 examples
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:47:16 - INFO - __main__ - ['positive']
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:47:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:47:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:47:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:47:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:47:29 - INFO - __main__ - Starting training!
03/19/2022 08:47:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:47:30 - INFO - __main__ - Starting training!
03/19/2022 08:47:35 - INFO - __main__ - Step 10 Global step 10 Train loss 23.086369 on epoch=4
03/19/2022 08:47:39 - INFO - __main__ - Step 20 Global step 20 Train loss 16.556047 on epoch=9
03/19/2022 08:47:44 - INFO - __main__ - Step 30 Global step 30 Train loss 14.892534 on epoch=14
03/19/2022 08:47:50 - INFO - __main__ - Step 40 Global step 40 Train loss 12.523476 on epoch=19
03/19/2022 08:47:54 - INFO - __main__ - Step 50 Global step 50 Train loss 9.334984 on epoch=24
03/19/2022 08:47:55 - INFO - __main__ - Global step 50 Train loss 15.278683 Classification-F1 0.0 on epoch=24
03/19/2022 08:48:02 - INFO - __main__ - Step 60 Global step 60 Train loss 4.676122 on epoch=29
03/19/2022 08:48:07 - INFO - __main__ - Step 70 Global step 70 Train loss 3.128148 on epoch=34
03/19/2022 08:48:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.397223 on epoch=39
03/19/2022 08:48:17 - INFO - __main__ - Step 90 Global step 90 Train loss 3.089792 on epoch=44
03/19/2022 08:48:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.024976 on epoch=49
03/19/2022 08:48:23 - INFO - __main__ - Global step 100 Train loss 2.863252 Classification-F1 0.5636363636363637 on epoch=49
03/19/2022 08:48:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.990109 on epoch=54
03/19/2022 08:48:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.067141 on epoch=59
03/19/2022 08:48:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.440998 on epoch=64
03/19/2022 08:48:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.407120 on epoch=69
03/19/2022 08:48:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.386737 on epoch=74
03/19/2022 08:48:51 - INFO - __main__ - Global step 150 Train loss 0.658421 Classification-F1 0.5465587044534412 on epoch=74
03/19/2022 08:48:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.357458 on epoch=79
03/19/2022 08:49:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.364501 on epoch=84
03/19/2022 08:49:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.333538 on epoch=89
03/19/2022 08:49:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.342171 on epoch=94
03/19/2022 08:49:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.370173 on epoch=99
03/19/2022 08:49:17 - INFO - __main__ - Global step 200 Train loss 0.353568 Classification-F1 0.6532019704433498 on epoch=99
03/19/2022 08:49:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.341588 on epoch=104
03/19/2022 08:49:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.331165 on epoch=109
03/19/2022 08:49:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.364480 on epoch=114
03/19/2022 08:49:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.333424 on epoch=119
03/19/2022 08:49:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.299211 on epoch=124
03/19/2022 08:49:44 - INFO - __main__ - Global step 250 Train loss 0.333973 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 08:49:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.341300 on epoch=129
03/19/2022 08:49:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.319143 on epoch=134
03/19/2022 08:49:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.325866 on epoch=139
03/19/2022 08:50:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.313813 on epoch=144
03/19/2022 08:50:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.299301 on epoch=149
03/19/2022 08:50:10 - INFO - __main__ - Global step 300 Train loss 0.319885 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 08:50:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.293746 on epoch=154
03/19/2022 08:50:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342290 on epoch=159
03/19/2022 08:50:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.254230 on epoch=164
03/19/2022 08:50:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.274511 on epoch=169
03/19/2022 08:50:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.266479 on epoch=174
03/19/2022 08:50:35 - INFO - __main__ - Global step 350 Train loss 0.286251 Classification-F1 0.3992490613266583 on epoch=174
03/19/2022 08:50:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.205478 on epoch=179
03/19/2022 08:50:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.226710 on epoch=184
03/19/2022 08:50:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.124306 on epoch=189
03/19/2022 08:50:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.123454 on epoch=194
03/19/2022 08:51:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.150243 on epoch=199
03/19/2022 08:51:01 - INFO - __main__ - Global step 400 Train loss 0.166038 Classification-F1 0.5844155844155844 on epoch=199
03/19/2022 08:51:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.044122 on epoch=204
03/19/2022 08:51:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.025511 on epoch=209
03/19/2022 08:51:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.015307 on epoch=214
03/19/2022 08:51:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.036680 on epoch=219
03/19/2022 08:51:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.029284 on epoch=224
03/19/2022 08:51:27 - INFO - __main__ - Global step 450 Train loss 0.030181 Classification-F1 0.6559139784946237 on epoch=224
03/19/2022 08:51:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.037097 on epoch=229
03/19/2022 08:51:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.006960 on epoch=234
03/19/2022 08:51:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005173 on epoch=239
03/19/2022 08:51:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003050 on epoch=244
03/19/2022 08:51:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002854 on epoch=249
03/19/2022 08:51:54 - INFO - __main__ - Global step 500 Train loss 0.011027 Classification-F1 0.5835835835835835 on epoch=249
03/19/2022 08:51:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003615 on epoch=254
03/19/2022 08:52:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003432 on epoch=259
03/19/2022 08:52:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.020525 on epoch=264
03/19/2022 08:52:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.017018 on epoch=269
03/19/2022 08:52:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000707 on epoch=274
03/19/2022 08:52:20 - INFO - __main__ - Global step 550 Train loss 0.009059 Classification-F1 0.6235294117647059 on epoch=274
03/19/2022 08:52:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001306 on epoch=279
03/19/2022 08:52:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000364 on epoch=284
03/19/2022 08:52:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001329 on epoch=289
03/19/2022 08:52:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001290 on epoch=294
03/19/2022 08:52:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000384 on epoch=299
03/19/2022 08:52:45 - INFO - __main__ - Global step 600 Train loss 0.000935 Classification-F1 0.6862745098039216 on epoch=299
03/19/2022 08:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:52:46 - INFO - __main__ - Printing 3 examples
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:52:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:52:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:52:46 - INFO - __main__ - Printing 3 examples
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:52:46 - INFO - __main__ - ['positive']
03/19/2022 08:52:46 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:52:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:52:46 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:52:46 - INFO - __main__ - save last model!
03/19/2022 08:52:53 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:52:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 08:52:54 - INFO - __main__ - Printing 3 examples
03/19/2022 08:52:54 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 08:52:54 - INFO - __main__ - ['negative']
03/19/2022 08:52:54 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 08:52:54 - INFO - __main__ - ['negative']
03/19/2022 08:52:54 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 08:52:54 - INFO - __main__ - ['negative']
03/19/2022 08:52:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:52:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:52:56 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 08:52:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:52:59 - INFO - __main__ - Starting training!
03/19/2022 08:53:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0005_8_predictions.txt
03/19/2022 08:53:11 - INFO - __main__ - Classification-F1 on test data: 0.1637
03/19/2022 08:53:11 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.6862745098039216, test_performance=0.1636681977604291
03/19/2022 08:53:11 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0003, bsz=8 ...
03/19/2022 08:53:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:53:12 - INFO - __main__ - Printing 3 examples
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:53:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:53:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:53:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:53:12 - INFO - __main__ - Printing 3 examples
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:53:12 - INFO - __main__ - ['positive']
03/19/2022 08:53:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:53:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:53:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:53:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:53:25 - INFO - __main__ - Starting training!
03/19/2022 08:53:30 - INFO - __main__ - Step 10 Global step 10 Train loss 22.408432 on epoch=4
03/19/2022 08:53:35 - INFO - __main__ - Step 20 Global step 20 Train loss 17.342834 on epoch=9
03/19/2022 08:53:40 - INFO - __main__ - Step 30 Global step 30 Train loss 16.289265 on epoch=14
03/19/2022 08:53:45 - INFO - __main__ - Step 40 Global step 40 Train loss 14.196775 on epoch=19
03/19/2022 08:53:50 - INFO - __main__ - Step 50 Global step 50 Train loss 13.295032 on epoch=24
03/19/2022 08:53:56 - INFO - __main__ - Global step 50 Train loss 16.706467 Classification-F1 0.0 on epoch=24
03/19/2022 08:54:02 - INFO - __main__ - Step 60 Global step 60 Train loss 11.796650 on epoch=29
03/19/2022 08:54:07 - INFO - __main__ - Step 70 Global step 70 Train loss 8.553556 on epoch=34
03/19/2022 08:54:12 - INFO - __main__ - Step 80 Global step 80 Train loss 3.481055 on epoch=39
03/19/2022 08:54:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.549428 on epoch=44
03/19/2022 08:54:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.321768 on epoch=49
03/19/2022 08:54:22 - INFO - __main__ - Global step 100 Train loss 4.940492 Classification-F1 0.9372549019607843 on epoch=49
03/19/2022 08:54:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.287231 on epoch=54
03/19/2022 08:54:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.313096 on epoch=59
03/19/2022 08:54:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.178260 on epoch=64
03/19/2022 08:54:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.123092 on epoch=69
03/19/2022 08:54:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.029542 on epoch=74
03/19/2022 08:54:49 - INFO - __main__ - Global step 150 Train loss 0.186244 Classification-F1 0.9687194525904204 on epoch=74
03/19/2022 08:54:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.014884 on epoch=79
03/19/2022 08:54:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.006496 on epoch=84
03/19/2022 08:55:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.062042 on epoch=89
03/19/2022 08:55:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.005141 on epoch=94
03/19/2022 08:55:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001987 on epoch=99
03/19/2022 08:55:15 - INFO - __main__ - Global step 200 Train loss 0.018110 Classification-F1 1.0 on epoch=99
03/19/2022 08:55:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.007445 on epoch=104
03/19/2022 08:55:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.035317 on epoch=109
03/19/2022 08:55:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.034175 on epoch=114
03/19/2022 08:55:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000976 on epoch=119
03/19/2022 08:55:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000645 on epoch=124
03/19/2022 08:55:41 - INFO - __main__ - Global step 250 Train loss 0.015711 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 08:55:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000359 on epoch=129
03/19/2022 08:55:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.011316 on epoch=134
03/19/2022 08:55:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001212 on epoch=139
03/19/2022 08:56:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000402 on epoch=144
03/19/2022 08:56:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000387 on epoch=149
03/19/2022 08:56:07 - INFO - __main__ - Global step 300 Train loss 0.002735 Classification-F1 1.0 on epoch=149
03/19/2022 08:56:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000501 on epoch=154
03/19/2022 08:56:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000367 on epoch=159
03/19/2022 08:56:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004707 on epoch=164
03/19/2022 08:56:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000207 on epoch=169
03/19/2022 08:56:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000158 on epoch=174
03/19/2022 08:56:33 - INFO - __main__ - Global step 350 Train loss 0.001188 Classification-F1 1.0 on epoch=174
03/19/2022 08:56:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000072 on epoch=179
03/19/2022 08:56:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000068 on epoch=184
03/19/2022 08:56:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000063 on epoch=189
03/19/2022 08:56:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000106 on epoch=194
03/19/2022 08:56:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000061 on epoch=199
03/19/2022 08:56:59 - INFO - __main__ - Global step 400 Train loss 0.000074 Classification-F1 1.0 on epoch=199
03/19/2022 08:57:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000248 on epoch=204
03/19/2022 08:57:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000092 on epoch=209
03/19/2022 08:57:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003010 on epoch=214
03/19/2022 08:57:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000041 on epoch=219
03/19/2022 08:57:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000052 on epoch=224
03/19/2022 08:57:25 - INFO - __main__ - Global step 450 Train loss 0.000689 Classification-F1 1.0 on epoch=224
03/19/2022 08:57:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000165 on epoch=229
03/19/2022 08:57:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000041 on epoch=234
03/19/2022 08:57:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000046 on epoch=239
03/19/2022 08:57:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000045 on epoch=244
03/19/2022 08:57:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000054 on epoch=249
03/19/2022 08:57:51 - INFO - __main__ - Global step 500 Train loss 0.000070 Classification-F1 1.0 on epoch=249
03/19/2022 08:57:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000017 on epoch=254
03/19/2022 08:58:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000022 on epoch=259
03/19/2022 08:58:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000023 on epoch=264
03/19/2022 08:58:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000029 on epoch=269
03/19/2022 08:58:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000017 on epoch=274
03/19/2022 08:58:17 - INFO - __main__ - Global step 550 Train loss 0.000021 Classification-F1 1.0 on epoch=274
03/19/2022 08:58:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000033 on epoch=279
03/19/2022 08:58:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000041 on epoch=284
03/19/2022 08:58:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000021 on epoch=289
03/19/2022 08:58:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000022 on epoch=294
03/19/2022 08:58:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.053939 on epoch=299
03/19/2022 08:58:42 - INFO - __main__ - Global step 600 Train loss 0.010811 Classification-F1 1.0 on epoch=299
03/19/2022 08:58:42 - INFO - __main__ - save last model!
03/19/2022 08:58:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:58:43 - INFO - __main__ - Printing 3 examples
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 08:58:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:58:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:58:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:58:43 - INFO - __main__ - Printing 3 examples
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:58:43 - INFO - __main__ - ['positive']
03/19/2022 08:58:43 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:58:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:58:43 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:58:49 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 08:58:50 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 08:58:50 - INFO - __main__ - Printing 3 examples
03/19/2022 08:58:50 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 08:58:50 - INFO - __main__ - ['negative']
03/19/2022 08:58:50 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 08:58:50 - INFO - __main__ - ['negative']
03/19/2022 08:58:50 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 08:58:50 - INFO - __main__ - ['negative']
03/19/2022 08:58:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:58:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:58:52 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 08:58:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:58:57 - INFO - __main__ - Starting training!
03/19/2022 08:59:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0003_8_predictions.txt
03/19/2022 08:59:06 - INFO - __main__ - Classification-F1 on test data: 0.6147
03/19/2022 08:59:06 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.614732572367208
03/19/2022 08:59:06 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0002, bsz=8 ...
03/19/2022 08:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:59:07 - INFO - __main__ - Printing 3 examples
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 08:59:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:59:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 08:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 08:59:07 - INFO - __main__ - Printing 3 examples
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 08:59:07 - INFO - __main__ - ['positive']
03/19/2022 08:59:07 - INFO - __main__ - Tokenizing Input ...
03/19/2022 08:59:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 08:59:07 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 08:59:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 08:59:20 - INFO - __main__ - Starting training!
03/19/2022 08:59:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.253117 on epoch=4
03/19/2022 08:59:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.822865 on epoch=9
03/19/2022 08:59:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.805679 on epoch=14
03/19/2022 08:59:40 - INFO - __main__ - Step 40 Global step 40 Train loss 16.169350 on epoch=19
03/19/2022 08:59:45 - INFO - __main__ - Step 50 Global step 50 Train loss 14.921196 on epoch=24
03/19/2022 08:59:50 - INFO - __main__ - Global step 50 Train loss 17.794443 Classification-F1 0.0 on epoch=24
03/19/2022 08:59:56 - INFO - __main__ - Step 60 Global step 60 Train loss 14.495448 on epoch=29
03/19/2022 09:00:01 - INFO - __main__ - Step 70 Global step 70 Train loss 12.709044 on epoch=34
03/19/2022 09:00:06 - INFO - __main__ - Step 80 Global step 80 Train loss 12.637682 on epoch=39
03/19/2022 09:00:11 - INFO - __main__ - Step 90 Global step 90 Train loss 11.614586 on epoch=44
03/19/2022 09:00:16 - INFO - __main__ - Step 100 Global step 100 Train loss 10.474515 on epoch=49
03/19/2022 09:00:29 - INFO - __main__ - Global step 100 Train loss 12.386256 Classification-F1 0.0 on epoch=49
03/19/2022 09:00:34 - INFO - __main__ - Step 110 Global step 110 Train loss 3.438374 on epoch=54
03/19/2022 09:00:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.426199 on epoch=59
03/19/2022 09:00:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.149670 on epoch=64
03/19/2022 09:00:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.088664 on epoch=69
03/19/2022 09:00:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.121375 on epoch=74
03/19/2022 09:00:54 - INFO - __main__ - Global step 150 Train loss 0.844856 Classification-F1 1.0 on epoch=74
03/19/2022 09:01:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.040332 on epoch=79
03/19/2022 09:01:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.007862 on epoch=84
03/19/2022 09:01:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.006967 on epoch=89
03/19/2022 09:01:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001397 on epoch=94
03/19/2022 09:01:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000667 on epoch=99
03/19/2022 09:01:21 - INFO - __main__ - Global step 200 Train loss 0.011445 Classification-F1 0.9687194525904204 on epoch=99
03/19/2022 09:01:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000657 on epoch=104
03/19/2022 09:01:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005007 on epoch=109
03/19/2022 09:01:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001232 on epoch=114
03/19/2022 09:01:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001138 on epoch=119
03/19/2022 09:01:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000238 on epoch=124
03/19/2022 09:01:47 - INFO - __main__ - Global step 250 Train loss 0.001654 Classification-F1 1.0 on epoch=124
03/19/2022 09:01:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000259 on epoch=129
03/19/2022 09:01:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000326 on epoch=134
03/19/2022 09:02:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000241 on epoch=139
03/19/2022 09:02:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000126 on epoch=144
03/19/2022 09:02:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000116 on epoch=149
03/19/2022 09:02:13 - INFO - __main__ - Global step 300 Train loss 0.000214 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 09:02:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000457 on epoch=154
03/19/2022 09:02:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000190 on epoch=159
03/19/2022 09:02:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000049 on epoch=164
03/19/2022 09:02:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000123 on epoch=169
03/19/2022 09:02:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000103 on epoch=174
03/19/2022 09:02:38 - INFO - __main__ - Global step 350 Train loss 0.000184 Classification-F1 1.0 on epoch=174
03/19/2022 09:02:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.002366 on epoch=179
03/19/2022 09:02:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000071 on epoch=184
03/19/2022 09:02:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000025 on epoch=189
03/19/2022 09:02:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000065 on epoch=194
03/19/2022 09:03:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000039 on epoch=199
03/19/2022 09:03:04 - INFO - __main__ - Global step 400 Train loss 0.000513 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 09:03:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000025 on epoch=204
03/19/2022 09:03:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000014 on epoch=209
03/19/2022 09:03:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000032 on epoch=214
03/19/2022 09:03:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000100 on epoch=219
03/19/2022 09:03:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000032 on epoch=224
03/19/2022 09:03:30 - INFO - __main__ - Global step 450 Train loss 0.000041 Classification-F1 1.0 on epoch=224
03/19/2022 09:03:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000019 on epoch=229
03/19/2022 09:03:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.011959 on epoch=234
03/19/2022 09:03:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000117 on epoch=239
03/19/2022 09:03:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000048 on epoch=244
03/19/2022 09:03:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000093 on epoch=249
03/19/2022 09:03:57 - INFO - __main__ - Global step 500 Train loss 0.002447 Classification-F1 1.0 on epoch=249
03/19/2022 09:04:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000035 on epoch=254
03/19/2022 09:04:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000008 on epoch=259
03/19/2022 09:04:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006188 on epoch=264
03/19/2022 09:04:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000144 on epoch=269
03/19/2022 09:04:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000025 on epoch=274
03/19/2022 09:04:22 - INFO - __main__ - Global step 550 Train loss 0.001280 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 09:04:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000096 on epoch=279
03/19/2022 09:04:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000007 on epoch=284
03/19/2022 09:04:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000010 on epoch=289
03/19/2022 09:04:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000451 on epoch=294
03/19/2022 09:04:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.030040 on epoch=299
03/19/2022 09:04:48 - INFO - __main__ - Global step 600 Train loss 0.006121 Classification-F1 1.0 on epoch=299
03/19/2022 09:04:48 - INFO - __main__ - save last model!
03/19/2022 09:04:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:04:49 - INFO - __main__ - Printing 3 examples
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:04:49 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:04:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:04:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:04:49 - INFO - __main__ - Printing 3 examples
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 09:04:49 - INFO - __main__ - ['positive']
03/19/2022 09:04:49 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:04:49 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:04:50 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:04:55 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:04:56 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:04:56 - INFO - __main__ - Printing 3 examples
03/19/2022 09:04:56 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:04:56 - INFO - __main__ - ['negative']
03/19/2022 09:04:56 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:04:56 - INFO - __main__ - ['negative']
03/19/2022 09:04:56 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:04:56 - INFO - __main__ - ['negative']
03/19/2022 09:04:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:04:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:04:58 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:05:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:05:03 - INFO - __main__ - Starting training!
03/19/2022 09:05:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0002_8_predictions.txt
03/19/2022 09:05:12 - INFO - __main__ - Classification-F1 on test data: 0.9470
03/19/2022 09:05:12 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9469766166879594
03/19/2022 09:05:13 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0001, bsz=8 ...
03/19/2022 09:05:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:05:13 - INFO - __main__ - Printing 3 examples
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:05:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:05:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:05:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:05:13 - INFO - __main__ - Printing 3 examples
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/19/2022 09:05:13 - INFO - __main__ - ['positive']
03/19/2022 09:05:13 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:05:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:05:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:05:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:05:27 - INFO - __main__ - Starting training!
03/19/2022 09:05:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.349091 on epoch=4
03/19/2022 09:05:36 - INFO - __main__ - Step 20 Global step 20 Train loss 20.805000 on epoch=9
03/19/2022 09:05:40 - INFO - __main__ - Step 30 Global step 30 Train loss 19.240246 on epoch=14
03/19/2022 09:05:45 - INFO - __main__ - Step 40 Global step 40 Train loss 17.816250 on epoch=19
03/19/2022 09:05:50 - INFO - __main__ - Step 50 Global step 50 Train loss 17.849936 on epoch=24
03/19/2022 09:05:55 - INFO - __main__ - Global step 50 Train loss 19.812105 Classification-F1 0.0 on epoch=24
03/19/2022 09:06:01 - INFO - __main__ - Step 60 Global step 60 Train loss 16.751064 on epoch=29
03/19/2022 09:06:06 - INFO - __main__ - Step 70 Global step 70 Train loss 16.397701 on epoch=34
03/19/2022 09:06:11 - INFO - __main__ - Step 80 Global step 80 Train loss 16.302124 on epoch=39
03/19/2022 09:06:16 - INFO - __main__ - Step 90 Global step 90 Train loss 16.168848 on epoch=44
03/19/2022 09:06:21 - INFO - __main__ - Step 100 Global step 100 Train loss 14.871828 on epoch=49
03/19/2022 09:06:21 - INFO - __main__ - Global step 100 Train loss 16.098312 Classification-F1 0.0 on epoch=49
03/19/2022 09:06:26 - INFO - __main__ - Step 110 Global step 110 Train loss 14.225548 on epoch=54
03/19/2022 09:06:31 - INFO - __main__ - Step 120 Global step 120 Train loss 14.413385 on epoch=59
03/19/2022 09:06:36 - INFO - __main__ - Step 130 Global step 130 Train loss 13.865430 on epoch=64
03/19/2022 09:06:41 - INFO - __main__ - Step 140 Global step 140 Train loss 13.693471 on epoch=69
03/19/2022 09:06:46 - INFO - __main__ - Step 150 Global step 150 Train loss 13.297854 on epoch=74
03/19/2022 09:06:47 - INFO - __main__ - Global step 150 Train loss 13.899137 Classification-F1 0.0 on epoch=74
03/19/2022 09:06:52 - INFO - __main__ - Step 160 Global step 160 Train loss 13.286520 on epoch=79
03/19/2022 09:06:57 - INFO - __main__ - Step 170 Global step 170 Train loss 12.375179 on epoch=84
03/19/2022 09:07:02 - INFO - __main__ - Step 180 Global step 180 Train loss 12.154474 on epoch=89
03/19/2022 09:07:07 - INFO - __main__ - Step 190 Global step 190 Train loss 11.571712 on epoch=94
03/19/2022 09:07:12 - INFO - __main__ - Step 200 Global step 200 Train loss 11.186537 on epoch=99
03/19/2022 09:07:13 - INFO - __main__ - Global step 200 Train loss 12.114885 Classification-F1 0.0 on epoch=99
03/19/2022 09:07:18 - INFO - __main__ - Step 210 Global step 210 Train loss 10.877688 on epoch=104
03/19/2022 09:07:23 - INFO - __main__ - Step 220 Global step 220 Train loss 9.989186 on epoch=109
03/19/2022 09:07:28 - INFO - __main__ - Step 230 Global step 230 Train loss 9.370094 on epoch=114
03/19/2022 09:07:33 - INFO - __main__ - Step 240 Global step 240 Train loss 6.758595 on epoch=119
03/19/2022 09:07:38 - INFO - __main__ - Step 250 Global step 250 Train loss 5.381712 on epoch=124
03/19/2022 09:07:38 - INFO - __main__ - Global step 250 Train loss 8.475455 Classification-F1 0.539313399778516 on epoch=124
03/19/2022 09:07:44 - INFO - __main__ - Step 260 Global step 260 Train loss 3.419300 on epoch=129
03/19/2022 09:07:49 - INFO - __main__ - Step 270 Global step 270 Train loss 3.427347 on epoch=134
03/19/2022 09:07:54 - INFO - __main__ - Step 280 Global step 280 Train loss 4.480230 on epoch=139
03/19/2022 09:07:59 - INFO - __main__ - Step 290 Global step 290 Train loss 2.972924 on epoch=144
03/19/2022 09:08:03 - INFO - __main__ - Step 300 Global step 300 Train loss 2.597848 on epoch=149
03/19/2022 09:08:04 - INFO - __main__ - Global step 300 Train loss 3.379530 Classification-F1 0.38888888888888884 on epoch=149
03/19/2022 09:08:09 - INFO - __main__ - Step 310 Global step 310 Train loss 2.551507 on epoch=154
03/19/2022 09:08:14 - INFO - __main__ - Step 320 Global step 320 Train loss 2.034810 on epoch=159
03/19/2022 09:08:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.942202 on epoch=164
03/19/2022 09:08:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.768981 on epoch=169
03/19/2022 09:08:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.027411 on epoch=174
03/19/2022 09:08:29 - INFO - __main__ - Global step 350 Train loss 1.864982 Classification-F1 0.746031746031746 on epoch=174
03/19/2022 09:08:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.534946 on epoch=179
03/19/2022 09:08:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.537040 on epoch=184
03/19/2022 09:08:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.589484 on epoch=189
03/19/2022 09:08:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.374086 on epoch=194
03/19/2022 09:08:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.361874 on epoch=199
03/19/2022 09:08:55 - INFO - __main__ - Global step 400 Train loss 0.479486 Classification-F1 0.5636363636363637 on epoch=199
03/19/2022 09:09:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.396180 on epoch=204
03/19/2022 09:09:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.387567 on epoch=209
03/19/2022 09:09:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.458970 on epoch=214
03/19/2022 09:09:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.367323 on epoch=219
03/19/2022 09:09:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.370137 on epoch=224
03/19/2022 09:09:21 - INFO - __main__ - Global step 450 Train loss 0.396035 Classification-F1 0.805668016194332 on epoch=224
03/19/2022 09:09:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.490247 on epoch=229
03/19/2022 09:09:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.415001 on epoch=234
03/19/2022 09:09:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.318781 on epoch=239
03/19/2022 09:09:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.519590 on epoch=244
03/19/2022 09:09:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.352629 on epoch=249
03/19/2022 09:09:47 - INFO - __main__ - Global step 500 Train loss 0.419250 Classification-F1 0.6666666666666667 on epoch=249
03/19/2022 09:09:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.381283 on epoch=254
03/19/2022 09:09:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.333245 on epoch=259
03/19/2022 09:10:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.391465 on epoch=264
03/19/2022 09:10:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.606265 on epoch=269
03/19/2022 09:10:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.360468 on epoch=274
03/19/2022 09:10:12 - INFO - __main__ - Global step 550 Train loss 0.414545 Classification-F1 0.5636363636363637 on epoch=274
03/19/2022 09:10:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.334710 on epoch=279
03/19/2022 09:10:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.322215 on epoch=284
03/19/2022 09:10:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.398004 on epoch=289
03/19/2022 09:10:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.305571 on epoch=294
03/19/2022 09:10:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.500741 on epoch=299
03/19/2022 09:10:37 - INFO - __main__ - Global step 600 Train loss 0.372248 Classification-F1 0.5636363636363637 on epoch=299
03/19/2022 09:10:37 - INFO - __main__ - save last model!
03/19/2022 09:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:10:38 - INFO - __main__ - Printing 3 examples
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:10:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:10:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:10:38 - INFO - __main__ - Printing 3 examples
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:10:38 - INFO - __main__ - ['negative']
03/19/2022 09:10:38 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:10:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:10:38 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:10:44 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:10:44 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:10:44 - INFO - __main__ - Printing 3 examples
03/19/2022 09:10:44 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:10:44 - INFO - __main__ - ['negative']
03/19/2022 09:10:44 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:10:44 - INFO - __main__ - ['negative']
03/19/2022 09:10:44 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:10:44 - INFO - __main__ - ['negative']
03/19/2022 09:10:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:10:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:10:46 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:10:49 - INFO - __main__ - Starting training!
03/19/2022 09:11:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0001_8_predictions.txt
03/19/2022 09:11:01 - INFO - __main__ - Classification-F1 on test data: 0.7205
03/19/2022 09:11:02 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0001, bsz=8, dev_performance=0.805668016194332, test_performance=0.7205385532591415
03/19/2022 09:11:02 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0005, bsz=8 ...
03/19/2022 09:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:11:03 - INFO - __main__ - Printing 3 examples
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:11:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:11:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:11:03 - INFO - __main__ - Printing 3 examples
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:11:03 - INFO - __main__ - ['negative']
03/19/2022 09:11:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:11:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:11:03 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:11:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:11:16 - INFO - __main__ - Starting training!
03/19/2022 09:11:22 - INFO - __main__ - Step 10 Global step 10 Train loss 23.675089 on epoch=4
03/19/2022 09:11:27 - INFO - __main__ - Step 20 Global step 20 Train loss 16.755527 on epoch=9
03/19/2022 09:11:32 - INFO - __main__ - Step 30 Global step 30 Train loss 14.429403 on epoch=14
03/19/2022 09:11:37 - INFO - __main__ - Step 40 Global step 40 Train loss 12.219376 on epoch=19
03/19/2022 09:11:42 - INFO - __main__ - Step 50 Global step 50 Train loss 8.965490 on epoch=24
03/19/2022 09:11:43 - INFO - __main__ - Global step 50 Train loss 15.208979 Classification-F1 0.2782608695652174 on epoch=24
03/19/2022 09:11:48 - INFO - __main__ - Step 60 Global step 60 Train loss 5.343720 on epoch=29
03/19/2022 09:11:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.440024 on epoch=34
03/19/2022 09:11:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.606428 on epoch=39
03/19/2022 09:12:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.360006 on epoch=44
03/19/2022 09:12:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.279654 on epoch=49
03/19/2022 09:12:08 - INFO - __main__ - Global step 100 Train loss 1.605966 Classification-F1 0.5333333333333333 on epoch=49
03/19/2022 09:12:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.188254 on epoch=54
03/19/2022 09:12:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.185817 on epoch=59
03/19/2022 09:12:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.318316 on epoch=64
03/19/2022 09:12:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.348533 on epoch=69
03/19/2022 09:12:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.852321 on epoch=74
03/19/2022 09:12:35 - INFO - __main__ - Global step 150 Train loss 0.378648 Classification-F1 0.26821705426356585 on epoch=74
03/19/2022 09:12:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.518839 on epoch=79
03/19/2022 09:12:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.266446 on epoch=84
03/19/2022 09:12:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.257079 on epoch=89
03/19/2022 09:12:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.195124 on epoch=94
03/19/2022 09:13:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.144330 on epoch=99
03/19/2022 09:13:01 - INFO - __main__ - Global step 200 Train loss 0.276364 Classification-F1 0.6389743589743591 on epoch=99
03/19/2022 09:13:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.227102 on epoch=104
03/19/2022 09:13:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.087801 on epoch=109
03/19/2022 09:13:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.212753 on epoch=114
03/19/2022 09:13:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.049250 on epoch=119
03/19/2022 09:13:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.071401 on epoch=124
03/19/2022 09:13:28 - INFO - __main__ - Global step 250 Train loss 0.129661 Classification-F1 0.6559139784946237 on epoch=124
03/19/2022 09:13:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.040195 on epoch=129
03/19/2022 09:13:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.020129 on epoch=134
03/19/2022 09:13:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027627 on epoch=139
03/19/2022 09:13:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.108728 on epoch=144
03/19/2022 09:13:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.232695 on epoch=149
03/19/2022 09:13:54 - INFO - __main__ - Global step 300 Train loss 0.085875 Classification-F1 0.6476476476476476 on epoch=149
03/19/2022 09:13:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.165736 on epoch=154
03/19/2022 09:14:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.185903 on epoch=159
03/19/2022 09:14:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.102396 on epoch=164
03/19/2022 09:14:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.042426 on epoch=169
03/19/2022 09:14:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.335688 on epoch=174
03/19/2022 09:14:20 - INFO - __main__ - Global step 350 Train loss 0.166430 Classification-F1 0.6190476190476191 on epoch=174
03/19/2022 09:14:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.106342 on epoch=179
03/19/2022 09:14:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.027263 on epoch=184
03/19/2022 09:14:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.102122 on epoch=189
03/19/2022 09:14:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.057720 on epoch=194
03/19/2022 09:14:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.020352 on epoch=199
03/19/2022 09:14:46 - INFO - __main__ - Global step 400 Train loss 0.062760 Classification-F1 0.6532019704433498 on epoch=199
03/19/2022 09:14:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.065742 on epoch=204
03/19/2022 09:14:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.082215 on epoch=209
03/19/2022 09:15:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008445 on epoch=214
03/19/2022 09:15:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014008 on epoch=219
03/19/2022 09:15:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.019070 on epoch=224
03/19/2022 09:15:11 - INFO - __main__ - Global step 450 Train loss 0.037896 Classification-F1 0.7184750733137829 on epoch=224
03/19/2022 09:15:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.019016 on epoch=229
03/19/2022 09:15:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.009360 on epoch=234
03/19/2022 09:15:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.021388 on epoch=239
03/19/2022 09:15:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005412 on epoch=244
03/19/2022 09:15:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004502 on epoch=249
03/19/2022 09:15:38 - INFO - __main__ - Global step 500 Train loss 0.011936 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 09:15:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001261 on epoch=254
03/19/2022 09:15:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004261 on epoch=259
03/19/2022 09:15:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001206 on epoch=264
03/19/2022 09:15:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000928 on epoch=269
03/19/2022 09:16:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002311 on epoch=274
03/19/2022 09:16:04 - INFO - __main__ - Global step 550 Train loss 0.001994 Classification-F1 0.5555555555555556 on epoch=274
03/19/2022 09:16:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000626 on epoch=279
03/19/2022 09:16:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002162 on epoch=284
03/19/2022 09:16:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000408 on epoch=289
03/19/2022 09:16:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000650 on epoch=294
03/19/2022 09:16:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000341 on epoch=299
03/19/2022 09:16:30 - INFO - __main__ - Global step 600 Train loss 0.000837 Classification-F1 0.716256157635468 on epoch=299
03/19/2022 09:16:30 - INFO - __main__ - save last model!
03/19/2022 09:16:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:16:30 - INFO - __main__ - Printing 3 examples
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:16:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:16:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:16:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:16:30 - INFO - __main__ - Printing 3 examples
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:16:30 - INFO - __main__ - ['negative']
03/19/2022 09:16:30 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:16:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:16:31 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:16:36 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:16:37 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:16:37 - INFO - __main__ - Printing 3 examples
03/19/2022 09:16:37 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:16:37 - INFO - __main__ - ['negative']
03/19/2022 09:16:37 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:16:37 - INFO - __main__ - ['negative']
03/19/2022 09:16:37 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:16:37 - INFO - __main__ - ['negative']
03/19/2022 09:16:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:16:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:16:38 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:16:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:16:41 - INFO - __main__ - Starting training!
03/19/2022 09:16:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0005_8_predictions.txt
03/19/2022 09:16:54 - INFO - __main__ - Classification-F1 on test data: 0.6689
03/19/2022 09:16:54 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.7184750733137829, test_performance=0.6689490343001098
03/19/2022 09:16:54 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0003, bsz=8 ...
03/19/2022 09:16:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:16:55 - INFO - __main__ - Printing 3 examples
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:16:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:16:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:16:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:16:55 - INFO - __main__ - Printing 3 examples
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:16:55 - INFO - __main__ - ['negative']
03/19/2022 09:16:55 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:16:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:16:55 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:17:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:17:07 - INFO - __main__ - Starting training!
03/19/2022 09:17:11 - INFO - __main__ - Step 10 Global step 10 Train loss 22.132000 on epoch=4
03/19/2022 09:17:16 - INFO - __main__ - Step 20 Global step 20 Train loss 17.583401 on epoch=9
03/19/2022 09:17:21 - INFO - __main__ - Step 30 Global step 30 Train loss 15.722865 on epoch=14
03/19/2022 09:17:26 - INFO - __main__ - Step 40 Global step 40 Train loss 14.773193 on epoch=19
03/19/2022 09:17:31 - INFO - __main__ - Step 50 Global step 50 Train loss 14.056066 on epoch=24
03/19/2022 09:17:32 - INFO - __main__ - Global step 50 Train loss 16.853504 Classification-F1 0.0 on epoch=24
03/19/2022 09:17:38 - INFO - __main__ - Step 60 Global step 60 Train loss 12.546500 on epoch=29
03/19/2022 09:17:43 - INFO - __main__ - Step 70 Global step 70 Train loss 11.687912 on epoch=34
03/19/2022 09:17:48 - INFO - __main__ - Step 80 Global step 80 Train loss 9.749170 on epoch=39
03/19/2022 09:17:53 - INFO - __main__ - Step 90 Global step 90 Train loss 2.217134 on epoch=44
03/19/2022 09:17:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.476164 on epoch=49
03/19/2022 09:17:58 - INFO - __main__ - Global step 100 Train loss 7.335376 Classification-F1 0.9687194525904204 on epoch=49
03/19/2022 09:18:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.139041 on epoch=54
03/19/2022 09:18:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.600351 on epoch=59
03/19/2022 09:18:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.304625 on epoch=64
03/19/2022 09:18:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.044714 on epoch=69
03/19/2022 09:18:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.136340 on epoch=74
03/19/2022 09:18:25 - INFO - __main__ - Global step 150 Train loss 0.445014 Classification-F1 0.9687194525904204 on epoch=74
03/19/2022 09:18:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.004336 on epoch=79
03/19/2022 09:18:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.000365 on epoch=84
03/19/2022 09:18:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.005894 on epoch=89
03/19/2022 09:18:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001552 on epoch=94
03/19/2022 09:18:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000217 on epoch=99
03/19/2022 09:18:51 - INFO - __main__ - Global step 200 Train loss 0.002473 Classification-F1 0.9375 on epoch=99
03/19/2022 09:18:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000121 on epoch=104
03/19/2022 09:19:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000287 on epoch=109
03/19/2022 09:19:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000493 on epoch=114
03/19/2022 09:19:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000188 on epoch=119
03/19/2022 09:19:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000081 on epoch=124
03/19/2022 09:19:17 - INFO - __main__ - Global step 250 Train loss 0.000234 Classification-F1 0.9375 on epoch=124
03/19/2022 09:19:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000291 on epoch=129
03/19/2022 09:19:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000375 on epoch=134
03/19/2022 09:19:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000283 on epoch=139
03/19/2022 09:19:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.099645 on epoch=144
03/19/2022 09:19:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000619 on epoch=149
03/19/2022 09:19:43 - INFO - __main__ - Global step 300 Train loss 0.020243 Classification-F1 0.9375 on epoch=149
03/19/2022 09:19:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000345 on epoch=154
03/19/2022 09:19:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001002 on epoch=159
03/19/2022 09:19:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000193 on epoch=164
03/19/2022 09:20:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000057 on epoch=169
03/19/2022 09:20:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000091 on epoch=174
03/19/2022 09:20:09 - INFO - __main__ - Global step 350 Train loss 0.000338 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 09:20:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000133 on epoch=179
03/19/2022 09:20:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000068 on epoch=184
03/19/2022 09:20:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000079 on epoch=189
03/19/2022 09:20:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000049 on epoch=194
03/19/2022 09:20:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000021 on epoch=199
03/19/2022 09:20:34 - INFO - __main__ - Global step 400 Train loss 0.000070 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 09:20:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000103 on epoch=204
03/19/2022 09:20:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000033 on epoch=209
03/19/2022 09:20:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000065 on epoch=214
03/19/2022 09:20:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000026 on epoch=219
03/19/2022 09:21:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000033 on epoch=224
03/19/2022 09:21:01 - INFO - __main__ - Global step 450 Train loss 0.000052 Classification-F1 0.906158357771261 on epoch=224
03/19/2022 09:21:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000256 on epoch=229
03/19/2022 09:21:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000080 on epoch=234
03/19/2022 09:21:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000020 on epoch=239
03/19/2022 09:21:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000027 on epoch=244
03/19/2022 09:21:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000012 on epoch=249
03/19/2022 09:21:27 - INFO - __main__ - Global step 500 Train loss 0.000079 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 09:21:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000022 on epoch=254
03/19/2022 09:21:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000017 on epoch=259
03/19/2022 09:21:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000073 on epoch=264
03/19/2022 09:21:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000035 on epoch=269
03/19/2022 09:21:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000023 on epoch=274
03/19/2022 09:21:53 - INFO - __main__ - Global step 550 Train loss 0.000034 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 09:21:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000021 on epoch=279
03/19/2022 09:22:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000016 on epoch=284
03/19/2022 09:22:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000026 on epoch=289
03/19/2022 09:22:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000034 on epoch=294
03/19/2022 09:22:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000030 on epoch=299
03/19/2022 09:22:19 - INFO - __main__ - Global step 600 Train loss 0.000025 Classification-F1 0.906158357771261 on epoch=299
03/19/2022 09:22:19 - INFO - __main__ - save last model!
03/19/2022 09:22:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:22:20 - INFO - __main__ - Printing 3 examples
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:22:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:22:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:22:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:22:20 - INFO - __main__ - Printing 3 examples
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:22:20 - INFO - __main__ - ['negative']
03/19/2022 09:22:20 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:22:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:22:20 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:22:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:22:26 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:22:26 - INFO - __main__ - Printing 3 examples
03/19/2022 09:22:26 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:22:26 - INFO - __main__ - ['negative']
03/19/2022 09:22:26 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:22:26 - INFO - __main__ - ['negative']
03/19/2022 09:22:26 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:22:26 - INFO - __main__ - ['negative']
03/19/2022 09:22:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:22:27 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:22:28 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:22:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:22:34 - INFO - __main__ - Starting training!
03/19/2022 09:22:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0003_8_predictions.txt
03/19/2022 09:22:43 - INFO - __main__ - Classification-F1 on test data: 0.4648
03/19/2022 09:22:44 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.46482516317598777
03/19/2022 09:22:44 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0002, bsz=8 ...
03/19/2022 09:22:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:22:45 - INFO - __main__ - Printing 3 examples
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:22:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:22:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:22:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:22:45 - INFO - __main__ - Printing 3 examples
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:22:45 - INFO - __main__ - ['negative']
03/19/2022 09:22:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:22:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:22:45 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:22:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:22:55 - INFO - __main__ - Starting training!
03/19/2022 09:23:00 - INFO - __main__ - Step 10 Global step 10 Train loss 22.664967 on epoch=4
03/19/2022 09:23:05 - INFO - __main__ - Step 20 Global step 20 Train loss 17.591146 on epoch=9
03/19/2022 09:23:10 - INFO - __main__ - Step 30 Global step 30 Train loss 16.313623 on epoch=14
03/19/2022 09:23:15 - INFO - __main__ - Step 40 Global step 40 Train loss 15.486856 on epoch=19
03/19/2022 09:23:20 - INFO - __main__ - Step 50 Global step 50 Train loss 13.914873 on epoch=24
03/19/2022 09:23:21 - INFO - __main__ - Global step 50 Train loss 17.194294 Classification-F1 0.0 on epoch=24
03/19/2022 09:23:27 - INFO - __main__ - Step 60 Global step 60 Train loss 13.748464 on epoch=29
03/19/2022 09:23:31 - INFO - __main__ - Step 70 Global step 70 Train loss 12.864746 on epoch=34
03/19/2022 09:23:36 - INFO - __main__ - Step 80 Global step 80 Train loss 12.469000 on epoch=39
03/19/2022 09:23:41 - INFO - __main__ - Step 90 Global step 90 Train loss 12.229326 on epoch=44
03/19/2022 09:23:46 - INFO - __main__ - Step 100 Global step 100 Train loss 10.558973 on epoch=49
03/19/2022 09:23:47 - INFO - __main__ - Global step 100 Train loss 12.374103 Classification-F1 0.0 on epoch=49
03/19/2022 09:23:52 - INFO - __main__ - Step 110 Global step 110 Train loss 8.558019 on epoch=54
03/19/2022 09:23:57 - INFO - __main__ - Step 120 Global step 120 Train loss 3.808900 on epoch=59
03/19/2022 09:24:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.977277 on epoch=64
03/19/2022 09:24:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.328319 on epoch=69
03/19/2022 09:24:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.810444 on epoch=74
03/19/2022 09:24:12 - INFO - __main__ - Global step 150 Train loss 3.696591 Classification-F1 0.5844155844155844 on epoch=74
03/19/2022 09:24:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.402983 on epoch=79
03/19/2022 09:24:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.322649 on epoch=84
03/19/2022 09:24:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.156819 on epoch=89
03/19/2022 09:24:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.219954 on epoch=94
03/19/2022 09:24:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.137938 on epoch=99
03/19/2022 09:24:39 - INFO - __main__ - Global step 200 Train loss 0.248069 Classification-F1 0.746031746031746 on epoch=99
03/19/2022 09:24:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.093442 on epoch=104
03/19/2022 09:24:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.302654 on epoch=109
03/19/2022 09:24:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.095268 on epoch=114
03/19/2022 09:25:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.217540 on epoch=119
03/19/2022 09:25:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.059421 on epoch=124
03/19/2022 09:25:05 - INFO - __main__ - Global step 250 Train loss 0.153665 Classification-F1 0.7793103448275862 on epoch=124
03/19/2022 09:25:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.128752 on epoch=129
03/19/2022 09:25:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.118705 on epoch=134
03/19/2022 09:25:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.082103 on epoch=139
03/19/2022 09:25:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.068203 on epoch=144
03/19/2022 09:25:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.035277 on epoch=149
03/19/2022 09:25:32 - INFO - __main__ - Global step 300 Train loss 0.086608 Classification-F1 0.8117647058823529 on epoch=149
03/19/2022 09:25:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.026606 on epoch=154
03/19/2022 09:25:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.043856 on epoch=159
03/19/2022 09:25:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.221315 on epoch=164
03/19/2022 09:25:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.145502 on epoch=169
03/19/2022 09:25:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.163256 on epoch=174
03/19/2022 09:25:59 - INFO - __main__ - Global step 350 Train loss 0.120107 Classification-F1 0.41372141372141363 on epoch=174
03/19/2022 09:26:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.192433 on epoch=179
03/19/2022 09:26:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.110512 on epoch=184
03/19/2022 09:26:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.077954 on epoch=189
03/19/2022 09:26:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.183467 on epoch=194
03/19/2022 09:26:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.100389 on epoch=199
03/19/2022 09:26:25 - INFO - __main__ - Global step 400 Train loss 0.132951 Classification-F1 0.41473684210526324 on epoch=199
03/19/2022 09:26:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.048340 on epoch=204
03/19/2022 09:26:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.073566 on epoch=209
03/19/2022 09:26:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.050437 on epoch=214
03/19/2022 09:26:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.168196 on epoch=219
03/19/2022 09:26:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.024851 on epoch=224
03/19/2022 09:26:50 - INFO - __main__ - Global step 450 Train loss 0.073078 Classification-F1 0.4222222222222223 on epoch=224
03/19/2022 09:26:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.050437 on epoch=229
03/19/2022 09:27:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026893 on epoch=234
03/19/2022 09:27:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.022355 on epoch=239
03/19/2022 09:27:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.030891 on epoch=244
03/19/2022 09:27:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.044698 on epoch=249
03/19/2022 09:27:16 - INFO - __main__ - Global step 500 Train loss 0.035055 Classification-F1 0.43936243936243935 on epoch=249
03/19/2022 09:27:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.018669 on epoch=254
03/19/2022 09:27:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.034702 on epoch=259
03/19/2022 09:27:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.020101 on epoch=264
03/19/2022 09:27:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.017303 on epoch=269
03/19/2022 09:27:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.027270 on epoch=274
03/19/2022 09:27:42 - INFO - __main__ - Global step 550 Train loss 0.023609 Classification-F1 0.461904761904762 on epoch=274
03/19/2022 09:27:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.068747 on epoch=279
03/19/2022 09:27:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.062646 on epoch=284
03/19/2022 09:27:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020229 on epoch=289
03/19/2022 09:28:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.017354 on epoch=294
03/19/2022 09:28:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.038990 on epoch=299
03/19/2022 09:28:08 - INFO - __main__ - Global step 600 Train loss 0.041593 Classification-F1 0.6235294117647059 on epoch=299
03/19/2022 09:28:08 - INFO - __main__ - save last model!
03/19/2022 09:28:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:28:08 - INFO - __main__ - Printing 3 examples
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:28:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:28:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:28:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:28:08 - INFO - __main__ - Printing 3 examples
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:28:08 - INFO - __main__ - ['negative']
03/19/2022 09:28:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:28:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:28:09 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:28:15 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:28:15 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:28:15 - INFO - __main__ - Printing 3 examples
03/19/2022 09:28:15 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:28:15 - INFO - __main__ - ['negative']
03/19/2022 09:28:15 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:28:15 - INFO - __main__ - ['negative']
03/19/2022 09:28:15 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:28:15 - INFO - __main__ - ['negative']
03/19/2022 09:28:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:28:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:28:17 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:28:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:28:22 - INFO - __main__ - Starting training!
03/19/2022 09:28:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0002_8_predictions.txt
03/19/2022 09:28:33 - INFO - __main__ - Classification-F1 on test data: 0.8277
03/19/2022 09:28:33 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0002, bsz=8, dev_performance=0.8117647058823529, test_performance=0.8276812792941826
03/19/2022 09:28:33 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0001, bsz=8 ...
03/19/2022 09:28:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:28:34 - INFO - __main__ - Printing 3 examples
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:28:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:28:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:28:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:28:34 - INFO - __main__ - Printing 3 examples
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/19/2022 09:28:34 - INFO - __main__ - ['negative']
03/19/2022 09:28:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:28:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:28:34 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:28:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:28:47 - INFO - __main__ - Starting training!
03/19/2022 09:28:54 - INFO - __main__ - Step 10 Global step 10 Train loss 23.669025 on epoch=4
03/19/2022 09:28:59 - INFO - __main__ - Step 20 Global step 20 Train loss 18.597065 on epoch=9
03/19/2022 09:29:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.326239 on epoch=14
03/19/2022 09:29:09 - INFO - __main__ - Step 40 Global step 40 Train loss 17.083103 on epoch=19
03/19/2022 09:29:14 - INFO - __main__ - Step 50 Global step 50 Train loss 15.170850 on epoch=24
03/19/2022 09:29:22 - INFO - __main__ - Global step 50 Train loss 18.369257 Classification-F1 0.0 on epoch=24
03/19/2022 09:29:27 - INFO - __main__ - Step 60 Global step 60 Train loss 15.769275 on epoch=29
03/19/2022 09:29:32 - INFO - __main__ - Step 70 Global step 70 Train loss 14.821652 on epoch=34
03/19/2022 09:29:37 - INFO - __main__ - Step 80 Global step 80 Train loss 14.963165 on epoch=39
03/19/2022 09:29:42 - INFO - __main__ - Step 90 Global step 90 Train loss 15.041903 on epoch=44
03/19/2022 09:29:48 - INFO - __main__ - Step 100 Global step 100 Train loss 13.920359 on epoch=49
03/19/2022 09:29:57 - INFO - __main__ - Global step 100 Train loss 14.903271 Classification-F1 0.0 on epoch=49
03/19/2022 09:30:02 - INFO - __main__ - Step 110 Global step 110 Train loss 13.189634 on epoch=54
03/19/2022 09:30:07 - INFO - __main__ - Step 120 Global step 120 Train loss 13.324994 on epoch=59
03/19/2022 09:30:12 - INFO - __main__ - Step 130 Global step 130 Train loss 12.978557 on epoch=64
03/19/2022 09:30:17 - INFO - __main__ - Step 140 Global step 140 Train loss 12.216898 on epoch=69
03/19/2022 09:30:22 - INFO - __main__ - Step 150 Global step 150 Train loss 11.947448 on epoch=74
03/19/2022 09:30:27 - INFO - __main__ - Global step 150 Train loss 12.731506 Classification-F1 0.010582010582010581 on epoch=74
03/19/2022 09:30:33 - INFO - __main__ - Step 160 Global step 160 Train loss 11.231474 on epoch=79
03/19/2022 09:30:38 - INFO - __main__ - Step 170 Global step 170 Train loss 10.826574 on epoch=84
03/19/2022 09:30:43 - INFO - __main__ - Step 180 Global step 180 Train loss 9.407777 on epoch=89
03/19/2022 09:30:48 - INFO - __main__ - Step 190 Global step 190 Train loss 6.731619 on epoch=94
03/19/2022 09:30:53 - INFO - __main__ - Step 200 Global step 200 Train loss 2.088781 on epoch=99
03/19/2022 09:30:53 - INFO - __main__ - Global step 200 Train loss 8.057244 Classification-F1 0.6559139784946237 on epoch=99
03/19/2022 09:30:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.590964 on epoch=104
03/19/2022 09:31:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.221353 on epoch=109
03/19/2022 09:31:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.322175 on epoch=114
03/19/2022 09:31:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.121860 on epoch=119
03/19/2022 09:31:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.152943 on epoch=124
03/19/2022 09:31:20 - INFO - __main__ - Global step 250 Train loss 0.281859 Classification-F1 0.906158357771261 on epoch=124
03/19/2022 09:31:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.085713 on epoch=129
03/19/2022 09:31:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.138849 on epoch=134
03/19/2022 09:31:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.029636 on epoch=139
03/19/2022 09:31:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.025405 on epoch=144
03/19/2022 09:31:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.028345 on epoch=149
03/19/2022 09:31:46 - INFO - __main__ - Global step 300 Train loss 0.061590 Classification-F1 0.8745098039215686 on epoch=149
03/19/2022 09:31:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.033626 on epoch=154
03/19/2022 09:31:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.016396 on epoch=159
03/19/2022 09:32:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.020314 on epoch=164
03/19/2022 09:32:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.016768 on epoch=169
03/19/2022 09:32:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.019120 on epoch=174
03/19/2022 09:32:12 - INFO - __main__ - Global step 350 Train loss 0.021245 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 09:32:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.019584 on epoch=179
03/19/2022 09:32:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.002241 on epoch=184
03/19/2022 09:32:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003852 on epoch=189
03/19/2022 09:32:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001302 on epoch=194
03/19/2022 09:32:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002821 on epoch=199
03/19/2022 09:32:38 - INFO - __main__ - Global step 400 Train loss 0.005960 Classification-F1 0.8745098039215686 on epoch=199
03/19/2022 09:32:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.052897 on epoch=204
03/19/2022 09:32:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001523 on epoch=209
03/19/2022 09:32:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000952 on epoch=214
03/19/2022 09:32:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001309 on epoch=219
03/19/2022 09:33:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.033429 on epoch=224
03/19/2022 09:33:04 - INFO - __main__ - Global step 450 Train loss 0.018022 Classification-F1 0.9375 on epoch=224
03/19/2022 09:33:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001747 on epoch=229
03/19/2022 09:33:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000571 on epoch=234
03/19/2022 09:33:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000242 on epoch=239
03/19/2022 09:33:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000277 on epoch=244
03/19/2022 09:33:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001808 on epoch=249
03/19/2022 09:33:30 - INFO - __main__ - Global step 500 Train loss 0.000929 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 09:33:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.041946 on epoch=254
03/19/2022 09:33:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001513 on epoch=259
03/19/2022 09:33:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000244 on epoch=264
03/19/2022 09:33:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.027018 on epoch=269
03/19/2022 09:33:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000704 on epoch=274
03/19/2022 09:33:56 - INFO - __main__ - Global step 550 Train loss 0.014285 Classification-F1 0.9375 on epoch=274
03/19/2022 09:34:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000223 on epoch=279
03/19/2022 09:34:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000379 on epoch=284
03/19/2022 09:34:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000405 on epoch=289
03/19/2022 09:34:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000613 on epoch=294
03/19/2022 09:34:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000545 on epoch=299
03/19/2022 09:34:22 - INFO - __main__ - Global step 600 Train loss 0.000433 Classification-F1 0.8745098039215686 on epoch=299
03/19/2022 09:34:22 - INFO - __main__ - save last model!
03/19/2022 09:34:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:34:23 - INFO - __main__ - Printing 3 examples
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:34:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:34:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:34:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:34:23 - INFO - __main__ - Printing 3 examples
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:34:23 - INFO - __main__ - ['positive']
03/19/2022 09:34:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:34:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:34:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:34:28 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:34:29 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:34:29 - INFO - __main__ - Printing 3 examples
03/19/2022 09:34:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:34:29 - INFO - __main__ - ['negative']
03/19/2022 09:34:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:34:29 - INFO - __main__ - ['negative']
03/19/2022 09:34:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:34:29 - INFO - __main__ - ['negative']
03/19/2022 09:34:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:34:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:34:31 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:34:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:34:34 - INFO - __main__ - Starting training!
03/19/2022 09:34:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0001_8_predictions.txt
03/19/2022 09:34:46 - INFO - __main__ - Classification-F1 on test data: 0.9147
03/19/2022 09:34:46 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.9147229348152146
03/19/2022 09:34:46 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0005, bsz=8 ...
03/19/2022 09:34:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:34:47 - INFO - __main__ - Printing 3 examples
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:34:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:34:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:34:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:34:47 - INFO - __main__ - Printing 3 examples
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:34:47 - INFO - __main__ - ['positive']
03/19/2022 09:34:47 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:34:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:34:47 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:34:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:34:57 - INFO - __main__ - Starting training!
03/19/2022 09:35:02 - INFO - __main__ - Step 10 Global step 10 Train loss 23.535025 on epoch=4
03/19/2022 09:35:06 - INFO - __main__ - Step 20 Global step 20 Train loss 18.460752 on epoch=9
03/19/2022 09:35:11 - INFO - __main__ - Step 30 Global step 30 Train loss 15.295441 on epoch=14
03/19/2022 09:35:16 - INFO - __main__ - Step 40 Global step 40 Train loss 13.862806 on epoch=19
03/19/2022 09:35:21 - INFO - __main__ - Step 50 Global step 50 Train loss 12.449714 on epoch=24
03/19/2022 09:35:22 - INFO - __main__ - Global step 50 Train loss 16.720749 Classification-F1 0.0 on epoch=24
03/19/2022 09:35:28 - INFO - __main__ - Step 60 Global step 60 Train loss 10.024116 on epoch=29
03/19/2022 09:35:33 - INFO - __main__ - Step 70 Global step 70 Train loss 4.565025 on epoch=34
03/19/2022 09:35:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.959988 on epoch=39
03/19/2022 09:35:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.759230 on epoch=44
03/19/2022 09:35:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.405107 on epoch=49
03/19/2022 09:35:49 - INFO - __main__ - Global step 100 Train loss 3.342694 Classification-F1 0.3992490613266583 on epoch=49
03/19/2022 09:35:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.381644 on epoch=54
03/19/2022 09:36:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.335773 on epoch=59
03/19/2022 09:36:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.411821 on epoch=64
03/19/2022 09:36:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.326417 on epoch=69
03/19/2022 09:36:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.347575 on epoch=74
03/19/2022 09:36:15 - INFO - __main__ - Global step 150 Train loss 0.360646 Classification-F1 0.46843853820598 on epoch=74
03/19/2022 09:36:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.270022 on epoch=79
03/19/2022 09:36:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.320108 on epoch=84
03/19/2022 09:36:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.311734 on epoch=89
03/19/2022 09:36:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.277514 on epoch=94
03/19/2022 09:36:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.266663 on epoch=99
03/19/2022 09:36:42 - INFO - __main__ - Global step 200 Train loss 0.289208 Classification-F1 0.5151515151515151 on epoch=99
03/19/2022 09:36:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.390448 on epoch=104
03/19/2022 09:36:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.255999 on epoch=109
03/19/2022 09:36:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.227859 on epoch=114
03/19/2022 09:37:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.160340 on epoch=119
03/19/2022 09:37:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.075600 on epoch=124
03/19/2022 09:37:08 - INFO - __main__ - Global step 250 Train loss 0.222049 Classification-F1 0.4909862142099682 on epoch=124
03/19/2022 09:37:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.170046 on epoch=129
03/19/2022 09:37:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.071016 on epoch=134
03/19/2022 09:37:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.029083 on epoch=139
03/19/2022 09:37:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.017980 on epoch=144
03/19/2022 09:37:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.129408 on epoch=149
03/19/2022 09:37:34 - INFO - __main__ - Global step 300 Train loss 0.083507 Classification-F1 0.4909862142099682 on epoch=149
03/19/2022 09:37:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.016672 on epoch=154
03/19/2022 09:37:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.026948 on epoch=159
03/19/2022 09:37:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.044010 on epoch=164
03/19/2022 09:37:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.018190 on epoch=169
03/19/2022 09:37:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011798 on epoch=174
03/19/2022 09:38:00 - INFO - __main__ - Global step 350 Train loss 0.023524 Classification-F1 0.4231177094379639 on epoch=174
03/19/2022 09:38:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.137422 on epoch=179
03/19/2022 09:38:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.028273 on epoch=184
03/19/2022 09:38:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.006103 on epoch=189
03/19/2022 09:38:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.011017 on epoch=194
03/19/2022 09:38:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.010446 on epoch=199
03/19/2022 09:38:25 - INFO - __main__ - Global step 400 Train loss 0.038652 Classification-F1 0.41700404858299595 on epoch=199
03/19/2022 09:38:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035963 on epoch=204
03/19/2022 09:38:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.026611 on epoch=209
03/19/2022 09:38:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001867 on epoch=214
03/19/2022 09:38:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002036 on epoch=219
03/19/2022 09:38:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008835 on epoch=224
03/19/2022 09:38:51 - INFO - __main__ - Global step 450 Train loss 0.015062 Classification-F1 0.4385964912280702 on epoch=224
03/19/2022 09:38:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002270 on epoch=229
03/19/2022 09:39:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.040414 on epoch=234
03/19/2022 09:39:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.215651 on epoch=239
03/19/2022 09:39:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.038780 on epoch=244
03/19/2022 09:39:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.049374 on epoch=249
03/19/2022 09:39:18 - INFO - __main__ - Global step 500 Train loss 0.069298 Classification-F1 0.5465587044534412 on epoch=249
03/19/2022 09:39:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.034024 on epoch=254
03/19/2022 09:39:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.026893 on epoch=259
03/19/2022 09:39:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.022964 on epoch=264
03/19/2022 09:39:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.011248 on epoch=269
03/19/2022 09:39:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.007274 on epoch=274
03/19/2022 09:39:45 - INFO - __main__ - Global step 550 Train loss 0.020481 Classification-F1 0.3816425120772947 on epoch=274
03/19/2022 09:39:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002878 on epoch=279
03/19/2022 09:39:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.009829 on epoch=284
03/19/2022 09:40:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001899 on epoch=289
03/19/2022 09:40:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001556 on epoch=294
03/19/2022 09:40:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.016486 on epoch=299
03/19/2022 09:40:11 - INFO - __main__ - Global step 600 Train loss 0.006529 Classification-F1 0.6190476190476191 on epoch=299
03/19/2022 09:40:11 - INFO - __main__ - save last model!
03/19/2022 09:40:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:40:12 - INFO - __main__ - Printing 3 examples
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:40:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:40:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:40:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:40:12 - INFO - __main__ - Printing 3 examples
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:40:12 - INFO - __main__ - ['positive']
03/19/2022 09:40:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:40:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:40:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:40:18 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:40:19 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:40:19 - INFO - __main__ - Printing 3 examples
03/19/2022 09:40:19 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:40:19 - INFO - __main__ - ['negative']
03/19/2022 09:40:19 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:40:19 - INFO - __main__ - ['negative']
03/19/2022 09:40:19 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:40:19 - INFO - __main__ - ['negative']
03/19/2022 09:40:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:40:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:40:21 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:40:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:40:25 - INFO - __main__ - Starting training!
03/19/2022 09:40:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0005_8_predictions.txt
03/19/2022 09:40:36 - INFO - __main__ - Classification-F1 on test data: 0.6379
03/19/2022 09:40:36 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.6190476190476191, test_performance=0.6378551420568226
03/19/2022 09:40:36 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0003, bsz=8 ...
03/19/2022 09:40:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:40:37 - INFO - __main__ - Printing 3 examples
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:40:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:40:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:40:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:40:37 - INFO - __main__ - Printing 3 examples
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:40:37 - INFO - __main__ - ['positive']
03/19/2022 09:40:37 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:40:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:40:37 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:40:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:40:50 - INFO - __main__ - Starting training!
03/19/2022 09:40:55 - INFO - __main__ - Step 10 Global step 10 Train loss 22.214542 on epoch=4
03/19/2022 09:41:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.208233 on epoch=9
03/19/2022 09:41:05 - INFO - __main__ - Step 30 Global step 30 Train loss 16.573929 on epoch=14
03/19/2022 09:41:10 - INFO - __main__ - Step 40 Global step 40 Train loss 15.076937 on epoch=19
03/19/2022 09:41:15 - INFO - __main__ - Step 50 Global step 50 Train loss 13.386253 on epoch=24
03/19/2022 09:41:17 - INFO - __main__ - Global step 50 Train loss 17.091978 Classification-F1 0.0 on epoch=24
03/19/2022 09:41:22 - INFO - __main__ - Step 60 Global step 60 Train loss 12.706669 on epoch=29
03/19/2022 09:41:27 - INFO - __main__ - Step 70 Global step 70 Train loss 11.429474 on epoch=34
03/19/2022 09:41:32 - INFO - __main__ - Step 80 Global step 80 Train loss 9.335863 on epoch=39
03/19/2022 09:41:37 - INFO - __main__ - Step 90 Global step 90 Train loss 4.670783 on epoch=44
03/19/2022 09:41:43 - INFO - __main__ - Step 100 Global step 100 Train loss 2.210502 on epoch=49
03/19/2022 09:41:43 - INFO - __main__ - Global step 100 Train loss 8.070659 Classification-F1 0.5636363636363637 on epoch=49
03/19/2022 09:41:48 - INFO - __main__ - Step 110 Global step 110 Train loss 1.790781 on epoch=54
03/19/2022 09:41:53 - INFO - __main__ - Step 120 Global step 120 Train loss 1.579941 on epoch=59
03/19/2022 09:41:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.739626 on epoch=64
03/19/2022 09:42:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.003069 on epoch=69
03/19/2022 09:42:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.079423 on epoch=74
03/19/2022 09:42:09 - INFO - __main__ - Global step 150 Train loss 1.438568 Classification-F1 0.4589371980676329 on epoch=74
03/19/2022 09:42:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.755254 on epoch=79
03/19/2022 09:42:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.729264 on epoch=84
03/19/2022 09:42:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.343441 on epoch=89
03/19/2022 09:42:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.172476 on epoch=94
03/19/2022 09:42:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.184012 on epoch=99
03/19/2022 09:42:35 - INFO - __main__ - Global step 200 Train loss 0.436889 Classification-F1 0.9054187192118226 on epoch=99
03/19/2022 09:42:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.145690 on epoch=104
03/19/2022 09:42:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.138007 on epoch=109
03/19/2022 09:42:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.095641 on epoch=114
03/19/2022 09:42:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.046440 on epoch=119
03/19/2022 09:43:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.031209 on epoch=124
03/19/2022 09:43:02 - INFO - __main__ - Global step 250 Train loss 0.091397 Classification-F1 0.9054187192118226 on epoch=124
03/19/2022 09:43:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.008074 on epoch=129
03/19/2022 09:43:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002713 on epoch=134
03/19/2022 09:43:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.004326 on epoch=139
03/19/2022 09:43:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002354 on epoch=144
03/19/2022 09:43:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.004927 on epoch=149
03/19/2022 09:43:28 - INFO - __main__ - Global step 300 Train loss 0.004479 Classification-F1 0.9054187192118226 on epoch=149
03/19/2022 09:43:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003670 on epoch=154
03/19/2022 09:43:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002883 on epoch=159
03/19/2022 09:43:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000327 on epoch=164
03/19/2022 09:43:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001406 on epoch=169
03/19/2022 09:43:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001702 on epoch=174
03/19/2022 09:43:54 - INFO - __main__ - Global step 350 Train loss 0.001998 Classification-F1 0.9054187192118226 on epoch=174
03/19/2022 09:43:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000541 on epoch=179
03/19/2022 09:44:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.004171 on epoch=184
03/19/2022 09:44:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000682 on epoch=189
03/19/2022 09:44:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000299 on epoch=194
03/19/2022 09:44:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000907 on epoch=199
03/19/2022 09:44:20 - INFO - __main__ - Global step 400 Train loss 0.001320 Classification-F1 0.9054187192118226 on epoch=199
03/19/2022 09:44:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000185 on epoch=204
03/19/2022 09:44:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.007799 on epoch=209
03/19/2022 09:44:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000163 on epoch=214
03/19/2022 09:44:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000252 on epoch=219
03/19/2022 09:44:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.084230 on epoch=224
03/19/2022 09:44:46 - INFO - __main__ - Global step 450 Train loss 0.018526 Classification-F1 0.8398398398398398 on epoch=224
03/19/2022 09:44:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000392 on epoch=229
03/19/2022 09:44:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000355 on epoch=234
03/19/2022 09:45:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004221 on epoch=239
03/19/2022 09:45:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000144 on epoch=244
03/19/2022 09:45:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000147 on epoch=249
03/19/2022 09:45:11 - INFO - __main__ - Global step 500 Train loss 0.001052 Classification-F1 0.9054187192118226 on epoch=249
03/19/2022 09:45:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006707 on epoch=254
03/19/2022 09:45:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000125 on epoch=259
03/19/2022 09:45:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000659 on epoch=264
03/19/2022 09:45:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000338 on epoch=269
03/19/2022 09:45:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000141 on epoch=274
03/19/2022 09:45:37 - INFO - __main__ - Global step 550 Train loss 0.001594 Classification-F1 0.9054187192118226 on epoch=274
03/19/2022 09:45:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000092 on epoch=279
03/19/2022 09:45:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000048 on epoch=284
03/19/2022 09:45:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000072 on epoch=289
03/19/2022 09:45:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000304 on epoch=294
03/19/2022 09:46:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000082 on epoch=299
03/19/2022 09:46:03 - INFO - __main__ - Global step 600 Train loss 0.000119 Classification-F1 0.9054187192118226 on epoch=299
03/19/2022 09:46:03 - INFO - __main__ - save last model!
03/19/2022 09:46:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:46:04 - INFO - __main__ - Printing 3 examples
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:46:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:46:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:46:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:46:04 - INFO - __main__ - Printing 3 examples
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:46:04 - INFO - __main__ - ['positive']
03/19/2022 09:46:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:46:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:46:04 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:46:10 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:46:11 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:46:11 - INFO - __main__ - Printing 3 examples
03/19/2022 09:46:11 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:46:11 - INFO - __main__ - ['negative']
03/19/2022 09:46:11 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:46:11 - INFO - __main__ - ['negative']
03/19/2022 09:46:11 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:46:11 - INFO - __main__ - ['negative']
03/19/2022 09:46:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:46:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:46:13 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:46:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:46:15 - INFO - __main__ - Starting training!
03/19/2022 09:46:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0003_8_predictions.txt
03/19/2022 09:46:25 - INFO - __main__ - Classification-F1 on test data: 0.8890
03/19/2022 09:46:25 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9054187192118226, test_performance=0.8889865673746524
03/19/2022 09:46:25 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0002, bsz=8 ...
03/19/2022 09:46:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:46:26 - INFO - __main__ - Printing 3 examples
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:46:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:46:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:46:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:46:26 - INFO - __main__ - Printing 3 examples
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:46:26 - INFO - __main__ - ['positive']
03/19/2022 09:46:26 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:46:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:46:26 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:46:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:46:39 - INFO - __main__ - Starting training!
03/19/2022 09:46:43 - INFO - __main__ - Step 10 Global step 10 Train loss 23.904514 on epoch=4
03/19/2022 09:46:48 - INFO - __main__ - Step 20 Global step 20 Train loss 18.406780 on epoch=9
03/19/2022 09:46:53 - INFO - __main__ - Step 30 Global step 30 Train loss 17.238838 on epoch=14
03/19/2022 09:46:58 - INFO - __main__ - Step 40 Global step 40 Train loss 16.128811 on epoch=19
03/19/2022 09:47:03 - INFO - __main__ - Step 50 Global step 50 Train loss 15.341013 on epoch=24
03/19/2022 09:47:13 - INFO - __main__ - Global step 50 Train loss 18.203993 Classification-F1 0.0 on epoch=24
03/19/2022 09:47:19 - INFO - __main__ - Step 60 Global step 60 Train loss 14.890617 on epoch=29
03/19/2022 09:47:24 - INFO - __main__ - Step 70 Global step 70 Train loss 13.324102 on epoch=34
03/19/2022 09:47:29 - INFO - __main__ - Step 80 Global step 80 Train loss 12.951220 on epoch=39
03/19/2022 09:47:34 - INFO - __main__ - Step 90 Global step 90 Train loss 12.321015 on epoch=44
03/19/2022 09:47:39 - INFO - __main__ - Step 100 Global step 100 Train loss 11.080770 on epoch=49
03/19/2022 09:47:44 - INFO - __main__ - Global step 100 Train loss 12.913544 Classification-F1 0.0 on epoch=49
03/19/2022 09:47:49 - INFO - __main__ - Step 110 Global step 110 Train loss 6.302130 on epoch=54
03/19/2022 09:47:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.653578 on epoch=59
03/19/2022 09:47:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.904473 on epoch=64
03/19/2022 09:48:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.322420 on epoch=69
03/19/2022 09:48:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.079230 on epoch=74
03/19/2022 09:48:10 - INFO - __main__ - Global step 150 Train loss 1.852366 Classification-F1 0.9372549019607843 on epoch=74
03/19/2022 09:48:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.025533 on epoch=79
03/19/2022 09:48:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.018301 on epoch=84
03/19/2022 09:48:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.018951 on epoch=89
03/19/2022 09:48:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.011487 on epoch=94
03/19/2022 09:48:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.011947 on epoch=99
03/19/2022 09:48:36 - INFO - __main__ - Global step 200 Train loss 0.017244 Classification-F1 0.9687194525904204 on epoch=99
03/19/2022 09:48:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.002476 on epoch=104
03/19/2022 09:48:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005654 on epoch=109
03/19/2022 09:48:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000859 on epoch=114
03/19/2022 09:48:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.003454 on epoch=119
03/19/2022 09:49:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001884 on epoch=124
03/19/2022 09:49:03 - INFO - __main__ - Global step 250 Train loss 0.002865 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 09:49:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000941 on epoch=129
03/19/2022 09:49:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000634 on epoch=134
03/19/2022 09:49:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000301 on epoch=139
03/19/2022 09:49:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000150 on epoch=144
03/19/2022 09:49:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000439 on epoch=149
03/19/2022 09:49:29 - INFO - __main__ - Global step 300 Train loss 0.000493 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 09:49:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000394 on epoch=154
03/19/2022 09:49:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000196 on epoch=159
03/19/2022 09:49:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000284 on epoch=164
03/19/2022 09:49:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022473 on epoch=169
03/19/2022 09:49:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.014710 on epoch=174
03/19/2022 09:49:54 - INFO - __main__ - Global step 350 Train loss 0.007611 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 09:49:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000672 on epoch=179
03/19/2022 09:50:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000353 on epoch=184
03/19/2022 09:50:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000227 on epoch=189
03/19/2022 09:50:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000204 on epoch=194
03/19/2022 09:50:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000325 on epoch=199
03/19/2022 09:50:19 - INFO - __main__ - Global step 400 Train loss 0.000356 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 09:50:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000260 on epoch=204
03/19/2022 09:50:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000282 on epoch=209
03/19/2022 09:50:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000197 on epoch=214
03/19/2022 09:50:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000058 on epoch=219
03/19/2022 09:50:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000065 on epoch=224
03/19/2022 09:50:45 - INFO - __main__ - Global step 450 Train loss 0.000172 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 09:50:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000751 on epoch=229
03/19/2022 09:50:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000063 on epoch=234
03/19/2022 09:51:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000062 on epoch=239
03/19/2022 09:51:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.019812 on epoch=244
03/19/2022 09:51:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000938 on epoch=249
03/19/2022 09:51:10 - INFO - __main__ - Global step 500 Train loss 0.004325 Classification-F1 0.9372549019607843 on epoch=249
03/19/2022 09:51:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.091788 on epoch=254
03/19/2022 09:51:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.128674 on epoch=259
03/19/2022 09:51:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000145 on epoch=264
03/19/2022 09:51:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000086 on epoch=269
03/19/2022 09:51:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000052 on epoch=274
03/19/2022 09:51:36 - INFO - __main__ - Global step 550 Train loss 0.044149 Classification-F1 0.9372549019607843 on epoch=274
03/19/2022 09:51:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000185 on epoch=279
03/19/2022 09:51:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000164 on epoch=284
03/19/2022 09:51:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000036 on epoch=289
03/19/2022 09:51:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000117 on epoch=294
03/19/2022 09:52:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000055 on epoch=299
03/19/2022 09:52:01 - INFO - __main__ - Global step 600 Train loss 0.000111 Classification-F1 0.9372549019607843 on epoch=299
03/19/2022 09:52:01 - INFO - __main__ - save last model!
03/19/2022 09:52:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:52:02 - INFO - __main__ - Printing 3 examples
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:52:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:52:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:52:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:52:02 - INFO - __main__ - Printing 3 examples
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:52:02 - INFO - __main__ - ['positive']
03/19/2022 09:52:02 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:52:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:52:02 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:52:08 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:52:09 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:52:09 - INFO - __main__ - Printing 3 examples
03/19/2022 09:52:09 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:52:09 - INFO - __main__ - ['negative']
03/19/2022 09:52:09 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:52:09 - INFO - __main__ - ['negative']
03/19/2022 09:52:09 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:52:09 - INFO - __main__ - ['negative']
03/19/2022 09:52:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:52:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:52:11 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:52:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:52:15 - INFO - __main__ - Starting training!
03/19/2022 09:52:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0002_8_predictions.txt
03/19/2022 09:52:27 - INFO - __main__ - Classification-F1 on test data: 0.9389
03/19/2022 09:52:27 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9389413226110291
03/19/2022 09:52:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0001, bsz=8 ...
03/19/2022 09:52:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:52:28 - INFO - __main__ - Printing 3 examples
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:52:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:52:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:52:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:52:28 - INFO - __main__ - Printing 3 examples
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/19/2022 09:52:28 - INFO - __main__ - ['positive']
03/19/2022 09:52:28 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:52:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:52:28 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:52:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:52:39 - INFO - __main__ - Starting training!
03/19/2022 09:52:43 - INFO - __main__ - Step 10 Global step 10 Train loss 23.478724 on epoch=4
03/19/2022 09:52:48 - INFO - __main__ - Step 20 Global step 20 Train loss 19.425240 on epoch=9
03/19/2022 09:52:53 - INFO - __main__ - Step 30 Global step 30 Train loss 17.838175 on epoch=14
03/19/2022 09:52:58 - INFO - __main__ - Step 40 Global step 40 Train loss 17.801142 on epoch=19
03/19/2022 09:53:03 - INFO - __main__ - Step 50 Global step 50 Train loss 16.178104 on epoch=24
03/19/2022 09:53:14 - INFO - __main__ - Global step 50 Train loss 18.944277 Classification-F1 0.0 on epoch=24
03/19/2022 09:53:20 - INFO - __main__ - Step 60 Global step 60 Train loss 15.632803 on epoch=29
03/19/2022 09:53:25 - INFO - __main__ - Step 70 Global step 70 Train loss 15.527270 on epoch=34
03/19/2022 09:53:30 - INFO - __main__ - Step 80 Global step 80 Train loss 15.911084 on epoch=39
03/19/2022 09:53:35 - INFO - __main__ - Step 90 Global step 90 Train loss 14.589106 on epoch=44
03/19/2022 09:53:39 - INFO - __main__ - Step 100 Global step 100 Train loss 14.046804 on epoch=49
03/19/2022 09:53:50 - INFO - __main__ - Global step 100 Train loss 15.141413 Classification-F1 0.0 on epoch=49
03/19/2022 09:53:55 - INFO - __main__ - Step 110 Global step 110 Train loss 14.609421 on epoch=54
03/19/2022 09:54:00 - INFO - __main__ - Step 120 Global step 120 Train loss 14.624680 on epoch=59
03/19/2022 09:54:05 - INFO - __main__ - Step 130 Global step 130 Train loss 13.569441 on epoch=64
03/19/2022 09:54:10 - INFO - __main__ - Step 140 Global step 140 Train loss 13.017462 on epoch=69
03/19/2022 09:54:15 - INFO - __main__ - Step 150 Global step 150 Train loss 12.328792 on epoch=74
03/19/2022 09:54:24 - INFO - __main__ - Global step 150 Train loss 13.629960 Classification-F1 0.0 on epoch=74
03/19/2022 09:54:29 - INFO - __main__ - Step 160 Global step 160 Train loss 12.072322 on epoch=79
03/19/2022 09:54:34 - INFO - __main__ - Step 170 Global step 170 Train loss 10.661765 on epoch=84
03/19/2022 09:54:39 - INFO - __main__ - Step 180 Global step 180 Train loss 10.832061 on epoch=89
03/19/2022 09:54:44 - INFO - __main__ - Step 190 Global step 190 Train loss 9.969407 on epoch=94
03/19/2022 09:54:50 - INFO - __main__ - Step 200 Global step 200 Train loss 6.972814 on epoch=99
03/19/2022 09:55:02 - INFO - __main__ - Global step 200 Train loss 10.101675 Classification-F1 0.0 on epoch=99
03/19/2022 09:55:07 - INFO - __main__ - Step 210 Global step 210 Train loss 2.038918 on epoch=104
03/19/2022 09:55:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.889247 on epoch=109
03/19/2022 09:55:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.600086 on epoch=114
03/19/2022 09:55:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.309317 on epoch=119
03/19/2022 09:55:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.353202 on epoch=124
03/19/2022 09:55:28 - INFO - __main__ - Global step 250 Train loss 0.838154 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 09:55:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.342003 on epoch=129
03/19/2022 09:55:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.240737 on epoch=134
03/19/2022 09:55:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.163538 on epoch=139
03/19/2022 09:55:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.066164 on epoch=144
03/19/2022 09:55:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.108423 on epoch=149
03/19/2022 09:55:55 - INFO - __main__ - Global step 300 Train loss 0.184173 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 09:56:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.046248 on epoch=154
03/19/2022 09:56:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.025140 on epoch=159
03/19/2022 09:56:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.248682 on epoch=164
03/19/2022 09:56:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017658 on epoch=169
03/19/2022 09:56:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.064355 on epoch=174
03/19/2022 09:56:21 - INFO - __main__ - Global step 350 Train loss 0.080417 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 09:56:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.350030 on epoch=179
03/19/2022 09:56:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.014701 on epoch=184
03/19/2022 09:56:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.015588 on epoch=189
03/19/2022 09:56:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.012120 on epoch=194
03/19/2022 09:56:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.023683 on epoch=199
03/19/2022 09:56:47 - INFO - __main__ - Global step 400 Train loss 0.083224 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 09:56:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016939 on epoch=204
03/19/2022 09:56:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003284 on epoch=209
03/19/2022 09:57:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.097283 on epoch=214
03/19/2022 09:57:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.033538 on epoch=219
03/19/2022 09:57:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009587 on epoch=224
03/19/2022 09:57:13 - INFO - __main__ - Global step 450 Train loss 0.032126 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 09:57:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.006749 on epoch=229
03/19/2022 09:57:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002537 on epoch=234
03/19/2022 09:57:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012382 on epoch=239
03/19/2022 09:57:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000987 on epoch=244
03/19/2022 09:57:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004099 on epoch=249
03/19/2022 09:57:40 - INFO - __main__ - Global step 500 Train loss 0.005351 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 09:57:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001551 on epoch=254
03/19/2022 09:57:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.047875 on epoch=259
03/19/2022 09:57:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006598 on epoch=264
03/19/2022 09:58:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000456 on epoch=269
03/19/2022 09:58:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001117 on epoch=274
03/19/2022 09:58:06 - INFO - __main__ - Global step 550 Train loss 0.011520 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 09:58:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001390 on epoch=279
03/19/2022 09:58:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000632 on epoch=284
03/19/2022 09:58:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001161 on epoch=289
03/19/2022 09:58:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001023 on epoch=294
03/19/2022 09:58:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000700 on epoch=299
03/19/2022 09:58:32 - INFO - __main__ - Global step 600 Train loss 0.000981 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 09:58:32 - INFO - __main__ - save last model!
03/19/2022 09:58:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:58:32 - INFO - __main__ - Printing 3 examples
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 09:58:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:58:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:58:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:58:32 - INFO - __main__ - Printing 3 examples
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 09:58:32 - INFO - __main__ - ['negative']
03/19/2022 09:58:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:58:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:58:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:58:39 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 09:58:39 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 09:58:39 - INFO - __main__ - Printing 3 examples
03/19/2022 09:58:39 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 09:58:39 - INFO - __main__ - ['negative']
03/19/2022 09:58:39 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 09:58:39 - INFO - __main__ - ['negative']
03/19/2022 09:58:39 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 09:58:39 - INFO - __main__ - ['negative']
03/19/2022 09:58:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:58:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:58:41 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 09:58:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:58:46 - INFO - __main__ - Starting training!
03/19/2022 09:58:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0001_8_predictions.txt
03/19/2022 09:58:57 - INFO - __main__ - Classification-F1 on test data: 0.9220
03/19/2022 09:58:57 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9219550461065573
03/19/2022 09:58:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0005, bsz=8 ...
03/19/2022 09:58:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:58:58 - INFO - __main__ - Printing 3 examples
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 09:58:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:58:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 09:58:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 09:58:58 - INFO - __main__ - Printing 3 examples
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 09:58:58 - INFO - __main__ - ['negative']
03/19/2022 09:58:58 - INFO - __main__ - Tokenizing Input ...
03/19/2022 09:58:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 09:58:58 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 09:59:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 09:59:11 - INFO - __main__ - Starting training!
03/19/2022 09:59:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.864124 on epoch=4
03/19/2022 09:59:21 - INFO - __main__ - Step 20 Global step 20 Train loss 16.848709 on epoch=9
03/19/2022 09:59:26 - INFO - __main__ - Step 30 Global step 30 Train loss 14.996500 on epoch=14
03/19/2022 09:59:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.027969 on epoch=19
03/19/2022 09:59:36 - INFO - __main__ - Step 50 Global step 50 Train loss 11.108831 on epoch=24
03/19/2022 09:59:37 - INFO - __main__ - Global step 50 Train loss 15.769226 Classification-F1 0.0 on epoch=24
03/19/2022 09:59:42 - INFO - __main__ - Step 60 Global step 60 Train loss 6.338685 on epoch=29
03/19/2022 09:59:47 - INFO - __main__ - Step 70 Global step 70 Train loss 2.979499 on epoch=34
03/19/2022 09:59:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.879642 on epoch=39
03/19/2022 09:59:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.539747 on epoch=44
03/19/2022 10:00:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.463868 on epoch=49
03/19/2022 10:00:03 - INFO - __main__ - Global step 100 Train loss 2.840288 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 10:00:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.184974 on epoch=54
03/19/2022 10:00:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.315291 on epoch=59
03/19/2022 10:00:18 - INFO - __main__ - Step 130 Global step 130 Train loss 1.474793 on epoch=64
03/19/2022 10:00:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.230314 on epoch=69
03/19/2022 10:00:28 - INFO - __main__ - Step 150 Global step 150 Train loss 1.148954 on epoch=74
03/19/2022 10:00:29 - INFO - __main__ - Global step 150 Train loss 1.270865 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 10:00:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.880372 on epoch=79
03/19/2022 10:00:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.624473 on epoch=84
03/19/2022 10:00:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.465559 on epoch=89
03/19/2022 10:00:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.479427 on epoch=94
03/19/2022 10:00:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.457069 on epoch=99
03/19/2022 10:00:54 - INFO - __main__ - Global step 200 Train loss 0.581380 Classification-F1 0.6945917285259808 on epoch=99
03/19/2022 10:01:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.986491 on epoch=104
03/19/2022 10:01:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.933039 on epoch=109
03/19/2022 10:01:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.696922 on epoch=114
03/19/2022 10:01:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.403887 on epoch=119
03/19/2022 10:01:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.186225 on epoch=124
03/19/2022 10:01:20 - INFO - __main__ - Global step 250 Train loss 0.641313 Classification-F1 0.9054187192118226 on epoch=124
03/19/2022 10:01:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.114527 on epoch=129
03/19/2022 10:01:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.030706 on epoch=134
03/19/2022 10:01:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.008461 on epoch=139
03/19/2022 10:01:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.012221 on epoch=144
03/19/2022 10:01:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.010785 on epoch=149
03/19/2022 10:01:46 - INFO - __main__ - Global step 300 Train loss 0.035340 Classification-F1 0.9372549019607843 on epoch=149
03/19/2022 10:01:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.004702 on epoch=154
03/19/2022 10:01:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002734 on epoch=159
03/19/2022 10:02:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001010 on epoch=164
03/19/2022 10:02:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002098 on epoch=169
03/19/2022 10:02:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002727 on epoch=174
03/19/2022 10:02:12 - INFO - __main__ - Global step 350 Train loss 0.002654 Classification-F1 0.9372549019607843 on epoch=174
03/19/2022 10:02:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001676 on epoch=179
03/19/2022 10:02:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000444 on epoch=184
03/19/2022 10:02:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000954 on epoch=189
03/19/2022 10:02:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.003573 on epoch=194
03/19/2022 10:02:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003937 on epoch=199
03/19/2022 10:02:37 - INFO - __main__ - Global step 400 Train loss 0.002117 Classification-F1 0.9372549019607843 on epoch=199
03/19/2022 10:02:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.004982 on epoch=204
03/19/2022 10:02:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.010993 on epoch=209
03/19/2022 10:02:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.002365 on epoch=214
03/19/2022 10:02:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002038 on epoch=219
03/19/2022 10:03:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000679 on epoch=224
03/19/2022 10:03:02 - INFO - __main__ - Global step 450 Train loss 0.004211 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 10:03:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000099 on epoch=229
03/19/2022 10:03:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000313 on epoch=234
03/19/2022 10:03:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000388 on epoch=239
03/19/2022 10:03:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000294 on epoch=244
03/19/2022 10:03:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000372 on epoch=249
03/19/2022 10:03:28 - INFO - __main__ - Global step 500 Train loss 0.000293 Classification-F1 0.9372549019607843 on epoch=249
03/19/2022 10:03:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000221 on epoch=254
03/19/2022 10:03:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003140 on epoch=259
03/19/2022 10:03:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001867 on epoch=264
03/19/2022 10:03:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000161 on epoch=269
03/19/2022 10:03:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000610 on epoch=274
03/19/2022 10:03:53 - INFO - __main__ - Global step 550 Train loss 0.001200 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:03:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000542 on epoch=279
03/19/2022 10:04:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001366 on epoch=284
03/19/2022 10:04:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000057 on epoch=289
03/19/2022 10:04:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.017285 on epoch=294
03/19/2022 10:04:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000108 on epoch=299
03/19/2022 10:04:18 - INFO - __main__ - Global step 600 Train loss 0.003871 Classification-F1 0.9054187192118226 on epoch=299
03/19/2022 10:04:18 - INFO - __main__ - save last model!
03/19/2022 10:04:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:04:20 - INFO - __main__ - Printing 3 examples
03/19/2022 10:04:20 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:04:20 - INFO - __main__ - ['negative']
03/19/2022 10:04:20 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:04:20 - INFO - __main__ - ['negative']
03/19/2022 10:04:20 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:04:20 - INFO - __main__ - ['negative']
03/19/2022 10:04:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:04:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:04:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:04:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:04:21 - INFO - __main__ - Printing 3 examples
03/19/2022 10:04:21 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:04:21 - INFO - __main__ - ['negative']
03/19/2022 10:04:21 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:04:21 - INFO - __main__ - ['negative']
03/19/2022 10:04:21 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:04:21 - INFO - __main__ - ['negative']
03/19/2022 10:04:21 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:04:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:04:21 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:04:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:04:26 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:04:26 - INFO - __main__ - Printing 3 examples
03/19/2022 10:04:26 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:04:26 - INFO - __main__ - ['negative']
03/19/2022 10:04:26 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:04:26 - INFO - __main__ - ['negative']
03/19/2022 10:04:26 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:04:26 - INFO - __main__ - ['negative']
03/19/2022 10:04:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:04:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:04:27 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:04:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:04:34 - INFO - __main__ - Starting training!
03/19/2022 10:04:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0005_8_predictions.txt
03/19/2022 10:04:39 - INFO - __main__ - Classification-F1 on test data: 0.9190
03/19/2022 10:04:40 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9189960308055095
03/19/2022 10:04:40 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0003, bsz=8 ...
03/19/2022 10:04:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:04:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:04:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:04:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:04:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:04:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:04:41 - INFO - __main__ - ['negative']
03/19/2022 10:04:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:04:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:04:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:04:54 - INFO - __main__ - Starting training!
03/19/2022 10:04:58 - INFO - __main__ - Step 10 Global step 10 Train loss 22.600666 on epoch=4
03/19/2022 10:05:03 - INFO - __main__ - Step 20 Global step 20 Train loss 16.207081 on epoch=9
03/19/2022 10:05:08 - INFO - __main__ - Step 30 Global step 30 Train loss 15.587491 on epoch=14
03/19/2022 10:05:13 - INFO - __main__ - Step 40 Global step 40 Train loss 14.288622 on epoch=19
03/19/2022 10:05:18 - INFO - __main__ - Step 50 Global step 50 Train loss 12.754503 on epoch=24
03/19/2022 10:05:28 - INFO - __main__ - Global step 50 Train loss 16.287674 Classification-F1 0.0 on epoch=24
03/19/2022 10:05:34 - INFO - __main__ - Step 60 Global step 60 Train loss 11.924324 on epoch=29
03/19/2022 10:05:39 - INFO - __main__ - Step 70 Global step 70 Train loss 10.956310 on epoch=34
03/19/2022 10:05:43 - INFO - __main__ - Step 80 Global step 80 Train loss 4.957481 on epoch=39
03/19/2022 10:05:48 - INFO - __main__ - Step 90 Global step 90 Train loss 2.948825 on epoch=44
03/19/2022 10:05:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.468291 on epoch=49
03/19/2022 10:05:53 - INFO - __main__ - Global step 100 Train loss 6.651046 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 10:05:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.639811 on epoch=54
03/19/2022 10:06:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.481981 on epoch=59
03/19/2022 10:06:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.391927 on epoch=64
03/19/2022 10:06:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.403230 on epoch=69
03/19/2022 10:06:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.428593 on epoch=74
03/19/2022 10:06:19 - INFO - __main__ - Global step 150 Train loss 0.469108 Classification-F1 0.22727272727272727 on epoch=74
03/19/2022 10:06:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.445089 on epoch=79
03/19/2022 10:06:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.383257 on epoch=84
03/19/2022 10:06:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.344139 on epoch=89
03/19/2022 10:06:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.358230 on epoch=94
03/19/2022 10:06:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.317857 on epoch=99
03/19/2022 10:06:44 - INFO - __main__ - Global step 200 Train loss 0.369714 Classification-F1 0.3191489361702127 on epoch=99
03/19/2022 10:06:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.371789 on epoch=104
03/19/2022 10:06:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.366361 on epoch=109
03/19/2022 10:06:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.357118 on epoch=114
03/19/2022 10:07:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.361776 on epoch=119
03/19/2022 10:07:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.331155 on epoch=124
03/19/2022 10:07:10 - INFO - __main__ - Global step 250 Train loss 0.357640 Classification-F1 0.4920634920634921 on epoch=124
03/19/2022 10:07:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.334100 on epoch=129
03/19/2022 10:07:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.343091 on epoch=134
03/19/2022 10:07:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.347094 on epoch=139
03/19/2022 10:07:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.358730 on epoch=144
03/19/2022 10:07:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.364087 on epoch=149
03/19/2022 10:07:36 - INFO - __main__ - Global step 300 Train loss 0.349420 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 10:07:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.349192 on epoch=154
03/19/2022 10:07:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.339033 on epoch=159
03/19/2022 10:07:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.339070 on epoch=164
03/19/2022 10:07:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360596 on epoch=169
03/19/2022 10:08:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.338163 on epoch=174
03/19/2022 10:08:01 - INFO - __main__ - Global step 350 Train loss 0.345211 Classification-F1 0.4385964912280702 on epoch=174
03/19/2022 10:08:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.328840 on epoch=179
03/19/2022 10:08:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.337594 on epoch=184
03/19/2022 10:08:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.312970 on epoch=189
03/19/2022 10:08:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.322099 on epoch=194
03/19/2022 10:08:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.283567 on epoch=199
03/19/2022 10:08:27 - INFO - __main__ - Global step 400 Train loss 0.317014 Classification-F1 0.4554554554554554 on epoch=199
03/19/2022 10:08:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.341033 on epoch=204
03/19/2022 10:08:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.284611 on epoch=209
03/19/2022 10:08:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.296579 on epoch=214
03/19/2022 10:08:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.286904 on epoch=219
03/19/2022 10:08:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.278456 on epoch=224
03/19/2022 10:08:52 - INFO - __main__ - Global step 450 Train loss 0.297517 Classification-F1 0.5134502923976608 on epoch=224
03/19/2022 10:08:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.277931 on epoch=229
03/19/2022 10:09:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.261670 on epoch=234
03/19/2022 10:09:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.233029 on epoch=239
03/19/2022 10:09:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.255703 on epoch=244
03/19/2022 10:09:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.238193 on epoch=249
03/19/2022 10:09:18 - INFO - __main__ - Global step 500 Train loss 0.253305 Classification-F1 0.6532019704433498 on epoch=249
03/19/2022 10:09:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.301966 on epoch=254
03/19/2022 10:09:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.190964 on epoch=259
03/19/2022 10:09:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.261914 on epoch=264
03/19/2022 10:09:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.160307 on epoch=269
03/19/2022 10:09:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.150110 on epoch=274
03/19/2022 10:09:44 - INFO - __main__ - Global step 550 Train loss 0.213052 Classification-F1 0.6235294117647059 on epoch=274
03/19/2022 10:09:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.204685 on epoch=279
03/19/2022 10:09:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.268537 on epoch=284
03/19/2022 10:09:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167765 on epoch=289
03/19/2022 10:10:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.209820 on epoch=294
03/19/2022 10:10:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.137778 on epoch=299
03/19/2022 10:10:09 - INFO - __main__ - Global step 600 Train loss 0.197717 Classification-F1 0.6761133603238867 on epoch=299
03/19/2022 10:10:10 - INFO - __main__ - save last model!
03/19/2022 10:10:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:10:10 - INFO - __main__ - Printing 3 examples
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:10:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:10:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:10:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:10:10 - INFO - __main__ - Printing 3 examples
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:10:10 - INFO - __main__ - ['negative']
03/19/2022 10:10:10 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:10:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:10:10 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:10:17 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:10:17 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:10:17 - INFO - __main__ - Printing 3 examples
03/19/2022 10:10:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:10:17 - INFO - __main__ - ['negative']
03/19/2022 10:10:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:10:17 - INFO - __main__ - ['negative']
03/19/2022 10:10:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:10:17 - INFO - __main__ - ['negative']
03/19/2022 10:10:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:10:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:10:19 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:10:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:10:23 - INFO - __main__ - Starting training!
03/19/2022 10:10:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0003_8_predictions.txt
03/19/2022 10:10:35 - INFO - __main__ - Classification-F1 on test data: 0.2613
03/19/2022 10:10:35 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0003, bsz=8, dev_performance=0.6761133603238867, test_performance=0.26125996701570575
03/19/2022 10:10:35 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0002, bsz=8 ...
03/19/2022 10:10:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:10:36 - INFO - __main__ - Printing 3 examples
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:10:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:10:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:10:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:10:36 - INFO - __main__ - Printing 3 examples
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:10:36 - INFO - __main__ - ['negative']
03/19/2022 10:10:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:10:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:10:37 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:10:49 - INFO - __main__ - Starting training!
03/19/2022 10:10:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.687122 on epoch=4
03/19/2022 10:10:59 - INFO - __main__ - Step 20 Global step 20 Train loss 17.894886 on epoch=9
03/19/2022 10:11:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.910297 on epoch=14
03/19/2022 10:11:09 - INFO - __main__ - Step 40 Global step 40 Train loss 15.877165 on epoch=19
03/19/2022 10:11:14 - INFO - __main__ - Step 50 Global step 50 Train loss 15.468249 on epoch=24
03/19/2022 10:11:24 - INFO - __main__ - Global step 50 Train loss 17.967545 Classification-F1 0.0 on epoch=24
03/19/2022 10:11:29 - INFO - __main__ - Step 60 Global step 60 Train loss 14.733877 on epoch=29
03/19/2022 10:11:34 - INFO - __main__ - Step 70 Global step 70 Train loss 14.227895 on epoch=34
03/19/2022 10:11:39 - INFO - __main__ - Step 80 Global step 80 Train loss 13.690317 on epoch=39
03/19/2022 10:11:44 - INFO - __main__ - Step 90 Global step 90 Train loss 12.257775 on epoch=44
03/19/2022 10:11:49 - INFO - __main__ - Step 100 Global step 100 Train loss 11.248464 on epoch=49
03/19/2022 10:11:58 - INFO - __main__ - Global step 100 Train loss 13.231665 Classification-F1 0.0 on epoch=49
03/19/2022 10:12:03 - INFO - __main__ - Step 110 Global step 110 Train loss 9.883585 on epoch=54
03/19/2022 10:12:08 - INFO - __main__ - Step 120 Global step 120 Train loss 5.227834 on epoch=59
03/19/2022 10:12:13 - INFO - __main__ - Step 130 Global step 130 Train loss 3.287809 on epoch=64
03/19/2022 10:12:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.625956 on epoch=69
03/19/2022 10:12:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.414333 on epoch=74
03/19/2022 10:12:24 - INFO - __main__ - Global step 150 Train loss 3.887903 Classification-F1 0.906158357771261 on epoch=74
03/19/2022 10:12:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.366945 on epoch=79
03/19/2022 10:12:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.327100 on epoch=84
03/19/2022 10:12:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.472491 on epoch=89
03/19/2022 10:12:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.304991 on epoch=94
03/19/2022 10:12:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.360989 on epoch=99
03/19/2022 10:12:50 - INFO - __main__ - Global step 200 Train loss 0.366503 Classification-F1 0.9375 on epoch=99
03/19/2022 10:12:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.199003 on epoch=104
03/19/2022 10:13:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.189240 on epoch=109
03/19/2022 10:13:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.190801 on epoch=114
03/19/2022 10:13:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.091600 on epoch=119
03/19/2022 10:13:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.124179 on epoch=124
03/19/2022 10:13:17 - INFO - __main__ - Global step 250 Train loss 0.158964 Classification-F1 1.0 on epoch=124
03/19/2022 10:13:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.123854 on epoch=129
03/19/2022 10:13:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.051751 on epoch=134
03/19/2022 10:13:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.067761 on epoch=139
03/19/2022 10:13:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.104694 on epoch=144
03/19/2022 10:13:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.042833 on epoch=149
03/19/2022 10:13:44 - INFO - __main__ - Global step 300 Train loss 0.078179 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 10:13:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.018892 on epoch=154
03/19/2022 10:13:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.118403 on epoch=159
03/19/2022 10:13:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.016476 on epoch=164
03/19/2022 10:14:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.064665 on epoch=169
03/19/2022 10:14:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.044626 on epoch=174
03/19/2022 10:14:10 - INFO - __main__ - Global step 350 Train loss 0.052612 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 10:14:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.033043 on epoch=179
03/19/2022 10:14:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.013950 on epoch=184
03/19/2022 10:14:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.029689 on epoch=189
03/19/2022 10:14:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.042415 on epoch=194
03/19/2022 10:14:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.005020 on epoch=199
03/19/2022 10:14:36 - INFO - __main__ - Global step 400 Train loss 0.024823 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 10:14:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.019644 on epoch=204
03/19/2022 10:14:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.007309 on epoch=209
03/19/2022 10:14:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.022026 on epoch=214
03/19/2022 10:14:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003425 on epoch=219
03/19/2022 10:15:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.022810 on epoch=224
03/19/2022 10:15:02 - INFO - __main__ - Global step 450 Train loss 0.015043 Classification-F1 1.0 on epoch=224
03/19/2022 10:15:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.019123 on epoch=229
03/19/2022 10:15:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.029053 on epoch=234
03/19/2022 10:15:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.038041 on epoch=239
03/19/2022 10:15:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.053133 on epoch=244
03/19/2022 10:15:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.012126 on epoch=249
03/19/2022 10:15:28 - INFO - __main__ - Global step 500 Train loss 0.030295 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 10:15:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.015731 on epoch=254
03/19/2022 10:15:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004008 on epoch=259
03/19/2022 10:15:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.010958 on epoch=264
03/19/2022 10:15:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002395 on epoch=269
03/19/2022 10:15:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000721 on epoch=274
03/19/2022 10:15:55 - INFO - __main__ - Global step 550 Train loss 0.006763 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:16:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.011147 on epoch=279
03/19/2022 10:16:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017828 on epoch=284
03/19/2022 10:16:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007389 on epoch=289
03/19/2022 10:16:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000573 on epoch=294
03/19/2022 10:16:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003160 on epoch=299
03/19/2022 10:16:21 - INFO - __main__ - Global step 600 Train loss 0.008019 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 10:16:21 - INFO - __main__ - save last model!
03/19/2022 10:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:16:22 - INFO - __main__ - Printing 3 examples
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:16:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:16:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:16:22 - INFO - __main__ - Printing 3 examples
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:16:22 - INFO - __main__ - ['negative']
03/19/2022 10:16:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:16:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:16:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:16:28 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:16:29 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:16:29 - INFO - __main__ - Printing 3 examples
03/19/2022 10:16:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:16:29 - INFO - __main__ - ['negative']
03/19/2022 10:16:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:16:29 - INFO - __main__ - ['negative']
03/19/2022 10:16:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:16:29 - INFO - __main__ - ['negative']
03/19/2022 10:16:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:16:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:16:30 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:16:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:16:35 - INFO - __main__ - Starting training!
03/19/2022 10:16:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0002_8_predictions.txt
03/19/2022 10:16:45 - INFO - __main__ - Classification-F1 on test data: 0.9339
03/19/2022 10:16:46 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9338942307692307
03/19/2022 10:16:46 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0001, bsz=8 ...
03/19/2022 10:16:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:16:47 - INFO - __main__ - Printing 3 examples
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:16:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:16:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:16:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:16:47 - INFO - __main__ - Printing 3 examples
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/19/2022 10:16:47 - INFO - __main__ - ['negative']
03/19/2022 10:16:47 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:16:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:16:47 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:16:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:16:59 - INFO - __main__ - Starting training!
03/19/2022 10:17:04 - INFO - __main__ - Step 10 Global step 10 Train loss 23.636522 on epoch=4
03/19/2022 10:17:09 - INFO - __main__ - Step 20 Global step 20 Train loss 20.922213 on epoch=9
03/19/2022 10:17:14 - INFO - __main__ - Step 30 Global step 30 Train loss 17.326088 on epoch=14
03/19/2022 10:17:19 - INFO - __main__ - Step 40 Global step 40 Train loss 17.268976 on epoch=19
03/19/2022 10:17:24 - INFO - __main__ - Step 50 Global step 50 Train loss 16.102400 on epoch=24
03/19/2022 10:17:33 - INFO - __main__ - Global step 50 Train loss 19.051237 Classification-F1 0.0 on epoch=24
03/19/2022 10:17:39 - INFO - __main__ - Step 60 Global step 60 Train loss 16.165491 on epoch=29
03/19/2022 10:17:44 - INFO - __main__ - Step 70 Global step 70 Train loss 15.605452 on epoch=34
03/19/2022 10:17:49 - INFO - __main__ - Step 80 Global step 80 Train loss 14.521906 on epoch=39
03/19/2022 10:17:54 - INFO - __main__ - Step 90 Global step 90 Train loss 13.770462 on epoch=44
03/19/2022 10:17:59 - INFO - __main__ - Step 100 Global step 100 Train loss 13.968747 on epoch=49
03/19/2022 10:18:08 - INFO - __main__ - Global step 100 Train loss 14.806410 Classification-F1 0.0 on epoch=49
03/19/2022 10:18:14 - INFO - __main__ - Step 110 Global step 110 Train loss 13.469915 on epoch=54
03/19/2022 10:18:19 - INFO - __main__ - Step 120 Global step 120 Train loss 13.452800 on epoch=59
03/19/2022 10:18:24 - INFO - __main__ - Step 130 Global step 130 Train loss 12.744551 on epoch=64
03/19/2022 10:18:29 - INFO - __main__ - Step 140 Global step 140 Train loss 12.446413 on epoch=69
03/19/2022 10:18:34 - INFO - __main__ - Step 150 Global step 150 Train loss 12.241903 on epoch=74
03/19/2022 10:18:43 - INFO - __main__ - Global step 150 Train loss 12.871117 Classification-F1 0.0 on epoch=74
03/19/2022 10:18:48 - INFO - __main__ - Step 160 Global step 160 Train loss 11.334857 on epoch=79
03/19/2022 10:18:53 - INFO - __main__ - Step 170 Global step 170 Train loss 9.777437 on epoch=84
03/19/2022 10:18:58 - INFO - __main__ - Step 180 Global step 180 Train loss 5.086554 on epoch=89
03/19/2022 10:19:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.868899 on epoch=94
03/19/2022 10:19:08 - INFO - __main__ - Step 200 Global step 200 Train loss 1.571493 on epoch=99
03/19/2022 10:19:09 - INFO - __main__ - Global step 200 Train loss 5.727848 Classification-F1 0.5134502923976608 on epoch=99
03/19/2022 10:19:14 - INFO - __main__ - Step 210 Global step 210 Train loss 3.536801 on epoch=104
03/19/2022 10:19:19 - INFO - __main__ - Step 220 Global step 220 Train loss 1.915587 on epoch=109
03/19/2022 10:19:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.740953 on epoch=114
03/19/2022 10:19:29 - INFO - __main__ - Step 240 Global step 240 Train loss 4.354946 on epoch=119
03/19/2022 10:19:34 - INFO - __main__ - Step 250 Global step 250 Train loss 3.164692 on epoch=124
03/19/2022 10:19:34 - INFO - __main__ - Global step 250 Train loss 2.942596 Classification-F1 0.9054187192118226 on epoch=124
03/19/2022 10:19:40 - INFO - __main__ - Step 260 Global step 260 Train loss 3.673005 on epoch=129
03/19/2022 10:19:45 - INFO - __main__ - Step 270 Global step 270 Train loss 1.573056 on epoch=134
03/19/2022 10:19:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.507736 on epoch=139
03/19/2022 10:19:55 - INFO - __main__ - Step 290 Global step 290 Train loss 1.095346 on epoch=144
03/19/2022 10:20:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.557349 on epoch=149
03/19/2022 10:20:00 - INFO - __main__ - Global step 300 Train loss 1.681298 Classification-F1 0.7408906882591093 on epoch=149
03/19/2022 10:20:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.377529 on epoch=154
03/19/2022 10:20:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.351570 on epoch=159
03/19/2022 10:20:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.650468 on epoch=164
03/19/2022 10:20:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.362942 on epoch=169
03/19/2022 10:20:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.367529 on epoch=174
03/19/2022 10:20:26 - INFO - __main__ - Global step 350 Train loss 0.422007 Classification-F1 0.46843853820598 on epoch=174
03/19/2022 10:20:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.325360 on epoch=179
03/19/2022 10:20:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.343848 on epoch=184
03/19/2022 10:20:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.323478 on epoch=189
03/19/2022 10:20:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.427049 on epoch=194
03/19/2022 10:20:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.356462 on epoch=199
03/19/2022 10:20:51 - INFO - __main__ - Global step 400 Train loss 0.355239 Classification-F1 0.37662337662337664 on epoch=199
03/19/2022 10:20:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362705 on epoch=204
03/19/2022 10:21:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.359382 on epoch=209
03/19/2022 10:21:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.342187 on epoch=214
03/19/2022 10:21:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324495 on epoch=219
03/19/2022 10:21:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.336966 on epoch=224
03/19/2022 10:21:17 - INFO - __main__ - Global step 450 Train loss 0.345147 Classification-F1 0.3552492046659597 on epoch=224
03/19/2022 10:21:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.326690 on epoch=229
03/19/2022 10:21:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.301521 on epoch=234
03/19/2022 10:21:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.336869 on epoch=239
03/19/2022 10:21:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.306641 on epoch=244
03/19/2022 10:21:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.339768 on epoch=249
03/19/2022 10:21:43 - INFO - __main__ - Global step 500 Train loss 0.322298 Classification-F1 0.39756367663344405 on epoch=249
03/19/2022 10:21:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.326159 on epoch=254
03/19/2022 10:21:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.324758 on epoch=259
03/19/2022 10:21:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.333304 on epoch=264
03/19/2022 10:22:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.322040 on epoch=269
03/19/2022 10:22:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.289663 on epoch=274
03/19/2022 10:22:09 - INFO - __main__ - Global step 550 Train loss 0.319185 Classification-F1 0.5076923076923077 on epoch=274
03/19/2022 10:22:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.309117 on epoch=279
03/19/2022 10:22:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.291745 on epoch=284
03/19/2022 10:22:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.332900 on epoch=289
03/19/2022 10:22:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.320476 on epoch=294
03/19/2022 10:22:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.333843 on epoch=299
03/19/2022 10:22:36 - INFO - __main__ - Global step 600 Train loss 0.317616 Classification-F1 0.5636363636363637 on epoch=299
03/19/2022 10:22:36 - INFO - __main__ - save last model!
03/19/2022 10:22:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:22:36 - INFO - __main__ - Printing 3 examples
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:22:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:22:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:22:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:22:36 - INFO - __main__ - Printing 3 examples
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:22:36 - INFO - __main__ - ['negative']
03/19/2022 10:22:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:22:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:22:37 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:22:42 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:22:43 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:22:43 - INFO - __main__ - Printing 3 examples
03/19/2022 10:22:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:22:43 - INFO - __main__ - ['negative']
03/19/2022 10:22:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:22:43 - INFO - __main__ - ['negative']
03/19/2022 10:22:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:22:43 - INFO - __main__ - ['negative']
03/19/2022 10:22:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:22:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:22:44 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:22:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:22:48 - INFO - __main__ - Starting training!
03/19/2022 10:23:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0001_8_predictions.txt
03/19/2022 10:23:00 - INFO - __main__ - Classification-F1 on test data: 0.5412
03/19/2022 10:23:00 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9054187192118226, test_performance=0.5412495330977273
03/19/2022 10:23:00 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0005, bsz=8 ...
03/19/2022 10:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:23:01 - INFO - __main__ - Printing 3 examples
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:23:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:23:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:23:01 - INFO - __main__ - Printing 3 examples
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:23:01 - INFO - __main__ - ['negative']
03/19/2022 10:23:01 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:23:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:23:01 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:23:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:23:11 - INFO - __main__ - Starting training!
03/19/2022 10:23:15 - INFO - __main__ - Step 10 Global step 10 Train loss 22.252460 on epoch=4
03/19/2022 10:23:20 - INFO - __main__ - Step 20 Global step 20 Train loss 17.689583 on epoch=9
03/19/2022 10:23:26 - INFO - __main__ - Step 30 Global step 30 Train loss 14.705327 on epoch=14
03/19/2022 10:23:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.769567 on epoch=19
03/19/2022 10:23:36 - INFO - __main__ - Step 50 Global step 50 Train loss 9.745708 on epoch=24
03/19/2022 10:23:36 - INFO - __main__ - Global step 50 Train loss 15.432528 Classification-F1 0.0 on epoch=24
03/19/2022 10:23:42 - INFO - __main__ - Step 60 Global step 60 Train loss 5.513101 on epoch=29
03/19/2022 10:23:47 - INFO - __main__ - Step 70 Global step 70 Train loss 1.823686 on epoch=34
03/19/2022 10:23:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.712249 on epoch=39
03/19/2022 10:23:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.462164 on epoch=44
03/19/2022 10:24:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.403858 on epoch=49
03/19/2022 10:24:02 - INFO - __main__ - Global step 100 Train loss 1.783012 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 10:24:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.344014 on epoch=54
03/19/2022 10:24:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.346783 on epoch=59
03/19/2022 10:24:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.299234 on epoch=64
03/19/2022 10:24:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.208749 on epoch=69
03/19/2022 10:24:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.101906 on epoch=74
03/19/2022 10:24:29 - INFO - __main__ - Global step 150 Train loss 0.260137 Classification-F1 1.0 on epoch=74
03/19/2022 10:24:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.006866 on epoch=79
03/19/2022 10:24:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003030 on epoch=84
03/19/2022 10:24:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.021958 on epoch=89
03/19/2022 10:24:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.008324 on epoch=94
03/19/2022 10:24:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001244 on epoch=99
03/19/2022 10:24:55 - INFO - __main__ - Global step 200 Train loss 0.008284 Classification-F1 0.9687194525904204 on epoch=99
03/19/2022 10:25:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001225 on epoch=104
03/19/2022 10:25:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.001147 on epoch=109
03/19/2022 10:25:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000307 on epoch=114
03/19/2022 10:25:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.083615 on epoch=119
03/19/2022 10:25:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.122608 on epoch=124
03/19/2022 10:25:21 - INFO - __main__ - Global step 250 Train loss 0.041780 Classification-F1 0.9687194525904204 on epoch=124
03/19/2022 10:25:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003969 on epoch=129
03/19/2022 10:25:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001571 on epoch=134
03/19/2022 10:25:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000652 on epoch=139
03/19/2022 10:25:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000612 on epoch=144
03/19/2022 10:25:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000216 on epoch=149
03/19/2022 10:25:47 - INFO - __main__ - Global step 300 Train loss 0.001404 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 10:25:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.023391 on epoch=154
03/19/2022 10:25:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002006 on epoch=159
03/19/2022 10:26:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000367 on epoch=164
03/19/2022 10:26:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000735 on epoch=169
03/19/2022 10:26:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000381 on epoch=174
03/19/2022 10:26:13 - INFO - __main__ - Global step 350 Train loss 0.005376 Classification-F1 0.9375 on epoch=174
03/19/2022 10:26:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000157 on epoch=179
03/19/2022 10:26:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.010024 on epoch=184
03/19/2022 10:26:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000718 on epoch=189
03/19/2022 10:26:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.094938 on epoch=194
03/19/2022 10:26:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.013584 on epoch=199
03/19/2022 10:26:39 - INFO - __main__ - Global step 400 Train loss 0.023884 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 10:26:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000238 on epoch=204
03/19/2022 10:26:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000120 on epoch=209
03/19/2022 10:26:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004854 on epoch=214
03/19/2022 10:26:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000058 on epoch=219
03/19/2022 10:27:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000973 on epoch=224
03/19/2022 10:27:05 - INFO - __main__ - Global step 450 Train loss 0.001249 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 10:27:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000173 on epoch=229
03/19/2022 10:27:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000057 on epoch=234
03/19/2022 10:27:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000885 on epoch=239
03/19/2022 10:27:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000074 on epoch=244
03/19/2022 10:27:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000118 on epoch=249
03/19/2022 10:27:30 - INFO - __main__ - Global step 500 Train loss 0.000261 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 10:27:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000066 on epoch=254
03/19/2022 10:27:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000157 on epoch=259
03/19/2022 10:27:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000065 on epoch=264
03/19/2022 10:27:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000038 on epoch=269
03/19/2022 10:27:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000010 on epoch=274
03/19/2022 10:27:56 - INFO - __main__ - Global step 550 Train loss 0.000067 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:28:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000012 on epoch=279
03/19/2022 10:28:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000015 on epoch=284
03/19/2022 10:28:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000011 on epoch=289
03/19/2022 10:28:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000015 on epoch=294
03/19/2022 10:28:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000013 on epoch=299
03/19/2022 10:28:22 - INFO - __main__ - Global step 600 Train loss 0.000013 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 10:28:22 - INFO - __main__ - save last model!
03/19/2022 10:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:28:23 - INFO - __main__ - Printing 3 examples
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:28:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:28:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:28:23 - INFO - __main__ - Printing 3 examples
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:28:23 - INFO - __main__ - ['negative']
03/19/2022 10:28:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:28:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:28:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:28:29 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:28:29 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:28:29 - INFO - __main__ - Printing 3 examples
03/19/2022 10:28:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:28:29 - INFO - __main__ - ['negative']
03/19/2022 10:28:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:28:29 - INFO - __main__ - ['negative']
03/19/2022 10:28:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:28:29 - INFO - __main__ - ['negative']
03/19/2022 10:28:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:28:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:28:31 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:28:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:28:34 - INFO - __main__ - Starting training!
03/19/2022 10:28:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0005_8_predictions.txt
03/19/2022 10:28:46 - INFO - __main__ - Classification-F1 on test data: 0.9340
03/19/2022 10:28:47 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.9339989439831038
03/19/2022 10:28:47 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0003, bsz=8 ...
03/19/2022 10:28:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:28:48 - INFO - __main__ - Printing 3 examples
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:28:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:28:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:28:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:28:48 - INFO - __main__ - Printing 3 examples
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:28:48 - INFO - __main__ - ['negative']
03/19/2022 10:28:48 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:28:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:28:48 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:28:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:28:58 - INFO - __main__ - Starting training!
03/19/2022 10:29:03 - INFO - __main__ - Step 10 Global step 10 Train loss 21.260986 on epoch=4
03/19/2022 10:29:08 - INFO - __main__ - Step 20 Global step 20 Train loss 18.582760 on epoch=9
03/19/2022 10:29:13 - INFO - __main__ - Step 30 Global step 30 Train loss 15.400706 on epoch=14
03/19/2022 10:29:18 - INFO - __main__ - Step 40 Global step 40 Train loss 14.807630 on epoch=19
03/19/2022 10:29:23 - INFO - __main__ - Step 50 Global step 50 Train loss 13.815634 on epoch=24
03/19/2022 10:29:23 - INFO - __main__ - Global step 50 Train loss 16.773542 Classification-F1 0.0 on epoch=24
03/19/2022 10:29:29 - INFO - __main__ - Step 60 Global step 60 Train loss 12.896817 on epoch=29
03/19/2022 10:29:34 - INFO - __main__ - Step 70 Global step 70 Train loss 11.817392 on epoch=34
03/19/2022 10:29:39 - INFO - __main__ - Step 80 Global step 80 Train loss 10.440182 on epoch=39
03/19/2022 10:29:44 - INFO - __main__ - Step 90 Global step 90 Train loss 7.116118 on epoch=44
03/19/2022 10:29:49 - INFO - __main__ - Step 100 Global step 100 Train loss 3.817454 on epoch=49
03/19/2022 10:29:50 - INFO - __main__ - Global step 100 Train loss 9.217592 Classification-F1 0.2074074074074074 on epoch=49
03/19/2022 10:29:56 - INFO - __main__ - Step 110 Global step 110 Train loss 2.822284 on epoch=54
03/19/2022 10:30:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.902154 on epoch=59
03/19/2022 10:30:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.817556 on epoch=64
03/19/2022 10:30:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.893334 on epoch=69
03/19/2022 10:30:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.830970 on epoch=74
03/19/2022 10:30:17 - INFO - __main__ - Global step 150 Train loss 2.053260 Classification-F1 0.3816425120772947 on epoch=74
03/19/2022 10:30:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.429451 on epoch=79
03/19/2022 10:30:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.819825 on epoch=84
03/19/2022 10:30:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.806125 on epoch=89
03/19/2022 10:30:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.682027 on epoch=94
03/19/2022 10:30:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.472912 on epoch=99
03/19/2022 10:30:43 - INFO - __main__ - Global step 200 Train loss 0.842068 Classification-F1 0.8398398398398398 on epoch=99
03/19/2022 10:30:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.252961 on epoch=104
03/19/2022 10:30:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.159572 on epoch=109
03/19/2022 10:30:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.136647 on epoch=114
03/19/2022 10:31:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.133814 on epoch=119
03/19/2022 10:31:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.252484 on epoch=124
03/19/2022 10:31:10 - INFO - __main__ - Global step 250 Train loss 0.187096 Classification-F1 0.9375 on epoch=124
03/19/2022 10:31:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.079614 on epoch=129
03/19/2022 10:31:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.207788 on epoch=134
03/19/2022 10:31:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.058988 on epoch=139
03/19/2022 10:31:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.029321 on epoch=144
03/19/2022 10:31:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.069155 on epoch=149
03/19/2022 10:31:36 - INFO - __main__ - Global step 300 Train loss 0.088973 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 10:31:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.044784 on epoch=154
03/19/2022 10:31:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.037716 on epoch=159
03/19/2022 10:31:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.059463 on epoch=164
03/19/2022 10:31:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.062692 on epoch=169
03/19/2022 10:32:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.078228 on epoch=174
03/19/2022 10:32:03 - INFO - __main__ - Global step 350 Train loss 0.056577 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 10:32:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.012558 on epoch=179
03/19/2022 10:32:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.186267 on epoch=184
03/19/2022 10:32:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.044424 on epoch=189
03/19/2022 10:32:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006353 on epoch=194
03/19/2022 10:32:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015722 on epoch=199
03/19/2022 10:32:29 - INFO - __main__ - Global step 400 Train loss 0.053065 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 10:32:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000685 on epoch=204
03/19/2022 10:32:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001995 on epoch=209
03/19/2022 10:32:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000973 on epoch=214
03/19/2022 10:32:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.053229 on epoch=219
03/19/2022 10:32:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001119 on epoch=224
03/19/2022 10:32:55 - INFO - __main__ - Global step 450 Train loss 0.011600 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 10:33:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002734 on epoch=229
03/19/2022 10:33:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000862 on epoch=234
03/19/2022 10:33:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.009605 on epoch=239
03/19/2022 10:33:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000417 on epoch=244
03/19/2022 10:33:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.009088 on epoch=249
03/19/2022 10:33:20 - INFO - __main__ - Global step 500 Train loss 0.004541 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 10:33:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001765 on epoch=254
03/19/2022 10:33:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.008722 on epoch=259
03/19/2022 10:33:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000425 on epoch=264
03/19/2022 10:33:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002373 on epoch=269
03/19/2022 10:33:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000871 on epoch=274
03/19/2022 10:33:46 - INFO - __main__ - Global step 550 Train loss 0.002831 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:33:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.037754 on epoch=279
03/19/2022 10:33:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000367 on epoch=284
03/19/2022 10:34:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000425 on epoch=289
03/19/2022 10:34:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003994 on epoch=294
03/19/2022 10:34:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000472 on epoch=299
03/19/2022 10:34:12 - INFO - __main__ - Global step 600 Train loss 0.008602 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 10:34:12 - INFO - __main__ - save last model!
03/19/2022 10:34:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:34:12 - INFO - __main__ - Printing 3 examples
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:34:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:34:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:34:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:34:12 - INFO - __main__ - Printing 3 examples
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:34:12 - INFO - __main__ - ['negative']
03/19/2022 10:34:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:34:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:34:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:34:19 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:34:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:34:20 - INFO - __main__ - Printing 3 examples
03/19/2022 10:34:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:34:20 - INFO - __main__ - ['negative']
03/19/2022 10:34:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:34:20 - INFO - __main__ - ['negative']
03/19/2022 10:34:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:34:20 - INFO - __main__ - ['negative']
03/19/2022 10:34:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:34:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:34:21 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:34:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:34:26 - INFO - __main__ - Starting training!
03/19/2022 10:34:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0003_8_predictions.txt
03/19/2022 10:34:36 - INFO - __main__ - Classification-F1 on test data: 0.4574
03/19/2022 10:34:36 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.4573670654744273
03/19/2022 10:34:36 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0002, bsz=8 ...
03/19/2022 10:34:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:34:37 - INFO - __main__ - Printing 3 examples
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:34:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:34:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:34:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:34:37 - INFO - __main__ - Printing 3 examples
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:34:37 - INFO - __main__ - ['negative']
03/19/2022 10:34:37 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:34:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:34:37 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:34:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:34:48 - INFO - __main__ - Starting training!
03/19/2022 10:34:54 - INFO - __main__ - Step 10 Global step 10 Train loss 22.691990 on epoch=4
03/19/2022 10:34:59 - INFO - __main__ - Step 20 Global step 20 Train loss 17.898081 on epoch=9
03/19/2022 10:35:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.071083 on epoch=14
03/19/2022 10:35:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.761020 on epoch=19
03/19/2022 10:35:14 - INFO - __main__ - Step 50 Global step 50 Train loss 14.381559 on epoch=24
03/19/2022 10:35:18 - INFO - __main__ - Global step 50 Train loss 17.160748 Classification-F1 0.0 on epoch=24
03/19/2022 10:35:24 - INFO - __main__ - Step 60 Global step 60 Train loss 13.977664 on epoch=29
03/19/2022 10:35:29 - INFO - __main__ - Step 70 Global step 70 Train loss 13.438663 on epoch=34
03/19/2022 10:35:34 - INFO - __main__ - Step 80 Global step 80 Train loss 12.242777 on epoch=39
03/19/2022 10:35:39 - INFO - __main__ - Step 90 Global step 90 Train loss 11.451501 on epoch=44
03/19/2022 10:35:44 - INFO - __main__ - Step 100 Global step 100 Train loss 9.964098 on epoch=49
03/19/2022 10:35:47 - INFO - __main__ - Global step 100 Train loss 12.214939 Classification-F1 0.0 on epoch=49
03/19/2022 10:35:52 - INFO - __main__ - Step 110 Global step 110 Train loss 8.418207 on epoch=54
03/19/2022 10:35:57 - INFO - __main__ - Step 120 Global step 120 Train loss 4.243278 on epoch=59
03/19/2022 10:36:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.144121 on epoch=64
03/19/2022 10:36:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.628422 on epoch=69
03/19/2022 10:36:12 - INFO - __main__ - Step 150 Global step 150 Train loss 2.162203 on epoch=74
03/19/2022 10:36:13 - INFO - __main__ - Global step 150 Train loss 3.919246 Classification-F1 0.3992490613266583 on epoch=74
03/19/2022 10:36:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.980938 on epoch=79
03/19/2022 10:36:23 - INFO - __main__ - Step 170 Global step 170 Train loss 2.120234 on epoch=84
03/19/2022 10:36:28 - INFO - __main__ - Step 180 Global step 180 Train loss 1.525001 on epoch=89
03/19/2022 10:36:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.461510 on epoch=94
03/19/2022 10:36:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.895554 on epoch=99
03/19/2022 10:36:39 - INFO - __main__ - Global step 200 Train loss 1.796648 Classification-F1 0.6267232237539766 on epoch=99
03/19/2022 10:36:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.202258 on epoch=104
03/19/2022 10:36:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.861211 on epoch=109
03/19/2022 10:36:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.113812 on epoch=114
03/19/2022 10:37:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.673968 on epoch=119
03/19/2022 10:37:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.048563 on epoch=124
03/19/2022 10:37:06 - INFO - __main__ - Global step 250 Train loss 0.979962 Classification-F1 0.8745098039215686 on epoch=124
03/19/2022 10:37:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.650832 on epoch=129
03/19/2022 10:37:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.306183 on epoch=134
03/19/2022 10:37:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.086244 on epoch=139
03/19/2022 10:37:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.127176 on epoch=144
03/19/2022 10:37:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.079595 on epoch=149
03/19/2022 10:37:32 - INFO - __main__ - Global step 300 Train loss 0.250006 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 10:37:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.059060 on epoch=154
03/19/2022 10:37:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.034164 on epoch=159
03/19/2022 10:37:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.059554 on epoch=164
03/19/2022 10:37:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.050010 on epoch=169
03/19/2022 10:37:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.044600 on epoch=174
03/19/2022 10:37:58 - INFO - __main__ - Global step 350 Train loss 0.049478 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 10:38:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.049667 on epoch=179
03/19/2022 10:38:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.038811 on epoch=184
03/19/2022 10:38:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.027851 on epoch=189
03/19/2022 10:38:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.029860 on epoch=194
03/19/2022 10:38:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.024207 on epoch=199
03/19/2022 10:38:24 - INFO - __main__ - Global step 400 Train loss 0.034079 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 10:38:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015849 on epoch=204
03/19/2022 10:38:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004152 on epoch=209
03/19/2022 10:38:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000788 on epoch=214
03/19/2022 10:38:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.016954 on epoch=219
03/19/2022 10:38:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000472 on epoch=224
03/19/2022 10:38:50 - INFO - __main__ - Global step 450 Train loss 0.007643 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 10:38:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000288 on epoch=229
03/19/2022 10:39:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001280 on epoch=234
03/19/2022 10:39:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000252 on epoch=239
03/19/2022 10:39:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000935 on epoch=244
03/19/2022 10:39:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000330 on epoch=249
03/19/2022 10:39:16 - INFO - __main__ - Global step 500 Train loss 0.000617 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 10:39:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001662 on epoch=254
03/19/2022 10:39:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001148 on epoch=259
03/19/2022 10:39:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000320 on epoch=264
03/19/2022 10:39:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000176 on epoch=269
03/19/2022 10:39:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000429 on epoch=274
03/19/2022 10:39:42 - INFO - __main__ - Global step 550 Train loss 0.000747 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:39:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000165 on epoch=279
03/19/2022 10:39:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000147 on epoch=284
03/19/2022 10:39:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000177 on epoch=289
03/19/2022 10:40:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000242 on epoch=294
03/19/2022 10:40:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000082 on epoch=299
03/19/2022 10:40:08 - INFO - __main__ - Global step 600 Train loss 0.000162 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 10:40:08 - INFO - __main__ - save last model!
03/19/2022 10:40:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:40:08 - INFO - __main__ - Printing 3 examples
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:40:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:40:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:40:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:40:08 - INFO - __main__ - Printing 3 examples
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:40:08 - INFO - __main__ - ['negative']
03/19/2022 10:40:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:40:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:40:09 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:40:14 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:40:15 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:40:15 - INFO - __main__ - Printing 3 examples
03/19/2022 10:40:15 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:40:15 - INFO - __main__ - ['negative']
03/19/2022 10:40:15 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:40:15 - INFO - __main__ - ['negative']
03/19/2022 10:40:15 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:40:15 - INFO - __main__ - ['negative']
03/19/2022 10:40:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:40:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:40:16 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:40:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:40:20 - INFO - __main__ - Starting training!
03/19/2022 10:40:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0002_8_predictions.txt
03/19/2022 10:40:31 - INFO - __main__ - Classification-F1 on test data: 0.9120
03/19/2022 10:40:32 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9119647859143658
03/19/2022 10:40:32 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0001, bsz=8 ...
03/19/2022 10:40:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:40:33 - INFO - __main__ - Printing 3 examples
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:40:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:40:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:40:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:40:33 - INFO - __main__ - Printing 3 examples
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/19/2022 10:40:33 - INFO - __main__ - ['negative']
03/19/2022 10:40:33 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:40:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:40:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:40:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:40:43 - INFO - __main__ - Starting training!
03/19/2022 10:40:48 - INFO - __main__ - Step 10 Global step 10 Train loss 22.068359 on epoch=4
03/19/2022 10:40:53 - INFO - __main__ - Step 20 Global step 20 Train loss 18.386393 on epoch=9
03/19/2022 10:40:58 - INFO - __main__ - Step 30 Global step 30 Train loss 17.667543 on epoch=14
03/19/2022 10:41:03 - INFO - __main__ - Step 40 Global step 40 Train loss 17.304499 on epoch=19
03/19/2022 10:41:08 - INFO - __main__ - Step 50 Global step 50 Train loss 16.303635 on epoch=24
03/19/2022 10:41:15 - INFO - __main__ - Global step 50 Train loss 18.346087 Classification-F1 0.0 on epoch=24
03/19/2022 10:41:20 - INFO - __main__ - Step 60 Global step 60 Train loss 15.579813 on epoch=29
03/19/2022 10:41:25 - INFO - __main__ - Step 70 Global step 70 Train loss 14.935605 on epoch=34
03/19/2022 10:41:30 - INFO - __main__ - Step 80 Global step 80 Train loss 14.825693 on epoch=39
03/19/2022 10:41:36 - INFO - __main__ - Step 90 Global step 90 Train loss 14.933243 on epoch=44
03/19/2022 10:41:41 - INFO - __main__ - Step 100 Global step 100 Train loss 13.549576 on epoch=49
03/19/2022 10:41:44 - INFO - __main__ - Global step 100 Train loss 14.764785 Classification-F1 0.0 on epoch=49
03/19/2022 10:41:49 - INFO - __main__ - Step 110 Global step 110 Train loss 13.591347 on epoch=54
03/19/2022 10:41:54 - INFO - __main__ - Step 120 Global step 120 Train loss 12.897276 on epoch=59
03/19/2022 10:41:59 - INFO - __main__ - Step 130 Global step 130 Train loss 12.628206 on epoch=64
03/19/2022 10:42:04 - INFO - __main__ - Step 140 Global step 140 Train loss 12.254139 on epoch=69
03/19/2022 10:42:09 - INFO - __main__ - Step 150 Global step 150 Train loss 12.225073 on epoch=74
03/19/2022 10:42:11 - INFO - __main__ - Global step 150 Train loss 12.719207 Classification-F1 0.0 on epoch=74
03/19/2022 10:42:16 - INFO - __main__ - Step 160 Global step 160 Train loss 11.410249 on epoch=79
03/19/2022 10:42:21 - INFO - __main__ - Step 170 Global step 170 Train loss 11.398025 on epoch=84
03/19/2022 10:42:26 - INFO - __main__ - Step 180 Global step 180 Train loss 10.291488 on epoch=89
03/19/2022 10:42:31 - INFO - __main__ - Step 190 Global step 190 Train loss 5.905591 on epoch=94
03/19/2022 10:42:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.367416 on epoch=99
03/19/2022 10:42:36 - INFO - __main__ - Global step 200 Train loss 8.074553 Classification-F1 0.8745098039215686 on epoch=99
03/19/2022 10:42:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.696983 on epoch=104
03/19/2022 10:42:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.335602 on epoch=109
03/19/2022 10:42:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.058805 on epoch=114
03/19/2022 10:42:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.244021 on epoch=119
03/19/2022 10:43:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.342435 on epoch=124
03/19/2022 10:43:03 - INFO - __main__ - Global step 250 Train loss 0.335569 Classification-F1 1.0 on epoch=124
03/19/2022 10:43:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.461399 on epoch=129
03/19/2022 10:43:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.093348 on epoch=134
03/19/2022 10:43:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.031755 on epoch=139
03/19/2022 10:43:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.111603 on epoch=144
03/19/2022 10:43:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002534 on epoch=149
03/19/2022 10:43:29 - INFO - __main__ - Global step 300 Train loss 0.140128 Classification-F1 0.9687194525904204 on epoch=149
03/19/2022 10:43:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.049442 on epoch=154
03/19/2022 10:43:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.109558 on epoch=159
03/19/2022 10:43:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.110517 on epoch=164
03/19/2022 10:43:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000832 on epoch=169
03/19/2022 10:43:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.161580 on epoch=174
03/19/2022 10:43:55 - INFO - __main__ - Global step 350 Train loss 0.086386 Classification-F1 0.9687194525904204 on epoch=174
03/19/2022 10:44:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.004284 on epoch=179
03/19/2022 10:44:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.017921 on epoch=184
03/19/2022 10:44:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001052 on epoch=189
03/19/2022 10:44:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001033 on epoch=194
03/19/2022 10:44:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.161371 on epoch=199
03/19/2022 10:44:21 - INFO - __main__ - Global step 400 Train loss 0.037132 Classification-F1 0.9687194525904204 on epoch=199
03/19/2022 10:44:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003450 on epoch=204
03/19/2022 10:44:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001595 on epoch=209
03/19/2022 10:44:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.078973 on epoch=214
03/19/2022 10:44:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001097 on epoch=219
03/19/2022 10:44:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002428 on epoch=224
03/19/2022 10:44:46 - INFO - __main__ - Global step 450 Train loss 0.017509 Classification-F1 0.9687194525904204 on epoch=224
03/19/2022 10:44:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000608 on epoch=229
03/19/2022 10:44:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000851 on epoch=234
03/19/2022 10:45:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000724 on epoch=239
03/19/2022 10:45:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000225 on epoch=244
03/19/2022 10:45:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001810 on epoch=249
03/19/2022 10:45:12 - INFO - __main__ - Global step 500 Train loss 0.000844 Classification-F1 0.9687194525904204 on epoch=249
03/19/2022 10:45:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.029782 on epoch=254
03/19/2022 10:45:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000259 on epoch=259
03/19/2022 10:45:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001669 on epoch=264
03/19/2022 10:45:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000168 on epoch=269
03/19/2022 10:45:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000117 on epoch=274
03/19/2022 10:45:38 - INFO - __main__ - Global step 550 Train loss 0.006399 Classification-F1 0.9687194525904204 on epoch=274
03/19/2022 10:45:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000050 on epoch=279
03/19/2022 10:45:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003558 on epoch=284
03/19/2022 10:45:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.038034 on epoch=289
03/19/2022 10:45:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000133 on epoch=294
03/19/2022 10:46:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000072 on epoch=299
03/19/2022 10:46:04 - INFO - __main__ - Global step 600 Train loss 0.008369 Classification-F1 0.9687194525904204 on epoch=299
03/19/2022 10:46:04 - INFO - __main__ - save last model!
03/19/2022 10:46:11 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:46:12 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 10:46:12 - INFO - __main__ - Printing 3 examples
03/19/2022 10:46:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/19/2022 10:46:12 - INFO - __main__ - ['negative']
03/19/2022 10:46:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/19/2022 10:46:12 - INFO - __main__ - ['negative']
03/19/2022 10:46:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/19/2022 10:46:12 - INFO - __main__ - ['negative']
03/19/2022 10:46:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:46:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:46:14 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 10:46:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0001_8_predictions.txt
03/19/2022 10:46:29 - INFO - __main__ - Classification-F1 on test data: 0.9490
03/19/2022 10:46:29 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.9489774990770931
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22358): No such process
Task: tab_fact, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 10:46:34 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 10:46:34 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-tab_fact
Output directory () already exists and is not empty.
03/19/2022 10:46:34 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 10:46:34 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-tab_fact
03/19/2022 10:46:36 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 10:46:36 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 10:46:36 - INFO - __main__ - args.device: cuda:0
03/19/2022 10:46:36 - INFO - __main__ - Using 2 gpus
03/19/2022 10:46:36 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/19/2022 10:46:36 - INFO - __main__ - args.device: cuda:1
03/19/2022 10:46:36 - INFO - __main__ - Using 2 gpus
03/19/2022 10:46:36 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/19/2022 10:46:40 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.0005, bsz=8 ...
03/19/2022 10:46:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:46:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:46:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:46:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:46:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:46:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:46:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:46:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:46:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:46:41 - INFO - __main__ - Printing 3 examples
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 10:46:41 - INFO - __main__ - ['refuted']
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:46:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:46:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:46:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:46:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:46:54 - INFO - __main__ - Starting training!
03/19/2022 10:46:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:46:55 - INFO - __main__ - Starting training!
03/19/2022 10:47:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.397655 on epoch=4
03/19/2022 10:47:06 - INFO - __main__ - Step 20 Global step 20 Train loss 13.430031 on epoch=9
03/19/2022 10:47:12 - INFO - __main__ - Step 30 Global step 30 Train loss 9.844991 on epoch=14
03/19/2022 10:47:18 - INFO - __main__ - Step 40 Global step 40 Train loss 8.167175 on epoch=19
03/19/2022 10:47:24 - INFO - __main__ - Step 50 Global step 50 Train loss 7.777761 on epoch=24
03/19/2022 10:47:25 - INFO - __main__ - Global step 50 Train loss 12.123522 Classification-F1 0.04 on epoch=24
03/19/2022 10:47:32 - INFO - __main__ - Step 60 Global step 60 Train loss 6.683145 on epoch=29
03/19/2022 10:47:38 - INFO - __main__ - Step 70 Global step 70 Train loss 5.494188 on epoch=34
03/19/2022 10:47:44 - INFO - __main__ - Step 80 Global step 80 Train loss 4.155151 on epoch=39
03/19/2022 10:47:51 - INFO - __main__ - Step 90 Global step 90 Train loss 3.023946 on epoch=44
03/19/2022 10:47:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.983968 on epoch=49
03/19/2022 10:47:58 - INFO - __main__ - Global step 100 Train loss 4.268080 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 10:48:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.709476 on epoch=54
03/19/2022 10:48:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.519168 on epoch=59
03/19/2022 10:48:18 - INFO - __main__ - Step 130 Global step 130 Train loss 1.365636 on epoch=64
03/19/2022 10:48:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.664138 on epoch=69
03/19/2022 10:48:30 - INFO - __main__ - Step 150 Global step 150 Train loss 1.246885 on epoch=74
03/19/2022 10:48:31 - INFO - __main__ - Global step 150 Train loss 1.501061 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 10:48:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.222538 on epoch=79
03/19/2022 10:48:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.417848 on epoch=84
03/19/2022 10:48:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.897373 on epoch=89
03/19/2022 10:48:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.162539 on epoch=94
03/19/2022 10:49:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.961168 on epoch=99
03/19/2022 10:49:03 - INFO - __main__ - Global step 200 Train loss 1.132293 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 10:49:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.830359 on epoch=104
03/19/2022 10:49:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.289228 on epoch=109
03/19/2022 10:49:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.765292 on epoch=114
03/19/2022 10:49:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.584853 on epoch=119
03/19/2022 10:49:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.731281 on epoch=124
03/19/2022 10:49:34 - INFO - __main__ - Global step 250 Train loss 0.840203 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 10:49:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.822414 on epoch=129
03/19/2022 10:49:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.648933 on epoch=134
03/19/2022 10:49:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.429781 on epoch=139
03/19/2022 10:49:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.494077 on epoch=144
03/19/2022 10:50:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.685906 on epoch=149
03/19/2022 10:50:05 - INFO - __main__ - Global step 300 Train loss 0.616222 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 10:50:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.533054 on epoch=154
03/19/2022 10:50:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.447413 on epoch=159
03/19/2022 10:50:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.386365 on epoch=164
03/19/2022 10:50:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.408542 on epoch=169
03/19/2022 10:50:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.339908 on epoch=174
03/19/2022 10:50:36 - INFO - __main__ - Global step 350 Train loss 0.423057 Classification-F1 0.5333333333333333 on epoch=174
03/19/2022 10:50:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.502445 on epoch=179
03/19/2022 10:50:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.376918 on epoch=184
03/19/2022 10:50:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.354584 on epoch=189
03/19/2022 10:51:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.398215 on epoch=194
03/19/2022 10:51:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.318858 on epoch=199
03/19/2022 10:51:08 - INFO - __main__ - Global step 400 Train loss 0.390204 Classification-F1 0.4817813765182186 on epoch=199
03/19/2022 10:51:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.328794 on epoch=204
03/19/2022 10:51:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.293607 on epoch=209
03/19/2022 10:51:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.317971 on epoch=214
03/19/2022 10:51:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.312309 on epoch=219
03/19/2022 10:51:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.331503 on epoch=224
03/19/2022 10:51:39 - INFO - __main__ - Global step 450 Train loss 0.316837 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 10:51:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.241484 on epoch=229
03/19/2022 10:51:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.234224 on epoch=234
03/19/2022 10:51:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.263112 on epoch=239
03/19/2022 10:52:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.284527 on epoch=244
03/19/2022 10:52:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.248772 on epoch=249
03/19/2022 10:52:10 - INFO - __main__ - Global step 500 Train loss 0.254424 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 10:52:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.261623 on epoch=254
03/19/2022 10:52:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.257422 on epoch=259
03/19/2022 10:52:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.238611 on epoch=264
03/19/2022 10:52:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.274031 on epoch=269
03/19/2022 10:52:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.212595 on epoch=274
03/19/2022 10:52:41 - INFO - __main__ - Global step 550 Train loss 0.248856 Classification-F1 0.5844155844155844 on epoch=274
03/19/2022 10:52:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.207257 on epoch=279
03/19/2022 10:52:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.206695 on epoch=284
03/19/2022 10:53:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211619 on epoch=289
03/19/2022 10:53:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.227257 on epoch=294
03/19/2022 10:53:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.172049 on epoch=299
03/19/2022 10:53:14 - INFO - __main__ - Global step 600 Train loss 0.204975 Classification-F1 0.464039408866995 on epoch=299
03/19/2022 10:53:14 - INFO - __main__ - save last model!
03/19/2022 10:53:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:53:14 - INFO - __main__ - Printing 3 examples
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 10:53:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:53:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:53:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:53:14 - INFO - __main__ - Printing 3 examples
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 10:53:14 - INFO - __main__ - ['refuted']
03/19/2022 10:53:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:53:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:53:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:53:20 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 10:53:21 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 10:53:22 - INFO - __main__ - Printing 3 examples
03/19/2022 10:53:22 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 10:53:22 - INFO - __main__ - ['entailed']
03/19/2022 10:53:22 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 10:53:22 - INFO - __main__ - ['entailed']
03/19/2022 10:53:22 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 10:53:22 - INFO - __main__ - ['entailed']
03/19/2022 10:53:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:53:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:53:25 - INFO - __main__ - Starting training!
03/19/2022 10:53:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:53:58 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 10:59:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_100_0.0005_8_predictions.txt
03/19/2022 10:59:46 - INFO - __main__ - Classification-F1 on test data: 0.4079
03/19/2022 10:59:47 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.0005, bsz=8, dev_performance=0.5844155844155844, test_performance=0.4078517932628992
03/19/2022 10:59:47 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.0003, bsz=8 ...
03/19/2022 10:59:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:59:48 - INFO - __main__ - Printing 3 examples
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 10:59:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:59:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 10:59:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 10:59:48 - INFO - __main__ - Printing 3 examples
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 10:59:48 - INFO - __main__ - ['refuted']
03/19/2022 10:59:48 - INFO - __main__ - Tokenizing Input ...
03/19/2022 10:59:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 10:59:48 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 10:59:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 10:59:58 - INFO - __main__ - Starting training!
03/19/2022 11:00:04 - INFO - __main__ - Step 10 Global step 10 Train loss 20.585590 on epoch=4
03/19/2022 11:00:10 - INFO - __main__ - Step 20 Global step 20 Train loss 13.159114 on epoch=9
03/19/2022 11:00:16 - INFO - __main__ - Step 30 Global step 30 Train loss 9.263483 on epoch=14
03/19/2022 11:00:22 - INFO - __main__ - Step 40 Global step 40 Train loss 8.235019 on epoch=19
03/19/2022 11:00:28 - INFO - __main__ - Step 50 Global step 50 Train loss 7.893733 on epoch=24
03/19/2022 11:00:29 - INFO - __main__ - Global step 50 Train loss 11.827387 Classification-F1 0.3333333333333333 on epoch=24
03/19/2022 11:00:36 - INFO - __main__ - Step 60 Global step 60 Train loss 7.458694 on epoch=29
03/19/2022 11:00:42 - INFO - __main__ - Step 70 Global step 70 Train loss 6.959148 on epoch=34
03/19/2022 11:00:48 - INFO - __main__ - Step 80 Global step 80 Train loss 6.046876 on epoch=39
03/19/2022 11:00:54 - INFO - __main__ - Step 90 Global step 90 Train loss 5.543180 on epoch=44
03/19/2022 11:01:00 - INFO - __main__ - Step 100 Global step 100 Train loss 4.608692 on epoch=49
03/19/2022 11:01:01 - INFO - __main__ - Global step 100 Train loss 6.123319 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 11:01:07 - INFO - __main__ - Step 110 Global step 110 Train loss 2.169828 on epoch=54
03/19/2022 11:01:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.300387 on epoch=59
03/19/2022 11:01:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.205667 on epoch=64
03/19/2022 11:01:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.120920 on epoch=69
03/19/2022 11:01:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.088052 on epoch=74
03/19/2022 11:01:33 - INFO - __main__ - Global step 150 Train loss 0.576971 Classification-F1 0.4458874458874459 on epoch=74
03/19/2022 11:01:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.040288 on epoch=79
03/19/2022 11:01:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.043249 on epoch=84
03/19/2022 11:01:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.023880 on epoch=89
03/19/2022 11:01:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.010284 on epoch=94
03/19/2022 11:02:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.005682 on epoch=99
03/19/2022 11:02:05 - INFO - __main__ - Global step 200 Train loss 0.024677 Classification-F1 0.5195195195195195 on epoch=99
03/19/2022 11:02:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005558 on epoch=104
03/19/2022 11:02:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005826 on epoch=109
03/19/2022 11:02:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.004077 on epoch=114
03/19/2022 11:02:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001395 on epoch=119
03/19/2022 11:02:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000970 on epoch=124
03/19/2022 11:02:36 - INFO - __main__ - Global step 250 Train loss 0.003565 Classification-F1 0.5195195195195195 on epoch=124
03/19/2022 11:02:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.002197 on epoch=129
03/19/2022 11:02:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001986 on epoch=134
03/19/2022 11:02:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000582 on epoch=139
03/19/2022 11:03:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000635 on epoch=144
03/19/2022 11:03:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005300 on epoch=149
03/19/2022 11:03:07 - INFO - __main__ - Global step 300 Train loss 0.002140 Classification-F1 0.4817813765182186 on epoch=149
03/19/2022 11:03:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000530 on epoch=154
03/19/2022 11:03:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001179 on epoch=159
03/19/2022 11:03:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000716 on epoch=164
03/19/2022 11:03:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000365 on epoch=169
03/19/2022 11:03:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000322 on epoch=174
03/19/2022 11:03:38 - INFO - __main__ - Global step 350 Train loss 0.000622 Classification-F1 0.5901477832512315 on epoch=174
03/19/2022 11:03:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000967 on epoch=179
03/19/2022 11:03:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000813 on epoch=184
03/19/2022 11:03:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000346 on epoch=189
03/19/2022 11:04:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001212 on epoch=194
03/19/2022 11:04:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002512 on epoch=199
03/19/2022 11:04:10 - INFO - __main__ - Global step 400 Train loss 0.001170 Classification-F1 0.5195195195195195 on epoch=199
03/19/2022 11:04:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.105536 on epoch=204
03/19/2022 11:04:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.003248 on epoch=209
03/19/2022 11:04:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003380 on epoch=214
03/19/2022 11:04:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001406 on epoch=219
03/19/2022 11:04:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000529 on epoch=224
03/19/2022 11:04:41 - INFO - __main__ - Global step 450 Train loss 0.222820 Classification-F1 0.5835835835835835 on epoch=224
03/19/2022 11:04:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000900 on epoch=229
03/19/2022 11:04:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000317 on epoch=234
03/19/2022 11:04:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000614 on epoch=239
03/19/2022 11:05:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000274 on epoch=244
03/19/2022 11:05:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000235 on epoch=249
03/19/2022 11:05:12 - INFO - __main__ - Global step 500 Train loss 0.000468 Classification-F1 0.5901477832512315 on epoch=249
03/19/2022 11:05:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000406 on epoch=254
03/19/2022 11:05:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000262 on epoch=259
03/19/2022 11:05:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000397 on epoch=264
03/19/2022 11:05:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.036902 on epoch=269
03/19/2022 11:05:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.338481 on epoch=274
03/19/2022 11:05:43 - INFO - __main__ - Global step 550 Train loss 0.275290 Classification-F1 0.5555555555555556 on epoch=274
03/19/2022 11:05:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.244469 on epoch=279
03/19/2022 11:05:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.032693 on epoch=284
03/19/2022 11:06:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004341 on epoch=289
03/19/2022 11:06:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001269 on epoch=294
03/19/2022 11:06:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000482 on epoch=299
03/19/2022 11:06:14 - INFO - __main__ - Global step 600 Train loss 0.056651 Classification-F1 0.5555555555555556 on epoch=299
03/19/2022 11:06:14 - INFO - __main__ - save last model!
03/19/2022 11:06:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:06:14 - INFO - __main__ - Printing 3 examples
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 11:06:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:06:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:06:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:06:14 - INFO - __main__ - Printing 3 examples
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 11:06:14 - INFO - __main__ - ['refuted']
03/19/2022 11:06:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:06:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:06:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:06:21 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 11:06:21 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 11:06:21 - INFO - __main__ - Printing 3 examples
03/19/2022 11:06:21 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:06:21 - INFO - __main__ - ['entailed']
03/19/2022 11:06:21 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:06:21 - INFO - __main__ - ['entailed']
03/19/2022 11:06:21 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:06:21 - INFO - __main__ - ['entailed']
03/19/2022 11:06:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:06:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:06:25 - INFO - __main__ - Starting training!
03/19/2022 11:06:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:06:58 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 11:13:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_100_0.0003_8_predictions.txt
03/19/2022 11:13:08 - INFO - __main__ - Classification-F1 on test data: 0.4981
03/19/2022 11:13:09 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.0003, bsz=8, dev_performance=0.5901477832512315, test_performance=0.49812373871626237
03/19/2022 11:13:09 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.0002, bsz=8 ...
03/19/2022 11:13:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:13:09 - INFO - __main__ - Printing 3 examples
03/19/2022 11:13:09 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 11:13:09 - INFO - __main__ - ['refuted']
03/19/2022 11:13:09 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 11:13:09 - INFO - __main__ - ['refuted']
03/19/2022 11:13:09 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 11:13:09 - INFO - __main__ - ['refuted']
03/19/2022 11:13:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:13:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:13:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:13:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:13:10 - INFO - __main__ - Printing 3 examples
03/19/2022 11:13:10 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 11:13:10 - INFO - __main__ - ['refuted']
03/19/2022 11:13:10 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 11:13:10 - INFO - __main__ - ['refuted']
03/19/2022 11:13:10 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 11:13:10 - INFO - __main__ - ['refuted']
03/19/2022 11:13:10 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:13:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:13:10 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:13:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:13:22 - INFO - __main__ - Starting training!
03/19/2022 11:13:28 - INFO - __main__ - Step 10 Global step 10 Train loss 20.149860 on epoch=4
03/19/2022 11:13:34 - INFO - __main__ - Step 20 Global step 20 Train loss 12.765253 on epoch=9
03/19/2022 11:13:40 - INFO - __main__ - Step 30 Global step 30 Train loss 9.547712 on epoch=14
03/19/2022 11:13:46 - INFO - __main__ - Step 40 Global step 40 Train loss 9.574750 on epoch=19
03/19/2022 11:13:53 - INFO - __main__ - Step 50 Global step 50 Train loss 9.180789 on epoch=24
03/19/2022 11:13:54 - INFO - __main__ - Global step 50 Train loss 12.243673 Classification-F1 0.027777777777777776 on epoch=24
03/19/2022 11:14:00 - INFO - __main__ - Step 60 Global step 60 Train loss 11.296112 on epoch=29
03/19/2022 11:14:06 - INFO - __main__ - Step 70 Global step 70 Train loss 10.296744 on epoch=34
03/19/2022 11:14:13 - INFO - __main__ - Step 80 Global step 80 Train loss 9.298242 on epoch=39
03/19/2022 11:14:19 - INFO - __main__ - Step 90 Global step 90 Train loss 7.089925 on epoch=44
03/19/2022 11:14:25 - INFO - __main__ - Step 100 Global step 100 Train loss 6.177835 on epoch=49
03/19/2022 11:14:26 - INFO - __main__ - Global step 100 Train loss 8.831772 Classification-F1 0.04301075268817205 on epoch=49
03/19/2022 11:14:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.109692 on epoch=54
03/19/2022 11:14:39 - INFO - __main__ - Step 120 Global step 120 Train loss 5.574500 on epoch=59
03/19/2022 11:14:45 - INFO - __main__ - Step 130 Global step 130 Train loss 5.089638 on epoch=64
03/19/2022 11:14:51 - INFO - __main__ - Step 140 Global step 140 Train loss 4.182778 on epoch=69
03/19/2022 11:14:57 - INFO - __main__ - Step 150 Global step 150 Train loss 3.791370 on epoch=74
03/19/2022 11:14:58 - INFO - __main__ - Global step 150 Train loss 4.949595 Classification-F1 0.09615384615384616 on epoch=74
03/19/2022 11:15:05 - INFO - __main__ - Step 160 Global step 160 Train loss 2.765003 on epoch=79
03/19/2022 11:15:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.876359 on epoch=84
03/19/2022 11:15:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.611202 on epoch=89
03/19/2022 11:15:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.185102 on epoch=94
03/19/2022 11:15:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.132926 on epoch=99
03/19/2022 11:15:30 - INFO - __main__ - Global step 200 Train loss 0.914118 Classification-F1 0.4666666666666667 on epoch=99
03/19/2022 11:15:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.097625 on epoch=104
03/19/2022 11:15:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.094209 on epoch=109
03/19/2022 11:15:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.052778 on epoch=114
03/19/2022 11:15:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.045253 on epoch=119
03/19/2022 11:16:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.017365 on epoch=124
03/19/2022 11:16:03 - INFO - __main__ - Global step 250 Train loss 0.061446 Classification-F1 0.5195195195195195 on epoch=124
03/19/2022 11:16:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.010093 on epoch=129
03/19/2022 11:16:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007971 on epoch=134
03/19/2022 11:16:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.005803 on epoch=139
03/19/2022 11:16:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.008118 on epoch=144
03/19/2022 11:16:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.004493 on epoch=149
03/19/2022 11:16:35 - INFO - __main__ - Global step 300 Train loss 0.007296 Classification-F1 0.4682306940371457 on epoch=149
03/19/2022 11:16:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002132 on epoch=154
03/19/2022 11:16:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002383 on epoch=159
03/19/2022 11:16:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001543 on epoch=164
03/19/2022 11:16:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.003713 on epoch=169
03/19/2022 11:17:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001443 on epoch=174
03/19/2022 11:17:06 - INFO - __main__ - Global step 350 Train loss 0.002243 Classification-F1 0.5270935960591133 on epoch=174
03/19/2022 11:17:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.003868 on epoch=179
03/19/2022 11:17:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001233 on epoch=184
03/19/2022 11:17:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.014407 on epoch=189
03/19/2022 11:17:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001263 on epoch=194
03/19/2022 11:17:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000278 on epoch=199
03/19/2022 11:17:38 - INFO - __main__ - Global step 400 Train loss 0.004210 Classification-F1 0.464039408866995 on epoch=199
03/19/2022 11:17:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000526 on epoch=204
03/19/2022 11:17:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.002494 on epoch=209
03/19/2022 11:17:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000367 on epoch=214
03/19/2022 11:18:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000158 on epoch=219
03/19/2022 11:18:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005448 on epoch=224
03/19/2022 11:18:10 - INFO - __main__ - Global step 450 Train loss 0.001798 Classification-F1 0.5333333333333333 on epoch=224
03/19/2022 11:18:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000953 on epoch=229
03/19/2022 11:18:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000244 on epoch=234
03/19/2022 11:18:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001790 on epoch=239
03/19/2022 11:18:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000499 on epoch=244
03/19/2022 11:18:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000389 on epoch=249
03/19/2022 11:18:41 - INFO - __main__ - Global step 500 Train loss 0.000775 Classification-F1 0.5195195195195195 on epoch=249
03/19/2022 11:18:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000053 on epoch=254
03/19/2022 11:18:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000187 on epoch=259
03/19/2022 11:19:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000954 on epoch=264
03/19/2022 11:19:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000130 on epoch=269
03/19/2022 11:19:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000768 on epoch=274
03/19/2022 11:19:13 - INFO - __main__ - Global step 550 Train loss 0.000418 Classification-F1 0.4980392156862745 on epoch=274
03/19/2022 11:19:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000079 on epoch=279
03/19/2022 11:19:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000037 on epoch=284
03/19/2022 11:19:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000099 on epoch=289
03/19/2022 11:19:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000109 on epoch=294
03/19/2022 11:19:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000048 on epoch=299
03/19/2022 11:19:43 - INFO - __main__ - Global step 600 Train loss 0.000074 Classification-F1 0.4980392156862745 on epoch=299
03/19/2022 11:19:43 - INFO - __main__ - save last model!
03/19/2022 11:19:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:19:44 - INFO - __main__ - Printing 3 examples
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 11:19:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:19:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:19:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:19:44 - INFO - __main__ - Printing 3 examples
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 11:19:44 - INFO - __main__ - ['refuted']
03/19/2022 11:19:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:19:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:19:44 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:19:50 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 11:19:51 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 11:19:51 - INFO - __main__ - Printing 3 examples
03/19/2022 11:19:51 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:19:51 - INFO - __main__ - ['entailed']
03/19/2022 11:19:51 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:19:51 - INFO - __main__ - ['entailed']
03/19/2022 11:19:51 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:19:51 - INFO - __main__ - ['entailed']
03/19/2022 11:19:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:19:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:19:55 - INFO - __main__ - Starting training!
03/19/2022 11:20:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:20:28 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 11:26:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_100_0.0002_8_predictions.txt
03/19/2022 11:26:36 - INFO - __main__ - Classification-F1 on test data: 0.4478
03/19/2022 11:26:36 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.0002, bsz=8, dev_performance=0.5333333333333333, test_performance=0.44778503591588814
03/19/2022 11:26:36 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.0001, bsz=8 ...
03/19/2022 11:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:26:37 - INFO - __main__ - Printing 3 examples
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:26:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:26:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:26:37 - INFO - __main__ - Printing 3 examples
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/19/2022 11:26:37 - INFO - __main__ - ['refuted']
03/19/2022 11:26:37 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:26:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:26:37 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:26:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:26:50 - INFO - __main__ - Starting training!
03/19/2022 11:26:56 - INFO - __main__ - Step 10 Global step 10 Train loss 19.744522 on epoch=4
03/19/2022 11:27:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.004810 on epoch=9
03/19/2022 11:27:08 - INFO - __main__ - Step 30 Global step 30 Train loss 11.639465 on epoch=14
03/19/2022 11:27:14 - INFO - __main__ - Step 40 Global step 40 Train loss 10.284620 on epoch=19
03/19/2022 11:27:20 - INFO - __main__ - Step 50 Global step 50 Train loss 9.726605 on epoch=24
03/19/2022 11:27:21 - INFO - __main__ - Global step 50 Train loss 13.480003 Classification-F1 0.05263157894736842 on epoch=24
03/19/2022 11:27:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.313162 on epoch=29
03/19/2022 11:27:34 - INFO - __main__ - Step 70 Global step 70 Train loss 8.494821 on epoch=34
03/19/2022 11:27:40 - INFO - __main__ - Step 80 Global step 80 Train loss 8.157900 on epoch=39
03/19/2022 11:27:46 - INFO - __main__ - Step 90 Global step 90 Train loss 8.378793 on epoch=44
03/19/2022 11:27:53 - INFO - __main__ - Step 100 Global step 100 Train loss 8.297653 on epoch=49
03/19/2022 11:27:53 - INFO - __main__ - Global step 100 Train loss 8.528466 Classification-F1 0.13333333333333336 on epoch=49
03/19/2022 11:28:00 - INFO - __main__ - Step 110 Global step 110 Train loss 8.091539 on epoch=54
03/19/2022 11:28:06 - INFO - __main__ - Step 120 Global step 120 Train loss 8.004492 on epoch=59
03/19/2022 11:28:12 - INFO - __main__ - Step 130 Global step 130 Train loss 7.283998 on epoch=64
03/19/2022 11:28:19 - INFO - __main__ - Step 140 Global step 140 Train loss 7.436607 on epoch=69
03/19/2022 11:28:25 - INFO - __main__ - Step 150 Global step 150 Train loss 6.864960 on epoch=74
03/19/2022 11:28:26 - INFO - __main__ - Global step 150 Train loss 7.536320 Classification-F1 0.17045454545454544 on epoch=74
03/19/2022 11:28:32 - INFO - __main__ - Step 160 Global step 160 Train loss 6.687453 on epoch=79
03/19/2022 11:28:38 - INFO - __main__ - Step 170 Global step 170 Train loss 6.487521 on epoch=84
03/19/2022 11:28:44 - INFO - __main__ - Step 180 Global step 180 Train loss 6.615879 on epoch=89
03/19/2022 11:28:51 - INFO - __main__ - Step 190 Global step 190 Train loss 6.314492 on epoch=94
03/19/2022 11:28:57 - INFO - __main__ - Step 200 Global step 200 Train loss 6.253614 on epoch=99
03/19/2022 11:28:58 - INFO - __main__ - Global step 200 Train loss 6.471792 Classification-F1 0.24242424242424243 on epoch=99
03/19/2022 11:29:04 - INFO - __main__ - Step 210 Global step 210 Train loss 5.755997 on epoch=104
03/19/2022 11:29:10 - INFO - __main__ - Step 220 Global step 220 Train loss 5.148787 on epoch=109
03/19/2022 11:29:17 - INFO - __main__ - Step 230 Global step 230 Train loss 5.285888 on epoch=114
03/19/2022 11:29:23 - INFO - __main__ - Step 240 Global step 240 Train loss 4.357442 on epoch=119
03/19/2022 11:29:29 - INFO - __main__ - Step 250 Global step 250 Train loss 3.811822 on epoch=124
03/19/2022 11:29:42 - INFO - __main__ - Global step 250 Train loss 4.871987 Classification-F1 0.11224489795918367 on epoch=124
03/19/2022 11:29:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.746199 on epoch=129
03/19/2022 11:29:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.390956 on epoch=134
03/19/2022 11:30:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.345102 on epoch=139
03/19/2022 11:30:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.325372 on epoch=144
03/19/2022 11:30:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.201707 on epoch=149
03/19/2022 11:30:13 - INFO - __main__ - Global step 300 Train loss 0.401867 Classification-F1 0.3191489361702127 on epoch=149
03/19/2022 11:30:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.176111 on epoch=154
03/19/2022 11:30:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.121467 on epoch=159
03/19/2022 11:30:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.112480 on epoch=164
03/19/2022 11:30:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.067069 on epoch=169
03/19/2022 11:30:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.046570 on epoch=174
03/19/2022 11:30:45 - INFO - __main__ - Global step 350 Train loss 0.104740 Classification-F1 0.5901477832512315 on epoch=174
03/19/2022 11:30:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.060070 on epoch=179
03/19/2022 11:30:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.036940 on epoch=184
03/19/2022 11:31:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034319 on epoch=189
03/19/2022 11:31:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.014961 on epoch=194
03/19/2022 11:31:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.121254 on epoch=199
03/19/2022 11:31:17 - INFO - __main__ - Global step 400 Train loss 0.053509 Classification-F1 0.4817813765182186 on epoch=199
03/19/2022 11:31:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014360 on epoch=204
03/19/2022 11:31:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.016199 on epoch=209
03/19/2022 11:31:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.006295 on epoch=214
03/19/2022 11:31:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006136 on epoch=219
03/19/2022 11:31:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009970 on epoch=224
03/19/2022 11:31:48 - INFO - __main__ - Global step 450 Train loss 0.010592 Classification-F1 0.5270935960591133 on epoch=224
03/19/2022 11:31:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003959 on epoch=229
03/19/2022 11:32:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002397 on epoch=234
03/19/2022 11:32:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.015294 on epoch=239
03/19/2022 11:32:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003052 on epoch=244
03/19/2022 11:32:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001892 on epoch=249
03/19/2022 11:32:19 - INFO - __main__ - Global step 500 Train loss 0.005319 Classification-F1 0.4920634920634921 on epoch=249
03/19/2022 11:32:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002577 on epoch=254
03/19/2022 11:32:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002991 on epoch=259
03/19/2022 11:32:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008182 on epoch=264
03/19/2022 11:32:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001574 on epoch=269
03/19/2022 11:32:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001511 on epoch=274
03/19/2022 11:32:50 - INFO - __main__ - Global step 550 Train loss 0.003367 Classification-F1 0.5195195195195195 on epoch=274
03/19/2022 11:32:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001691 on epoch=279
03/19/2022 11:33:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001917 on epoch=284
03/19/2022 11:33:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001102 on epoch=289
03/19/2022 11:33:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004278 on epoch=294
03/19/2022 11:33:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003262 on epoch=299
03/19/2022 11:33:21 - INFO - __main__ - Global step 600 Train loss 0.002450 Classification-F1 0.4920634920634921 on epoch=299
03/19/2022 11:33:21 - INFO - __main__ - save last model!
03/19/2022 11:33:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:33:22 - INFO - __main__ - Printing 3 examples
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 11:33:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:33:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:33:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:33:22 - INFO - __main__ - Printing 3 examples
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 11:33:22 - INFO - __main__ - ['refuted']
03/19/2022 11:33:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:33:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:33:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:33:28 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 11:33:29 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 11:33:29 - INFO - __main__ - Printing 3 examples
03/19/2022 11:33:29 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:33:29 - INFO - __main__ - ['entailed']
03/19/2022 11:33:29 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:33:29 - INFO - __main__ - ['entailed']
03/19/2022 11:33:29 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:33:29 - INFO - __main__ - ['entailed']
03/19/2022 11:33:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:33:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:33:35 - INFO - __main__ - Starting training!
03/19/2022 11:33:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:34:05 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 11:40:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_100_0.0001_8_predictions.txt
03/19/2022 11:40:22 - INFO - __main__ - Classification-F1 on test data: 0.4942
03/19/2022 11:40:22 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.0001, bsz=8, dev_performance=0.5901477832512315, test_performance=0.49423556084038145
03/19/2022 11:40:22 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.0005, bsz=8 ...
03/19/2022 11:40:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:40:23 - INFO - __main__ - Printing 3 examples
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:40:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:40:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:40:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:40:23 - INFO - __main__ - Printing 3 examples
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 11:40:23 - INFO - __main__ - ['refuted']
03/19/2022 11:40:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:40:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:40:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:40:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:40:36 - INFO - __main__ - Starting training!
03/19/2022 11:40:41 - INFO - __main__ - Step 10 Global step 10 Train loss 19.523247 on epoch=4
03/19/2022 11:40:47 - INFO - __main__ - Step 20 Global step 20 Train loss 14.448599 on epoch=9
03/19/2022 11:40:53 - INFO - __main__ - Step 30 Global step 30 Train loss 9.628779 on epoch=14
03/19/2022 11:40:59 - INFO - __main__ - Step 40 Global step 40 Train loss 8.200812 on epoch=19
03/19/2022 11:41:05 - INFO - __main__ - Step 50 Global step 50 Train loss 6.892316 on epoch=24
03/19/2022 11:41:06 - INFO - __main__ - Global step 50 Train loss 11.738750 Classification-F1 0.06818181818181818 on epoch=24
03/19/2022 11:41:13 - INFO - __main__ - Step 60 Global step 60 Train loss 5.956716 on epoch=29
03/19/2022 11:41:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.867719 on epoch=34
03/19/2022 11:41:25 - INFO - __main__ - Step 80 Global step 80 Train loss 3.249312 on epoch=39
03/19/2022 11:41:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.693790 on epoch=44
03/19/2022 11:41:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.228695 on epoch=49
03/19/2022 11:41:38 - INFO - __main__ - Global step 100 Train loss 2.999246 Classification-F1 0.4980392156862745 on epoch=49
03/19/2022 11:41:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.229003 on epoch=54
03/19/2022 11:41:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.116910 on epoch=59
03/19/2022 11:41:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.102504 on epoch=64
03/19/2022 11:42:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.029083 on epoch=69
03/19/2022 11:42:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.013080 on epoch=74
03/19/2022 11:42:10 - INFO - __main__ - Global step 150 Train loss 0.098116 Classification-F1 0.4920634920634921 on epoch=74
03/19/2022 11:42:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005635 on epoch=79
03/19/2022 11:42:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.007033 on epoch=84
03/19/2022 11:42:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.005161 on epoch=89
03/19/2022 11:42:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001379 on epoch=94
03/19/2022 11:42:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001909 on epoch=99
03/19/2022 11:42:41 - INFO - __main__ - Global step 200 Train loss 0.004223 Classification-F1 0.5333333333333333 on epoch=99
03/19/2022 11:42:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000897 on epoch=104
03/19/2022 11:42:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000888 on epoch=109
03/19/2022 11:43:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000696 on epoch=114
03/19/2022 11:43:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.458090 on epoch=119
03/19/2022 11:43:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.006836 on epoch=124
03/19/2022 11:43:13 - INFO - __main__ - Global step 250 Train loss 0.093481 Classification-F1 0.5195195195195195 on epoch=124
03/19/2022 11:43:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001954 on epoch=129
03/19/2022 11:43:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001305 on epoch=134
03/19/2022 11:43:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001823 on epoch=139
03/19/2022 11:43:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001513 on epoch=144
03/19/2022 11:43:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.008725 on epoch=149
03/19/2022 11:43:44 - INFO - __main__ - Global step 300 Train loss 0.003064 Classification-F1 0.4920634920634921 on epoch=149
03/19/2022 11:43:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002156 on epoch=154
03/19/2022 11:43:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000350 on epoch=159
03/19/2022 11:44:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002514 on epoch=164
03/19/2022 11:44:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000371 on epoch=169
03/19/2022 11:44:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000351 on epoch=174
03/19/2022 11:44:16 - INFO - __main__ - Global step 350 Train loss 0.001148 Classification-F1 0.4817813765182186 on epoch=174
03/19/2022 11:44:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000374 on epoch=179
03/19/2022 11:44:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000205 on epoch=184
03/19/2022 11:44:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000338 on epoch=189
03/19/2022 11:44:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000322 on epoch=194
03/19/2022 11:44:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000405 on epoch=199
03/19/2022 11:44:47 - INFO - __main__ - Global step 400 Train loss 0.000329 Classification-F1 0.4909862142099682 on epoch=199
03/19/2022 11:44:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000161 on epoch=204
03/19/2022 11:44:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000064 on epoch=209
03/19/2022 11:45:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000138 on epoch=214
03/19/2022 11:45:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000114 on epoch=219
03/19/2022 11:45:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000127 on epoch=224
03/19/2022 11:45:18 - INFO - __main__ - Global step 450 Train loss 0.000121 Classification-F1 0.4817813765182186 on epoch=224
03/19/2022 11:45:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000308 on epoch=229
03/19/2022 11:45:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000197 on epoch=234
03/19/2022 11:45:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000203 on epoch=239
03/19/2022 11:45:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000168 on epoch=244
03/19/2022 11:45:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000140 on epoch=249
03/19/2022 11:45:50 - INFO - __main__ - Global step 500 Train loss 0.000203 Classification-F1 0.5076923076923077 on epoch=249
03/19/2022 11:45:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000156 on epoch=254
03/19/2022 11:46:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000319 on epoch=259
03/19/2022 11:46:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000107 on epoch=264
03/19/2022 11:46:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000145 on epoch=269
03/19/2022 11:46:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000104 on epoch=274
03/19/2022 11:46:21 - INFO - __main__ - Global step 550 Train loss 0.000166 Classification-F1 0.5076923076923077 on epoch=274
03/19/2022 11:46:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000132 on epoch=279
03/19/2022 11:46:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000141 on epoch=284
03/19/2022 11:46:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000129 on epoch=289
03/19/2022 11:46:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000182 on epoch=294
03/19/2022 11:46:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.364529 on epoch=299
03/19/2022 11:46:53 - INFO - __main__ - Global step 600 Train loss 0.073023 Classification-F1 0.3333333333333333 on epoch=299
03/19/2022 11:46:53 - INFO - __main__ - save last model!
03/19/2022 11:46:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:46:53 - INFO - __main__ - Printing 3 examples
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 11:46:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:46:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:46:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:46:53 - INFO - __main__ - Printing 3 examples
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 11:46:53 - INFO - __main__ - ['refuted']
03/19/2022 11:46:53 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:46:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:46:53 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:47:00 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 11:47:01 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 11:47:01 - INFO - __main__ - Printing 3 examples
03/19/2022 11:47:01 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:47:01 - INFO - __main__ - ['entailed']
03/19/2022 11:47:01 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:47:01 - INFO - __main__ - ['entailed']
03/19/2022 11:47:01 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 11:47:01 - INFO - __main__ - ['entailed']
03/19/2022 11:47:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:47:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:47:04 - INFO - __main__ - Starting training!
03/19/2022 11:47:25 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:47:38 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 11:53:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_13_0.0005_8_predictions.txt
03/19/2022 11:53:56 - INFO - __main__ - Classification-F1 on test data: 0.4821
03/19/2022 11:53:56 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.0005, bsz=8, dev_performance=0.5333333333333333, test_performance=0.4820979119467972
03/19/2022 11:53:56 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.0003, bsz=8 ...
03/19/2022 11:53:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:53:57 - INFO - __main__ - Printing 3 examples
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 11:53:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:53:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 11:53:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 11:53:57 - INFO - __main__ - Printing 3 examples
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 11:53:57 - INFO - __main__ - ['refuted']
03/19/2022 11:53:57 - INFO - __main__ - Tokenizing Input ...
03/19/2022 11:53:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 11:53:57 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 11:54:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 11:54:08 - INFO - __main__ - Starting training!
03/19/2022 11:54:13 - INFO - __main__ - Step 10 Global step 10 Train loss 19.982115 on epoch=4
03/19/2022 11:54:20 - INFO - __main__ - Step 20 Global step 20 Train loss 15.063011 on epoch=9
03/19/2022 11:54:26 - INFO - __main__ - Step 30 Global step 30 Train loss 9.812912 on epoch=14
03/19/2022 11:54:32 - INFO - __main__ - Step 40 Global step 40 Train loss 8.961974 on epoch=19
03/19/2022 11:54:38 - INFO - __main__ - Step 50 Global step 50 Train loss 7.975126 on epoch=24
03/19/2022 11:54:39 - INFO - __main__ - Global step 50 Train loss 12.359027 Classification-F1 0.16666666666666669 on epoch=24
03/19/2022 11:54:46 - INFO - __main__ - Step 60 Global step 60 Train loss 7.463722 on epoch=29
03/19/2022 11:54:52 - INFO - __main__ - Step 70 Global step 70 Train loss 6.831075 on epoch=34
03/19/2022 11:54:59 - INFO - __main__ - Step 80 Global step 80 Train loss 6.009775 on epoch=39
03/19/2022 11:55:05 - INFO - __main__ - Step 90 Global step 90 Train loss 5.272068 on epoch=44
03/19/2022 11:55:11 - INFO - __main__ - Step 100 Global step 100 Train loss 4.151920 on epoch=49
03/19/2022 11:55:12 - INFO - __main__ - Global step 100 Train loss 5.945712 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 11:55:19 - INFO - __main__ - Step 110 Global step 110 Train loss 3.550806 on epoch=54
03/19/2022 11:55:25 - INFO - __main__ - Step 120 Global step 120 Train loss 2.138898 on epoch=59
03/19/2022 11:55:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.979158 on epoch=64
03/19/2022 11:55:38 - INFO - __main__ - Step 140 Global step 140 Train loss 1.740138 on epoch=69
03/19/2022 11:55:44 - INFO - __main__ - Step 150 Global step 150 Train loss 2.349062 on epoch=74
03/19/2022 11:55:45 - INFO - __main__ - Global step 150 Train loss 2.351613 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 11:55:51 - INFO - __main__ - Step 160 Global step 160 Train loss 1.312342 on epoch=79
03/19/2022 11:55:57 - INFO - __main__ - Step 170 Global step 170 Train loss 1.554428 on epoch=84
03/19/2022 11:56:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.465801 on epoch=89
03/19/2022 11:56:10 - INFO - __main__ - Step 190 Global step 190 Train loss 1.461778 on epoch=94
03/19/2022 11:56:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.273323 on epoch=99
03/19/2022 11:56:17 - INFO - __main__ - Global step 200 Train loss 1.413534 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 11:56:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.606989 on epoch=104
03/19/2022 11:56:29 - INFO - __main__ - Step 220 Global step 220 Train loss 1.303684 on epoch=109
03/19/2022 11:56:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.356255 on epoch=114
03/19/2022 11:56:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.206808 on epoch=119
03/19/2022 11:56:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.924239 on epoch=124
03/19/2022 11:56:49 - INFO - __main__ - Global step 250 Train loss 1.279595 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 11:56:55 - INFO - __main__ - Step 260 Global step 260 Train loss 1.194073 on epoch=129
03/19/2022 11:57:01 - INFO - __main__ - Step 270 Global step 270 Train loss 1.021638 on epoch=134
03/19/2022 11:57:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.048375 on epoch=139
03/19/2022 11:57:13 - INFO - __main__ - Step 290 Global step 290 Train loss 1.255802 on epoch=144
03/19/2022 11:57:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.916153 on epoch=149
03/19/2022 11:57:20 - INFO - __main__ - Global step 300 Train loss 1.087208 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 11:57:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.687252 on epoch=154
03/19/2022 11:57:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.812039 on epoch=159
03/19/2022 11:57:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.923131 on epoch=164
03/19/2022 11:57:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.697373 on epoch=169
03/19/2022 11:57:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.650396 on epoch=174
03/19/2022 11:57:52 - INFO - __main__ - Global step 350 Train loss 0.754038 Classification-F1 0.3992490613266583 on epoch=174
03/19/2022 11:57:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.714248 on epoch=179
03/19/2022 11:58:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.685287 on epoch=184
03/19/2022 11:58:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.623750 on epoch=189
03/19/2022 11:58:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.402952 on epoch=194
03/19/2022 11:58:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.325643 on epoch=199
03/19/2022 11:58:25 - INFO - __main__ - Global step 400 Train loss 0.550376 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 11:58:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.544099 on epoch=204
03/19/2022 11:58:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.263519 on epoch=209
03/19/2022 11:58:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.231072 on epoch=214
03/19/2022 11:58:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.203762 on epoch=219
03/19/2022 11:58:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.136223 on epoch=224
03/19/2022 11:58:56 - INFO - __main__ - Global step 450 Train loss 0.275735 Classification-F1 0.39756367663344405 on epoch=224
03/19/2022 11:59:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.194401 on epoch=229
03/19/2022 11:59:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.096761 on epoch=234
03/19/2022 11:59:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.099712 on epoch=239
03/19/2022 11:59:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.067597 on epoch=244
03/19/2022 11:59:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.029494 on epoch=249
03/19/2022 11:59:28 - INFO - __main__ - Global step 500 Train loss 0.097593 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 11:59:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.065063 on epoch=254
03/19/2022 11:59:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.030786 on epoch=259
03/19/2022 11:59:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003087 on epoch=264
03/19/2022 11:59:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003559 on epoch=269
03/19/2022 11:59:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001029 on epoch=274
03/19/2022 11:59:59 - INFO - __main__ - Global step 550 Train loss 0.020705 Classification-F1 0.6190476190476191 on epoch=274
03/19/2022 12:00:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001280 on epoch=279
03/19/2022 12:00:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000938 on epoch=284
03/19/2022 12:00:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000635 on epoch=289
03/19/2022 12:00:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000648 on epoch=294
03/19/2022 12:00:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000810 on epoch=299
03/19/2022 12:00:32 - INFO - __main__ - Global step 600 Train loss 0.000862 Classification-F1 0.6190476190476191 on epoch=299
03/19/2022 12:00:32 - INFO - __main__ - save last model!
03/19/2022 12:00:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:00:32 - INFO - __main__ - Printing 3 examples
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 12:00:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:00:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:00:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:00:32 - INFO - __main__ - Printing 3 examples
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 12:00:32 - INFO - __main__ - ['refuted']
03/19/2022 12:00:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:00:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:00:32 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:00:39 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 12:00:40 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 12:00:40 - INFO - __main__ - Printing 3 examples
03/19/2022 12:00:40 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:00:40 - INFO - __main__ - ['entailed']
03/19/2022 12:00:40 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:00:40 - INFO - __main__ - ['entailed']
03/19/2022 12:00:40 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:00:40 - INFO - __main__ - ['entailed']
03/19/2022 12:00:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:00:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:00:45 - INFO - __main__ - Starting training!
03/19/2022 12:01:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:01:16 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 12:07:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_13_0.0003_8_predictions.txt
03/19/2022 12:07:49 - INFO - __main__ - Classification-F1 on test data: 0.2489
03/19/2022 12:07:49 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.0003, bsz=8, dev_performance=0.6190476190476191, test_performance=0.248861534335563
03/19/2022 12:07:49 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.0002, bsz=8 ...
03/19/2022 12:07:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:07:50 - INFO - __main__ - Printing 3 examples
03/19/2022 12:07:50 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 12:07:50 - INFO - __main__ - ['refuted']
03/19/2022 12:07:50 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 12:07:50 - INFO - __main__ - ['refuted']
03/19/2022 12:07:50 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 12:07:50 - INFO - __main__ - ['refuted']
03/19/2022 12:07:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:07:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:07:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:07:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:07:51 - INFO - __main__ - Printing 3 examples
03/19/2022 12:07:51 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 12:07:51 - INFO - __main__ - ['refuted']
03/19/2022 12:07:51 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 12:07:51 - INFO - __main__ - ['refuted']
03/19/2022 12:07:51 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 12:07:51 - INFO - __main__ - ['refuted']
03/19/2022 12:07:51 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:07:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:07:51 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:08:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:08:03 - INFO - __main__ - Starting training!
03/19/2022 12:08:09 - INFO - __main__ - Step 10 Global step 10 Train loss 20.621479 on epoch=4
03/19/2022 12:08:15 - INFO - __main__ - Step 20 Global step 20 Train loss 17.369991 on epoch=9
03/19/2022 12:08:21 - INFO - __main__ - Step 30 Global step 30 Train loss 11.459219 on epoch=14
03/19/2022 12:08:27 - INFO - __main__ - Step 40 Global step 40 Train loss 9.620743 on epoch=19
03/19/2022 12:08:33 - INFO - __main__ - Step 50 Global step 50 Train loss 8.739548 on epoch=24
03/19/2022 12:08:34 - INFO - __main__ - Global step 50 Train loss 13.562196 Classification-F1 0.10322580645161292 on epoch=24
03/19/2022 12:08:41 - INFO - __main__ - Step 60 Global step 60 Train loss 8.281491 on epoch=29
03/19/2022 12:08:47 - INFO - __main__ - Step 70 Global step 70 Train loss 8.047088 on epoch=34
03/19/2022 12:08:53 - INFO - __main__ - Step 80 Global step 80 Train loss 7.109332 on epoch=39
03/19/2022 12:08:59 - INFO - __main__ - Step 90 Global step 90 Train loss 7.008375 on epoch=44
03/19/2022 12:09:05 - INFO - __main__ - Step 100 Global step 100 Train loss 7.011329 on epoch=49
03/19/2022 12:09:06 - INFO - __main__ - Global step 100 Train loss 7.491523 Classification-F1 0.05454545454545454 on epoch=49
03/19/2022 12:09:12 - INFO - __main__ - Step 110 Global step 110 Train loss 5.930712 on epoch=54
03/19/2022 12:09:18 - INFO - __main__ - Step 120 Global step 120 Train loss 5.746733 on epoch=59
03/19/2022 12:09:24 - INFO - __main__ - Step 130 Global step 130 Train loss 5.328552 on epoch=64
03/19/2022 12:09:30 - INFO - __main__ - Step 140 Global step 140 Train loss 4.066175 on epoch=69
03/19/2022 12:09:37 - INFO - __main__ - Step 150 Global step 150 Train loss 3.998437 on epoch=74
03/19/2022 12:09:37 - INFO - __main__ - Global step 150 Train loss 5.014122 Classification-F1 0.0 on epoch=74
03/19/2022 12:09:44 - INFO - __main__ - Step 160 Global step 160 Train loss 3.105975 on epoch=79
03/19/2022 12:09:50 - INFO - __main__ - Step 170 Global step 170 Train loss 2.446343 on epoch=84
03/19/2022 12:09:56 - INFO - __main__ - Step 180 Global step 180 Train loss 1.992894 on epoch=89
03/19/2022 12:10:02 - INFO - __main__ - Step 190 Global step 190 Train loss 2.352474 on epoch=94
03/19/2022 12:10:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.502992 on epoch=99
03/19/2022 12:10:09 - INFO - __main__ - Global step 200 Train loss 2.080136 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 12:10:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.182544 on epoch=104
03/19/2022 12:10:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.135956 on epoch=109
03/19/2022 12:10:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.083596 on epoch=114
03/19/2022 12:10:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.055513 on epoch=119
03/19/2022 12:10:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.015293 on epoch=124
03/19/2022 12:10:41 - INFO - __main__ - Global step 250 Train loss 0.094580 Classification-F1 0.5270935960591133 on epoch=124
03/19/2022 12:10:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.007424 on epoch=129
03/19/2022 12:10:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.012400 on epoch=134
03/19/2022 12:11:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.006860 on epoch=139
03/19/2022 12:11:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.005094 on epoch=144
03/19/2022 12:11:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002553 on epoch=149
03/19/2022 12:11:13 - INFO - __main__ - Global step 300 Train loss 0.006866 Classification-F1 0.4909862142099682 on epoch=149
03/19/2022 12:11:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.016743 on epoch=154
03/19/2022 12:11:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003655 on epoch=159
03/19/2022 12:11:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.008387 on epoch=164
03/19/2022 12:11:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001777 on epoch=169
03/19/2022 12:11:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001632 on epoch=174
03/19/2022 12:11:44 - INFO - __main__ - Global step 350 Train loss 0.006439 Classification-F1 0.6000000000000001 on epoch=174
03/19/2022 12:11:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001821 on epoch=179
03/19/2022 12:11:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001016 on epoch=184
03/19/2022 12:12:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002371 on epoch=189
03/19/2022 12:12:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000695 on epoch=194
03/19/2022 12:12:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000395 on epoch=199
03/19/2022 12:12:16 - INFO - __main__ - Global step 400 Train loss 0.001260 Classification-F1 0.5588547189819725 on epoch=199
03/19/2022 12:12:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001567 on epoch=204
03/19/2022 12:12:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000528 on epoch=209
03/19/2022 12:12:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000335 on epoch=214
03/19/2022 12:12:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001554 on epoch=219
03/19/2022 12:12:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001705 on epoch=224
03/19/2022 12:12:48 - INFO - __main__ - Global step 450 Train loss 0.001138 Classification-F1 0.5588547189819725 on epoch=224
03/19/2022 12:12:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000595 on epoch=229
03/19/2022 12:13:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001505 on epoch=234
03/19/2022 12:13:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000205 on epoch=239
03/19/2022 12:13:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000205 on epoch=244
03/19/2022 12:13:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000212 on epoch=249
03/19/2022 12:13:19 - INFO - __main__ - Global step 500 Train loss 0.000545 Classification-F1 0.5588547189819725 on epoch=249
03/19/2022 12:13:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000321 on epoch=254
03/19/2022 12:13:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000102 on epoch=259
03/19/2022 12:13:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000793 on epoch=264
03/19/2022 12:13:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000176 on epoch=269
03/19/2022 12:13:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000233 on epoch=274
03/19/2022 12:13:50 - INFO - __main__ - Global step 550 Train loss 0.000325 Classification-F1 0.6113360323886641 on epoch=274
03/19/2022 12:13:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000111 on epoch=279
03/19/2022 12:14:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000043 on epoch=284
03/19/2022 12:14:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000157 on epoch=289
03/19/2022 12:14:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000405 on epoch=294
03/19/2022 12:14:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000198 on epoch=299
03/19/2022 12:14:22 - INFO - __main__ - Global step 600 Train loss 0.000183 Classification-F1 0.6113360323886641 on epoch=299
03/19/2022 12:14:22 - INFO - __main__ - save last model!
03/19/2022 12:14:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:14:22 - INFO - __main__ - Printing 3 examples
03/19/2022 12:14:22 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 12:14:22 - INFO - __main__ - ['refuted']
03/19/2022 12:14:22 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 12:14:22 - INFO - __main__ - ['refuted']
03/19/2022 12:14:22 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 12:14:22 - INFO - __main__ - ['refuted']
03/19/2022 12:14:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 12:14:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:14:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:14:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:14:23 - INFO - __main__ - Printing 3 examples
03/19/2022 12:14:23 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 12:14:23 - INFO - __main__ - ['refuted']
03/19/2022 12:14:23 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 12:14:23 - INFO - __main__ - ['refuted']
03/19/2022 12:14:23 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 12:14:23 - INFO - __main__ - ['refuted']
03/19/2022 12:14:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:14:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:14:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:14:29 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 12:14:30 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 12:14:30 - INFO - __main__ - Printing 3 examples
03/19/2022 12:14:30 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:14:30 - INFO - __main__ - ['entailed']
03/19/2022 12:14:30 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:14:30 - INFO - __main__ - ['entailed']
03/19/2022 12:14:30 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:14:30 - INFO - __main__ - ['entailed']
03/19/2022 12:14:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:14:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:14:34 - INFO - __main__ - Starting training!
03/19/2022 12:14:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:15:06 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 12:21:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_13_0.0002_8_predictions.txt
03/19/2022 12:21:16 - INFO - __main__ - Classification-F1 on test data: 0.4856
03/19/2022 12:21:16 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.0002, bsz=8, dev_performance=0.6113360323886641, test_performance=0.4856051610083671
03/19/2022 12:21:16 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.0001, bsz=8 ...
03/19/2022 12:21:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:21:17 - INFO - __main__ - Printing 3 examples
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:21:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:21:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:21:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:21:17 - INFO - __main__ - Printing 3 examples
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/19/2022 12:21:17 - INFO - __main__ - ['refuted']
03/19/2022 12:21:17 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:21:17 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:21:17 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:21:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:21:28 - INFO - __main__ - Starting training!
03/19/2022 12:21:33 - INFO - __main__ - Step 10 Global step 10 Train loss 20.895670 on epoch=4
03/19/2022 12:21:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.105274 on epoch=9
03/19/2022 12:21:45 - INFO - __main__ - Step 30 Global step 30 Train loss 13.272586 on epoch=14
03/19/2022 12:21:51 - INFO - __main__ - Step 40 Global step 40 Train loss 11.470700 on epoch=19
03/19/2022 12:21:57 - INFO - __main__ - Step 50 Global step 50 Train loss 10.214282 on epoch=24
03/19/2022 12:22:01 - INFO - __main__ - Global step 50 Train loss 14.791703 Classification-F1 0.02564102564102564 on epoch=24
03/19/2022 12:22:08 - INFO - __main__ - Step 60 Global step 60 Train loss 9.543732 on epoch=29
03/19/2022 12:22:14 - INFO - __main__ - Step 70 Global step 70 Train loss 8.861859 on epoch=34
03/19/2022 12:22:20 - INFO - __main__ - Step 80 Global step 80 Train loss 9.114820 on epoch=39
03/19/2022 12:22:26 - INFO - __main__ - Step 90 Global step 90 Train loss 7.887377 on epoch=44
03/19/2022 12:22:32 - INFO - __main__ - Step 100 Global step 100 Train loss 8.363313 on epoch=49
03/19/2022 12:22:34 - INFO - __main__ - Global step 100 Train loss 8.754221 Classification-F1 0.045454545454545456 on epoch=49
03/19/2022 12:22:41 - INFO - __main__ - Step 110 Global step 110 Train loss 7.916979 on epoch=54
03/19/2022 12:22:47 - INFO - __main__ - Step 120 Global step 120 Train loss 7.988322 on epoch=59
03/19/2022 12:22:53 - INFO - __main__ - Step 130 Global step 130 Train loss 7.539696 on epoch=64
03/19/2022 12:22:59 - INFO - __main__ - Step 140 Global step 140 Train loss 7.291156 on epoch=69
03/19/2022 12:23:05 - INFO - __main__ - Step 150 Global step 150 Train loss 7.161693 on epoch=74
03/19/2022 12:23:06 - INFO - __main__ - Global step 150 Train loss 7.579568 Classification-F1 0.11538461538461538 on epoch=74
03/19/2022 12:23:13 - INFO - __main__ - Step 160 Global step 160 Train loss 6.741590 on epoch=79
03/19/2022 12:23:19 - INFO - __main__ - Step 170 Global step 170 Train loss 7.073640 on epoch=84
03/19/2022 12:23:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.613016 on epoch=89
03/19/2022 12:23:31 - INFO - __main__ - Step 190 Global step 190 Train loss 5.975205 on epoch=94
03/19/2022 12:23:37 - INFO - __main__ - Step 200 Global step 200 Train loss 6.744082 on epoch=99
03/19/2022 12:23:38 - INFO - __main__ - Global step 200 Train loss 6.629507 Classification-F1 0.09333333333333334 on epoch=99
03/19/2022 12:23:44 - INFO - __main__ - Step 210 Global step 210 Train loss 5.915871 on epoch=104
03/19/2022 12:23:50 - INFO - __main__ - Step 220 Global step 220 Train loss 5.881471 on epoch=109
03/19/2022 12:23:56 - INFO - __main__ - Step 230 Global step 230 Train loss 5.604241 on epoch=114
03/19/2022 12:24:02 - INFO - __main__ - Step 240 Global step 240 Train loss 5.193666 on epoch=119
03/19/2022 12:24:08 - INFO - __main__ - Step 250 Global step 250 Train loss 4.630229 on epoch=124
03/19/2022 12:24:09 - INFO - __main__ - Global step 250 Train loss 5.445095 Classification-F1 0.13333333333333333 on epoch=124
03/19/2022 12:24:16 - INFO - __main__ - Step 260 Global step 260 Train loss 4.381995 on epoch=129
03/19/2022 12:24:22 - INFO - __main__ - Step 270 Global step 270 Train loss 4.276118 on epoch=134
03/19/2022 12:24:28 - INFO - __main__ - Step 280 Global step 280 Train loss 2.922648 on epoch=139
03/19/2022 12:24:34 - INFO - __main__ - Step 290 Global step 290 Train loss 2.753217 on epoch=144
03/19/2022 12:24:40 - INFO - __main__ - Step 300 Global step 300 Train loss 2.942823 on epoch=149
03/19/2022 12:24:41 - INFO - __main__ - Global step 300 Train loss 3.455360 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 12:24:48 - INFO - __main__ - Step 310 Global step 310 Train loss 2.521304 on epoch=154
03/19/2022 12:24:54 - INFO - __main__ - Step 320 Global step 320 Train loss 2.151173 on epoch=159
03/19/2022 12:25:00 - INFO - __main__ - Step 330 Global step 330 Train loss 1.541982 on epoch=164
03/19/2022 12:25:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.950272 on epoch=169
03/19/2022 12:25:12 - INFO - __main__ - Step 350 Global step 350 Train loss 1.366956 on epoch=174
03/19/2022 12:25:13 - INFO - __main__ - Global step 350 Train loss 1.906337 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 12:25:19 - INFO - __main__ - Step 360 Global step 360 Train loss 2.075382 on epoch=179
03/19/2022 12:25:25 - INFO - __main__ - Step 370 Global step 370 Train loss 1.986267 on epoch=184
03/19/2022 12:25:31 - INFO - __main__ - Step 380 Global step 380 Train loss 1.595729 on epoch=189
03/19/2022 12:25:37 - INFO - __main__ - Step 390 Global step 390 Train loss 1.538759 on epoch=194
03/19/2022 12:25:43 - INFO - __main__ - Step 400 Global step 400 Train loss 1.227146 on epoch=199
03/19/2022 12:25:44 - INFO - __main__ - Global step 400 Train loss 1.684657 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 12:25:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.368502 on epoch=204
03/19/2022 12:25:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.560361 on epoch=209
03/19/2022 12:26:02 - INFO - __main__ - Step 430 Global step 430 Train loss 1.277951 on epoch=214
03/19/2022 12:26:08 - INFO - __main__ - Step 440 Global step 440 Train loss 1.400653 on epoch=219
03/19/2022 12:26:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.164727 on epoch=224
03/19/2022 12:26:16 - INFO - __main__ - Global step 450 Train loss 1.354439 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 12:26:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.381000 on epoch=229
03/19/2022 12:26:28 - INFO - __main__ - Step 470 Global step 470 Train loss 1.648286 on epoch=234
03/19/2022 12:26:34 - INFO - __main__ - Step 480 Global step 480 Train loss 1.288904 on epoch=239
03/19/2022 12:26:40 - INFO - __main__ - Step 490 Global step 490 Train loss 1.481530 on epoch=244
03/19/2022 12:26:46 - INFO - __main__ - Step 500 Global step 500 Train loss 1.239258 on epoch=249
03/19/2022 12:26:47 - INFO - __main__ - Global step 500 Train loss 1.407796 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 12:26:53 - INFO - __main__ - Step 510 Global step 510 Train loss 1.511792 on epoch=254
03/19/2022 12:26:59 - INFO - __main__ - Step 520 Global step 520 Train loss 1.257593 on epoch=259
03/19/2022 12:27:05 - INFO - __main__ - Step 530 Global step 530 Train loss 1.086733 on epoch=264
03/19/2022 12:27:11 - INFO - __main__ - Step 540 Global step 540 Train loss 1.850314 on epoch=269
03/19/2022 12:27:17 - INFO - __main__ - Step 550 Global step 550 Train loss 1.369399 on epoch=274
03/19/2022 12:27:18 - INFO - __main__ - Global step 550 Train loss 1.415166 Classification-F1 0.3333333333333333 on epoch=274
03/19/2022 12:27:24 - INFO - __main__ - Step 560 Global step 560 Train loss 1.479031 on epoch=279
03/19/2022 12:27:30 - INFO - __main__ - Step 570 Global step 570 Train loss 1.095325 on epoch=284
03/19/2022 12:27:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.106909 on epoch=289
03/19/2022 12:27:42 - INFO - __main__ - Step 590 Global step 590 Train loss 1.146231 on epoch=294
03/19/2022 12:27:48 - INFO - __main__ - Step 600 Global step 600 Train loss 1.070804 on epoch=299
03/19/2022 12:27:49 - INFO - __main__ - Global step 600 Train loss 1.179660 Classification-F1 0.3333333333333333 on epoch=299
03/19/2022 12:27:49 - INFO - __main__ - save last model!
03/19/2022 12:27:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:27:51 - INFO - __main__ - Printing 3 examples
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 12:27:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:27:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:27:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:27:51 - INFO - __main__ - Printing 3 examples
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 12:27:51 - INFO - __main__ - ['entailed']
03/19/2022 12:27:51 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:27:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:27:51 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:27:56 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 12:27:57 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 12:27:57 - INFO - __main__ - Printing 3 examples
03/19/2022 12:27:57 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:27:57 - INFO - __main__ - ['entailed']
03/19/2022 12:27:57 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:27:57 - INFO - __main__ - ['entailed']
03/19/2022 12:27:57 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:27:57 - INFO - __main__ - ['entailed']
03/19/2022 12:27:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:28:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:28:02 - INFO - __main__ - Starting training!
03/19/2022 12:28:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:28:34 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 12:34:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_13_0.0001_8_predictions.txt
03/19/2022 12:34:35 - INFO - __main__ - Classification-F1 on test data: 0.3362
03/19/2022 12:34:35 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33617021276595743
03/19/2022 12:34:35 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.0005, bsz=8 ...
03/19/2022 12:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:34:36 - INFO - __main__ - Printing 3 examples
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:34:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:34:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:34:36 - INFO - __main__ - Printing 3 examples
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 12:34:36 - INFO - __main__ - ['entailed']
03/19/2022 12:34:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:34:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:34:36 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:34:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:34:47 - INFO - __main__ - Starting training!
03/19/2022 12:34:55 - INFO - __main__ - Step 10 Global step 10 Train loss 20.580193 on epoch=4
03/19/2022 12:35:00 - INFO - __main__ - Step 20 Global step 20 Train loss 14.459173 on epoch=9
03/19/2022 12:35:06 - INFO - __main__ - Step 30 Global step 30 Train loss 11.508804 on epoch=14
03/19/2022 12:35:13 - INFO - __main__ - Step 40 Global step 40 Train loss 7.709014 on epoch=19
03/19/2022 12:35:19 - INFO - __main__ - Step 50 Global step 50 Train loss 7.313709 on epoch=24
03/19/2022 12:35:21 - INFO - __main__ - Global step 50 Train loss 12.314178 Classification-F1 0.16666666666666669 on epoch=24
03/19/2022 12:35:27 - INFO - __main__ - Step 60 Global step 60 Train loss 6.291573 on epoch=29
03/19/2022 12:35:34 - INFO - __main__ - Step 70 Global step 70 Train loss 5.078035 on epoch=34
03/19/2022 12:35:40 - INFO - __main__ - Step 80 Global step 80 Train loss 3.142401 on epoch=39
03/19/2022 12:35:46 - INFO - __main__ - Step 90 Global step 90 Train loss 2.111511 on epoch=44
03/19/2022 12:35:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.737808 on epoch=49
03/19/2022 12:35:53 - INFO - __main__ - Global step 100 Train loss 3.672266 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 12:36:00 - INFO - __main__ - Step 110 Global step 110 Train loss 2.119095 on epoch=54
03/19/2022 12:36:07 - INFO - __main__ - Step 120 Global step 120 Train loss 1.336979 on epoch=59
03/19/2022 12:36:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.293765 on epoch=64
03/19/2022 12:36:19 - INFO - __main__ - Step 140 Global step 140 Train loss 1.197187 on epoch=69
03/19/2022 12:36:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.307135 on epoch=74
03/19/2022 12:36:26 - INFO - __main__ - Global step 150 Train loss 1.450832 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 12:36:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.745431 on epoch=79
03/19/2022 12:36:39 - INFO - __main__ - Step 170 Global step 170 Train loss 1.160555 on epoch=84
03/19/2022 12:36:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.144437 on epoch=89
03/19/2022 12:36:51 - INFO - __main__ - Step 190 Global step 190 Train loss 1.104861 on epoch=94
03/19/2022 12:36:58 - INFO - __main__ - Step 200 Global step 200 Train loss 1.047198 on epoch=99
03/19/2022 12:36:59 - INFO - __main__ - Global step 200 Train loss 1.040496 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 12:37:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.748541 on epoch=104
03/19/2022 12:37:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.826576 on epoch=109
03/19/2022 12:37:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.613789 on epoch=114
03/19/2022 12:37:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.939266 on epoch=119
03/19/2022 12:37:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.606112 on epoch=124
03/19/2022 12:37:31 - INFO - __main__ - Global step 250 Train loss 0.746857 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 12:37:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.602844 on epoch=129
03/19/2022 12:37:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.566941 on epoch=134
03/19/2022 12:37:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.377395 on epoch=139
03/19/2022 12:37:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.415066 on epoch=144
03/19/2022 12:38:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.322151 on epoch=149
03/19/2022 12:38:03 - INFO - __main__ - Global step 300 Train loss 0.456879 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 12:38:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.416831 on epoch=154
03/19/2022 12:38:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.478093 on epoch=159
03/19/2022 12:38:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.265207 on epoch=164
03/19/2022 12:38:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.263744 on epoch=169
03/19/2022 12:38:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.271567 on epoch=174
03/19/2022 12:38:36 - INFO - __main__ - Global step 350 Train loss 0.339089 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 12:38:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.320720 on epoch=179
03/19/2022 12:38:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.291924 on epoch=184
03/19/2022 12:38:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.400820 on epoch=189
03/19/2022 12:39:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.292415 on epoch=194
03/19/2022 12:39:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.292068 on epoch=199
03/19/2022 12:39:08 - INFO - __main__ - Global step 400 Train loss 0.319589 Classification-F1 0.4980392156862745 on epoch=199
03/19/2022 12:39:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.240351 on epoch=204
03/19/2022 12:39:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.215790 on epoch=209
03/19/2022 12:39:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.165218 on epoch=214
03/19/2022 12:39:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.200914 on epoch=219
03/19/2022 12:39:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.193471 on epoch=224
03/19/2022 12:39:41 - INFO - __main__ - Global step 450 Train loss 0.203149 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 12:39:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.135995 on epoch=229
03/19/2022 12:39:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.182616 on epoch=234
03/19/2022 12:40:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.157772 on epoch=239
03/19/2022 12:40:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.093599 on epoch=244
03/19/2022 12:40:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.103514 on epoch=249
03/19/2022 12:40:13 - INFO - __main__ - Global step 500 Train loss 0.134699 Classification-F1 0.6559139784946237 on epoch=249
03/19/2022 12:40:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.770267 on epoch=254
03/19/2022 12:40:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.041112 on epoch=259
03/19/2022 12:40:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.045303 on epoch=264
03/19/2022 12:40:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.012216 on epoch=269
03/19/2022 12:40:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016096 on epoch=274
03/19/2022 12:40:46 - INFO - __main__ - Global step 550 Train loss 0.176999 Classification-F1 0.6389743589743591 on epoch=274
03/19/2022 12:40:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005342 on epoch=279
03/19/2022 12:40:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.119635 on epoch=284
03/19/2022 12:41:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.050828 on epoch=289
03/19/2022 12:41:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014229 on epoch=294
03/19/2022 12:41:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.021703 on epoch=299
03/19/2022 12:41:18 - INFO - __main__ - Global step 600 Train loss 0.042347 Classification-F1 0.4458874458874459 on epoch=299
03/19/2022 12:41:18 - INFO - __main__ - save last model!
03/19/2022 12:41:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:41:19 - INFO - __main__ - Printing 3 examples
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 12:41:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:41:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:41:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:41:19 - INFO - __main__ - Printing 3 examples
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 12:41:19 - INFO - __main__ - ['entailed']
03/19/2022 12:41:19 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:41:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:41:19 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:41:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 12:41:26 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 12:41:26 - INFO - __main__ - Printing 3 examples
03/19/2022 12:41:26 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:41:26 - INFO - __main__ - ['entailed']
03/19/2022 12:41:26 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:41:26 - INFO - __main__ - ['entailed']
03/19/2022 12:41:26 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:41:26 - INFO - __main__ - ['entailed']
03/19/2022 12:41:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:41:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:41:32 - INFO - __main__ - Starting training!
03/19/2022 12:41:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:42:03 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 12:47:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_21_0.0005_8_predictions.txt
03/19/2022 12:47:48 - INFO - __main__ - Classification-F1 on test data: 0.4880
03/19/2022 12:47:49 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.0005, bsz=8, dev_performance=0.6559139784946237, test_performance=0.48795412404810834
03/19/2022 12:47:49 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.0003, bsz=8 ...
03/19/2022 12:47:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:47:50 - INFO - __main__ - Printing 3 examples
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:47:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:47:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:47:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:47:50 - INFO - __main__ - Printing 3 examples
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 12:47:50 - INFO - __main__ - ['entailed']
03/19/2022 12:47:50 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:47:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:47:50 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:48:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:48:01 - INFO - __main__ - Starting training!
03/19/2022 12:48:06 - INFO - __main__ - Step 10 Global step 10 Train loss 20.623119 on epoch=4
03/19/2022 12:48:12 - INFO - __main__ - Step 20 Global step 20 Train loss 14.947571 on epoch=9
03/19/2022 12:48:18 - INFO - __main__ - Step 30 Global step 30 Train loss 9.853830 on epoch=14
03/19/2022 12:48:25 - INFO - __main__ - Step 40 Global step 40 Train loss 8.427679 on epoch=19
03/19/2022 12:48:31 - INFO - __main__ - Step 50 Global step 50 Train loss 7.527611 on epoch=24
03/19/2022 12:48:32 - INFO - __main__ - Global step 50 Train loss 12.275962 Classification-F1 0.029411764705882353 on epoch=24
03/19/2022 12:48:39 - INFO - __main__ - Step 60 Global step 60 Train loss 7.428224 on epoch=29
03/19/2022 12:48:45 - INFO - __main__ - Step 70 Global step 70 Train loss 6.410548 on epoch=34
03/19/2022 12:48:51 - INFO - __main__ - Step 80 Global step 80 Train loss 5.640590 on epoch=39
03/19/2022 12:48:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.777733 on epoch=44
03/19/2022 12:49:03 - INFO - __main__ - Step 100 Global step 100 Train loss 4.575424 on epoch=49
03/19/2022 12:49:04 - INFO - __main__ - Global step 100 Train loss 5.766504 Classification-F1 0.0392156862745098 on epoch=49
03/19/2022 12:49:11 - INFO - __main__ - Step 110 Global step 110 Train loss 2.814927 on epoch=54
03/19/2022 12:49:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.887882 on epoch=59
03/19/2022 12:49:23 - INFO - __main__ - Step 130 Global step 130 Train loss 1.747285 on epoch=64
03/19/2022 12:49:30 - INFO - __main__ - Step 140 Global step 140 Train loss 1.288157 on epoch=69
03/19/2022 12:49:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.810440 on epoch=74
03/19/2022 12:49:37 - INFO - __main__ - Global step 150 Train loss 1.909738 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 12:49:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.797732 on epoch=79
03/19/2022 12:49:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.803425 on epoch=84
03/19/2022 12:49:55 - INFO - __main__ - Step 180 Global step 180 Train loss 2.180289 on epoch=89
03/19/2022 12:50:01 - INFO - __main__ - Step 190 Global step 190 Train loss 1.349081 on epoch=94
03/19/2022 12:50:07 - INFO - __main__ - Step 200 Global step 200 Train loss 1.127196 on epoch=99
03/19/2022 12:50:08 - INFO - __main__ - Global step 200 Train loss 1.651544 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 12:50:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.329311 on epoch=104
03/19/2022 12:50:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.446704 on epoch=109
03/19/2022 12:50:27 - INFO - __main__ - Step 230 Global step 230 Train loss 1.149431 on epoch=114
03/19/2022 12:50:33 - INFO - __main__ - Step 240 Global step 240 Train loss 1.329260 on epoch=119
03/19/2022 12:50:39 - INFO - __main__ - Step 250 Global step 250 Train loss 1.625809 on epoch=124
03/19/2022 12:50:40 - INFO - __main__ - Global step 250 Train loss 1.376103 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 12:50:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.212470 on epoch=129
03/19/2022 12:50:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.289178 on epoch=134
03/19/2022 12:50:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.213300 on epoch=139
03/19/2022 12:51:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.827736 on epoch=144
03/19/2022 12:51:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.879561 on epoch=149
03/19/2022 12:51:12 - INFO - __main__ - Global step 300 Train loss 1.084449 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 12:51:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.751593 on epoch=154
03/19/2022 12:51:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.782443 on epoch=159
03/19/2022 12:51:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.996634 on epoch=164
03/19/2022 12:51:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.838469 on epoch=169
03/19/2022 12:51:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.871841 on epoch=174
03/19/2022 12:51:44 - INFO - __main__ - Global step 350 Train loss 0.848196 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 12:51:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.681471 on epoch=179
03/19/2022 12:51:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.732538 on epoch=184
03/19/2022 12:52:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.761059 on epoch=189
03/19/2022 12:52:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.925010 on epoch=194
03/19/2022 12:52:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.560799 on epoch=199
03/19/2022 12:52:16 - INFO - __main__ - Global step 400 Train loss 0.732175 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 12:52:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.433937 on epoch=204
03/19/2022 12:52:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.588043 on epoch=209
03/19/2022 12:52:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.647498 on epoch=214
03/19/2022 12:52:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.487502 on epoch=219
03/19/2022 12:52:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.421981 on epoch=224
03/19/2022 12:52:48 - INFO - __main__ - Global step 450 Train loss 0.515792 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 12:52:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.434327 on epoch=229
03/19/2022 12:53:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.473840 on epoch=234
03/19/2022 12:53:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.407395 on epoch=239
03/19/2022 12:53:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.408743 on epoch=244
03/19/2022 12:53:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.435979 on epoch=249
03/19/2022 12:53:20 - INFO - __main__ - Global step 500 Train loss 0.432057 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 12:53:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.319726 on epoch=254
03/19/2022 12:53:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.388605 on epoch=259
03/19/2022 12:53:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.311072 on epoch=264
03/19/2022 12:53:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.379171 on epoch=269
03/19/2022 12:53:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.247778 on epoch=274
03/19/2022 12:53:52 - INFO - __main__ - Global step 550 Train loss 0.329270 Classification-F1 0.3333333333333333 on epoch=274
03/19/2022 12:53:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.269759 on epoch=279
03/19/2022 12:54:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.328703 on epoch=284
03/19/2022 12:54:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.157691 on epoch=289
03/19/2022 12:54:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.141656 on epoch=294
03/19/2022 12:54:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.053560 on epoch=299
03/19/2022 12:54:23 - INFO - __main__ - Global step 600 Train loss 0.190274 Classification-F1 0.36374269005847953 on epoch=299
03/19/2022 12:54:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:54:24 - INFO - __main__ - Printing 3 examples
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 12:54:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:54:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 12:54:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 12:54:24 - INFO - __main__ - Printing 3 examples
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 12:54:24 - INFO - __main__ - ['entailed']
03/19/2022 12:54:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 12:54:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:54:24 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 12:54:24 - INFO - __main__ - save last model!
03/19/2022 12:54:31 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 12:54:32 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 12:54:32 - INFO - __main__ - Printing 3 examples
03/19/2022 12:54:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:54:32 - INFO - __main__ - ['entailed']
03/19/2022 12:54:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:54:32 - INFO - __main__ - ['entailed']
03/19/2022 12:54:32 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 12:54:32 - INFO - __main__ - ['entailed']
03/19/2022 12:54:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 12:54:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 12:54:37 - INFO - __main__ - Starting training!
03/19/2022 12:54:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 12:55:08 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 13:01:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_21_0.0003_8_predictions.txt
03/19/2022 13:01:14 - INFO - __main__ - Classification-F1 on test data: 0.4242
03/19/2022 13:01:15 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.0003, bsz=8, dev_performance=0.36374269005847953, test_performance=0.4241542120294617
03/19/2022 13:01:15 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.0002, bsz=8 ...
03/19/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:01:16 - INFO - __main__ - Printing 3 examples
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:01:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:01:16 - INFO - __main__ - Printing 3 examples
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 13:01:16 - INFO - __main__ - ['entailed']
03/19/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:01:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:01:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:01:26 - INFO - __main__ - Starting training!
03/19/2022 13:01:32 - INFO - __main__ - Step 10 Global step 10 Train loss 20.409775 on epoch=4
03/19/2022 13:01:38 - INFO - __main__ - Step 20 Global step 20 Train loss 16.180462 on epoch=9
03/19/2022 13:01:44 - INFO - __main__ - Step 30 Global step 30 Train loss 9.962389 on epoch=14
03/19/2022 13:01:50 - INFO - __main__ - Step 40 Global step 40 Train loss 8.899101 on epoch=19
03/19/2022 13:01:57 - INFO - __main__ - Step 50 Global step 50 Train loss 8.631056 on epoch=24
03/19/2022 13:02:04 - INFO - __main__ - Global step 50 Train loss 12.816556 Classification-F1 0.03968253968253969 on epoch=24
03/19/2022 13:02:11 - INFO - __main__ - Step 60 Global step 60 Train loss 8.061422 on epoch=29
03/19/2022 13:02:17 - INFO - __main__ - Step 70 Global step 70 Train loss 7.352246 on epoch=34
03/19/2022 13:02:23 - INFO - __main__ - Step 80 Global step 80 Train loss 7.209571 on epoch=39
03/19/2022 13:02:29 - INFO - __main__ - Step 90 Global step 90 Train loss 7.125749 on epoch=44
03/19/2022 13:02:35 - INFO - __main__ - Step 100 Global step 100 Train loss 6.518779 on epoch=49
03/19/2022 13:02:37 - INFO - __main__ - Global step 100 Train loss 7.253553 Classification-F1 0.10769230769230768 on epoch=49
03/19/2022 13:02:44 - INFO - __main__ - Step 110 Global step 110 Train loss 5.971811 on epoch=54
03/19/2022 13:02:50 - INFO - __main__ - Step 120 Global step 120 Train loss 5.085959 on epoch=59
03/19/2022 13:02:57 - INFO - __main__ - Step 130 Global step 130 Train loss 4.963400 on epoch=64
03/19/2022 13:03:03 - INFO - __main__ - Step 140 Global step 140 Train loss 4.175961 on epoch=69
03/19/2022 13:03:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.536729 on epoch=74
03/19/2022 13:03:10 - INFO - __main__ - Global step 150 Train loss 4.346772 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 13:03:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.303814 on epoch=79
03/19/2022 13:03:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.269932 on epoch=84
03/19/2022 13:03:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.205702 on epoch=89
03/19/2022 13:03:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.118238 on epoch=94
03/19/2022 13:03:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.063539 on epoch=99
03/19/2022 13:03:43 - INFO - __main__ - Global step 200 Train loss 0.192245 Classification-F1 0.5307917888563051 on epoch=99
03/19/2022 13:03:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.044305 on epoch=104
03/19/2022 13:03:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.030224 on epoch=109
03/19/2022 13:04:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.018022 on epoch=114
03/19/2022 13:04:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.044257 on epoch=119
03/19/2022 13:04:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.025105 on epoch=124
03/19/2022 13:04:16 - INFO - __main__ - Global step 250 Train loss 0.032383 Classification-F1 0.37662337662337664 on epoch=124
03/19/2022 13:04:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.004202 on epoch=129
03/19/2022 13:04:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.008262 on epoch=134
03/19/2022 13:04:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003812 on epoch=139
03/19/2022 13:04:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.012624 on epoch=144
03/19/2022 13:04:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002076 on epoch=149
03/19/2022 13:04:49 - INFO - __main__ - Global step 300 Train loss 0.006195 Classification-F1 0.41700404858299595 on epoch=149
03/19/2022 13:04:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001481 on epoch=154
03/19/2022 13:05:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002839 on epoch=159
03/19/2022 13:05:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.003291 on epoch=164
03/19/2022 13:05:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002496 on epoch=169
03/19/2022 13:05:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001013 on epoch=174
03/19/2022 13:05:21 - INFO - __main__ - Global step 350 Train loss 0.002224 Classification-F1 0.464039408866995 on epoch=174
03/19/2022 13:05:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.008783 on epoch=179
03/19/2022 13:05:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001533 on epoch=184
03/19/2022 13:05:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003025 on epoch=189
03/19/2022 13:05:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001006 on epoch=194
03/19/2022 13:05:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001149 on epoch=199
03/19/2022 13:05:54 - INFO - __main__ - Global step 400 Train loss 0.003099 Classification-F1 0.4285714285714286 on epoch=199
03/19/2022 13:06:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000496 on epoch=204
03/19/2022 13:06:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001075 on epoch=209
03/19/2022 13:06:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000642 on epoch=214
03/19/2022 13:06:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000215 on epoch=219
03/19/2022 13:06:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.014878 on epoch=224
03/19/2022 13:06:27 - INFO - __main__ - Global step 450 Train loss 0.003461 Classification-F1 0.4920634920634921 on epoch=224
03/19/2022 13:06:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000678 on epoch=229
03/19/2022 13:06:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000209 on epoch=234
03/19/2022 13:06:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000399 on epoch=239
03/19/2022 13:06:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000262 on epoch=244
03/19/2022 13:06:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000348 on epoch=249
03/19/2022 13:07:00 - INFO - __main__ - Global step 500 Train loss 0.000379 Classification-F1 0.4920634920634921 on epoch=249
03/19/2022 13:07:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000115 on epoch=254
03/19/2022 13:07:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000351 on epoch=259
03/19/2022 13:07:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000393 on epoch=264
03/19/2022 13:07:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018662 on epoch=269
03/19/2022 13:07:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000371 on epoch=274
03/19/2022 13:07:32 - INFO - __main__ - Global step 550 Train loss 0.003979 Classification-F1 0.4554554554554554 on epoch=274
03/19/2022 13:07:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004865 on epoch=279
03/19/2022 13:07:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000106 on epoch=284
03/19/2022 13:07:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000248 on epoch=289
03/19/2022 13:07:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000110 on epoch=294
03/19/2022 13:08:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000127 on epoch=299
03/19/2022 13:08:05 - INFO - __main__ - Global step 600 Train loss 0.001091 Classification-F1 0.464039408866995 on epoch=299
03/19/2022 13:08:05 - INFO - __main__ - save last model!
03/19/2022 13:08:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:08:05 - INFO - __main__ - Printing 3 examples
03/19/2022 13:08:05 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 13:08:05 - INFO - __main__ - ['entailed']
03/19/2022 13:08:05 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 13:08:05 - INFO - __main__ - ['entailed']
03/19/2022 13:08:05 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 13:08:05 - INFO - __main__ - ['entailed']
03/19/2022 13:08:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 13:08:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:08:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:08:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:08:06 - INFO - __main__ - Printing 3 examples
03/19/2022 13:08:06 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 13:08:06 - INFO - __main__ - ['entailed']
03/19/2022 13:08:06 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 13:08:06 - INFO - __main__ - ['entailed']
03/19/2022 13:08:06 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 13:08:06 - INFO - __main__ - ['entailed']
03/19/2022 13:08:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:08:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:08:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:08:12 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 13:08:13 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 13:08:13 - INFO - __main__ - Printing 3 examples
03/19/2022 13:08:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:08:13 - INFO - __main__ - ['entailed']
03/19/2022 13:08:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:08:13 - INFO - __main__ - ['entailed']
03/19/2022 13:08:13 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:08:13 - INFO - __main__ - ['entailed']
03/19/2022 13:08:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:08:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:08:18 - INFO - __main__ - Starting training!
03/19/2022 13:08:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:08:50 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 13:15:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_21_0.0002_8_predictions.txt
03/19/2022 13:15:12 - INFO - __main__ - Classification-F1 on test data: 0.4999
03/19/2022 13:15:13 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.0002, bsz=8, dev_performance=0.5307917888563051, test_performance=0.49991452823523364
03/19/2022 13:15:13 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.0001, bsz=8 ...
03/19/2022 13:15:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:15:14 - INFO - __main__ - Printing 3 examples
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:15:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:15:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:15:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:15:14 - INFO - __main__ - Printing 3 examples
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/19/2022 13:15:14 - INFO - __main__ - ['entailed']
03/19/2022 13:15:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:15:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:15:14 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:15:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:15:25 - INFO - __main__ - Starting training!
03/19/2022 13:15:30 - INFO - __main__ - Step 10 Global step 10 Train loss 20.816679 on epoch=4
03/19/2022 13:15:36 - INFO - __main__ - Step 20 Global step 20 Train loss 17.159115 on epoch=9
03/19/2022 13:15:42 - INFO - __main__ - Step 30 Global step 30 Train loss 12.305685 on epoch=14
03/19/2022 13:15:49 - INFO - __main__ - Step 40 Global step 40 Train loss 9.929840 on epoch=19
03/19/2022 13:15:55 - INFO - __main__ - Step 50 Global step 50 Train loss 9.405775 on epoch=24
03/19/2022 13:16:01 - INFO - __main__ - Global step 50 Train loss 13.923418 Classification-F1 0.04081632653061224 on epoch=24
03/19/2022 13:16:08 - INFO - __main__ - Step 60 Global step 60 Train loss 9.182813 on epoch=29
03/19/2022 13:16:14 - INFO - __main__ - Step 70 Global step 70 Train loss 8.748213 on epoch=34
03/19/2022 13:16:21 - INFO - __main__ - Step 80 Global step 80 Train loss 9.010386 on epoch=39
03/19/2022 13:16:27 - INFO - __main__ - Step 90 Global step 90 Train loss 8.290255 on epoch=44
03/19/2022 13:16:33 - INFO - __main__ - Step 100 Global step 100 Train loss 7.792306 on epoch=49
03/19/2022 13:16:34 - INFO - __main__ - Global step 100 Train loss 8.604795 Classification-F1 0.07518796992481203 on epoch=49
03/19/2022 13:16:41 - INFO - __main__ - Step 110 Global step 110 Train loss 7.908300 on epoch=54
03/19/2022 13:16:48 - INFO - __main__ - Step 120 Global step 120 Train loss 7.978176 on epoch=59
03/19/2022 13:16:54 - INFO - __main__ - Step 130 Global step 130 Train loss 7.268243 on epoch=64
03/19/2022 13:17:00 - INFO - __main__ - Step 140 Global step 140 Train loss 7.095618 on epoch=69
03/19/2022 13:17:06 - INFO - __main__ - Step 150 Global step 150 Train loss 6.838290 on epoch=74
03/19/2022 13:17:07 - INFO - __main__ - Global step 150 Train loss 7.417726 Classification-F1 0.028985507246376812 on epoch=74
03/19/2022 13:17:13 - INFO - __main__ - Step 160 Global step 160 Train loss 6.831749 on epoch=79
03/19/2022 13:17:20 - INFO - __main__ - Step 170 Global step 170 Train loss 6.291613 on epoch=84
03/19/2022 13:17:26 - INFO - __main__ - Step 180 Global step 180 Train loss 6.212982 on epoch=89
03/19/2022 13:17:32 - INFO - __main__ - Step 190 Global step 190 Train loss 6.419137 on epoch=94
03/19/2022 13:17:38 - INFO - __main__ - Step 200 Global step 200 Train loss 5.835303 on epoch=99
03/19/2022 13:17:39 - INFO - __main__ - Global step 200 Train loss 6.318156 Classification-F1 0.1627906976744186 on epoch=99
03/19/2022 13:17:46 - INFO - __main__ - Step 210 Global step 210 Train loss 5.867710 on epoch=104
03/19/2022 13:17:52 - INFO - __main__ - Step 220 Global step 220 Train loss 5.817023 on epoch=109
03/19/2022 13:17:59 - INFO - __main__ - Step 230 Global step 230 Train loss 5.183700 on epoch=114
03/19/2022 13:18:05 - INFO - __main__ - Step 240 Global step 240 Train loss 4.886093 on epoch=119
03/19/2022 13:18:11 - INFO - __main__ - Step 250 Global step 250 Train loss 4.463332 on epoch=124
03/19/2022 13:18:12 - INFO - __main__ - Global step 250 Train loss 5.243572 Classification-F1 0.12698412698412698 on epoch=124
03/19/2022 13:18:18 - INFO - __main__ - Step 260 Global step 260 Train loss 4.114206 on epoch=129
03/19/2022 13:18:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.961642 on epoch=134
03/19/2022 13:18:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.364307 on epoch=139
03/19/2022 13:18:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.158758 on epoch=144
03/19/2022 13:18:43 - INFO - __main__ - Step 300 Global step 300 Train loss 3.322755 on epoch=149
03/19/2022 13:18:44 - INFO - __main__ - Global step 300 Train loss 2.184334 Classification-F1 0.14102564102564105 on epoch=149
03/19/2022 13:18:50 - INFO - __main__ - Step 310 Global step 310 Train loss 2.957469 on epoch=154
03/19/2022 13:18:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.336632 on epoch=159
03/19/2022 13:19:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.206885 on epoch=164
03/19/2022 13:19:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.200818 on epoch=169
03/19/2022 13:19:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.223119 on epoch=174
03/19/2022 13:19:16 - INFO - __main__ - Global step 350 Train loss 0.784985 Classification-F1 0.36374269005847953 on epoch=174
03/19/2022 13:19:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.131292 on epoch=179
03/19/2022 13:19:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.089361 on epoch=184
03/19/2022 13:19:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.109519 on epoch=189
03/19/2022 13:19:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.066987 on epoch=194
03/19/2022 13:19:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.057357 on epoch=199
03/19/2022 13:19:48 - INFO - __main__ - Global step 400 Train loss 0.090903 Classification-F1 0.4554554554554554 on epoch=199
03/19/2022 13:19:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.071825 on epoch=204
03/19/2022 13:20:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.030041 on epoch=209
03/19/2022 13:20:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.049635 on epoch=214
03/19/2022 13:20:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.037671 on epoch=219
03/19/2022 13:20:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011915 on epoch=224
03/19/2022 13:20:21 - INFO - __main__ - Global step 450 Train loss 0.040217 Classification-F1 0.4009852216748768 on epoch=224
03/19/2022 13:20:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.012082 on epoch=229
03/19/2022 13:20:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019840 on epoch=234
03/19/2022 13:20:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.010431 on epoch=239
03/19/2022 13:20:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009426 on epoch=244
03/19/2022 13:20:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006971 on epoch=249
03/19/2022 13:20:52 - INFO - __main__ - Global step 500 Train loss 0.011750 Classification-F1 0.36374269005847953 on epoch=249
03/19/2022 13:20:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007447 on epoch=254
03/19/2022 13:21:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004805 on epoch=259
03/19/2022 13:21:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008780 on epoch=264
03/19/2022 13:21:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004197 on epoch=269
03/19/2022 13:21:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004952 on epoch=274
03/19/2022 13:21:24 - INFO - __main__ - Global step 550 Train loss 0.006036 Classification-F1 0.39756367663344405 on epoch=274
03/19/2022 13:21:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005466 on epoch=279
03/19/2022 13:21:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011576 on epoch=284
03/19/2022 13:21:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002480 on epoch=289
03/19/2022 13:21:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003616 on epoch=294
03/19/2022 13:21:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003488 on epoch=299
03/19/2022 13:21:56 - INFO - __main__ - Global step 600 Train loss 0.005325 Classification-F1 0.43529411764705883 on epoch=299
03/19/2022 13:21:56 - INFO - __main__ - save last model!
03/19/2022 13:21:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:21:57 - INFO - __main__ - Printing 3 examples
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 13:21:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:21:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:21:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:21:57 - INFO - __main__ - Printing 3 examples
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:21:57 - INFO - __main__ - ['refuted']
03/19/2022 13:21:57 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:21:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:21:57 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:22:03 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 13:22:04 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 13:22:04 - INFO - __main__ - Printing 3 examples
03/19/2022 13:22:04 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:22:04 - INFO - __main__ - ['entailed']
03/19/2022 13:22:04 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:22:04 - INFO - __main__ - ['entailed']
03/19/2022 13:22:04 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:22:04 - INFO - __main__ - ['entailed']
03/19/2022 13:22:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:22:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:22:08 - INFO - __main__ - Starting training!
03/19/2022 13:22:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:22:41 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 13:28:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_21_0.0001_8_predictions.txt
03/19/2022 13:28:48 - INFO - __main__ - Classification-F1 on test data: 0.4984
03/19/2022 13:28:49 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.0001, bsz=8, dev_performance=0.4554554554554554, test_performance=0.49836572716460986
03/19/2022 13:28:49 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.0005, bsz=8 ...
03/19/2022 13:28:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:28:50 - INFO - __main__ - Printing 3 examples
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:28:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:28:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:28:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:28:50 - INFO - __main__ - Printing 3 examples
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:28:50 - INFO - __main__ - ['refuted']
03/19/2022 13:28:50 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:28:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:28:50 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:29:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:29:01 - INFO - __main__ - Starting training!
03/19/2022 13:29:07 - INFO - __main__ - Step 10 Global step 10 Train loss 20.709343 on epoch=4
03/19/2022 13:29:13 - INFO - __main__ - Step 20 Global step 20 Train loss 13.743960 on epoch=9
03/19/2022 13:29:19 - INFO - __main__ - Step 30 Global step 30 Train loss 14.003842 on epoch=14
03/19/2022 13:29:24 - INFO - __main__ - Step 40 Global step 40 Train loss 12.871196 on epoch=19
03/19/2022 13:29:30 - INFO - __main__ - Step 50 Global step 50 Train loss 11.409735 on epoch=24
03/19/2022 13:29:31 - INFO - __main__ - Global step 50 Train loss 14.547615 Classification-F1 0.0 on epoch=24
03/19/2022 13:29:38 - INFO - __main__ - Step 60 Global step 60 Train loss 10.304097 on epoch=29
03/19/2022 13:29:44 - INFO - __main__ - Step 70 Global step 70 Train loss 9.054260 on epoch=34
03/19/2022 13:29:51 - INFO - __main__ - Step 80 Global step 80 Train loss 7.193780 on epoch=39
03/19/2022 13:29:57 - INFO - __main__ - Step 90 Global step 90 Train loss 5.818528 on epoch=44
03/19/2022 13:30:03 - INFO - __main__ - Step 100 Global step 100 Train loss 3.715971 on epoch=49
03/19/2022 13:30:04 - INFO - __main__ - Global step 100 Train loss 7.217327 Classification-F1 0.0 on epoch=49
03/19/2022 13:30:10 - INFO - __main__ - Step 110 Global step 110 Train loss 2.193053 on epoch=54
03/19/2022 13:30:16 - INFO - __main__ - Step 120 Global step 120 Train loss 1.443983 on epoch=59
03/19/2022 13:30:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.204691 on epoch=64
03/19/2022 13:30:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.294550 on epoch=69
03/19/2022 13:30:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.218177 on epoch=74
03/19/2022 13:30:35 - INFO - __main__ - Global step 150 Train loss 1.470891 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 13:30:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.259455 on epoch=79
03/19/2022 13:30:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.298304 on epoch=84
03/19/2022 13:30:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.307865 on epoch=89
03/19/2022 13:31:01 - INFO - __main__ - Step 190 Global step 190 Train loss 1.069466 on epoch=94
03/19/2022 13:31:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.785487 on epoch=99
03/19/2022 13:31:08 - INFO - __main__ - Global step 200 Train loss 1.144115 Classification-F1 0.28888888888888886 on epoch=99
03/19/2022 13:31:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.682936 on epoch=104
03/19/2022 13:31:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.756245 on epoch=109
03/19/2022 13:31:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.727040 on epoch=114
03/19/2022 13:31:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.765990 on epoch=119
03/19/2022 13:31:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.514968 on epoch=124
03/19/2022 13:31:40 - INFO - __main__ - Global step 250 Train loss 0.689436 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 13:31:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.438629 on epoch=129
03/19/2022 13:31:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.828582 on epoch=134
03/19/2022 13:31:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.877973 on epoch=139
03/19/2022 13:32:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.462795 on epoch=144
03/19/2022 13:32:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.514159 on epoch=149
03/19/2022 13:32:11 - INFO - __main__ - Global step 300 Train loss 0.624428 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 13:32:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.383207 on epoch=154
03/19/2022 13:32:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.388810 on epoch=159
03/19/2022 13:32:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.358914 on epoch=164
03/19/2022 13:32:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.323724 on epoch=169
03/19/2022 13:32:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.302705 on epoch=174
03/19/2022 13:32:43 - INFO - __main__ - Global step 350 Train loss 0.351472 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 13:32:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314366 on epoch=179
03/19/2022 13:32:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.367031 on epoch=184
03/19/2022 13:33:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.365975 on epoch=189
03/19/2022 13:33:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.306196 on epoch=194
03/19/2022 13:33:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.322180 on epoch=199
03/19/2022 13:33:15 - INFO - __main__ - Global step 400 Train loss 0.335149 Classification-F1 0.4231177094379639 on epoch=199
03/19/2022 13:33:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.249929 on epoch=204
03/19/2022 13:33:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.249270 on epoch=209
03/19/2022 13:33:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.310922 on epoch=214
03/19/2022 13:33:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.251549 on epoch=219
03/19/2022 13:33:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.229812 on epoch=224
03/19/2022 13:33:47 - INFO - __main__ - Global step 450 Train loss 0.258296 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 13:33:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.247425 on epoch=229
03/19/2022 13:33:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.243695 on epoch=234
03/19/2022 13:34:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.224110 on epoch=239
03/19/2022 13:34:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.248450 on epoch=244
03/19/2022 13:34:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.226214 on epoch=249
03/19/2022 13:34:18 - INFO - __main__ - Global step 500 Train loss 0.237979 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 13:34:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.234320 on epoch=254
03/19/2022 13:34:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.644495 on epoch=259
03/19/2022 13:34:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.220466 on epoch=264
03/19/2022 13:34:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.217294 on epoch=269
03/19/2022 13:34:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.207715 on epoch=274
03/19/2022 13:34:49 - INFO - __main__ - Global step 550 Train loss 0.304858 Classification-F1 0.3333333333333333 on epoch=274
03/19/2022 13:34:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.235456 on epoch=279
03/19/2022 13:35:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.229131 on epoch=284
03/19/2022 13:35:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.181058 on epoch=289
03/19/2022 13:35:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.219228 on epoch=294
03/19/2022 13:35:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.238856 on epoch=299
03/19/2022 13:35:20 - INFO - __main__ - Global step 600 Train loss 0.220746 Classification-F1 0.3333333333333333 on epoch=299
03/19/2022 13:35:20 - INFO - __main__ - save last model!
03/19/2022 13:35:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:35:21 - INFO - __main__ - Printing 3 examples
03/19/2022 13:35:21 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:35:21 - INFO - __main__ - ['refuted']
03/19/2022 13:35:21 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:35:21 - INFO - __main__ - ['refuted']
03/19/2022 13:35:21 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:35:21 - INFO - __main__ - ['refuted']
03/19/2022 13:35:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 13:35:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:35:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:35:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:35:22 - INFO - __main__ - Printing 3 examples
03/19/2022 13:35:22 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:35:22 - INFO - __main__ - ['refuted']
03/19/2022 13:35:22 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:35:22 - INFO - __main__ - ['refuted']
03/19/2022 13:35:22 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:35:22 - INFO - __main__ - ['refuted']
03/19/2022 13:35:22 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:35:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:35:22 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:35:27 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 13:35:29 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 13:35:29 - INFO - __main__ - Printing 3 examples
03/19/2022 13:35:29 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:35:29 - INFO - __main__ - ['entailed']
03/19/2022 13:35:29 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:35:29 - INFO - __main__ - ['entailed']
03/19/2022 13:35:29 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:35:29 - INFO - __main__ - ['entailed']
03/19/2022 13:35:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:35:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:35:34 - INFO - __main__ - Starting training!
03/19/2022 13:35:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:36:05 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 13:41:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_42_0.0005_8_predictions.txt
03/19/2022 13:41:33 - INFO - __main__ - Classification-F1 on test data: 0.4555
03/19/2022 13:41:33 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.0005, bsz=8, dev_performance=0.4231177094379639, test_performance=0.4555165173017646
03/19/2022 13:41:33 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.0003, bsz=8 ...
03/19/2022 13:41:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:41:34 - INFO - __main__ - Printing 3 examples
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:41:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:41:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:41:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:41:34 - INFO - __main__ - Printing 3 examples
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:41:34 - INFO - __main__ - ['refuted']
03/19/2022 13:41:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:41:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:41:34 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:41:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:41:45 - INFO - __main__ - Starting training!
03/19/2022 13:41:50 - INFO - __main__ - Step 10 Global step 10 Train loss 21.120518 on epoch=4
03/19/2022 13:41:57 - INFO - __main__ - Step 20 Global step 20 Train loss 11.275981 on epoch=9
03/19/2022 13:42:03 - INFO - __main__ - Step 30 Global step 30 Train loss 8.577967 on epoch=14
03/19/2022 13:42:09 - INFO - __main__ - Step 40 Global step 40 Train loss 7.955789 on epoch=19
03/19/2022 13:42:15 - INFO - __main__ - Step 50 Global step 50 Train loss 7.786731 on epoch=24
03/19/2022 13:42:17 - INFO - __main__ - Global step 50 Train loss 11.343397 Classification-F1 0.20370370370370372 on epoch=24
03/19/2022 13:42:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.439495 on epoch=29
03/19/2022 13:42:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.621449 on epoch=34
03/19/2022 13:42:36 - INFO - __main__ - Step 80 Global step 80 Train loss 5.806682 on epoch=39
03/19/2022 13:42:42 - INFO - __main__ - Step 90 Global step 90 Train loss 4.017653 on epoch=44
03/19/2022 13:42:48 - INFO - __main__ - Step 100 Global step 100 Train loss 3.298065 on epoch=49
03/19/2022 13:42:49 - INFO - __main__ - Global step 100 Train loss 5.236668 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 13:42:56 - INFO - __main__ - Step 110 Global step 110 Train loss 2.827531 on epoch=54
03/19/2022 13:43:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.418435 on epoch=59
03/19/2022 13:43:08 - INFO - __main__ - Step 130 Global step 130 Train loss 1.881135 on epoch=64
03/19/2022 13:43:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.195073 on epoch=69
03/19/2022 13:43:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.997156 on epoch=74
03/19/2022 13:43:21 - INFO - __main__ - Global step 150 Train loss 1.863866 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 13:43:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.101229 on epoch=79
03/19/2022 13:43:34 - INFO - __main__ - Step 170 Global step 170 Train loss 1.324005 on epoch=84
03/19/2022 13:43:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.405665 on epoch=89
03/19/2022 13:43:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.328754 on epoch=94
03/19/2022 13:43:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.182688 on epoch=99
03/19/2022 13:43:53 - INFO - __main__ - Global step 200 Train loss 0.868468 Classification-F1 0.3992490613266583 on epoch=99
03/19/2022 13:44:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.163207 on epoch=104
03/19/2022 13:44:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.198959 on epoch=109
03/19/2022 13:44:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.121224 on epoch=114
03/19/2022 13:44:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.085464 on epoch=119
03/19/2022 13:44:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.039344 on epoch=124
03/19/2022 13:44:25 - INFO - __main__ - Global step 250 Train loss 0.121640 Classification-F1 0.4181818181818182 on epoch=124
03/19/2022 13:44:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.019457 on epoch=129
03/19/2022 13:44:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.010371 on epoch=134
03/19/2022 13:44:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.006450 on epoch=139
03/19/2022 13:44:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.004867 on epoch=144
03/19/2022 13:44:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.010887 on epoch=149
03/19/2022 13:44:58 - INFO - __main__ - Global step 300 Train loss 0.010406 Classification-F1 0.4817813765182186 on epoch=149
03/19/2022 13:45:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.004162 on epoch=154
03/19/2022 13:45:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001341 on epoch=159
03/19/2022 13:45:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002405 on epoch=164
03/19/2022 13:45:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000636 on epoch=169
03/19/2022 13:45:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000863 on epoch=174
03/19/2022 13:45:30 - INFO - __main__ - Global step 350 Train loss 0.001882 Classification-F1 0.4554554554554554 on epoch=174
03/19/2022 13:45:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000642 on epoch=179
03/19/2022 13:45:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000772 on epoch=184
03/19/2022 13:45:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000730 on epoch=189
03/19/2022 13:45:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001737 on epoch=194
03/19/2022 13:46:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000864 on epoch=199
03/19/2022 13:46:02 - INFO - __main__ - Global step 400 Train loss 0.000949 Classification-F1 0.5270935960591133 on epoch=199
03/19/2022 13:46:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000234 on epoch=204
03/19/2022 13:46:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000951 on epoch=209
03/19/2022 13:46:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000227 on epoch=214
03/19/2022 13:46:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000116 on epoch=219
03/19/2022 13:46:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000075 on epoch=224
03/19/2022 13:46:34 - INFO - __main__ - Global step 450 Train loss 0.000321 Classification-F1 0.4682306940371457 on epoch=224
03/19/2022 13:46:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001288 on epoch=229
03/19/2022 13:46:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.031138 on epoch=234
03/19/2022 13:46:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000864 on epoch=239
03/19/2022 13:46:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000304 on epoch=244
03/19/2022 13:47:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.010687 on epoch=249
03/19/2022 13:47:06 - INFO - __main__ - Global step 500 Train loss 0.008856 Classification-F1 0.4682306940371457 on epoch=249
03/19/2022 13:47:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000244 on epoch=254
03/19/2022 13:47:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000682 on epoch=259
03/19/2022 13:47:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000096 on epoch=264
03/19/2022 13:47:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000162 on epoch=269
03/19/2022 13:47:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000281 on epoch=274
03/19/2022 13:47:37 - INFO - __main__ - Global step 550 Train loss 0.000293 Classification-F1 0.4980392156862745 on epoch=274
03/19/2022 13:47:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000086 on epoch=279
03/19/2022 13:47:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000078 on epoch=284
03/19/2022 13:47:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000119 on epoch=289
03/19/2022 13:48:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002915 on epoch=294
03/19/2022 13:48:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000098 on epoch=299
03/19/2022 13:48:09 - INFO - __main__ - Global step 600 Train loss 0.000659 Classification-F1 0.4666666666666667 on epoch=299
03/19/2022 13:48:09 - INFO - __main__ - save last model!
03/19/2022 13:48:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:48:09 - INFO - __main__ - Printing 3 examples
03/19/2022 13:48:09 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:48:09 - INFO - __main__ - ['refuted']
03/19/2022 13:48:09 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:48:09 - INFO - __main__ - ['refuted']
03/19/2022 13:48:09 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:48:09 - INFO - __main__ - ['refuted']
03/19/2022 13:48:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 13:48:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:48:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:48:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:48:10 - INFO - __main__ - Printing 3 examples
03/19/2022 13:48:10 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:48:10 - INFO - __main__ - ['refuted']
03/19/2022 13:48:10 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:48:10 - INFO - __main__ - ['refuted']
03/19/2022 13:48:10 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:48:10 - INFO - __main__ - ['refuted']
03/19/2022 13:48:10 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:48:10 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:48:10 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:48:16 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 13:48:17 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 13:48:17 - INFO - __main__ - Printing 3 examples
03/19/2022 13:48:17 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:48:17 - INFO - __main__ - ['entailed']
03/19/2022 13:48:17 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:48:17 - INFO - __main__ - ['entailed']
03/19/2022 13:48:17 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 13:48:17 - INFO - __main__ - ['entailed']
03/19/2022 13:48:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:48:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:48:21 - INFO - __main__ - Starting training!
03/19/2022 13:48:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:48:53 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 13:55:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_42_0.0003_8_predictions.txt
03/19/2022 13:55:07 - INFO - __main__ - Classification-F1 on test data: 0.5017
03/19/2022 13:55:07 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.0003, bsz=8, dev_performance=0.5270935960591133, test_performance=0.501737557266969
03/19/2022 13:55:07 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.0002, bsz=8 ...
03/19/2022 13:55:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:55:08 - INFO - __main__ - Printing 3 examples
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 13:55:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:55:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 13:55:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 13:55:08 - INFO - __main__ - Printing 3 examples
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 13:55:08 - INFO - __main__ - ['refuted']
03/19/2022 13:55:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 13:55:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 13:55:08 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 13:55:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 13:55:19 - INFO - __main__ - Starting training!
03/19/2022 13:55:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.462730 on epoch=4
03/19/2022 13:55:31 - INFO - __main__ - Step 20 Global step 20 Train loss 15.169096 on epoch=9
03/19/2022 13:55:37 - INFO - __main__ - Step 30 Global step 30 Train loss 10.385528 on epoch=14
03/19/2022 13:55:43 - INFO - __main__ - Step 40 Global step 40 Train loss 8.922340 on epoch=19
03/19/2022 13:55:49 - INFO - __main__ - Step 50 Global step 50 Train loss 9.082050 on epoch=24
03/19/2022 13:55:51 - INFO - __main__ - Global step 50 Train loss 12.804349 Classification-F1 0.0 on epoch=24
03/19/2022 13:55:58 - INFO - __main__ - Step 60 Global step 60 Train loss 7.707583 on epoch=29
03/19/2022 13:56:04 - INFO - __main__ - Step 70 Global step 70 Train loss 7.420368 on epoch=34
03/19/2022 13:56:10 - INFO - __main__ - Step 80 Global step 80 Train loss 6.957091 on epoch=39
03/19/2022 13:56:16 - INFO - __main__ - Step 90 Global step 90 Train loss 6.871007 on epoch=44
03/19/2022 13:56:22 - INFO - __main__ - Step 100 Global step 100 Train loss 6.143007 on epoch=49
03/19/2022 13:56:23 - INFO - __main__ - Global step 100 Train loss 7.019812 Classification-F1 0.08333333333333333 on epoch=49
03/19/2022 13:56:30 - INFO - __main__ - Step 110 Global step 110 Train loss 6.114844 on epoch=54
03/19/2022 13:56:36 - INFO - __main__ - Step 120 Global step 120 Train loss 5.638136 on epoch=59
03/19/2022 13:56:42 - INFO - __main__ - Step 130 Global step 130 Train loss 5.031865 on epoch=64
03/19/2022 13:56:48 - INFO - __main__ - Step 140 Global step 140 Train loss 4.050483 on epoch=69
03/19/2022 13:56:54 - INFO - __main__ - Step 150 Global step 150 Train loss 2.905053 on epoch=74
03/19/2022 13:56:55 - INFO - __main__ - Global step 150 Train loss 4.748076 Classification-F1 0.21276595744680848 on epoch=74
03/19/2022 13:57:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.390917 on epoch=79
03/19/2022 13:57:08 - INFO - __main__ - Step 170 Global step 170 Train loss 2.251530 on epoch=84
03/19/2022 13:57:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.827717 on epoch=89
03/19/2022 13:57:20 - INFO - __main__ - Step 190 Global step 190 Train loss 1.220486 on epoch=94
03/19/2022 13:57:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.909089 on epoch=99
03/19/2022 13:57:27 - INFO - __main__ - Global step 200 Train loss 1.919948 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 13:57:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.785868 on epoch=104
03/19/2022 13:57:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.557319 on epoch=109
03/19/2022 13:57:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.247996 on epoch=114
03/19/2022 13:57:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.170439 on epoch=119
03/19/2022 13:57:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.124889 on epoch=124
03/19/2022 13:57:59 - INFO - __main__ - Global step 250 Train loss 0.377302 Classification-F1 0.4458874458874459 on epoch=124
03/19/2022 13:58:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.183796 on epoch=129
03/19/2022 13:58:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.356016 on epoch=134
03/19/2022 13:58:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.046949 on epoch=139
03/19/2022 13:58:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.021323 on epoch=144
03/19/2022 13:58:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.014936 on epoch=149
03/19/2022 13:58:31 - INFO - __main__ - Global step 300 Train loss 0.124604 Classification-F1 0.39999999999999997 on epoch=149
03/19/2022 13:58:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.011025 on epoch=154
03/19/2022 13:58:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.023791 on epoch=159
03/19/2022 13:58:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.013767 on epoch=164
03/19/2022 13:58:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.028086 on epoch=169
03/19/2022 13:59:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011390 on epoch=174
03/19/2022 13:59:02 - INFO - __main__ - Global step 350 Train loss 0.017612 Classification-F1 0.3454545454545454 on epoch=174
03/19/2022 13:59:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.008182 on epoch=179
03/19/2022 13:59:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.002748 on epoch=184
03/19/2022 13:59:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.063959 on epoch=189
03/19/2022 13:59:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.066247 on epoch=194
03/19/2022 13:59:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001279 on epoch=199
03/19/2022 13:59:34 - INFO - __main__ - Global step 400 Train loss 0.028483 Classification-F1 0.39756367663344405 on epoch=199
03/19/2022 13:59:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001360 on epoch=204
03/19/2022 13:59:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000652 on epoch=209
03/19/2022 13:59:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001897 on epoch=214
03/19/2022 13:59:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001264 on epoch=219
03/19/2022 14:00:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000453 on epoch=224
03/19/2022 14:00:05 - INFO - __main__ - Global step 450 Train loss 0.001125 Classification-F1 0.39756367663344405 on epoch=224
03/19/2022 14:00:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000372 on epoch=229
03/19/2022 14:00:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001552 on epoch=234
03/19/2022 14:00:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000482 on epoch=239
03/19/2022 14:00:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000709 on epoch=244
03/19/2022 14:00:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000608 on epoch=249
03/19/2022 14:00:36 - INFO - __main__ - Global step 500 Train loss 0.000745 Classification-F1 0.3454545454545454 on epoch=249
03/19/2022 14:00:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000796 on epoch=254
03/19/2022 14:00:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000126 on epoch=259
03/19/2022 14:00:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000622 on epoch=264
03/19/2022 14:01:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.006346 on epoch=269
03/19/2022 14:01:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001137 on epoch=274
03/19/2022 14:01:08 - INFO - __main__ - Global step 550 Train loss 0.001805 Classification-F1 0.4181818181818182 on epoch=274
03/19/2022 14:01:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.030998 on epoch=279
03/19/2022 14:01:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000386 on epoch=284
03/19/2022 14:01:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000192 on epoch=289
03/19/2022 14:01:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000370 on epoch=294
03/19/2022 14:01:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000300 on epoch=299
03/19/2022 14:01:39 - INFO - __main__ - Global step 600 Train loss 0.006449 Classification-F1 0.39756367663344405 on epoch=299
03/19/2022 14:01:39 - INFO - __main__ - save last model!
03/19/2022 14:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:01:39 - INFO - __main__ - Printing 3 examples
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 14:01:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:01:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:01:39 - INFO - __main__ - Printing 3 examples
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 14:01:39 - INFO - __main__ - ['refuted']
03/19/2022 14:01:39 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:01:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:01:40 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:01:46 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 14:01:47 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 14:01:47 - INFO - __main__ - Printing 3 examples
03/19/2022 14:01:47 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:01:47 - INFO - __main__ - ['entailed']
03/19/2022 14:01:47 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:01:47 - INFO - __main__ - ['entailed']
03/19/2022 14:01:47 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:01:47 - INFO - __main__ - ['entailed']
03/19/2022 14:01:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:01:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:01:50 - INFO - __main__ - Starting training!
03/19/2022 14:02:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:02:24 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 14:08:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_42_0.0002_8_predictions.txt
03/19/2022 14:08:41 - INFO - __main__ - Classification-F1 on test data: 0.4794
03/19/2022 14:08:42 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.0002, bsz=8, dev_performance=0.4458874458874459, test_performance=0.47944245040037636
03/19/2022 14:08:42 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.0001, bsz=8 ...
03/19/2022 14:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:08:43 - INFO - __main__ - Printing 3 examples
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:08:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:08:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:08:43 - INFO - __main__ - Printing 3 examples
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/19/2022 14:08:43 - INFO - __main__ - ['refuted']
03/19/2022 14:08:43 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:08:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:08:43 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:08:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:08:54 - INFO - __main__ - Starting training!
03/19/2022 14:08:59 - INFO - __main__ - Step 10 Global step 10 Train loss 19.518322 on epoch=4
03/19/2022 14:09:05 - INFO - __main__ - Step 20 Global step 20 Train loss 17.454258 on epoch=9
03/19/2022 14:09:11 - INFO - __main__ - Step 30 Global step 30 Train loss 14.852753 on epoch=14
03/19/2022 14:09:17 - INFO - __main__ - Step 40 Global step 40 Train loss 11.615215 on epoch=19
03/19/2022 14:09:23 - INFO - __main__ - Step 50 Global step 50 Train loss 9.447598 on epoch=24
03/19/2022 14:09:26 - INFO - __main__ - Global step 50 Train loss 14.577628 Classification-F1 0.025641025641025644 on epoch=24
03/19/2022 14:09:33 - INFO - __main__ - Step 60 Global step 60 Train loss 9.743042 on epoch=29
03/19/2022 14:09:39 - INFO - __main__ - Step 70 Global step 70 Train loss 9.217594 on epoch=34
03/19/2022 14:09:45 - INFO - __main__ - Step 80 Global step 80 Train loss 8.991879 on epoch=39
03/19/2022 14:09:51 - INFO - __main__ - Step 90 Global step 90 Train loss 8.941603 on epoch=44
03/19/2022 14:09:57 - INFO - __main__ - Step 100 Global step 100 Train loss 8.113668 on epoch=49
03/19/2022 14:09:58 - INFO - __main__ - Global step 100 Train loss 9.001557 Classification-F1 0.0 on epoch=49
03/19/2022 14:10:04 - INFO - __main__ - Step 110 Global step 110 Train loss 8.233076 on epoch=54
03/19/2022 14:10:11 - INFO - __main__ - Step 120 Global step 120 Train loss 7.611178 on epoch=59
03/19/2022 14:10:17 - INFO - __main__ - Step 130 Global step 130 Train loss 7.356208 on epoch=64
03/19/2022 14:10:23 - INFO - __main__ - Step 140 Global step 140 Train loss 7.260856 on epoch=69
03/19/2022 14:10:29 - INFO - __main__ - Step 150 Global step 150 Train loss 6.632689 on epoch=74
03/19/2022 14:10:30 - INFO - __main__ - Global step 150 Train loss 7.418802 Classification-F1 0.012345679012345678 on epoch=74
03/19/2022 14:10:36 - INFO - __main__ - Step 160 Global step 160 Train loss 7.080173 on epoch=79
03/19/2022 14:10:42 - INFO - __main__ - Step 170 Global step 170 Train loss 6.822912 on epoch=84
03/19/2022 14:10:48 - INFO - __main__ - Step 180 Global step 180 Train loss 6.644404 on epoch=89
03/19/2022 14:10:55 - INFO - __main__ - Step 190 Global step 190 Train loss 6.501541 on epoch=94
03/19/2022 14:11:01 - INFO - __main__ - Step 200 Global step 200 Train loss 6.234818 on epoch=99
03/19/2022 14:11:02 - INFO - __main__ - Global step 200 Train loss 6.656770 Classification-F1 0.03007518796992481 on epoch=99
03/19/2022 14:11:09 - INFO - __main__ - Step 210 Global step 210 Train loss 5.940608 on epoch=104
03/19/2022 14:11:15 - INFO - __main__ - Step 220 Global step 220 Train loss 5.729408 on epoch=109
03/19/2022 14:11:21 - INFO - __main__ - Step 230 Global step 230 Train loss 5.182252 on epoch=114
03/19/2022 14:11:27 - INFO - __main__ - Step 240 Global step 240 Train loss 5.422028 on epoch=119
03/19/2022 14:11:33 - INFO - __main__ - Step 250 Global step 250 Train loss 4.772902 on epoch=124
03/19/2022 14:11:34 - INFO - __main__ - Global step 250 Train loss 5.409441 Classification-F1 0.05454545454545454 on epoch=124
03/19/2022 14:11:41 - INFO - __main__ - Step 260 Global step 260 Train loss 4.316350 on epoch=129
03/19/2022 14:11:47 - INFO - __main__ - Step 270 Global step 270 Train loss 4.027999 on epoch=134
03/19/2022 14:11:53 - INFO - __main__ - Step 280 Global step 280 Train loss 3.220344 on epoch=139
03/19/2022 14:11:59 - INFO - __main__ - Step 290 Global step 290 Train loss 2.925392 on epoch=144
03/19/2022 14:12:05 - INFO - __main__ - Step 300 Global step 300 Train loss 3.117905 on epoch=149
03/19/2022 14:12:06 - INFO - __main__ - Global step 300 Train loss 3.521598 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 14:12:13 - INFO - __main__ - Step 310 Global step 310 Train loss 2.372402 on epoch=154
03/19/2022 14:12:19 - INFO - __main__ - Step 320 Global step 320 Train loss 2.212471 on epoch=159
03/19/2022 14:12:25 - INFO - __main__ - Step 330 Global step 330 Train loss 1.793217 on epoch=164
03/19/2022 14:12:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.664973 on epoch=169
03/19/2022 14:12:37 - INFO - __main__ - Step 350 Global step 350 Train loss 1.612679 on epoch=174
03/19/2022 14:12:38 - INFO - __main__ - Global step 350 Train loss 1.931149 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 14:12:44 - INFO - __main__ - Step 360 Global step 360 Train loss 1.072052 on epoch=179
03/19/2022 14:12:50 - INFO - __main__ - Step 370 Global step 370 Train loss 1.788388 on epoch=184
03/19/2022 14:12:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.323163 on epoch=189
03/19/2022 14:13:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.529663 on epoch=194
03/19/2022 14:13:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.470274 on epoch=199
03/19/2022 14:13:10 - INFO - __main__ - Global step 400 Train loss 1.436708 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 14:13:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.513527 on epoch=204
03/19/2022 14:13:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.851251 on epoch=209
03/19/2022 14:13:28 - INFO - __main__ - Step 430 Global step 430 Train loss 1.656038 on epoch=214
03/19/2022 14:13:34 - INFO - __main__ - Step 440 Global step 440 Train loss 1.310370 on epoch=219
03/19/2022 14:13:40 - INFO - __main__ - Step 450 Global step 450 Train loss 1.499604 on epoch=224
03/19/2022 14:13:41 - INFO - __main__ - Global step 450 Train loss 1.566158 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 14:13:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.301380 on epoch=229
03/19/2022 14:13:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.208183 on epoch=234
03/19/2022 14:13:59 - INFO - __main__ - Step 480 Global step 480 Train loss 1.046480 on epoch=239
03/19/2022 14:14:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.486544 on epoch=244
03/19/2022 14:14:11 - INFO - __main__ - Step 500 Global step 500 Train loss 1.443714 on epoch=249
03/19/2022 14:14:12 - INFO - __main__ - Global step 500 Train loss 1.297260 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 14:14:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.262315 on epoch=254
03/19/2022 14:14:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.302031 on epoch=259
03/19/2022 14:14:30 - INFO - __main__ - Step 530 Global step 530 Train loss 1.291862 on epoch=264
03/19/2022 14:14:36 - INFO - __main__ - Step 540 Global step 540 Train loss 1.276195 on epoch=269
03/19/2022 14:14:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.969332 on epoch=274
03/19/2022 14:14:43 - INFO - __main__ - Global step 550 Train loss 1.220347 Classification-F1 0.3333333333333333 on epoch=274
03/19/2022 14:14:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.998881 on epoch=279
03/19/2022 14:14:55 - INFO - __main__ - Step 570 Global step 570 Train loss 1.207003 on epoch=284
03/19/2022 14:15:01 - INFO - __main__ - Step 580 Global step 580 Train loss 1.003425 on epoch=289
03/19/2022 14:15:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.799654 on epoch=294
03/19/2022 14:15:14 - INFO - __main__ - Step 600 Global step 600 Train loss 1.042567 on epoch=299
03/19/2022 14:15:15 - INFO - __main__ - Global step 600 Train loss 1.010306 Classification-F1 0.3333333333333333 on epoch=299
03/19/2022 14:15:15 - INFO - __main__ - save last model!
03/19/2022 14:15:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:15:15 - INFO - __main__ - Printing 3 examples
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 14:15:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:15:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:15:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:15:15 - INFO - __main__ - Printing 3 examples
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:15:15 - INFO - __main__ - ['entailed']
03/19/2022 14:15:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:15:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:15:15 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:15:22 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 14:15:22 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 14:15:22 - INFO - __main__ - Printing 3 examples
03/19/2022 14:15:22 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:15:22 - INFO - __main__ - ['entailed']
03/19/2022 14:15:22 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:15:22 - INFO - __main__ - ['entailed']
03/19/2022 14:15:22 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:15:22 - INFO - __main__ - ['entailed']
03/19/2022 14:15:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:15:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:15:26 - INFO - __main__ - Starting training!
03/19/2022 14:15:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:15:59 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 14:21:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_42_0.0001_8_predictions.txt
03/19/2022 14:21:51 - INFO - __main__ - Classification-F1 on test data: 0.1680
03/19/2022 14:21:52 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.16803427681121785
03/19/2022 14:21:52 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.0005, bsz=8 ...
03/19/2022 14:21:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:21:53 - INFO - __main__ - Printing 3 examples
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:21:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:21:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:21:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:21:53 - INFO - __main__ - Printing 3 examples
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:21:53 - INFO - __main__ - ['entailed']
03/19/2022 14:21:53 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:21:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:21:53 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:22:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:22:05 - INFO - __main__ - Starting training!
03/19/2022 14:22:11 - INFO - __main__ - Step 10 Global step 10 Train loss 20.334763 on epoch=4
03/19/2022 14:22:17 - INFO - __main__ - Step 20 Global step 20 Train loss 14.490526 on epoch=9
03/19/2022 14:22:22 - INFO - __main__ - Step 30 Global step 30 Train loss 10.188284 on epoch=14
03/19/2022 14:22:28 - INFO - __main__ - Step 40 Global step 40 Train loss 10.655993 on epoch=19
03/19/2022 14:22:34 - INFO - __main__ - Step 50 Global step 50 Train loss 7.679278 on epoch=24
03/19/2022 14:22:35 - INFO - __main__ - Global step 50 Train loss 12.669768 Classification-F1 0.05102040816326531 on epoch=24
03/19/2022 14:22:42 - INFO - __main__ - Step 60 Global step 60 Train loss 7.062373 on epoch=29
03/19/2022 14:22:48 - INFO - __main__ - Step 70 Global step 70 Train loss 5.037339 on epoch=34
03/19/2022 14:22:54 - INFO - __main__ - Step 80 Global step 80 Train loss 4.347704 on epoch=39
03/19/2022 14:23:00 - INFO - __main__ - Step 90 Global step 90 Train loss 3.180058 on epoch=44
03/19/2022 14:23:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.918802 on epoch=49
03/19/2022 14:23:07 - INFO - __main__ - Global step 100 Train loss 4.309255 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 14:23:14 - INFO - __main__ - Step 110 Global step 110 Train loss 1.234600 on epoch=54
03/19/2022 14:23:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.508120 on epoch=59
03/19/2022 14:23:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.449259 on epoch=64
03/19/2022 14:23:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.380479 on epoch=69
03/19/2022 14:23:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.215154 on epoch=74
03/19/2022 14:23:39 - INFO - __main__ - Global step 150 Train loss 1.357522 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 14:23:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.898955 on epoch=79
03/19/2022 14:23:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.254812 on epoch=84
03/19/2022 14:23:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.036118 on epoch=89
03/19/2022 14:24:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.235192 on epoch=94
03/19/2022 14:24:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.741748 on epoch=99
03/19/2022 14:24:11 - INFO - __main__ - Global step 200 Train loss 1.033365 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 14:24:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.571728 on epoch=104
03/19/2022 14:24:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.777077 on epoch=109
03/19/2022 14:24:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.771889 on epoch=114
03/19/2022 14:24:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.764261 on epoch=119
03/19/2022 14:24:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.640377 on epoch=124
03/19/2022 14:24:42 - INFO - __main__ - Global step 250 Train loss 0.705066 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 14:24:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.660936 on epoch=129
03/19/2022 14:24:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.536198 on epoch=134
03/19/2022 14:25:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.504265 on epoch=139
03/19/2022 14:25:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.456895 on epoch=144
03/19/2022 14:25:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.451836 on epoch=149
03/19/2022 14:25:13 - INFO - __main__ - Global step 300 Train loss 0.522026 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 14:25:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381563 on epoch=154
03/19/2022 14:25:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.383709 on epoch=159
03/19/2022 14:25:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.308822 on epoch=164
03/19/2022 14:25:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.275888 on epoch=169
03/19/2022 14:25:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.305124 on epoch=174
03/19/2022 14:25:45 - INFO - __main__ - Global step 350 Train loss 0.331021 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 14:25:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314881 on epoch=179
03/19/2022 14:25:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.293310 on epoch=184
03/19/2022 14:26:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.245731 on epoch=189
03/19/2022 14:26:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.250490 on epoch=194
03/19/2022 14:26:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.297211 on epoch=199
03/19/2022 14:26:16 - INFO - __main__ - Global step 400 Train loss 0.280324 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 14:26:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.256175 on epoch=204
03/19/2022 14:26:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.292279 on epoch=209
03/19/2022 14:26:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.277543 on epoch=214
03/19/2022 14:26:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.335556 on epoch=219
03/19/2022 14:26:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.230741 on epoch=224
03/19/2022 14:26:47 - INFO - __main__ - Global step 450 Train loss 0.278459 Classification-F1 0.3333333333333333 on epoch=224
03/19/2022 14:26:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.213277 on epoch=229
03/19/2022 14:27:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.200522 on epoch=234
03/19/2022 14:27:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.236241 on epoch=239
03/19/2022 14:27:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.221770 on epoch=244
03/19/2022 14:27:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.246311 on epoch=249
03/19/2022 14:27:19 - INFO - __main__ - Global step 500 Train loss 0.223624 Classification-F1 0.40566959921798634 on epoch=249
03/19/2022 14:27:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.263685 on epoch=254
03/19/2022 14:27:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.199420 on epoch=259
03/19/2022 14:27:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.191190 on epoch=264
03/19/2022 14:27:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.237816 on epoch=269
03/19/2022 14:27:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.221583 on epoch=274
03/19/2022 14:27:51 - INFO - __main__ - Global step 550 Train loss 0.222739 Classification-F1 0.3333333333333333 on epoch=274
03/19/2022 14:27:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.247715 on epoch=279
03/19/2022 14:28:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.204052 on epoch=284
03/19/2022 14:28:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.186063 on epoch=289
03/19/2022 14:28:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.209612 on epoch=294
03/19/2022 14:28:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.185212 on epoch=299
03/19/2022 14:28:22 - INFO - __main__ - Global step 600 Train loss 0.206531 Classification-F1 0.3333333333333333 on epoch=299
03/19/2022 14:28:22 - INFO - __main__ - save last model!
03/19/2022 14:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:28:23 - INFO - __main__ - Printing 3 examples
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 14:28:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:28:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:28:23 - INFO - __main__ - Printing 3 examples
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:28:23 - INFO - __main__ - ['entailed']
03/19/2022 14:28:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:28:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:28:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:28:30 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 14:28:31 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 14:28:31 - INFO - __main__ - Printing 3 examples
03/19/2022 14:28:31 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:28:31 - INFO - __main__ - ['entailed']
03/19/2022 14:28:31 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:28:31 - INFO - __main__ - ['entailed']
03/19/2022 14:28:31 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:28:31 - INFO - __main__ - ['entailed']
03/19/2022 14:28:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:28:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:28:36 - INFO - __main__ - Starting training!
03/19/2022 14:28:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:29:09 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 14:35:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_87_0.0005_8_predictions.txt
03/19/2022 14:35:17 - INFO - __main__ - Classification-F1 on test data: 0.4996
03/19/2022 14:35:17 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.0005, bsz=8, dev_performance=0.40566959921798634, test_performance=0.4995722291282639
03/19/2022 14:35:17 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.0003, bsz=8 ...
03/19/2022 14:35:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:35:18 - INFO - __main__ - Printing 3 examples
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:35:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:35:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:35:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:35:18 - INFO - __main__ - Printing 3 examples
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:35:18 - INFO - __main__ - ['entailed']
03/19/2022 14:35:18 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:35:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:35:18 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:35:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:35:31 - INFO - __main__ - Starting training!
03/19/2022 14:35:36 - INFO - __main__ - Step 10 Global step 10 Train loss 19.870090 on epoch=4
03/19/2022 14:35:42 - INFO - __main__ - Step 20 Global step 20 Train loss 15.489858 on epoch=9
03/19/2022 14:35:49 - INFO - __main__ - Step 30 Global step 30 Train loss 10.082951 on epoch=14
03/19/2022 14:35:55 - INFO - __main__ - Step 40 Global step 40 Train loss 8.370687 on epoch=19
03/19/2022 14:36:01 - INFO - __main__ - Step 50 Global step 50 Train loss 8.277069 on epoch=24
03/19/2022 14:36:02 - INFO - __main__ - Global step 50 Train loss 12.418131 Classification-F1 0.047619047619047616 on epoch=24
03/19/2022 14:36:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.847610 on epoch=29
03/19/2022 14:36:15 - INFO - __main__ - Step 70 Global step 70 Train loss 6.902707 on epoch=34
03/19/2022 14:36:21 - INFO - __main__ - Step 80 Global step 80 Train loss 5.963235 on epoch=39
03/19/2022 14:36:27 - INFO - __main__ - Step 90 Global step 90 Train loss 5.183342 on epoch=44
03/19/2022 14:36:33 - INFO - __main__ - Step 100 Global step 100 Train loss 4.739891 on epoch=49
03/19/2022 14:36:34 - INFO - __main__ - Global step 100 Train loss 5.927357 Classification-F1 0.14634146341463414 on epoch=49
03/19/2022 14:36:41 - INFO - __main__ - Step 110 Global step 110 Train loss 3.160250 on epoch=54
03/19/2022 14:36:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.811546 on epoch=59
03/19/2022 14:36:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.183219 on epoch=64
03/19/2022 14:36:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.136132 on epoch=69
03/19/2022 14:37:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.196410 on epoch=74
03/19/2022 14:37:06 - INFO - __main__ - Global step 150 Train loss 0.897511 Classification-F1 0.5270935960591133 on epoch=74
03/19/2022 14:37:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.108892 on epoch=79
03/19/2022 14:37:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.043524 on epoch=84
03/19/2022 14:37:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.035348 on epoch=89
03/19/2022 14:37:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.030955 on epoch=94
03/19/2022 14:37:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.009471 on epoch=99
03/19/2022 14:37:39 - INFO - __main__ - Global step 200 Train loss 0.045638 Classification-F1 0.5270935960591133 on epoch=99
03/19/2022 14:37:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.009265 on epoch=104
03/19/2022 14:37:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005101 on epoch=109
03/19/2022 14:37:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003962 on epoch=114
03/19/2022 14:38:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001792 on epoch=119
03/19/2022 14:38:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.003078 on epoch=124
03/19/2022 14:38:10 - INFO - __main__ - Global step 250 Train loss 0.004640 Classification-F1 0.41700404858299595 on epoch=124
03/19/2022 14:38:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.002351 on epoch=129
03/19/2022 14:38:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001243 on epoch=134
03/19/2022 14:38:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001609 on epoch=139
03/19/2022 14:38:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.003088 on epoch=144
03/19/2022 14:38:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005298 on epoch=149
03/19/2022 14:38:42 - INFO - __main__ - Global step 300 Train loss 0.002718 Classification-F1 0.4666666666666667 on epoch=149
03/19/2022 14:38:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001252 on epoch=154
03/19/2022 14:38:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.182053 on epoch=159
03/19/2022 14:39:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002811 on epoch=164
03/19/2022 14:39:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001703 on epoch=169
03/19/2022 14:39:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000561 on epoch=174
03/19/2022 14:39:14 - INFO - __main__ - Global step 350 Train loss 0.037676 Classification-F1 0.4920634920634921 on epoch=174
03/19/2022 14:39:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000924 on epoch=179
03/19/2022 14:39:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000717 on epoch=184
03/19/2022 14:39:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000559 on epoch=189
03/19/2022 14:39:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000295 on epoch=194
03/19/2022 14:39:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000531 on epoch=199
03/19/2022 14:39:45 - INFO - __main__ - Global step 400 Train loss 0.000605 Classification-F1 0.4817813765182186 on epoch=199
03/19/2022 14:39:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000252 on epoch=204
03/19/2022 14:39:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000196 on epoch=209
03/19/2022 14:40:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000319 on epoch=214
03/19/2022 14:40:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000730 on epoch=219
03/19/2022 14:40:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000261 on epoch=224
03/19/2022 14:40:17 - INFO - __main__ - Global step 450 Train loss 0.000352 Classification-F1 0.4920634920634921 on epoch=224
03/19/2022 14:40:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000277 on epoch=229
03/19/2022 14:40:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000800 on epoch=234
03/19/2022 14:40:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000980 on epoch=239
03/19/2022 14:40:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000299 on epoch=244
03/19/2022 14:40:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000056 on epoch=249
03/19/2022 14:40:49 - INFO - __main__ - Global step 500 Train loss 0.000482 Classification-F1 0.4920634920634921 on epoch=249
03/19/2022 14:40:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000156 on epoch=254
03/19/2022 14:41:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000024 on epoch=259
03/19/2022 14:41:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000226 on epoch=264
03/19/2022 14:41:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000094 on epoch=269
03/19/2022 14:41:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.080337 on epoch=274
03/19/2022 14:41:20 - INFO - __main__ - Global step 550 Train loss 0.016167 Classification-F1 0.36374269005847953 on epoch=274
03/19/2022 14:41:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009437 on epoch=279
03/19/2022 14:41:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001018 on epoch=284
03/19/2022 14:41:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000272 on epoch=289
03/19/2022 14:41:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000390 on epoch=294
03/19/2022 14:41:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000373 on epoch=299
03/19/2022 14:41:52 - INFO - __main__ - Global step 600 Train loss 0.002298 Classification-F1 0.5195195195195195 on epoch=299
03/19/2022 14:41:52 - INFO - __main__ - save last model!
03/19/2022 14:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:41:52 - INFO - __main__ - Printing 3 examples
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 14:41:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:41:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:41:52 - INFO - __main__ - Printing 3 examples
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:41:52 - INFO - __main__ - ['entailed']
03/19/2022 14:41:52 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:41:52 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:41:52 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:41:59 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 14:42:00 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 14:42:00 - INFO - __main__ - Printing 3 examples
03/19/2022 14:42:00 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:42:00 - INFO - __main__ - ['entailed']
03/19/2022 14:42:00 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:42:00 - INFO - __main__ - ['entailed']
03/19/2022 14:42:00 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:42:00 - INFO - __main__ - ['entailed']
03/19/2022 14:42:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:42:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:42:05 - INFO - __main__ - Starting training!
03/19/2022 14:42:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:42:36 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 14:48:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_87_0.0003_8_predictions.txt
03/19/2022 14:48:36 - INFO - __main__ - Classification-F1 on test data: 0.4883
03/19/2022 14:48:37 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.0003, bsz=8, dev_performance=0.5270935960591133, test_performance=0.4882980027872949
03/19/2022 14:48:37 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.0002, bsz=8 ...
03/19/2022 14:48:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:48:38 - INFO - __main__ - Printing 3 examples
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:48:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:48:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:48:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:48:38 - INFO - __main__ - Printing 3 examples
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:48:38 - INFO - __main__ - ['entailed']
03/19/2022 14:48:38 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:48:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:48:38 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:48:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:48:49 - INFO - __main__ - Starting training!
03/19/2022 14:48:55 - INFO - __main__ - Step 10 Global step 10 Train loss 21.893871 on epoch=4
03/19/2022 14:49:01 - INFO - __main__ - Step 20 Global step 20 Train loss 13.032102 on epoch=9
03/19/2022 14:49:07 - INFO - __main__ - Step 30 Global step 30 Train loss 9.366933 on epoch=14
03/19/2022 14:49:13 - INFO - __main__ - Step 40 Global step 40 Train loss 8.394778 on epoch=19
03/19/2022 14:49:19 - INFO - __main__ - Step 50 Global step 50 Train loss 7.825212 on epoch=24
03/19/2022 14:49:21 - INFO - __main__ - Global step 50 Train loss 12.102578 Classification-F1 0.3333333333333333 on epoch=24
03/19/2022 14:49:28 - INFO - __main__ - Step 60 Global step 60 Train loss 7.873114 on epoch=29
03/19/2022 14:49:34 - INFO - __main__ - Step 70 Global step 70 Train loss 7.447913 on epoch=34
03/19/2022 14:49:40 - INFO - __main__ - Step 80 Global step 80 Train loss 6.643411 on epoch=39
03/19/2022 14:49:47 - INFO - __main__ - Step 90 Global step 90 Train loss 6.464113 on epoch=44
03/19/2022 14:49:53 - INFO - __main__ - Step 100 Global step 100 Train loss 5.697532 on epoch=49
03/19/2022 14:49:54 - INFO - __main__ - Global step 100 Train loss 6.825216 Classification-F1 0.0 on epoch=49
03/19/2022 14:50:00 - INFO - __main__ - Step 110 Global step 110 Train loss 4.574802 on epoch=54
03/19/2022 14:50:06 - INFO - __main__ - Step 120 Global step 120 Train loss 3.286499 on epoch=59
03/19/2022 14:50:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.618165 on epoch=64
03/19/2022 14:50:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.222121 on epoch=69
03/19/2022 14:50:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.196905 on epoch=74
03/19/2022 14:50:25 - INFO - __main__ - Global step 150 Train loss 1.779698 Classification-F1 0.3043478260869565 on epoch=74
03/19/2022 14:50:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.075661 on epoch=79
03/19/2022 14:50:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.075764 on epoch=84
03/19/2022 14:50:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.024676 on epoch=89
03/19/2022 14:50:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.009724 on epoch=94
03/19/2022 14:50:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.011932 on epoch=99
03/19/2022 14:50:57 - INFO - __main__ - Global step 200 Train loss 0.039551 Classification-F1 0.464039408866995 on epoch=99
03/19/2022 14:51:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005492 on epoch=104
03/19/2022 14:51:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.009664 on epoch=109
03/19/2022 14:51:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.005642 on epoch=114
03/19/2022 14:51:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.004067 on epoch=119
03/19/2022 14:51:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.002106 on epoch=124
03/19/2022 14:51:29 - INFO - __main__ - Global step 250 Train loss 0.005394 Classification-F1 0.4980392156862745 on epoch=124
03/19/2022 14:51:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003153 on epoch=129
03/19/2022 14:51:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000791 on epoch=134
03/19/2022 14:51:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001034 on epoch=139
03/19/2022 14:51:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002472 on epoch=144
03/19/2022 14:52:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000684 on epoch=149
03/19/2022 14:52:02 - INFO - __main__ - Global step 300 Train loss 0.001626 Classification-F1 0.4920634920634921 on epoch=149
03/19/2022 14:52:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000723 on epoch=154
03/19/2022 14:52:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000554 on epoch=159
03/19/2022 14:52:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001551 on epoch=164
03/19/2022 14:52:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000589 on epoch=169
03/19/2022 14:52:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000639 on epoch=174
03/19/2022 14:52:33 - INFO - __main__ - Global step 350 Train loss 0.000811 Classification-F1 0.5465587044534412 on epoch=174
03/19/2022 14:52:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000149 on epoch=179
03/19/2022 14:52:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000189 on epoch=184
03/19/2022 14:52:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000193 on epoch=189
03/19/2022 14:52:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000356 on epoch=194
03/19/2022 14:53:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000106 on epoch=199
03/19/2022 14:53:06 - INFO - __main__ - Global step 400 Train loss 0.000198 Classification-F1 0.4980392156862745 on epoch=199
03/19/2022 14:53:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000768 on epoch=204
03/19/2022 14:53:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000120 on epoch=209
03/19/2022 14:53:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000111 on epoch=214
03/19/2022 14:53:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000090 on epoch=219
03/19/2022 14:53:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000053 on epoch=224
03/19/2022 14:53:37 - INFO - __main__ - Global step 450 Train loss 0.000229 Classification-F1 0.4980392156862745 on epoch=224
03/19/2022 14:53:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000120 on epoch=229
03/19/2022 14:53:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000108 on epoch=234
03/19/2022 14:53:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.137504 on epoch=239
03/19/2022 14:54:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.076723 on epoch=244
03/19/2022 14:54:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.029170 on epoch=249
03/19/2022 14:54:09 - INFO - __main__ - Global step 500 Train loss 0.048725 Classification-F1 0.43529411764705883 on epoch=249
03/19/2022 14:54:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001401 on epoch=254
03/19/2022 14:54:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001117 on epoch=259
03/19/2022 14:54:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000734 on epoch=264
03/19/2022 14:54:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000731 on epoch=269
03/19/2022 14:54:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000187 on epoch=274
03/19/2022 14:54:41 - INFO - __main__ - Global step 550 Train loss 0.000834 Classification-F1 0.5307917888563051 on epoch=274
03/19/2022 14:54:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000177 on epoch=279
03/19/2022 14:54:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000385 on epoch=284
03/19/2022 14:54:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000228 on epoch=289
03/19/2022 14:55:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000656 on epoch=294
03/19/2022 14:55:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000206 on epoch=299
03/19/2022 14:55:13 - INFO - __main__ - Global step 600 Train loss 0.000330 Classification-F1 0.4980392156862745 on epoch=299
03/19/2022 14:55:13 - INFO - __main__ - save last model!
03/19/2022 14:55:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:55:13 - INFO - __main__ - Printing 3 examples
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 14:55:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:55:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 14:55:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 14:55:13 - INFO - __main__ - Printing 3 examples
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 14:55:13 - INFO - __main__ - ['entailed']
03/19/2022 14:55:13 - INFO - __main__ - Tokenizing Input ...
03/19/2022 14:55:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:55:13 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 14:55:19 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 14:55:20 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 14:55:20 - INFO - __main__ - Printing 3 examples
03/19/2022 14:55:20 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:55:20 - INFO - __main__ - ['entailed']
03/19/2022 14:55:20 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:55:20 - INFO - __main__ - ['entailed']
03/19/2022 14:55:20 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 14:55:20 - INFO - __main__ - ['entailed']
03/19/2022 14:55:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 14:55:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 14:55:24 - INFO - __main__ - Starting training!
03/19/2022 14:55:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 14:55:57 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 15:02:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_87_0.0002_8_predictions.txt
03/19/2022 15:02:05 - INFO - __main__ - Classification-F1 on test data: 0.4951
03/19/2022 15:02:05 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.0002, bsz=8, dev_performance=0.5465587044534412, test_performance=0.4951053621223653
03/19/2022 15:02:05 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.0001, bsz=8 ...
03/19/2022 15:02:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 15:02:06 - INFO - __main__ - Printing 3 examples
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:02:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:02:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 15:02:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 15:02:06 - INFO - __main__ - Printing 3 examples
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/19/2022 15:02:06 - INFO - __main__ - ['entailed']
03/19/2022 15:02:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:02:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:02:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 15:02:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:02:17 - INFO - __main__ - Starting training!
03/19/2022 15:02:24 - INFO - __main__ - Step 10 Global step 10 Train loss 20.547686 on epoch=4
03/19/2022 15:02:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.222307 on epoch=9
03/19/2022 15:02:36 - INFO - __main__ - Step 30 Global step 30 Train loss 14.311026 on epoch=14
03/19/2022 15:02:42 - INFO - __main__ - Step 40 Global step 40 Train loss 10.455775 on epoch=19
03/19/2022 15:02:48 - INFO - __main__ - Step 50 Global step 50 Train loss 9.943132 on epoch=24
03/19/2022 15:02:52 - INFO - __main__ - Global step 50 Train loss 14.695987 Classification-F1 0.0 on epoch=24
03/19/2022 15:02:59 - INFO - __main__ - Step 60 Global step 60 Train loss 9.444934 on epoch=29
03/19/2022 15:03:05 - INFO - __main__ - Step 70 Global step 70 Train loss 8.698346 on epoch=34
03/19/2022 15:03:11 - INFO - __main__ - Step 80 Global step 80 Train loss 8.505036 on epoch=39
03/19/2022 15:03:17 - INFO - __main__ - Step 90 Global step 90 Train loss 8.495481 on epoch=44
03/19/2022 15:03:23 - INFO - __main__ - Step 100 Global step 100 Train loss 8.031480 on epoch=49
03/19/2022 15:03:25 - INFO - __main__ - Global step 100 Train loss 8.635056 Classification-F1 0.0 on epoch=49
03/19/2022 15:03:31 - INFO - __main__ - Step 110 Global step 110 Train loss 7.525146 on epoch=54
03/19/2022 15:03:37 - INFO - __main__ - Step 120 Global step 120 Train loss 7.876161 on epoch=59
03/19/2022 15:03:43 - INFO - __main__ - Step 130 Global step 130 Train loss 7.203808 on epoch=64
03/19/2022 15:03:49 - INFO - __main__ - Step 140 Global step 140 Train loss 7.052205 on epoch=69
03/19/2022 15:03:55 - INFO - __main__ - Step 150 Global step 150 Train loss 7.093679 on epoch=74
03/19/2022 15:03:56 - INFO - __main__ - Global step 150 Train loss 7.350200 Classification-F1 0.0 on epoch=74
03/19/2022 15:04:02 - INFO - __main__ - Step 160 Global step 160 Train loss 7.398131 on epoch=79
03/19/2022 15:04:08 - INFO - __main__ - Step 170 Global step 170 Train loss 6.587692 on epoch=84
03/19/2022 15:04:14 - INFO - __main__ - Step 180 Global step 180 Train loss 6.434164 on epoch=89
03/19/2022 15:04:20 - INFO - __main__ - Step 190 Global step 190 Train loss 5.932445 on epoch=94
03/19/2022 15:04:26 - INFO - __main__ - Step 200 Global step 200 Train loss 5.925248 on epoch=99
03/19/2022 15:04:27 - INFO - __main__ - Global step 200 Train loss 6.455536 Classification-F1 0.037037037037037035 on epoch=99
03/19/2022 15:04:34 - INFO - __main__ - Step 210 Global step 210 Train loss 5.685133 on epoch=104
03/19/2022 15:04:40 - INFO - __main__ - Step 220 Global step 220 Train loss 5.072544 on epoch=109
03/19/2022 15:04:46 - INFO - __main__ - Step 230 Global step 230 Train loss 5.187388 on epoch=114
03/19/2022 15:04:52 - INFO - __main__ - Step 240 Global step 240 Train loss 4.946572 on epoch=119
03/19/2022 15:04:58 - INFO - __main__ - Step 250 Global step 250 Train loss 4.497457 on epoch=124
03/19/2022 15:04:59 - INFO - __main__ - Global step 250 Train loss 5.077819 Classification-F1 0.0 on epoch=124
03/19/2022 15:05:05 - INFO - __main__ - Step 260 Global step 260 Train loss 3.959818 on epoch=129
03/19/2022 15:05:11 - INFO - __main__ - Step 270 Global step 270 Train loss 3.741762 on epoch=134
03/19/2022 15:05:17 - INFO - __main__ - Step 280 Global step 280 Train loss 2.933289 on epoch=139
03/19/2022 15:05:24 - INFO - __main__ - Step 290 Global step 290 Train loss 2.338968 on epoch=144
03/19/2022 15:05:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.784988 on epoch=149
03/19/2022 15:05:30 - INFO - __main__ - Global step 300 Train loss 2.951765 Classification-F1 0.06349206349206349 on epoch=149
03/19/2022 15:05:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.297297 on epoch=154
03/19/2022 15:05:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.332291 on epoch=159
03/19/2022 15:05:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.183803 on epoch=164
03/19/2022 15:05:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.161354 on epoch=169
03/19/2022 15:06:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.156577 on epoch=174
03/19/2022 15:06:03 - INFO - __main__ - Global step 350 Train loss 0.226265 Classification-F1 0.3992490613266583 on epoch=174
03/19/2022 15:06:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.128861 on epoch=179
03/19/2022 15:06:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.061761 on epoch=184
03/19/2022 15:06:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.051829 on epoch=189
03/19/2022 15:06:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.045506 on epoch=194
03/19/2022 15:06:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.021180 on epoch=199
03/19/2022 15:06:35 - INFO - __main__ - Global step 400 Train loss 0.061827 Classification-F1 0.4458874458874459 on epoch=199
03/19/2022 15:06:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.020000 on epoch=204
03/19/2022 15:06:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.036364 on epoch=209
03/19/2022 15:06:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.017100 on epoch=214
03/19/2022 15:07:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008387 on epoch=219
03/19/2022 15:07:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.006870 on epoch=224
03/19/2022 15:07:07 - INFO - __main__ - Global step 450 Train loss 0.017744 Classification-F1 0.4817813765182186 on epoch=224
03/19/2022 15:07:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.013309 on epoch=229
03/19/2022 15:07:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.006317 on epoch=234
03/19/2022 15:07:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006287 on epoch=239
03/19/2022 15:07:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.076817 on epoch=244
03/19/2022 15:07:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002175 on epoch=249
03/19/2022 15:07:39 - INFO - __main__ - Global step 500 Train loss 0.020981 Classification-F1 0.4666666666666667 on epoch=249
03/19/2022 15:07:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001259 on epoch=254
03/19/2022 15:07:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001383 on epoch=259
03/19/2022 15:07:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002122 on epoch=264
03/19/2022 15:08:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.008607 on epoch=269
03/19/2022 15:08:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001006 on epoch=274
03/19/2022 15:08:10 - INFO - __main__ - Global step 550 Train loss 0.002875 Classification-F1 0.4554554554554554 on epoch=274
03/19/2022 15:08:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000867 on epoch=279
03/19/2022 15:08:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001622 on epoch=284
03/19/2022 15:08:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000827 on epoch=289
03/19/2022 15:08:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002376 on epoch=294
03/19/2022 15:08:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000592 on epoch=299
03/19/2022 15:08:42 - INFO - __main__ - Global step 600 Train loss 0.001257 Classification-F1 0.4666666666666667 on epoch=299
03/19/2022 15:08:42 - INFO - __main__ - save last model!
03/19/2022 15:08:49 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:08:50 - INFO - __main__ - Start tokenizing ... 12792 instances
03/19/2022 15:08:50 - INFO - __main__ - Printing 3 examples
03/19/2022 15:08:50 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 15:08:50 - INFO - __main__ - ['entailed']
03/19/2022 15:08:50 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 15:08:50 - INFO - __main__ - ['entailed']
03/19/2022 15:08:50 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/19/2022 15:08:50 - INFO - __main__ - ['entailed']
03/19/2022 15:08:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:09:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:09:28 - INFO - __main__ - Loaded 12792 examples from test data
03/19/2022 15:15:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-tab_fact/tab_fact_16_87_0.0001_8_predictions.txt
03/19/2022 15:15:49 - INFO - __main__ - Classification-F1 on test data: 0.4914
03/19/2022 15:15:51 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.0001, bsz=8, dev_performance=0.4817813765182186, test_performance=0.49144946748410073
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22411): No such process
Task: anli, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 15:15:57 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 15:15:57 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-anli
Output directory () already exists and is not empty.
03/19/2022 15:15:57 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 15:15:57 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-anli
03/19/2022 15:15:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 15:15:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 15:15:59 - INFO - __main__ - args.device: cuda:0
03/19/2022 15:15:59 - INFO - __main__ - Using 2 gpus
03/19/2022 15:15:59 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/19/2022 15:15:59 - INFO - __main__ - args.device: cuda:1
03/19/2022 15:15:59 - INFO - __main__ - Using 2 gpus
03/19/2022 15:15:59 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/19/2022 15:16:03 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.0005, bsz=8 ...
03/19/2022 15:16:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:16:04 - INFO - __main__ - Printing 3 examples
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:16:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:16:04 - INFO - __main__ - Printing 3 examples
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:16:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:16:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:16:04 - INFO - __main__ - Printing 3 examples
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:16:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:16:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:16:04 - INFO - __main__ - Printing 3 examples
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:16:04 - INFO - __main__ - ['neutral']
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:16:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:16:04 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:16:04 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:16:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:16:18 - INFO - __main__ - Starting training!
03/19/2022 15:16:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:16:19 - INFO - __main__ - Starting training!
03/19/2022 15:16:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.839777 on epoch=3
03/19/2022 15:16:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.677299 on epoch=6
03/19/2022 15:16:35 - INFO - __main__ - Step 30 Global step 30 Train loss 12.412600 on epoch=9
03/19/2022 15:16:40 - INFO - __main__ - Step 40 Global step 40 Train loss 9.637185 on epoch=13
03/19/2022 15:16:45 - INFO - __main__ - Step 50 Global step 50 Train loss 9.037419 on epoch=16
03/19/2022 15:16:47 - INFO - __main__ - Global step 50 Train loss 14.720856 Classification-F1 0.030095759233926125 on epoch=16
03/19/2022 15:16:54 - INFO - __main__ - Step 60 Global step 60 Train loss 7.620870 on epoch=19
03/19/2022 15:16:59 - INFO - __main__ - Step 70 Global step 70 Train loss 6.770022 on epoch=23
03/19/2022 15:17:04 - INFO - __main__ - Step 80 Global step 80 Train loss 5.283179 on epoch=26
03/19/2022 15:17:09 - INFO - __main__ - Step 90 Global step 90 Train loss 3.110994 on epoch=29
03/19/2022 15:17:14 - INFO - __main__ - Step 100 Global step 100 Train loss 2.176736 on epoch=33
03/19/2022 15:17:15 - INFO - __main__ - Global step 100 Train loss 4.992361 Classification-F1 0.16666666666666666 on epoch=33
03/19/2022 15:17:23 - INFO - __main__ - Step 110 Global step 110 Train loss 2.680611 on epoch=36
03/19/2022 15:17:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.977461 on epoch=39
03/19/2022 15:17:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.849636 on epoch=43
03/19/2022 15:17:38 - INFO - __main__ - Step 140 Global step 140 Train loss 2.352970 on epoch=46
03/19/2022 15:17:43 - INFO - __main__ - Step 150 Global step 150 Train loss 2.225949 on epoch=49
03/19/2022 15:17:44 - INFO - __main__ - Global step 150 Train loss 2.217326 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 15:17:49 - INFO - __main__ - Step 160 Global step 160 Train loss 1.862528 on epoch=53
03/19/2022 15:17:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.929862 on epoch=56
03/19/2022 15:17:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.546382 on epoch=59
03/19/2022 15:18:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.386971 on epoch=63
03/19/2022 15:18:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.395158 on epoch=66
03/19/2022 15:18:10 - INFO - __main__ - Global step 200 Train loss 1.624180 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 15:18:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.224480 on epoch=69
03/19/2022 15:18:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.103401 on epoch=73
03/19/2022 15:18:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.023084 on epoch=76
03/19/2022 15:18:30 - INFO - __main__ - Step 240 Global step 240 Train loss 1.236938 on epoch=79
03/19/2022 15:18:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.809488 on epoch=83
03/19/2022 15:18:36 - INFO - __main__ - Global step 250 Train loss 1.079478 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 15:18:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.944611 on epoch=86
03/19/2022 15:18:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.793232 on epoch=89
03/19/2022 15:18:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.739147 on epoch=93
03/19/2022 15:18:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.721918 on epoch=96
03/19/2022 15:19:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.709340 on epoch=99
03/19/2022 15:19:01 - INFO - __main__ - Global step 300 Train loss 0.781650 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 15:19:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.679152 on epoch=103
03/19/2022 15:19:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.698269 on epoch=106
03/19/2022 15:19:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.522684 on epoch=109
03/19/2022 15:19:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.663251 on epoch=113
03/19/2022 15:19:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.641246 on epoch=116
03/19/2022 15:19:27 - INFO - __main__ - Global step 350 Train loss 0.640921 Classification-F1 0.24611708482676223 on epoch=116
03/19/2022 15:19:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.528137 on epoch=119
03/19/2022 15:19:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.630800 on epoch=123
03/19/2022 15:19:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.594524 on epoch=126
03/19/2022 15:19:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.565365 on epoch=129
03/19/2022 15:19:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.583047 on epoch=133
03/19/2022 15:19:54 - INFO - __main__ - Global step 400 Train loss 0.580375 Classification-F1 0.2913165266106443 on epoch=133
03/19/2022 15:20:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.938206 on epoch=136
03/19/2022 15:20:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.429739 on epoch=139
03/19/2022 15:20:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.316303 on epoch=143
03/19/2022 15:20:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.251123 on epoch=146
03/19/2022 15:20:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.229658 on epoch=149
03/19/2022 15:20:21 - INFO - __main__ - Global step 450 Train loss 0.433006 Classification-F1 0.2261188554658841 on epoch=149
03/19/2022 15:20:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.123613 on epoch=153
03/19/2022 15:20:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.068771 on epoch=156
03/19/2022 15:20:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.056328 on epoch=159
03/19/2022 15:20:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.039070 on epoch=163
03/19/2022 15:20:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.010219 on epoch=166
03/19/2022 15:20:47 - INFO - __main__ - Global step 500 Train loss 0.059600 Classification-F1 0.28336629611948444 on epoch=166
03/19/2022 15:20:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.004116 on epoch=169
03/19/2022 15:20:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003871 on epoch=173
03/19/2022 15:21:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000648 on epoch=176
03/19/2022 15:21:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000192 on epoch=179
03/19/2022 15:21:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000282 on epoch=183
03/19/2022 15:21:13 - INFO - __main__ - Global step 550 Train loss 0.001822 Classification-F1 0.3700288892986661 on epoch=183
03/19/2022 15:21:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000718 on epoch=186
03/19/2022 15:21:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003378 on epoch=189
03/19/2022 15:21:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000695 on epoch=193
03/19/2022 15:21:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002098 on epoch=196
03/19/2022 15:21:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003114 on epoch=199
03/19/2022 15:21:39 - INFO - __main__ - Global step 600 Train loss 0.002001 Classification-F1 0.3547008547008547 on epoch=199
03/19/2022 15:21:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000250 on epoch=203
03/19/2022 15:21:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004684 on epoch=206
03/19/2022 15:21:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000332 on epoch=209
03/19/2022 15:21:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000203 on epoch=213
03/19/2022 15:22:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000069 on epoch=216
03/19/2022 15:22:05 - INFO - __main__ - Global step 650 Train loss 0.001108 Classification-F1 0.3147729789590255 on epoch=216
03/19/2022 15:22:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000159 on epoch=219
03/19/2022 15:22:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000144 on epoch=223
03/19/2022 15:22:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001168 on epoch=226
03/19/2022 15:22:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.038881 on epoch=229
03/19/2022 15:22:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002540 on epoch=233
03/19/2022 15:22:31 - INFO - __main__ - Global step 700 Train loss 0.008578 Classification-F1 0.32120209539564376 on epoch=233
03/19/2022 15:22:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000083 on epoch=236
03/19/2022 15:22:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000146 on epoch=239
03/19/2022 15:22:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000247 on epoch=243
03/19/2022 15:22:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000087 on epoch=246
03/19/2022 15:22:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000926 on epoch=249
03/19/2022 15:22:57 - INFO - __main__ - Global step 750 Train loss 0.000298 Classification-F1 0.3116883116883117 on epoch=249
03/19/2022 15:23:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000220 on epoch=253
03/19/2022 15:23:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000046 on epoch=256
03/19/2022 15:23:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000114 on epoch=259
03/19/2022 15:23:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000046 on epoch=263
03/19/2022 15:23:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000023 on epoch=266
03/19/2022 15:23:23 - INFO - __main__ - Global step 800 Train loss 0.000090 Classification-F1 0.3477078477078477 on epoch=266
03/19/2022 15:23:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007142 on epoch=269
03/19/2022 15:23:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.045867 on epoch=273
03/19/2022 15:23:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005019 on epoch=276
03/19/2022 15:23:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.126090 on epoch=279
03/19/2022 15:23:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.043281 on epoch=283
03/19/2022 15:23:49 - INFO - __main__ - Global step 850 Train loss 0.045480 Classification-F1 0.27735876959479444 on epoch=283
03/19/2022 15:23:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000398 on epoch=286
03/19/2022 15:23:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002901 on epoch=289
03/19/2022 15:24:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001068 on epoch=293
03/19/2022 15:24:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000055 on epoch=296
03/19/2022 15:24:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000127 on epoch=299
03/19/2022 15:24:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:24:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 15:24:15 - INFO - __main__ - Global step 900 Train loss 0.000910 Classification-F1 0.32405278842060453 on epoch=299
03/19/2022 15:24:15 - INFO - __main__ - save last model!
03/19/2022 15:24:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:24:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:24:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:24:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:24:15 - INFO - __main__ - ['neutral']
03/19/2022 15:24:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:24:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:24:15 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:24:22 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:24:23 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 15:24:23 - INFO - __main__ - Printing 3 examples
03/19/2022 15:24:23 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 15:24:23 - INFO - __main__ - ['contradiction']
03/19/2022 15:24:23 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 15:24:23 - INFO - __main__ - ['entailment']
03/19/2022 15:24:23 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 15:24:23 - INFO - __main__ - ['contradiction']
03/19/2022 15:24:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:24:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:24:24 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 15:24:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:24:26 - INFO - __main__ - Starting training!
03/19/2022 15:24:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_100_0.0005_8_predictions.txt
03/19/2022 15:24:43 - INFO - __main__ - Classification-F1 on test data: 0.2522
03/19/2022 15:24:44 - INFO - __main__ - prefix=anli_16_100, lr=0.0005, bsz=8, dev_performance=0.3700288892986661, test_performance=0.2521683245251143
03/19/2022 15:24:44 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.0003, bsz=8 ...
03/19/2022 15:24:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:24:44 - INFO - __main__ - Printing 3 examples
03/19/2022 15:24:44 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:24:44 - INFO - __main__ - ['neutral']
03/19/2022 15:24:44 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:24:44 - INFO - __main__ - ['neutral']
03/19/2022 15:24:44 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:24:44 - INFO - __main__ - ['neutral']
03/19/2022 15:24:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:24:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:24:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:24:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:24:45 - INFO - __main__ - Printing 3 examples
03/19/2022 15:24:45 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:24:45 - INFO - __main__ - ['neutral']
03/19/2022 15:24:45 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:24:45 - INFO - __main__ - ['neutral']
03/19/2022 15:24:45 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:24:45 - INFO - __main__ - ['neutral']
03/19/2022 15:24:45 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:24:45 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:24:45 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:24:56 - INFO - __main__ - Starting training!
03/19/2022 15:25:00 - INFO - __main__ - Step 10 Global step 10 Train loss 24.599522 on epoch=3
03/19/2022 15:25:05 - INFO - __main__ - Step 20 Global step 20 Train loss 19.769762 on epoch=6
03/19/2022 15:25:10 - INFO - __main__ - Step 30 Global step 30 Train loss 14.466150 on epoch=9
03/19/2022 15:25:15 - INFO - __main__ - Step 40 Global step 40 Train loss 11.903758 on epoch=13
03/19/2022 15:25:20 - INFO - __main__ - Step 50 Global step 50 Train loss 11.025898 on epoch=16
03/19/2022 15:25:22 - INFO - __main__ - Global step 50 Train loss 16.353020 Classification-F1 0.0 on epoch=16
03/19/2022 15:25:27 - INFO - __main__ - Step 60 Global step 60 Train loss 9.982864 on epoch=19
03/19/2022 15:25:32 - INFO - __main__ - Step 70 Global step 70 Train loss 9.106046 on epoch=23
03/19/2022 15:25:37 - INFO - __main__ - Step 80 Global step 80 Train loss 8.715548 on epoch=26
03/19/2022 15:25:42 - INFO - __main__ - Step 90 Global step 90 Train loss 7.728837 on epoch=29
03/19/2022 15:25:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.996265 on epoch=33
03/19/2022 15:25:48 - INFO - __main__ - Global step 100 Train loss 8.505912 Classification-F1 0.0 on epoch=33
03/19/2022 15:25:54 - INFO - __main__ - Step 110 Global step 110 Train loss 6.792338 on epoch=36
03/19/2022 15:25:59 - INFO - __main__ - Step 120 Global step 120 Train loss 6.334195 on epoch=39
03/19/2022 15:26:04 - INFO - __main__ - Step 130 Global step 130 Train loss 4.865385 on epoch=43
03/19/2022 15:26:09 - INFO - __main__ - Step 140 Global step 140 Train loss 3.881044 on epoch=46
03/19/2022 15:26:14 - INFO - __main__ - Step 150 Global step 150 Train loss 3.162510 on epoch=49
03/19/2022 15:26:15 - INFO - __main__ - Global step 150 Train loss 5.007094 Classification-F1 0.0 on epoch=49
03/19/2022 15:26:20 - INFO - __main__ - Step 160 Global step 160 Train loss 2.464532 on epoch=53
03/19/2022 15:26:24 - INFO - __main__ - Step 170 Global step 170 Train loss 2.380409 on epoch=56
03/19/2022 15:26:30 - INFO - __main__ - Step 180 Global step 180 Train loss 2.527734 on epoch=59
03/19/2022 15:26:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.943652 on epoch=63
03/19/2022 15:26:40 - INFO - __main__ - Step 200 Global step 200 Train loss 2.131131 on epoch=66
03/19/2022 15:26:41 - INFO - __main__ - Global step 200 Train loss 2.289492 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 15:26:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.953935 on epoch=69
03/19/2022 15:26:51 - INFO - __main__ - Step 220 Global step 220 Train loss 1.661980 on epoch=73
03/19/2022 15:26:56 - INFO - __main__ - Step 230 Global step 230 Train loss 1.941669 on epoch=76
03/19/2022 15:27:01 - INFO - __main__ - Step 240 Global step 240 Train loss 1.759528 on epoch=79
03/19/2022 15:27:06 - INFO - __main__ - Step 250 Global step 250 Train loss 1.939881 on epoch=83
03/19/2022 15:27:07 - INFO - __main__ - Global step 250 Train loss 1.851398 Classification-F1 0.24611708482676223 on epoch=83
03/19/2022 15:27:13 - INFO - __main__ - Step 260 Global step 260 Train loss 1.794768 on epoch=86
03/19/2022 15:27:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.477665 on epoch=89
03/19/2022 15:27:23 - INFO - __main__ - Step 280 Global step 280 Train loss 1.266620 on epoch=93
03/19/2022 15:27:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.321813 on epoch=96
03/19/2022 15:27:33 - INFO - __main__ - Step 300 Global step 300 Train loss 1.381236 on epoch=99
03/19/2022 15:27:34 - INFO - __main__ - Global step 300 Train loss 1.448420 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 15:27:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.397060 on epoch=103
03/19/2022 15:27:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.061877 on epoch=106
03/19/2022 15:27:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.015377 on epoch=109
03/19/2022 15:27:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.104871 on epoch=113
03/19/2022 15:27:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.075285 on epoch=116
03/19/2022 15:28:00 - INFO - __main__ - Global step 350 Train loss 1.130894 Classification-F1 0.2085278555866791 on epoch=116
03/19/2022 15:28:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.137058 on epoch=119
03/19/2022 15:28:10 - INFO - __main__ - Step 370 Global step 370 Train loss 1.107926 on epoch=123
03/19/2022 15:28:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.976845 on epoch=126
03/19/2022 15:28:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.782538 on epoch=129
03/19/2022 15:28:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.698436 on epoch=133
03/19/2022 15:28:25 - INFO - __main__ - Global step 400 Train loss 0.940560 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 15:28:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.931419 on epoch=136
03/19/2022 15:28:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.735672 on epoch=139
03/19/2022 15:28:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.707364 on epoch=143
03/19/2022 15:28:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.604645 on epoch=146
03/19/2022 15:28:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.671431 on epoch=149
03/19/2022 15:28:51 - INFO - __main__ - Global step 450 Train loss 0.730106 Classification-F1 0.22786647314949202 on epoch=149
03/19/2022 15:28:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.577812 on epoch=153
03/19/2022 15:29:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.619216 on epoch=156
03/19/2022 15:29:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.610393 on epoch=159
03/19/2022 15:29:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.483285 on epoch=163
03/19/2022 15:29:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.456720 on epoch=166
03/19/2022 15:29:17 - INFO - __main__ - Global step 500 Train loss 0.549485 Classification-F1 0.3830065359477124 on epoch=166
03/19/2022 15:29:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.450507 on epoch=169
03/19/2022 15:29:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.480463 on epoch=173
03/19/2022 15:29:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.468686 on epoch=176
03/19/2022 15:29:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.337333 on epoch=179
03/19/2022 15:29:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.379686 on epoch=183
03/19/2022 15:29:44 - INFO - __main__ - Global step 550 Train loss 0.423335 Classification-F1 0.3194444444444444 on epoch=183
03/19/2022 15:29:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.300270 on epoch=186
03/19/2022 15:29:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.296225 on epoch=189
03/19/2022 15:29:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.125982 on epoch=193
03/19/2022 15:30:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.147839 on epoch=196
03/19/2022 15:30:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.082731 on epoch=199
03/19/2022 15:30:10 - INFO - __main__ - Global step 600 Train loss 0.190609 Classification-F1 0.3969519758993443 on epoch=199
03/19/2022 15:30:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.059328 on epoch=203
03/19/2022 15:30:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.096542 on epoch=206
03/19/2022 15:30:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.142977 on epoch=209
03/19/2022 15:30:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.048865 on epoch=213
03/19/2022 15:30:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.024941 on epoch=216
03/19/2022 15:30:37 - INFO - __main__ - Global step 650 Train loss 0.074531 Classification-F1 0.32037037037037036 on epoch=216
03/19/2022 15:30:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.065637 on epoch=219
03/19/2022 15:30:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022590 on epoch=223
03/19/2022 15:30:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.004096 on epoch=226
03/19/2022 15:30:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.006767 on epoch=229
03/19/2022 15:31:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.021958 on epoch=233
03/19/2022 15:31:03 - INFO - __main__ - Global step 700 Train loss 0.024210 Classification-F1 0.3477183477183477 on epoch=233
03/19/2022 15:31:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.032537 on epoch=236
03/19/2022 15:31:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.006267 on epoch=239
03/19/2022 15:31:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.008021 on epoch=243
03/19/2022 15:31:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.012305 on epoch=246
03/19/2022 15:31:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.006263 on epoch=249
03/19/2022 15:31:29 - INFO - __main__ - Global step 750 Train loss 0.013078 Classification-F1 0.3455489930975592 on epoch=249
03/19/2022 15:31:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001950 on epoch=253
03/19/2022 15:31:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003775 on epoch=256
03/19/2022 15:31:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.005996 on epoch=259
03/19/2022 15:31:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001632 on epoch=263
03/19/2022 15:31:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001123 on epoch=266
03/19/2022 15:31:55 - INFO - __main__ - Global step 800 Train loss 0.002895 Classification-F1 0.41709770114942524 on epoch=266
03/19/2022 15:32:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.010863 on epoch=269
03/19/2022 15:32:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001334 on epoch=273
03/19/2022 15:32:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005269 on epoch=276
03/19/2022 15:32:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001522 on epoch=279
03/19/2022 15:32:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001183 on epoch=283
03/19/2022 15:32:21 - INFO - __main__ - Global step 850 Train loss 0.004034 Classification-F1 0.395109792773864 on epoch=283
03/19/2022 15:32:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001033 on epoch=286
03/19/2022 15:32:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.005243 on epoch=289
03/19/2022 15:32:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000634 on epoch=293
03/19/2022 15:32:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002366 on epoch=296
03/19/2022 15:32:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000961 on epoch=299
03/19/2022 15:32:47 - INFO - __main__ - Global step 900 Train loss 0.002048 Classification-F1 0.3803418803418803 on epoch=299
03/19/2022 15:32:47 - INFO - __main__ - save last model!
03/19/2022 15:32:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:32:47 - INFO - __main__ - Printing 3 examples
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 15:32:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:32:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:32:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:32:47 - INFO - __main__ - Printing 3 examples
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:32:47 - INFO - __main__ - ['neutral']
03/19/2022 15:32:47 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:32:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:32:47 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:32:54 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:32:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 15:32:54 - INFO - __main__ - Printing 3 examples
03/19/2022 15:32:54 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 15:32:54 - INFO - __main__ - ['contradiction']
03/19/2022 15:32:54 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 15:32:54 - INFO - __main__ - ['entailment']
03/19/2022 15:32:54 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 15:32:54 - INFO - __main__ - ['contradiction']
03/19/2022 15:32:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:32:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:32:56 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 15:32:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:32:58 - INFO - __main__ - Starting training!
03/19/2022 15:33:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_100_0.0003_8_predictions.txt
03/19/2022 15:33:14 - INFO - __main__ - Classification-F1 on test data: 0.3314
03/19/2022 15:33:14 - INFO - __main__ - prefix=anli_16_100, lr=0.0003, bsz=8, dev_performance=0.41709770114942524, test_performance=0.331365587321266
03/19/2022 15:33:14 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.0002, bsz=8 ...
03/19/2022 15:33:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:33:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:33:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:33:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:33:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:33:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:33:15 - INFO - __main__ - ['neutral']
03/19/2022 15:33:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:33:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:33:15 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:33:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:33:27 - INFO - __main__ - Starting training!
03/19/2022 15:33:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.985876 on epoch=3
03/19/2022 15:33:36 - INFO - __main__ - Step 20 Global step 20 Train loss 18.059238 on epoch=6
03/19/2022 15:33:41 - INFO - __main__ - Step 30 Global step 30 Train loss 15.119387 on epoch=9
03/19/2022 15:33:46 - INFO - __main__ - Step 40 Global step 40 Train loss 11.614658 on epoch=13
03/19/2022 15:33:51 - INFO - __main__ - Step 50 Global step 50 Train loss 11.791349 on epoch=16
03/19/2022 15:33:52 - INFO - __main__ - Global step 50 Train loss 16.114100 Classification-F1 0.07142857142857142 on epoch=16
03/19/2022 15:33:58 - INFO - __main__ - Step 60 Global step 60 Train loss 10.559164 on epoch=19
03/19/2022 15:34:03 - INFO - __main__ - Step 70 Global step 70 Train loss 10.264511 on epoch=23
03/19/2022 15:34:08 - INFO - __main__ - Step 80 Global step 80 Train loss 9.959099 on epoch=26
03/19/2022 15:34:13 - INFO - __main__ - Step 90 Global step 90 Train loss 9.008096 on epoch=29
03/19/2022 15:34:18 - INFO - __main__ - Step 100 Global step 100 Train loss 8.270582 on epoch=33
03/19/2022 15:34:19 - INFO - __main__ - Global step 100 Train loss 9.612290 Classification-F1 0.0627177700348432 on epoch=33
03/19/2022 15:34:24 - INFO - __main__ - Step 110 Global step 110 Train loss 8.671756 on epoch=36
03/19/2022 15:34:29 - INFO - __main__ - Step 120 Global step 120 Train loss 7.669721 on epoch=39
03/19/2022 15:34:34 - INFO - __main__ - Step 130 Global step 130 Train loss 6.944086 on epoch=43
03/19/2022 15:34:39 - INFO - __main__ - Step 140 Global step 140 Train loss 7.044395 on epoch=46
03/19/2022 15:34:44 - INFO - __main__ - Step 150 Global step 150 Train loss 6.151842 on epoch=49
03/19/2022 15:34:45 - INFO - __main__ - Global step 150 Train loss 7.296360 Classification-F1 0.023529411764705882 on epoch=49
03/19/2022 15:34:50 - INFO - __main__ - Step 160 Global step 160 Train loss 5.115983 on epoch=53
03/19/2022 15:34:55 - INFO - __main__ - Step 170 Global step 170 Train loss 4.526613 on epoch=56
03/19/2022 15:35:00 - INFO - __main__ - Step 180 Global step 180 Train loss 3.462995 on epoch=59
03/19/2022 15:35:05 - INFO - __main__ - Step 190 Global step 190 Train loss 2.848080 on epoch=63
03/19/2022 15:35:10 - INFO - __main__ - Step 200 Global step 200 Train loss 2.746192 on epoch=66
03/19/2022 15:35:10 - INFO - __main__ - Global step 200 Train loss 3.739973 Classification-F1 0.2333333333333333 on epoch=66
03/19/2022 15:35:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.263839 on epoch=69
03/19/2022 15:35:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.831905 on epoch=73
03/19/2022 15:35:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.518411 on epoch=76
03/19/2022 15:35:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.451327 on epoch=79
03/19/2022 15:35:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.460858 on epoch=83
03/19/2022 15:35:37 - INFO - __main__ - Global step 250 Train loss 0.705268 Classification-F1 0.38383838383838387 on epoch=83
03/19/2022 15:35:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.352604 on epoch=86
03/19/2022 15:35:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.326454 on epoch=89
03/19/2022 15:35:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.322711 on epoch=93
03/19/2022 15:35:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.352771 on epoch=96
03/19/2022 15:36:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.206004 on epoch=99
03/19/2022 15:36:04 - INFO - __main__ - Global step 300 Train loss 0.312109 Classification-F1 0.23615819209039554 on epoch=99
03/19/2022 15:36:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.992860 on epoch=103
03/19/2022 15:36:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.125206 on epoch=106
03/19/2022 15:36:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.111699 on epoch=109
03/19/2022 15:36:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.114938 on epoch=113
03/19/2022 15:36:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.035827 on epoch=116
03/19/2022 15:36:30 - INFO - __main__ - Global step 350 Train loss 0.276106 Classification-F1 0.3493416493416493 on epoch=116
03/19/2022 15:36:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.085841 on epoch=119
03/19/2022 15:36:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.058403 on epoch=123
03/19/2022 15:36:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.019145 on epoch=126
03/19/2022 15:36:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.024857 on epoch=129
03/19/2022 15:36:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.013362 on epoch=133
03/19/2022 15:36:56 - INFO - __main__ - Global step 400 Train loss 0.040321 Classification-F1 0.4271255060728745 on epoch=133
03/19/2022 15:37:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.008907 on epoch=136
03/19/2022 15:37:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004211 on epoch=139
03/19/2022 15:37:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003366 on epoch=143
03/19/2022 15:37:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005443 on epoch=146
03/19/2022 15:37:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009317 on epoch=149
03/19/2022 15:37:22 - INFO - __main__ - Global step 450 Train loss 0.006249 Classification-F1 0.31982570806100213 on epoch=149
03/19/2022 15:37:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001564 on epoch=153
03/19/2022 15:37:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003138 on epoch=156
03/19/2022 15:37:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002091 on epoch=159
03/19/2022 15:37:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000764 on epoch=163
03/19/2022 15:37:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004247 on epoch=166
03/19/2022 15:37:48 - INFO - __main__ - Global step 500 Train loss 0.002361 Classification-F1 0.3685185185185185 on epoch=166
03/19/2022 15:37:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005256 on epoch=169
03/19/2022 15:37:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000696 on epoch=173
03/19/2022 15:38:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003342 on epoch=176
03/19/2022 15:38:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002767 on epoch=179
03/19/2022 15:38:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001551 on epoch=183
03/19/2022 15:38:13 - INFO - __main__ - Global step 550 Train loss 0.002722 Classification-F1 0.30498791474401227 on epoch=183
03/19/2022 15:38:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002530 on epoch=186
03/19/2022 15:38:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001690 on epoch=189
03/19/2022 15:38:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002839 on epoch=193
03/19/2022 15:38:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.017885 on epoch=196
03/19/2022 15:38:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007310 on epoch=199
03/19/2022 15:38:39 - INFO - __main__ - Global step 600 Train loss 0.006451 Classification-F1 0.33419913419913416 on epoch=199
03/19/2022 15:38:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002378 on epoch=203
03/19/2022 15:38:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000693 on epoch=206
03/19/2022 15:38:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.025348 on epoch=209
03/19/2022 15:38:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000784 on epoch=213
03/19/2022 15:39:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001424 on epoch=216
03/19/2022 15:39:05 - INFO - __main__ - Global step 650 Train loss 0.006125 Classification-F1 0.4162798300098071 on epoch=216
03/19/2022 15:39:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000405 on epoch=219
03/19/2022 15:39:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000546 on epoch=223
03/19/2022 15:39:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001158 on epoch=226
03/19/2022 15:39:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001235 on epoch=229
03/19/2022 15:39:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001361 on epoch=233
03/19/2022 15:39:31 - INFO - __main__ - Global step 700 Train loss 0.000941 Classification-F1 0.31937984496124033 on epoch=233
03/19/2022 15:39:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001068 on epoch=236
03/19/2022 15:39:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004289 on epoch=239
03/19/2022 15:39:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000827 on epoch=243
03/19/2022 15:39:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000647 on epoch=246
03/19/2022 15:39:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.102501 on epoch=249
03/19/2022 15:39:57 - INFO - __main__ - Global step 750 Train loss 0.021866 Classification-F1 0.44370959194488596 on epoch=249
03/19/2022 15:40:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001398 on epoch=253
03/19/2022 15:40:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.018360 on epoch=256
03/19/2022 15:40:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.147161 on epoch=259
03/19/2022 15:40:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.068115 on epoch=263
03/19/2022 15:40:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.015774 on epoch=266
03/19/2022 15:40:23 - INFO - __main__ - Global step 800 Train loss 0.050161 Classification-F1 0.34982686909664595 on epoch=266
03/19/2022 15:40:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000465 on epoch=269
03/19/2022 15:40:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000623 on epoch=273
03/19/2022 15:40:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001718 on epoch=276
03/19/2022 15:40:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001801 on epoch=279
03/19/2022 15:40:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000329 on epoch=283
03/19/2022 15:40:49 - INFO - __main__ - Global step 850 Train loss 0.000987 Classification-F1 0.38383838383838387 on epoch=283
03/19/2022 15:40:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000265 on epoch=286
03/19/2022 15:40:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003123 on epoch=289
03/19/2022 15:41:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000245 on epoch=293
03/19/2022 15:41:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000238 on epoch=296
03/19/2022 15:41:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001042 on epoch=299
03/19/2022 15:41:15 - INFO - __main__ - Global step 900 Train loss 0.000983 Classification-F1 0.300197628458498 on epoch=299
03/19/2022 15:41:15 - INFO - __main__ - save last model!
03/19/2022 15:41:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:41:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 15:41:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:41:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:41:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:41:15 - INFO - __main__ - Printing 3 examples
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:41:15 - INFO - __main__ - ['neutral']
03/19/2022 15:41:15 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:41:15 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:41:15 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:41:21 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:41:22 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 15:41:22 - INFO - __main__ - Printing 3 examples
03/19/2022 15:41:22 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 15:41:22 - INFO - __main__ - ['contradiction']
03/19/2022 15:41:22 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 15:41:22 - INFO - __main__ - ['entailment']
03/19/2022 15:41:22 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 15:41:22 - INFO - __main__ - ['contradiction']
03/19/2022 15:41:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:41:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:41:24 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 15:41:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:41:26 - INFO - __main__ - Starting training!
03/19/2022 15:41:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_100_0.0002_8_predictions.txt
03/19/2022 15:41:43 - INFO - __main__ - Classification-F1 on test data: 0.3323
03/19/2022 15:41:43 - INFO - __main__ - prefix=anli_16_100, lr=0.0002, bsz=8, dev_performance=0.44370959194488596, test_performance=0.3323224454469017
03/19/2022 15:41:43 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.0001, bsz=8 ...
03/19/2022 15:41:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:41:44 - INFO - __main__ - Printing 3 examples
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian:    ; 3 May 1925  25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:41:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:41:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:41:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:41:44 - INFO - __main__ - Printing 3 examples
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946  12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (SA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current SA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
03/19/2022 15:41:44 - INFO - __main__ - ['neutral']
03/19/2022 15:41:44 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:41:44 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:41:44 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:41:56 - INFO - __main__ - Starting training!
03/19/2022 15:42:00 - INFO - __main__ - Step 10 Global step 10 Train loss 24.279110 on epoch=3
03/19/2022 15:42:04 - INFO - __main__ - Step 20 Global step 20 Train loss 21.773415 on epoch=6
03/19/2022 15:42:09 - INFO - __main__ - Step 30 Global step 30 Train loss 17.207525 on epoch=9
03/19/2022 15:42:14 - INFO - __main__ - Step 40 Global step 40 Train loss 14.513575 on epoch=13
03/19/2022 15:42:19 - INFO - __main__ - Step 50 Global step 50 Train loss 13.680684 on epoch=16
03/19/2022 15:42:26 - INFO - __main__ - Global step 50 Train loss 18.290861 Classification-F1 0.0 on epoch=16
03/19/2022 15:42:32 - INFO - __main__ - Step 60 Global step 60 Train loss 12.990069 on epoch=19
03/19/2022 15:42:36 - INFO - __main__ - Step 70 Global step 70 Train loss 12.329251 on epoch=23
03/19/2022 15:42:41 - INFO - __main__ - Step 80 Global step 80 Train loss 11.929899 on epoch=26
03/19/2022 15:42:46 - INFO - __main__ - Step 90 Global step 90 Train loss 11.842033 on epoch=29
03/19/2022 15:42:51 - INFO - __main__ - Step 100 Global step 100 Train loss 11.300304 on epoch=33
03/19/2022 15:42:52 - INFO - __main__ - Global step 100 Train loss 12.078311 Classification-F1 0.0 on epoch=33
03/19/2022 15:42:57 - INFO - __main__ - Step 110 Global step 110 Train loss 10.816689 on epoch=36
03/19/2022 15:43:02 - INFO - __main__ - Step 120 Global step 120 Train loss 11.244028 on epoch=39
03/19/2022 15:43:07 - INFO - __main__ - Step 130 Global step 130 Train loss 10.159324 on epoch=43
03/19/2022 15:43:12 - INFO - __main__ - Step 140 Global step 140 Train loss 10.208005 on epoch=46
03/19/2022 15:43:17 - INFO - __main__ - Step 150 Global step 150 Train loss 10.453112 on epoch=49
03/19/2022 15:43:18 - INFO - __main__ - Global step 150 Train loss 10.576232 Classification-F1 0.0 on epoch=49
03/19/2022 15:43:23 - INFO - __main__ - Step 160 Global step 160 Train loss 9.450329 on epoch=53
03/19/2022 15:43:28 - INFO - __main__ - Step 170 Global step 170 Train loss 9.258436 on epoch=56
03/19/2022 15:43:33 - INFO - __main__ - Step 180 Global step 180 Train loss 8.897729 on epoch=59
03/19/2022 15:43:38 - INFO - __main__ - Step 190 Global step 190 Train loss 8.921599 on epoch=63
03/19/2022 15:43:43 - INFO - __main__ - Step 200 Global step 200 Train loss 8.941122 on epoch=66
03/19/2022 15:43:44 - INFO - __main__ - Global step 200 Train loss 9.093843 Classification-F1 0.0 on epoch=66
03/19/2022 15:43:49 - INFO - __main__ - Step 210 Global step 210 Train loss 8.179027 on epoch=69
03/19/2022 15:43:54 - INFO - __main__ - Step 220 Global step 220 Train loss 8.319910 on epoch=73
03/19/2022 15:43:59 - INFO - __main__ - Step 230 Global step 230 Train loss 7.617733 on epoch=76
03/19/2022 15:44:04 - INFO - __main__ - Step 240 Global step 240 Train loss 8.103316 on epoch=79
03/19/2022 15:44:09 - INFO - __main__ - Step 250 Global step 250 Train loss 7.734134 on epoch=83
03/19/2022 15:44:10 - INFO - __main__ - Global step 250 Train loss 7.990825 Classification-F1 0.0 on epoch=83
03/19/2022 15:44:15 - INFO - __main__ - Step 260 Global step 260 Train loss 7.442535 on epoch=86
03/19/2022 15:44:20 - INFO - __main__ - Step 270 Global step 270 Train loss 7.318263 on epoch=89
03/19/2022 15:44:25 - INFO - __main__ - Step 280 Global step 280 Train loss 7.351883 on epoch=93
03/19/2022 15:44:30 - INFO - __main__ - Step 290 Global step 290 Train loss 6.723672 on epoch=96
03/19/2022 15:44:35 - INFO - __main__ - Step 300 Global step 300 Train loss 6.567253 on epoch=99
03/19/2022 15:44:36 - INFO - __main__ - Global step 300 Train loss 7.080721 Classification-F1 0.0 on epoch=99
03/19/2022 15:44:41 - INFO - __main__ - Step 310 Global step 310 Train loss 6.089034 on epoch=103
03/19/2022 15:44:46 - INFO - __main__ - Step 320 Global step 320 Train loss 5.579828 on epoch=106
03/19/2022 15:44:51 - INFO - __main__ - Step 330 Global step 330 Train loss 5.353449 on epoch=109
03/19/2022 15:44:56 - INFO - __main__ - Step 340 Global step 340 Train loss 5.372260 on epoch=113
03/19/2022 15:45:01 - INFO - __main__ - Step 350 Global step 350 Train loss 4.884266 on epoch=116
03/19/2022 15:45:02 - INFO - __main__ - Global step 350 Train loss 5.455767 Classification-F1 0.0 on epoch=116
03/19/2022 15:45:07 - INFO - __main__ - Step 360 Global step 360 Train loss 4.538621 on epoch=119
03/19/2022 15:45:12 - INFO - __main__ - Step 370 Global step 370 Train loss 4.356885 on epoch=123
03/19/2022 15:45:17 - INFO - __main__ - Step 380 Global step 380 Train loss 3.784359 on epoch=126
03/19/2022 15:45:22 - INFO - __main__ - Step 390 Global step 390 Train loss 3.870624 on epoch=129
03/19/2022 15:45:27 - INFO - __main__ - Step 400 Global step 400 Train loss 3.177230 on epoch=133
03/19/2022 15:45:28 - INFO - __main__ - Global step 400 Train loss 3.945544 Classification-F1 0.0 on epoch=133
03/19/2022 15:45:33 - INFO - __main__ - Step 410 Global step 410 Train loss 2.991730 on epoch=136
03/19/2022 15:45:38 - INFO - __main__ - Step 420 Global step 420 Train loss 3.055188 on epoch=139
03/19/2022 15:45:43 - INFO - __main__ - Step 430 Global step 430 Train loss 2.737537 on epoch=143
03/19/2022 15:45:48 - INFO - __main__ - Step 440 Global step 440 Train loss 2.467674 on epoch=146
03/19/2022 15:45:53 - INFO - __main__ - Step 450 Global step 450 Train loss 2.050843 on epoch=149
03/19/2022 15:45:54 - INFO - __main__ - Global step 450 Train loss 2.660594 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 15:46:00 - INFO - __main__ - Step 460 Global step 460 Train loss 2.341051 on epoch=153
03/19/2022 15:46:04 - INFO - __main__ - Step 470 Global step 470 Train loss 2.369200 on epoch=156
03/19/2022 15:46:09 - INFO - __main__ - Step 480 Global step 480 Train loss 2.766693 on epoch=159
03/19/2022 15:46:14 - INFO - __main__ - Step 490 Global step 490 Train loss 2.202038 on epoch=163
03/19/2022 15:46:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.767990 on epoch=166
03/19/2022 15:46:20 - INFO - __main__ - Global step 500 Train loss 2.289395 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 15:46:25 - INFO - __main__ - Step 510 Global step 510 Train loss 2.029315 on epoch=169
03/19/2022 15:46:30 - INFO - __main__ - Step 520 Global step 520 Train loss 2.288934 on epoch=173
03/19/2022 15:46:35 - INFO - __main__ - Step 530 Global step 530 Train loss 1.806087 on epoch=176
03/19/2022 15:46:40 - INFO - __main__ - Step 540 Global step 540 Train loss 2.271824 on epoch=179
03/19/2022 15:46:45 - INFO - __main__ - Step 550 Global step 550 Train loss 2.562201 on epoch=183
03/19/2022 15:46:46 - INFO - __main__ - Global step 550 Train loss 2.191672 Classification-F1 0.16666666666666666 on epoch=183
03/19/2022 15:46:51 - INFO - __main__ - Step 560 Global step 560 Train loss 2.077693 on epoch=186
03/19/2022 15:46:56 - INFO - __main__ - Step 570 Global step 570 Train loss 2.116658 on epoch=189
03/19/2022 15:47:01 - INFO - __main__ - Step 580 Global step 580 Train loss 2.888938 on epoch=193
03/19/2022 15:47:06 - INFO - __main__ - Step 590 Global step 590 Train loss 1.874321 on epoch=196
03/19/2022 15:47:11 - INFO - __main__ - Step 600 Global step 600 Train loss 2.137158 on epoch=199
03/19/2022 15:47:12 - INFO - __main__ - Global step 600 Train loss 2.218954 Classification-F1 0.16666666666666666 on epoch=199
03/19/2022 15:47:17 - INFO - __main__ - Step 610 Global step 610 Train loss 2.014710 on epoch=203
03/19/2022 15:47:22 - INFO - __main__ - Step 620 Global step 620 Train loss 2.002818 on epoch=206
03/19/2022 15:47:27 - INFO - __main__ - Step 630 Global step 630 Train loss 2.039438 on epoch=209
03/19/2022 15:47:32 - INFO - __main__ - Step 640 Global step 640 Train loss 1.701537 on epoch=213
03/19/2022 15:47:37 - INFO - __main__ - Step 650 Global step 650 Train loss 2.298721 on epoch=216
03/19/2022 15:47:37 - INFO - __main__ - Global step 650 Train loss 2.011445 Classification-F1 0.16666666666666666 on epoch=216
03/19/2022 15:47:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.785492 on epoch=219
03/19/2022 15:47:47 - INFO - __main__ - Step 670 Global step 670 Train loss 2.394973 on epoch=223
03/19/2022 15:47:52 - INFO - __main__ - Step 680 Global step 680 Train loss 1.811737 on epoch=226
03/19/2022 15:47:57 - INFO - __main__ - Step 690 Global step 690 Train loss 1.878900 on epoch=229
03/19/2022 15:48:02 - INFO - __main__ - Step 700 Global step 700 Train loss 1.955703 on epoch=233
03/19/2022 15:48:03 - INFO - __main__ - Global step 700 Train loss 1.965361 Classification-F1 0.16666666666666666 on epoch=233
03/19/2022 15:48:08 - INFO - __main__ - Step 710 Global step 710 Train loss 1.852798 on epoch=236
03/19/2022 15:48:13 - INFO - __main__ - Step 720 Global step 720 Train loss 1.875003 on epoch=239
03/19/2022 15:48:18 - INFO - __main__ - Step 730 Global step 730 Train loss 1.761898 on epoch=243
03/19/2022 15:48:23 - INFO - __main__ - Step 740 Global step 740 Train loss 1.812736 on epoch=246
03/19/2022 15:48:28 - INFO - __main__ - Step 750 Global step 750 Train loss 1.485772 on epoch=249
03/19/2022 15:48:29 - INFO - __main__ - Global step 750 Train loss 1.757641 Classification-F1 0.2333333333333333 on epoch=249
03/19/2022 15:48:35 - INFO - __main__ - Step 760 Global step 760 Train loss 1.601738 on epoch=253
03/19/2022 15:48:40 - INFO - __main__ - Step 770 Global step 770 Train loss 1.830861 on epoch=256
03/19/2022 15:48:45 - INFO - __main__ - Step 780 Global step 780 Train loss 1.614520 on epoch=259
03/19/2022 15:48:50 - INFO - __main__ - Step 790 Global step 790 Train loss 1.553257 on epoch=263
03/19/2022 15:48:55 - INFO - __main__ - Step 800 Global step 800 Train loss 1.565350 on epoch=266
03/19/2022 15:48:55 - INFO - __main__ - Global step 800 Train loss 1.633145 Classification-F1 0.16666666666666666 on epoch=266
03/19/2022 15:49:00 - INFO - __main__ - Step 810 Global step 810 Train loss 1.795397 on epoch=269
03/19/2022 15:49:05 - INFO - __main__ - Step 820 Global step 820 Train loss 1.634585 on epoch=273
03/19/2022 15:49:10 - INFO - __main__ - Step 830 Global step 830 Train loss 1.558449 on epoch=276
03/19/2022 15:49:15 - INFO - __main__ - Step 840 Global step 840 Train loss 1.619818 on epoch=279
03/19/2022 15:49:20 - INFO - __main__ - Step 850 Global step 850 Train loss 1.536279 on epoch=283
03/19/2022 15:49:21 - INFO - __main__ - Global step 850 Train loss 1.628906 Classification-F1 0.16666666666666666 on epoch=283
03/19/2022 15:49:26 - INFO - __main__ - Step 860 Global step 860 Train loss 1.273237 on epoch=286
03/19/2022 15:49:31 - INFO - __main__ - Step 870 Global step 870 Train loss 1.291679 on epoch=289
03/19/2022 15:49:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.235604 on epoch=293
03/19/2022 15:49:41 - INFO - __main__ - Step 890 Global step 890 Train loss 1.347473 on epoch=296
03/19/2022 15:49:46 - INFO - __main__ - Step 900 Global step 900 Train loss 1.400038 on epoch=299
03/19/2022 15:49:47 - INFO - __main__ - Global step 900 Train loss 1.309606 Classification-F1 0.16666666666666666 on epoch=299
03/19/2022 15:49:47 - INFO - __main__ - save last model!
03/19/2022 15:49:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:49:47 - INFO - __main__ - Printing 3 examples
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 15:49:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:49:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:49:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:49:47 - INFO - __main__ - Printing 3 examples
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 15:49:47 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:47 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:49:47 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:49:47 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:49:54 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:49:55 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 15:49:55 - INFO - __main__ - Printing 3 examples
03/19/2022 15:49:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 15:49:55 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 15:49:55 - INFO - __main__ - ['entailment']
03/19/2022 15:49:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 15:49:55 - INFO - __main__ - ['contradiction']
03/19/2022 15:49:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:49:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:49:56 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 15:50:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:50:00 - INFO - __main__ - Starting training!
03/19/2022 15:50:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_100_0.0001_8_predictions.txt
03/19/2022 15:50:12 - INFO - __main__ - Classification-F1 on test data: 0.2177
03/19/2022 15:50:12 - INFO - __main__ - prefix=anli_16_100, lr=0.0001, bsz=8, dev_performance=0.2333333333333333, test_performance=0.21767433565186375
03/19/2022 15:50:12 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.0005, bsz=8 ...
03/19/2022 15:50:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:50:13 - INFO - __main__ - Printing 3 examples
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:50:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:50:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:50:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:50:13 - INFO - __main__ - Printing 3 examples
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 15:50:13 - INFO - __main__ - ['contradiction']
03/19/2022 15:50:13 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:50:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:50:13 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:50:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:50:24 - INFO - __main__ - Starting training!
03/19/2022 15:50:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.445116 on epoch=3
03/19/2022 15:50:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.321678 on epoch=6
03/19/2022 15:50:38 - INFO - __main__ - Step 30 Global step 30 Train loss 12.577871 on epoch=9
03/19/2022 15:50:43 - INFO - __main__ - Step 40 Global step 40 Train loss 9.943021 on epoch=13
03/19/2022 15:50:48 - INFO - __main__ - Step 50 Global step 50 Train loss 8.856099 on epoch=16
03/19/2022 15:50:49 - INFO - __main__ - Global step 50 Train loss 14.428756 Classification-F1 0.01818181818181818 on epoch=16
03/19/2022 15:50:55 - INFO - __main__ - Step 60 Global step 60 Train loss 8.244021 on epoch=19
03/19/2022 15:51:00 - INFO - __main__ - Step 70 Global step 70 Train loss 6.864455 on epoch=23
03/19/2022 15:51:05 - INFO - __main__ - Step 80 Global step 80 Train loss 6.066892 on epoch=26
03/19/2022 15:51:10 - INFO - __main__ - Step 90 Global step 90 Train loss 3.517105 on epoch=29
03/19/2022 15:51:15 - INFO - __main__ - Step 100 Global step 100 Train loss 2.814526 on epoch=33
03/19/2022 15:51:16 - INFO - __main__ - Global step 100 Train loss 5.501400 Classification-F1 0.16666666666666666 on epoch=33
03/19/2022 15:51:22 - INFO - __main__ - Step 110 Global step 110 Train loss 2.060370 on epoch=36
03/19/2022 15:51:27 - INFO - __main__ - Step 120 Global step 120 Train loss 2.481110 on epoch=39
03/19/2022 15:51:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.999067 on epoch=43
03/19/2022 15:51:37 - INFO - __main__ - Step 140 Global step 140 Train loss 1.770890 on epoch=46
03/19/2022 15:51:42 - INFO - __main__ - Step 150 Global step 150 Train loss 2.012037 on epoch=49
03/19/2022 15:51:43 - INFO - __main__ - Global step 150 Train loss 2.064695 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 15:51:48 - INFO - __main__ - Step 160 Global step 160 Train loss 1.640697 on epoch=53
03/19/2022 15:51:53 - INFO - __main__ - Step 170 Global step 170 Train loss 1.859433 on epoch=56
03/19/2022 15:51:58 - INFO - __main__ - Step 180 Global step 180 Train loss 1.536211 on epoch=59
03/19/2022 15:52:03 - INFO - __main__ - Step 190 Global step 190 Train loss 1.346381 on epoch=63
03/19/2022 15:52:08 - INFO - __main__ - Step 200 Global step 200 Train loss 1.520872 on epoch=66
03/19/2022 15:52:09 - INFO - __main__ - Global step 200 Train loss 1.580719 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 15:52:14 - INFO - __main__ - Step 210 Global step 210 Train loss 1.332997 on epoch=69
03/19/2022 15:52:19 - INFO - __main__ - Step 220 Global step 220 Train loss 1.345275 on epoch=73
03/19/2022 15:52:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.305659 on epoch=76
03/19/2022 15:52:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.859475 on epoch=79
03/19/2022 15:52:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.941273 on epoch=83
03/19/2022 15:52:35 - INFO - __main__ - Global step 250 Train loss 1.156936 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 15:52:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.865983 on epoch=86
03/19/2022 15:52:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.827645 on epoch=89
03/19/2022 15:52:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.745280 on epoch=93
03/19/2022 15:52:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.893223 on epoch=96
03/19/2022 15:53:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.710447 on epoch=99
03/19/2022 15:53:01 - INFO - __main__ - Global step 300 Train loss 0.808516 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 15:53:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.638762 on epoch=103
03/19/2022 15:53:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.606800 on epoch=106
03/19/2022 15:53:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.560410 on epoch=109
03/19/2022 15:53:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.714773 on epoch=113
03/19/2022 15:53:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.649836 on epoch=116
03/19/2022 15:53:27 - INFO - __main__ - Global step 350 Train loss 0.634116 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 15:53:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.620347 on epoch=119
03/19/2022 15:53:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.480037 on epoch=123
03/19/2022 15:53:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.583904 on epoch=126
03/19/2022 15:53:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.488047 on epoch=129
03/19/2022 15:53:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.494517 on epoch=133
03/19/2022 15:53:54 - INFO - __main__ - Global step 400 Train loss 0.533370 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 15:53:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.483911 on epoch=136
03/19/2022 15:54:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.544412 on epoch=139
03/19/2022 15:54:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.608201 on epoch=143
03/19/2022 15:54:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.497710 on epoch=146
03/19/2022 15:54:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.512182 on epoch=149
03/19/2022 15:54:20 - INFO - __main__ - Global step 450 Train loss 0.529283 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 15:54:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.577283 on epoch=153
03/19/2022 15:54:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.435457 on epoch=156
03/19/2022 15:54:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.473307 on epoch=159
03/19/2022 15:54:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.539433 on epoch=163
03/19/2022 15:54:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.447480 on epoch=166
03/19/2022 15:54:47 - INFO - __main__ - Global step 500 Train loss 0.494592 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 15:54:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.473410 on epoch=169
03/19/2022 15:54:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.648684 on epoch=173
03/19/2022 15:55:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.511797 on epoch=176
03/19/2022 15:55:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.503346 on epoch=179
03/19/2022 15:55:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.447257 on epoch=183
03/19/2022 15:55:13 - INFO - __main__ - Global step 550 Train loss 0.516899 Classification-F1 0.16666666666666666 on epoch=183
03/19/2022 15:55:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.407980 on epoch=186
03/19/2022 15:55:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.498242 on epoch=189
03/19/2022 15:55:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.497365 on epoch=193
03/19/2022 15:55:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.455293 on epoch=196
03/19/2022 15:55:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.487868 on epoch=199
03/19/2022 15:55:40 - INFO - __main__ - Global step 600 Train loss 0.469350 Classification-F1 0.16666666666666666 on epoch=199
03/19/2022 15:55:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.468990 on epoch=203
03/19/2022 15:55:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.450388 on epoch=206
03/19/2022 15:55:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.418386 on epoch=209
03/19/2022 15:56:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.447500 on epoch=213
03/19/2022 15:56:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.388315 on epoch=216
03/19/2022 15:56:06 - INFO - __main__ - Global step 650 Train loss 0.434716 Classification-F1 0.16666666666666666 on epoch=216
03/19/2022 15:56:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.424434 on epoch=219
03/19/2022 15:56:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.381202 on epoch=223
03/19/2022 15:56:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.388845 on epoch=226
03/19/2022 15:56:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.427635 on epoch=229
03/19/2022 15:56:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.386106 on epoch=233
03/19/2022 15:56:32 - INFO - __main__ - Global step 700 Train loss 0.401644 Classification-F1 0.16666666666666666 on epoch=233
03/19/2022 15:56:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.408996 on epoch=236
03/19/2022 15:56:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.417241 on epoch=239
03/19/2022 15:56:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.383411 on epoch=243
03/19/2022 15:56:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.390711 on epoch=246
03/19/2022 15:56:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.413937 on epoch=249
03/19/2022 15:56:58 - INFO - __main__ - Global step 750 Train loss 0.402859 Classification-F1 0.16666666666666666 on epoch=249
03/19/2022 15:57:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.399796 on epoch=253
03/19/2022 15:57:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.369146 on epoch=256
03/19/2022 15:57:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.382399 on epoch=259
03/19/2022 15:57:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.395621 on epoch=263
03/19/2022 15:57:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.378269 on epoch=266
03/19/2022 15:57:25 - INFO - __main__ - Global step 800 Train loss 0.385046 Classification-F1 0.16666666666666666 on epoch=266
03/19/2022 15:57:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.396172 on epoch=269
03/19/2022 15:57:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.399062 on epoch=273
03/19/2022 15:57:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.408975 on epoch=276
03/19/2022 15:57:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.420975 on epoch=279
03/19/2022 15:57:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.384422 on epoch=283
03/19/2022 15:57:51 - INFO - __main__ - Global step 850 Train loss 0.401921 Classification-F1 0.16666666666666666 on epoch=283
03/19/2022 15:57:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.393916 on epoch=286
03/19/2022 15:58:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.393019 on epoch=289
03/19/2022 15:58:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.391920 on epoch=293
03/19/2022 15:58:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.382555 on epoch=296
03/19/2022 15:58:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.405631 on epoch=299
03/19/2022 15:58:17 - INFO - __main__ - Global step 900 Train loss 0.393408 Classification-F1 0.2085278555866791 on epoch=299
03/19/2022 15:58:18 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:58:18 - INFO - __main__ - Printing 3 examples
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 15:58:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:58:18 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:58:18 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:58:18 - INFO - __main__ - Printing 3 examples
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 15:58:18 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:18 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:58:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:58:18 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:58:18 - INFO - __main__ - save last model!
03/19/2022 15:58:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 15:58:25 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 15:58:25 - INFO - __main__ - Printing 3 examples
03/19/2022 15:58:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 15:58:25 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 15:58:25 - INFO - __main__ - ['entailment']
03/19/2022 15:58:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 15:58:25 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:58:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:58:27 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 15:58:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:58:29 - INFO - __main__ - Starting training!
03/19/2022 15:58:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_13_0.0005_8_predictions.txt
03/19/2022 15:58:40 - INFO - __main__ - Classification-F1 on test data: 0.1720
03/19/2022 15:58:41 - INFO - __main__ - prefix=anli_16_13, lr=0.0005, bsz=8, dev_performance=0.2085278555866791, test_performance=0.17199404578404517
03/19/2022 15:58:41 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.0003, bsz=8 ...
03/19/2022 15:58:41 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:58:41 - INFO - __main__ - Printing 3 examples
03/19/2022 15:58:41 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 15:58:41 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:41 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 15:58:41 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:41 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 15:58:41 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 15:58:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:58:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 15:58:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 15:58:42 - INFO - __main__ - Printing 3 examples
03/19/2022 15:58:42 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 15:58:42 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:42 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 15:58:42 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:42 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 15:58:42 - INFO - __main__ - ['contradiction']
03/19/2022 15:58:42 - INFO - __main__ - Tokenizing Input ...
03/19/2022 15:58:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 15:58:42 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 15:58:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 15:58:54 - INFO - __main__ - Starting training!
03/19/2022 15:58:58 - INFO - __main__ - Step 10 Global step 10 Train loss 24.551653 on epoch=3
03/19/2022 15:59:03 - INFO - __main__ - Step 20 Global step 20 Train loss 16.660784 on epoch=6
03/19/2022 15:59:08 - INFO - __main__ - Step 30 Global step 30 Train loss 13.349371 on epoch=9
03/19/2022 15:59:13 - INFO - __main__ - Step 40 Global step 40 Train loss 11.097553 on epoch=13
03/19/2022 15:59:18 - INFO - __main__ - Step 50 Global step 50 Train loss 9.326780 on epoch=16
03/19/2022 15:59:20 - INFO - __main__ - Global step 50 Train loss 14.997228 Classification-F1 0.0 on epoch=16
03/19/2022 15:59:25 - INFO - __main__ - Step 60 Global step 60 Train loss 9.530859 on epoch=19
03/19/2022 15:59:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.594489 on epoch=23
03/19/2022 15:59:35 - INFO - __main__ - Step 80 Global step 80 Train loss 7.854173 on epoch=26
03/19/2022 15:59:40 - INFO - __main__ - Step 90 Global step 90 Train loss 7.516767 on epoch=29
03/19/2022 15:59:45 - INFO - __main__ - Step 100 Global step 100 Train loss 6.166133 on epoch=33
03/19/2022 15:59:47 - INFO - __main__ - Global step 100 Train loss 7.732483 Classification-F1 0.0 on epoch=33
03/19/2022 15:59:52 - INFO - __main__ - Step 110 Global step 110 Train loss 5.088535 on epoch=36
03/19/2022 15:59:57 - INFO - __main__ - Step 120 Global step 120 Train loss 4.098702 on epoch=39
03/19/2022 16:00:02 - INFO - __main__ - Step 130 Global step 130 Train loss 3.338125 on epoch=43
03/19/2022 16:00:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.339319 on epoch=46
03/19/2022 16:00:12 - INFO - __main__ - Step 150 Global step 150 Train loss 2.361464 on epoch=49
03/19/2022 16:00:13 - INFO - __main__ - Global step 150 Train loss 3.445230 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 16:00:18 - INFO - __main__ - Step 160 Global step 160 Train loss 2.077754 on epoch=53
03/19/2022 16:00:23 - INFO - __main__ - Step 170 Global step 170 Train loss 2.605243 on epoch=56
03/19/2022 16:00:28 - INFO - __main__ - Step 180 Global step 180 Train loss 2.325642 on epoch=59
03/19/2022 16:00:33 - INFO - __main__ - Step 190 Global step 190 Train loss 2.632253 on epoch=63
03/19/2022 16:00:38 - INFO - __main__ - Step 200 Global step 200 Train loss 2.301592 on epoch=66
03/19/2022 16:00:39 - INFO - __main__ - Global step 200 Train loss 2.388497 Classification-F1 0.17204301075268816 on epoch=66
03/19/2022 16:00:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.862889 on epoch=69
03/19/2022 16:00:50 - INFO - __main__ - Step 220 Global step 220 Train loss 2.494677 on epoch=73
03/19/2022 16:00:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.668148 on epoch=76
03/19/2022 16:01:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.731949 on epoch=79
03/19/2022 16:01:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.879375 on epoch=83
03/19/2022 16:01:06 - INFO - __main__ - Global step 250 Train loss 1.927407 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 16:01:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.390872 on epoch=86
03/19/2022 16:01:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.350104 on epoch=89
03/19/2022 16:01:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.632104 on epoch=93
03/19/2022 16:01:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.228062 on epoch=96
03/19/2022 16:01:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.196390 on epoch=99
03/19/2022 16:01:32 - INFO - __main__ - Global step 300 Train loss 1.359506 Classification-F1 0.1693121693121693 on epoch=99
03/19/2022 16:01:37 - INFO - __main__ - Step 310 Global step 310 Train loss 1.360247 on epoch=103
03/19/2022 16:01:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.218156 on epoch=106
03/19/2022 16:01:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.811017 on epoch=109
03/19/2022 16:01:52 - INFO - __main__ - Step 340 Global step 340 Train loss 1.286999 on epoch=113
03/19/2022 16:01:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.102793 on epoch=116
03/19/2022 16:01:58 - INFO - __main__ - Global step 350 Train loss 1.155842 Classification-F1 0.3343915343915344 on epoch=116
03/19/2022 16:02:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.920325 on epoch=119
03/19/2022 16:02:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.649823 on epoch=123
03/19/2022 16:02:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.745460 on epoch=126
03/19/2022 16:02:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494240 on epoch=129
03/19/2022 16:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.522577 on epoch=133
03/19/2022 16:02:24 - INFO - __main__ - Global step 400 Train loss 0.666485 Classification-F1 0.17316017316017315 on epoch=133
03/19/2022 16:02:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.371803 on epoch=136
03/19/2022 16:02:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.287453 on epoch=139
03/19/2022 16:02:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.268628 on epoch=143
03/19/2022 16:02:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.185345 on epoch=146
03/19/2022 16:02:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.301445 on epoch=149
03/19/2022 16:02:50 - INFO - __main__ - Global step 450 Train loss 0.282935 Classification-F1 0.2995337995337995 on epoch=149
03/19/2022 16:02:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.090007 on epoch=153
03/19/2022 16:03:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.075542 on epoch=156
03/19/2022 16:03:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.035648 on epoch=159
03/19/2022 16:03:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.037831 on epoch=163
03/19/2022 16:03:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027039 on epoch=166
03/19/2022 16:03:16 - INFO - __main__ - Global step 500 Train loss 0.053214 Classification-F1 0.3391215106732348 on epoch=166
03/19/2022 16:03:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.042108 on epoch=169
03/19/2022 16:03:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.014331 on epoch=173
03/19/2022 16:03:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006775 on epoch=176
03/19/2022 16:03:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.012993 on epoch=179
03/19/2022 16:03:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.031570 on epoch=183
03/19/2022 16:03:43 - INFO - __main__ - Global step 550 Train loss 0.021555 Classification-F1 0.30488639900404607 on epoch=183
03/19/2022 16:03:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004105 on epoch=186
03/19/2022 16:03:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004287 on epoch=189
03/19/2022 16:03:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016150 on epoch=193
03/19/2022 16:04:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001057 on epoch=196
03/19/2022 16:04:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003856 on epoch=199
03/19/2022 16:04:09 - INFO - __main__ - Global step 600 Train loss 0.005891 Classification-F1 0.2901709401709402 on epoch=199
03/19/2022 16:04:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002534 on epoch=203
03/19/2022 16:04:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002419 on epoch=206
03/19/2022 16:04:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001257 on epoch=209
03/19/2022 16:04:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000891 on epoch=213
03/19/2022 16:04:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001579 on epoch=216
03/19/2022 16:04:35 - INFO - __main__ - Global step 650 Train loss 0.001736 Classification-F1 0.2776719576719577 on epoch=216
03/19/2022 16:04:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001744 on epoch=219
03/19/2022 16:04:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.008701 on epoch=223
03/19/2022 16:04:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000607 on epoch=226
03/19/2022 16:04:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002262 on epoch=229
03/19/2022 16:05:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000628 on epoch=233
03/19/2022 16:05:01 - INFO - __main__ - Global step 700 Train loss 0.002788 Classification-F1 0.2973178137651821 on epoch=233
03/19/2022 16:05:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000049 on epoch=236
03/19/2022 16:05:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000202 on epoch=239
03/19/2022 16:05:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000583 on epoch=243
03/19/2022 16:05:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000574 on epoch=246
03/19/2022 16:05:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000102 on epoch=249
03/19/2022 16:05:27 - INFO - __main__ - Global step 750 Train loss 0.000302 Classification-F1 0.350140056022409 on epoch=249
03/19/2022 16:05:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000344 on epoch=253
03/19/2022 16:05:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000146 on epoch=256
03/19/2022 16:05:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000056 on epoch=259
03/19/2022 16:05:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000044 on epoch=263
03/19/2022 16:05:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000051 on epoch=266
03/19/2022 16:05:54 - INFO - __main__ - Global step 800 Train loss 0.000128 Classification-F1 0.3159879336349925 on epoch=266
03/19/2022 16:05:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000127 on epoch=269
03/19/2022 16:06:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000124 on epoch=273
03/19/2022 16:06:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000072 on epoch=276
03/19/2022 16:06:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000155 on epoch=279
03/19/2022 16:06:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000036 on epoch=283
03/19/2022 16:06:20 - INFO - __main__ - Global step 850 Train loss 0.000103 Classification-F1 0.34162895927601816 on epoch=283
03/19/2022 16:06:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000126 on epoch=286
03/19/2022 16:06:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000032 on epoch=289
03/19/2022 16:06:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000071 on epoch=293
03/19/2022 16:06:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000784 on epoch=296
03/19/2022 16:06:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001168 on epoch=299
03/19/2022 16:06:46 - INFO - __main__ - Global step 900 Train loss 0.000436 Classification-F1 0.28127205546560385 on epoch=299
03/19/2022 16:06:46 - INFO - __main__ - save last model!
03/19/2022 16:06:46 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:06:46 - INFO - __main__ - Printing 3 examples
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:06:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:06:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:06:46 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:06:46 - INFO - __main__ - Printing 3 examples
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 16:06:46 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:46 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:06:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:06:46 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:06:52 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:06:53 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:06:53 - INFO - __main__ - Printing 3 examples
03/19/2022 16:06:53 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:06:53 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:53 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:06:53 - INFO - __main__ - ['entailment']
03/19/2022 16:06:53 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:06:53 - INFO - __main__ - ['contradiction']
03/19/2022 16:06:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:06:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:06:55 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:06:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:06:58 - INFO - __main__ - Starting training!
03/19/2022 16:07:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_13_0.0003_8_predictions.txt
03/19/2022 16:07:13 - INFO - __main__ - Classification-F1 on test data: 0.3495
03/19/2022 16:07:13 - INFO - __main__ - prefix=anli_16_13, lr=0.0003, bsz=8, dev_performance=0.350140056022409, test_performance=0.34950483785822434
03/19/2022 16:07:13 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.0002, bsz=8 ...
03/19/2022 16:07:14 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:07:14 - INFO - __main__ - Printing 3 examples
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:07:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:07:14 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:07:14 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:07:14 - INFO - __main__ - Printing 3 examples
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 16:07:14 - INFO - __main__ - ['contradiction']
03/19/2022 16:07:14 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:07:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:07:14 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:07:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:07:26 - INFO - __main__ - Starting training!
03/19/2022 16:07:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.153046 on epoch=3
03/19/2022 16:07:36 - INFO - __main__ - Step 20 Global step 20 Train loss 16.558992 on epoch=6
03/19/2022 16:07:41 - INFO - __main__ - Step 30 Global step 30 Train loss 13.873157 on epoch=9
03/19/2022 16:07:46 - INFO - __main__ - Step 40 Global step 40 Train loss 11.450452 on epoch=13
03/19/2022 16:07:50 - INFO - __main__ - Step 50 Global step 50 Train loss 10.330820 on epoch=16
03/19/2022 16:07:51 - INFO - __main__ - Global step 50 Train loss 15.073292 Classification-F1 0.07526881720430108 on epoch=16
03/19/2022 16:07:57 - INFO - __main__ - Step 60 Global step 60 Train loss 10.270507 on epoch=19
03/19/2022 16:08:02 - INFO - __main__ - Step 70 Global step 70 Train loss 9.657210 on epoch=23
03/19/2022 16:08:07 - INFO - __main__ - Step 80 Global step 80 Train loss 9.119755 on epoch=26
03/19/2022 16:08:12 - INFO - __main__ - Step 90 Global step 90 Train loss 9.150506 on epoch=29
03/19/2022 16:08:17 - INFO - __main__ - Step 100 Global step 100 Train loss 8.512516 on epoch=33
03/19/2022 16:08:18 - INFO - __main__ - Global step 100 Train loss 9.342099 Classification-F1 0.06306306306306306 on epoch=33
03/19/2022 16:08:23 - INFO - __main__ - Step 110 Global step 110 Train loss 7.546302 on epoch=36
03/19/2022 16:08:28 - INFO - __main__ - Step 120 Global step 120 Train loss 7.201823 on epoch=39
03/19/2022 16:08:33 - INFO - __main__ - Step 130 Global step 130 Train loss 7.095311 on epoch=43
03/19/2022 16:08:38 - INFO - __main__ - Step 140 Global step 140 Train loss 5.946737 on epoch=46
03/19/2022 16:08:43 - INFO - __main__ - Step 150 Global step 150 Train loss 5.076548 on epoch=49
03/19/2022 16:08:44 - INFO - __main__ - Global step 150 Train loss 6.573343 Classification-F1 0.08571428571428572 on epoch=49
03/19/2022 16:08:50 - INFO - __main__ - Step 160 Global step 160 Train loss 4.025409 on epoch=53
03/19/2022 16:08:55 - INFO - __main__ - Step 170 Global step 170 Train loss 3.663531 on epoch=56
03/19/2022 16:09:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.737325 on epoch=59
03/19/2022 16:09:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.860616 on epoch=63
03/19/2022 16:09:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.566933 on epoch=66
03/19/2022 16:09:11 - INFO - __main__ - Global step 200 Train loss 2.370763 Classification-F1 0.20370370370370372 on epoch=66
03/19/2022 16:09:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.481290 on epoch=69
03/19/2022 16:09:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.601521 on epoch=73
03/19/2022 16:09:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.499654 on epoch=76
03/19/2022 16:09:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.413249 on epoch=79
03/19/2022 16:09:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.411337 on epoch=83
03/19/2022 16:09:38 - INFO - __main__ - Global step 250 Train loss 0.481410 Classification-F1 0.211258697027198 on epoch=83
03/19/2022 16:09:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.375517 on epoch=86
03/19/2022 16:09:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.711332 on epoch=89
03/19/2022 16:09:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.298002 on epoch=93
03/19/2022 16:09:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.425313 on epoch=96
03/19/2022 16:10:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.284195 on epoch=99
03/19/2022 16:10:04 - INFO - __main__ - Global step 300 Train loss 0.418872 Classification-F1 0.2085278555866791 on epoch=99
03/19/2022 16:10:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.243678 on epoch=103
03/19/2022 16:10:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.245465 on epoch=106
03/19/2022 16:10:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.184342 on epoch=109
03/19/2022 16:10:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.195335 on epoch=113
03/19/2022 16:10:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.239220 on epoch=116
03/19/2022 16:10:31 - INFO - __main__ - Global step 350 Train loss 0.221608 Classification-F1 0.25291571678726005 on epoch=116
03/19/2022 16:10:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.174434 on epoch=119
03/19/2022 16:10:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.109165 on epoch=123
03/19/2022 16:10:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.192904 on epoch=126
03/19/2022 16:10:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.075098 on epoch=129
03/19/2022 16:10:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.066669 on epoch=133
03/19/2022 16:10:57 - INFO - __main__ - Global step 400 Train loss 0.123654 Classification-F1 0.31216931216931215 on epoch=133
03/19/2022 16:11:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.089048 on epoch=136
03/19/2022 16:11:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042924 on epoch=139
03/19/2022 16:11:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.028139 on epoch=143
03/19/2022 16:11:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.017573 on epoch=146
03/19/2022 16:11:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.014651 on epoch=149
03/19/2022 16:11:24 - INFO - __main__ - Global step 450 Train loss 0.038467 Classification-F1 0.36700261291526687 on epoch=149
03/19/2022 16:11:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.009751 on epoch=153
03/19/2022 16:11:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.020643 on epoch=156
03/19/2022 16:11:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.013063 on epoch=159
03/19/2022 16:11:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.045751 on epoch=163
03/19/2022 16:11:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006581 on epoch=166
03/19/2022 16:11:51 - INFO - __main__ - Global step 500 Train loss 0.019158 Classification-F1 0.3365125947664465 on epoch=166
03/19/2022 16:11:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.015467 on epoch=169
03/19/2022 16:12:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.010471 on epoch=173
03/19/2022 16:12:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.017000 on epoch=176
03/19/2022 16:12:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.006536 on epoch=179
03/19/2022 16:12:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002513 on epoch=183
03/19/2022 16:12:17 - INFO - __main__ - Global step 550 Train loss 0.010397 Classification-F1 0.3177304964539007 on epoch=183
03/19/2022 16:12:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001264 on epoch=186
03/19/2022 16:12:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003642 on epoch=189
03/19/2022 16:12:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.015375 on epoch=193
03/19/2022 16:12:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000957 on epoch=196
03/19/2022 16:12:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004647 on epoch=199
03/19/2022 16:12:44 - INFO - __main__ - Global step 600 Train loss 0.005177 Classification-F1 0.3134057971014493 on epoch=199
03/19/2022 16:12:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003866 on epoch=203
03/19/2022 16:12:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001097 on epoch=206
03/19/2022 16:12:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001560 on epoch=209
03/19/2022 16:13:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000626 on epoch=213
03/19/2022 16:13:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001627 on epoch=216
03/19/2022 16:13:10 - INFO - __main__ - Global step 650 Train loss 0.001755 Classification-F1 0.30160919540229886 on epoch=216
03/19/2022 16:13:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000309 on epoch=219
03/19/2022 16:13:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001284 on epoch=223
03/19/2022 16:13:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000403 on epoch=226
03/19/2022 16:13:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000744 on epoch=229
03/19/2022 16:13:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000366 on epoch=233
03/19/2022 16:13:36 - INFO - __main__ - Global step 700 Train loss 0.000621 Classification-F1 0.3020541549953315 on epoch=233
03/19/2022 16:13:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002064 on epoch=236
03/19/2022 16:13:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000502 on epoch=239
03/19/2022 16:13:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000437 on epoch=243
03/19/2022 16:13:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000382 on epoch=246
03/19/2022 16:14:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000172 on epoch=249
03/19/2022 16:14:02 - INFO - __main__ - Global step 750 Train loss 0.000712 Classification-F1 0.34478039069304467 on epoch=249
03/19/2022 16:14:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000160 on epoch=253
03/19/2022 16:14:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000427 on epoch=256
03/19/2022 16:14:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000567 on epoch=259
03/19/2022 16:14:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001206 on epoch=263
03/19/2022 16:14:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000188 on epoch=266
03/19/2022 16:14:28 - INFO - __main__ - Global step 800 Train loss 0.000510 Classification-F1 0.30571428571428577 on epoch=266
03/19/2022 16:14:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000097 on epoch=269
03/19/2022 16:14:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000674 on epoch=273
03/19/2022 16:14:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000117 on epoch=276
03/19/2022 16:14:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000045 on epoch=279
03/19/2022 16:14:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000331 on epoch=283
03/19/2022 16:14:54 - INFO - __main__ - Global step 850 Train loss 0.000253 Classification-F1 0.30294396961063624 on epoch=283
03/19/2022 16:14:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000097 on epoch=286
03/19/2022 16:15:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000069 on epoch=289
03/19/2022 16:15:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.016371 on epoch=293
03/19/2022 16:15:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.046584 on epoch=296
03/19/2022 16:15:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001148 on epoch=299
03/19/2022 16:15:20 - INFO - __main__ - Global step 900 Train loss 0.012854 Classification-F1 0.37174178478526304 on epoch=299
03/19/2022 16:15:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:15:21 - INFO - __main__ - Printing 3 examples
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:15:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:15:21 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:15:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:15:21 - INFO - __main__ - Printing 3 examples
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 16:15:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:21 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:15:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:15:21 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:15:21 - INFO - __main__ - save last model!
03/19/2022 16:15:28 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:15:28 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:15:28 - INFO - __main__ - Printing 3 examples
03/19/2022 16:15:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:15:28 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:15:28 - INFO - __main__ - ['entailment']
03/19/2022 16:15:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:15:28 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:15:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:15:30 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:15:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:15:31 - INFO - __main__ - Starting training!
03/19/2022 16:15:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_13_0.0002_8_predictions.txt
03/19/2022 16:15:49 - INFO - __main__ - Classification-F1 on test data: 0.2350
03/19/2022 16:15:50 - INFO - __main__ - prefix=anli_16_13, lr=0.0002, bsz=8, dev_performance=0.37174178478526304, test_performance=0.23503505923841808
03/19/2022 16:15:50 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.0001, bsz=8 ...
03/19/2022 16:15:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:15:51 - INFO - __main__ - Printing 3 examples
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945  September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:15:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:15:51 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:15:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:15:51 - INFO - __main__ - Printing 3 examples
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
03/19/2022 16:15:51 - INFO - __main__ - ['contradiction']
03/19/2022 16:15:51 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:15:51 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:15:51 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:16:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:16:03 - INFO - __main__ - Starting training!
03/19/2022 16:16:08 - INFO - __main__ - Step 10 Global step 10 Train loss 23.076946 on epoch=3
03/19/2022 16:16:13 - INFO - __main__ - Step 20 Global step 20 Train loss 21.081942 on epoch=6
03/19/2022 16:16:18 - INFO - __main__ - Step 30 Global step 30 Train loss 17.541264 on epoch=9
03/19/2022 16:16:23 - INFO - __main__ - Step 40 Global step 40 Train loss 16.474146 on epoch=13
03/19/2022 16:16:28 - INFO - __main__ - Step 50 Global step 50 Train loss 14.356461 on epoch=16
03/19/2022 16:16:35 - INFO - __main__ - Global step 50 Train loss 18.506153 Classification-F1 0.0 on epoch=16
03/19/2022 16:16:40 - INFO - __main__ - Step 60 Global step 60 Train loss 13.898918 on epoch=19
03/19/2022 16:16:45 - INFO - __main__ - Step 70 Global step 70 Train loss 13.433243 on epoch=23
03/19/2022 16:16:50 - INFO - __main__ - Step 80 Global step 80 Train loss 12.161561 on epoch=26
03/19/2022 16:16:55 - INFO - __main__ - Step 90 Global step 90 Train loss 12.489772 on epoch=29
03/19/2022 16:17:00 - INFO - __main__ - Step 100 Global step 100 Train loss 11.871161 on epoch=33
03/19/2022 16:17:04 - INFO - __main__ - Global step 100 Train loss 12.770930 Classification-F1 0.0 on epoch=33
03/19/2022 16:17:10 - INFO - __main__ - Step 110 Global step 110 Train loss 10.937748 on epoch=36
03/19/2022 16:17:15 - INFO - __main__ - Step 120 Global step 120 Train loss 10.981685 on epoch=39
03/19/2022 16:17:20 - INFO - __main__ - Step 130 Global step 130 Train loss 10.877200 on epoch=43
03/19/2022 16:17:25 - INFO - __main__ - Step 140 Global step 140 Train loss 10.115650 on epoch=46
03/19/2022 16:17:30 - INFO - __main__ - Step 150 Global step 150 Train loss 9.842824 on epoch=49
03/19/2022 16:17:33 - INFO - __main__ - Global step 150 Train loss 10.551023 Classification-F1 0.0 on epoch=49
03/19/2022 16:17:38 - INFO - __main__ - Step 160 Global step 160 Train loss 9.560263 on epoch=53
03/19/2022 16:17:43 - INFO - __main__ - Step 170 Global step 170 Train loss 9.914869 on epoch=56
03/19/2022 16:17:48 - INFO - __main__ - Step 180 Global step 180 Train loss 8.799890 on epoch=59
03/19/2022 16:17:53 - INFO - __main__ - Step 190 Global step 190 Train loss 8.981155 on epoch=63
03/19/2022 16:17:58 - INFO - __main__ - Step 200 Global step 200 Train loss 8.954062 on epoch=66
03/19/2022 16:18:02 - INFO - __main__ - Global step 200 Train loss 9.242048 Classification-F1 0.0 on epoch=66
03/19/2022 16:18:07 - INFO - __main__ - Step 210 Global step 210 Train loss 8.255756 on epoch=69
03/19/2022 16:18:12 - INFO - __main__ - Step 220 Global step 220 Train loss 8.524823 on epoch=73
03/19/2022 16:18:17 - INFO - __main__ - Step 230 Global step 230 Train loss 7.935057 on epoch=76
03/19/2022 16:18:22 - INFO - __main__ - Step 240 Global step 240 Train loss 7.435405 on epoch=79
03/19/2022 16:18:27 - INFO - __main__ - Step 250 Global step 250 Train loss 7.851447 on epoch=83
03/19/2022 16:18:29 - INFO - __main__ - Global step 250 Train loss 8.000498 Classification-F1 0.0 on epoch=83
03/19/2022 16:18:34 - INFO - __main__ - Step 260 Global step 260 Train loss 7.464488 on epoch=86
03/19/2022 16:18:39 - INFO - __main__ - Step 270 Global step 270 Train loss 7.556284 on epoch=89
03/19/2022 16:18:44 - INFO - __main__ - Step 280 Global step 280 Train loss 6.583570 on epoch=93
03/19/2022 16:18:49 - INFO - __main__ - Step 290 Global step 290 Train loss 6.326851 on epoch=96
03/19/2022 16:18:54 - INFO - __main__ - Step 300 Global step 300 Train loss 7.001734 on epoch=99
03/19/2022 16:18:56 - INFO - __main__ - Global step 300 Train loss 6.986586 Classification-F1 0.0 on epoch=99
03/19/2022 16:19:01 - INFO - __main__ - Step 310 Global step 310 Train loss 6.187920 on epoch=103
03/19/2022 16:19:06 - INFO - __main__ - Step 320 Global step 320 Train loss 5.632468 on epoch=106
03/19/2022 16:19:11 - INFO - __main__ - Step 330 Global step 330 Train loss 5.571417 on epoch=109
03/19/2022 16:19:16 - INFO - __main__ - Step 340 Global step 340 Train loss 4.962799 on epoch=113
03/19/2022 16:19:21 - INFO - __main__ - Step 350 Global step 350 Train loss 4.583386 on epoch=116
03/19/2022 16:19:23 - INFO - __main__ - Global step 350 Train loss 5.387598 Classification-F1 0.0 on epoch=116
03/19/2022 16:19:28 - INFO - __main__ - Step 360 Global step 360 Train loss 4.695077 on epoch=119
03/19/2022 16:19:33 - INFO - __main__ - Step 370 Global step 370 Train loss 4.380131 on epoch=123
03/19/2022 16:19:38 - INFO - __main__ - Step 380 Global step 380 Train loss 4.022279 on epoch=126
03/19/2022 16:19:43 - INFO - __main__ - Step 390 Global step 390 Train loss 3.461987 on epoch=129
03/19/2022 16:19:48 - INFO - __main__ - Step 400 Global step 400 Train loss 3.690123 on epoch=133
03/19/2022 16:19:50 - INFO - __main__ - Global step 400 Train loss 4.049920 Classification-F1 0.0 on epoch=133
03/19/2022 16:19:55 - INFO - __main__ - Step 410 Global step 410 Train loss 3.117487 on epoch=136
03/19/2022 16:20:00 - INFO - __main__ - Step 420 Global step 420 Train loss 3.002326 on epoch=139
03/19/2022 16:20:05 - INFO - __main__ - Step 430 Global step 430 Train loss 2.607386 on epoch=143
03/19/2022 16:20:10 - INFO - __main__ - Step 440 Global step 440 Train loss 2.680681 on epoch=146
03/19/2022 16:20:15 - INFO - __main__ - Step 450 Global step 450 Train loss 2.441363 on epoch=149
03/19/2022 16:20:16 - INFO - __main__ - Global step 450 Train loss 2.769849 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 16:20:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.984065 on epoch=153
03/19/2022 16:20:26 - INFO - __main__ - Step 470 Global step 470 Train loss 2.428133 on epoch=156
03/19/2022 16:20:31 - INFO - __main__ - Step 480 Global step 480 Train loss 2.753399 on epoch=159
03/19/2022 16:20:36 - INFO - __main__ - Step 490 Global step 490 Train loss 2.559771 on epoch=163
03/19/2022 16:20:41 - INFO - __main__ - Step 500 Global step 500 Train loss 2.233224 on epoch=166
03/19/2022 16:20:42 - INFO - __main__ - Global step 500 Train loss 2.391718 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 16:20:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.914543 on epoch=169
03/19/2022 16:20:52 - INFO - __main__ - Step 520 Global step 520 Train loss 2.506451 on epoch=173
03/19/2022 16:20:57 - INFO - __main__ - Step 530 Global step 530 Train loss 2.729444 on epoch=176
03/19/2022 16:21:02 - INFO - __main__ - Step 540 Global step 540 Train loss 2.130790 on epoch=179
03/19/2022 16:21:07 - INFO - __main__ - Step 550 Global step 550 Train loss 2.815533 on epoch=183
03/19/2022 16:21:08 - INFO - __main__ - Global step 550 Train loss 2.419352 Classification-F1 0.16666666666666666 on epoch=183
03/19/2022 16:21:13 - INFO - __main__ - Step 560 Global step 560 Train loss 2.647802 on epoch=186
03/19/2022 16:21:18 - INFO - __main__ - Step 570 Global step 570 Train loss 2.917486 on epoch=189
03/19/2022 16:21:23 - INFO - __main__ - Step 580 Global step 580 Train loss 1.935350 on epoch=193
03/19/2022 16:21:28 - INFO - __main__ - Step 590 Global step 590 Train loss 2.163070 on epoch=196
03/19/2022 16:21:33 - INFO - __main__ - Step 600 Global step 600 Train loss 1.782218 on epoch=199
03/19/2022 16:21:34 - INFO - __main__ - Global step 600 Train loss 2.289185 Classification-F1 0.16666666666666666 on epoch=199
03/19/2022 16:21:39 - INFO - __main__ - Step 610 Global step 610 Train loss 1.894507 on epoch=203
03/19/2022 16:21:44 - INFO - __main__ - Step 620 Global step 620 Train loss 1.673757 on epoch=206
03/19/2022 16:21:49 - INFO - __main__ - Step 630 Global step 630 Train loss 1.884400 on epoch=209
03/19/2022 16:21:54 - INFO - __main__ - Step 640 Global step 640 Train loss 1.771767 on epoch=213
03/19/2022 16:21:59 - INFO - __main__ - Step 650 Global step 650 Train loss 1.628431 on epoch=216
03/19/2022 16:22:00 - INFO - __main__ - Global step 650 Train loss 1.770572 Classification-F1 0.16666666666666666 on epoch=216
03/19/2022 16:22:05 - INFO - __main__ - Step 660 Global step 660 Train loss 2.528847 on epoch=219
03/19/2022 16:22:10 - INFO - __main__ - Step 670 Global step 670 Train loss 1.713657 on epoch=223
03/19/2022 16:22:15 - INFO - __main__ - Step 680 Global step 680 Train loss 1.854251 on epoch=226
03/19/2022 16:22:20 - INFO - __main__ - Step 690 Global step 690 Train loss 1.828165 on epoch=229
03/19/2022 16:22:25 - INFO - __main__ - Step 700 Global step 700 Train loss 1.611048 on epoch=233
03/19/2022 16:22:26 - INFO - __main__ - Global step 700 Train loss 1.907193 Classification-F1 0.16666666666666666 on epoch=233
03/19/2022 16:22:31 - INFO - __main__ - Step 710 Global step 710 Train loss 2.054067 on epoch=236
03/19/2022 16:22:36 - INFO - __main__ - Step 720 Global step 720 Train loss 1.707827 on epoch=239
03/19/2022 16:22:41 - INFO - __main__ - Step 730 Global step 730 Train loss 1.778237 on epoch=243
03/19/2022 16:22:46 - INFO - __main__ - Step 740 Global step 740 Train loss 2.064959 on epoch=246
03/19/2022 16:22:51 - INFO - __main__ - Step 750 Global step 750 Train loss 1.960180 on epoch=249
03/19/2022 16:22:52 - INFO - __main__ - Global step 750 Train loss 1.913054 Classification-F1 0.1693121693121693 on epoch=249
03/19/2022 16:22:57 - INFO - __main__ - Step 760 Global step 760 Train loss 1.825659 on epoch=253
03/19/2022 16:23:02 - INFO - __main__ - Step 770 Global step 770 Train loss 2.004791 on epoch=256
03/19/2022 16:23:07 - INFO - __main__ - Step 780 Global step 780 Train loss 1.804384 on epoch=259
03/19/2022 16:23:12 - INFO - __main__ - Step 790 Global step 790 Train loss 1.641156 on epoch=263
03/19/2022 16:23:17 - INFO - __main__ - Step 800 Global step 800 Train loss 1.487194 on epoch=266
03/19/2022 16:23:18 - INFO - __main__ - Global step 800 Train loss 1.752637 Classification-F1 0.16666666666666666 on epoch=266
03/19/2022 16:23:23 - INFO - __main__ - Step 810 Global step 810 Train loss 1.593272 on epoch=269
03/19/2022 16:23:28 - INFO - __main__ - Step 820 Global step 820 Train loss 1.681188 on epoch=273
03/19/2022 16:23:33 - INFO - __main__ - Step 830 Global step 830 Train loss 1.449945 on epoch=276
03/19/2022 16:23:38 - INFO - __main__ - Step 840 Global step 840 Train loss 1.567160 on epoch=279
03/19/2022 16:23:43 - INFO - __main__ - Step 850 Global step 850 Train loss 1.529732 on epoch=283
03/19/2022 16:23:44 - INFO - __main__ - Global step 850 Train loss 1.564260 Classification-F1 0.2014652014652015 on epoch=283
03/19/2022 16:23:50 - INFO - __main__ - Step 860 Global step 860 Train loss 1.504222 on epoch=286
03/19/2022 16:23:55 - INFO - __main__ - Step 870 Global step 870 Train loss 1.567977 on epoch=289
03/19/2022 16:24:00 - INFO - __main__ - Step 880 Global step 880 Train loss 1.363479 on epoch=293
03/19/2022 16:24:05 - INFO - __main__ - Step 890 Global step 890 Train loss 1.600526 on epoch=296
03/19/2022 16:24:10 - INFO - __main__ - Step 900 Global step 900 Train loss 1.391522 on epoch=299
03/19/2022 16:24:10 - INFO - __main__ - Global step 900 Train loss 1.485545 Classification-F1 0.16666666666666666 on epoch=299
03/19/2022 16:24:10 - INFO - __main__ - save last model!
03/19/2022 16:24:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:24:11 - INFO - __main__ - Printing 3 examples
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:24:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:24:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:24:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:24:11 - INFO - __main__ - Printing 3 examples
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:24:11 - INFO - __main__ - ['entailment']
03/19/2022 16:24:11 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:24:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:24:11 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:24:17 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:24:18 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:24:18 - INFO - __main__ - Printing 3 examples
03/19/2022 16:24:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:24:18 - INFO - __main__ - ['contradiction']
03/19/2022 16:24:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:24:18 - INFO - __main__ - ['entailment']
03/19/2022 16:24:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:24:18 - INFO - __main__ - ['contradiction']
03/19/2022 16:24:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:24:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:24:20 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:24:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:24:22 - INFO - __main__ - Starting training!
03/19/2022 16:24:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_13_0.0001_8_predictions.txt
03/19/2022 16:24:35 - INFO - __main__ - Classification-F1 on test data: 0.2595
03/19/2022 16:24:35 - INFO - __main__ - prefix=anli_16_13, lr=0.0001, bsz=8, dev_performance=0.2014652014652015, test_performance=0.2594943502474992
03/19/2022 16:24:35 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.0005, bsz=8 ...
03/19/2022 16:24:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:24:36 - INFO - __main__ - Printing 3 examples
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:24:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:24:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:24:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:24:36 - INFO - __main__ - Printing 3 examples
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:24:36 - INFO - __main__ - ['entailment']
03/19/2022 16:24:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:24:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:24:36 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:24:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:24:47 - INFO - __main__ - Starting training!
03/19/2022 16:24:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.478992 on epoch=3
03/19/2022 16:24:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.245592 on epoch=6
03/19/2022 16:25:03 - INFO - __main__ - Step 30 Global step 30 Train loss 13.313559 on epoch=9
03/19/2022 16:25:08 - INFO - __main__ - Step 40 Global step 40 Train loss 11.471166 on epoch=13
03/19/2022 16:25:13 - INFO - __main__ - Step 50 Global step 50 Train loss 10.173036 on epoch=16
03/19/2022 16:25:14 - INFO - __main__ - Global step 50 Train loss 15.336470 Classification-F1 0.0 on epoch=16
03/19/2022 16:25:20 - INFO - __main__ - Step 60 Global step 60 Train loss 8.558900 on epoch=19
03/19/2022 16:25:25 - INFO - __main__ - Step 70 Global step 70 Train loss 7.663863 on epoch=23
03/19/2022 16:25:30 - INFO - __main__ - Step 80 Global step 80 Train loss 5.909463 on epoch=26
03/19/2022 16:25:35 - INFO - __main__ - Step 90 Global step 90 Train loss 3.594270 on epoch=29
03/19/2022 16:25:40 - INFO - __main__ - Step 100 Global step 100 Train loss 3.358803 on epoch=33
03/19/2022 16:25:41 - INFO - __main__ - Global step 100 Train loss 5.817060 Classification-F1 0.16666666666666666 on epoch=33
03/19/2022 16:25:47 - INFO - __main__ - Step 110 Global step 110 Train loss 2.532239 on epoch=36
03/19/2022 16:25:52 - INFO - __main__ - Step 120 Global step 120 Train loss 2.483498 on epoch=39
03/19/2022 16:25:57 - INFO - __main__ - Step 130 Global step 130 Train loss 2.517035 on epoch=43
03/19/2022 16:26:02 - INFO - __main__ - Step 140 Global step 140 Train loss 2.191560 on epoch=46
03/19/2022 16:26:07 - INFO - __main__ - Step 150 Global step 150 Train loss 2.223614 on epoch=49
03/19/2022 16:26:08 - INFO - __main__ - Global step 150 Train loss 2.389589 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 16:26:12 - INFO - __main__ - Step 160 Global step 160 Train loss 2.149717 on epoch=53
03/19/2022 16:26:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.763632 on epoch=56
03/19/2022 16:26:22 - INFO - __main__ - Step 180 Global step 180 Train loss 1.657388 on epoch=59
03/19/2022 16:26:27 - INFO - __main__ - Step 190 Global step 190 Train loss 1.502308 on epoch=63
03/19/2022 16:26:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.206639 on epoch=66
03/19/2022 16:26:33 - INFO - __main__ - Global step 200 Train loss 1.655936 Classification-F1 0.23298358891579232 on epoch=66
03/19/2022 16:26:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.483140 on epoch=69
03/19/2022 16:26:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.272466 on epoch=73
03/19/2022 16:26:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.232316 on epoch=76
03/19/2022 16:26:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.018199 on epoch=79
03/19/2022 16:26:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.187151 on epoch=83
03/19/2022 16:27:00 - INFO - __main__ - Global step 250 Train loss 1.238654 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 16:27:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.190079 on epoch=86
03/19/2022 16:27:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.993528 on epoch=89
03/19/2022 16:27:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.960425 on epoch=93
03/19/2022 16:27:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.721334 on epoch=96
03/19/2022 16:27:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.580471 on epoch=99
03/19/2022 16:27:25 - INFO - __main__ - Global step 300 Train loss 0.889167 Classification-F1 0.16733925208501477 on epoch=99
03/19/2022 16:27:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.734668 on epoch=103
03/19/2022 16:27:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.656408 on epoch=106
03/19/2022 16:27:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.669194 on epoch=109
03/19/2022 16:27:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.534289 on epoch=113
03/19/2022 16:27:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.610252 on epoch=116
03/19/2022 16:27:51 - INFO - __main__ - Global step 350 Train loss 0.840962 Classification-F1 0.31213603540647566 on epoch=116
03/19/2022 16:27:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.539386 on epoch=119
03/19/2022 16:28:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.671932 on epoch=123
03/19/2022 16:28:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.538875 on epoch=126
03/19/2022 16:28:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.533251 on epoch=129
03/19/2022 16:28:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.621417 on epoch=133
03/19/2022 16:28:17 - INFO - __main__ - Global step 400 Train loss 0.580972 Classification-F1 0.2619047619047619 on epoch=133
03/19/2022 16:28:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.508358 on epoch=136
03/19/2022 16:28:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.466597 on epoch=139
03/19/2022 16:28:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.509623 on epoch=143
03/19/2022 16:28:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.473363 on epoch=146
03/19/2022 16:28:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.432743 on epoch=149
03/19/2022 16:28:43 - INFO - __main__ - Global step 450 Train loss 0.478137 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 16:28:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.333861 on epoch=153
03/19/2022 16:28:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.423689 on epoch=156
03/19/2022 16:28:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.259374 on epoch=159
03/19/2022 16:29:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.364410 on epoch=163
03/19/2022 16:29:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.332492 on epoch=166
03/19/2022 16:29:08 - INFO - __main__ - Global step 500 Train loss 0.342765 Classification-F1 0.2634920634920635 on epoch=166
03/19/2022 16:29:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.209311 on epoch=169
03/19/2022 16:29:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.222522 on epoch=173
03/19/2022 16:29:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.099770 on epoch=176
03/19/2022 16:29:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.109723 on epoch=179
03/19/2022 16:29:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.029575 on epoch=183
03/19/2022 16:29:34 - INFO - __main__ - Global step 550 Train loss 0.134180 Classification-F1 0.35454545454545455 on epoch=183
03/19/2022 16:29:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.012835 on epoch=186
03/19/2022 16:29:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.022197 on epoch=189
03/19/2022 16:29:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003506 on epoch=193
03/19/2022 16:29:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001313 on epoch=196
03/19/2022 16:30:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002628 on epoch=199
03/19/2022 16:30:01 - INFO - __main__ - Global step 600 Train loss 0.008496 Classification-F1 0.2871148459383754 on epoch=199
03/19/2022 16:30:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001937 on epoch=203
03/19/2022 16:30:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000299 on epoch=206
03/19/2022 16:30:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000419 on epoch=209
03/19/2022 16:30:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000624 on epoch=213
03/19/2022 16:30:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.003164 on epoch=216
03/19/2022 16:30:26 - INFO - __main__ - Global step 650 Train loss 0.001289 Classification-F1 0.30908828739751987 on epoch=216
03/19/2022 16:30:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000310 on epoch=219
03/19/2022 16:30:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000860 on epoch=223
03/19/2022 16:30:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000216 on epoch=226
03/19/2022 16:30:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000352 on epoch=229
03/19/2022 16:30:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000335 on epoch=233
03/19/2022 16:30:52 - INFO - __main__ - Global step 700 Train loss 0.000415 Classification-F1 0.3133756924079505 on epoch=233
03/19/2022 16:30:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000382 on epoch=236
03/19/2022 16:31:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000486 on epoch=239
03/19/2022 16:31:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000474 on epoch=243
03/19/2022 16:31:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000064 on epoch=246
03/19/2022 16:31:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000244 on epoch=249
03/19/2022 16:31:17 - INFO - __main__ - Global step 750 Train loss 0.000330 Classification-F1 0.30782469727702994 on epoch=249
03/19/2022 16:31:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000139 on epoch=253
03/19/2022 16:31:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000033 on epoch=256
03/19/2022 16:31:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000105 on epoch=259
03/19/2022 16:31:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003903 on epoch=263
03/19/2022 16:31:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000361 on epoch=266
03/19/2022 16:31:43 - INFO - __main__ - Global step 800 Train loss 0.000908 Classification-F1 0.30811965811965814 on epoch=266
03/19/2022 16:31:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000446 on epoch=269
03/19/2022 16:31:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000109 on epoch=273
03/19/2022 16:31:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000556 on epoch=276
03/19/2022 16:32:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000077 on epoch=279
03/19/2022 16:32:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000076 on epoch=283
03/19/2022 16:32:09 - INFO - __main__ - Global step 850 Train loss 0.000253 Classification-F1 0.35375816993464054 on epoch=283
03/19/2022 16:32:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001662 on epoch=286
03/19/2022 16:32:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000213 on epoch=289
03/19/2022 16:32:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000070 on epoch=293
03/19/2022 16:32:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000085 on epoch=296
03/19/2022 16:32:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.008386 on epoch=299
03/19/2022 16:32:34 - INFO - __main__ - Global step 900 Train loss 0.002083 Classification-F1 0.41578947368421054 on epoch=299
03/19/2022 16:32:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:32:34 - INFO - __main__ - Printing 3 examples
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:32:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:32:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:32:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:32:34 - INFO - __main__ - Printing 3 examples
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:32:34 - INFO - __main__ - ['entailment']
03/19/2022 16:32:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:32:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:32:34 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:32:35 - INFO - __main__ - save last model!
03/19/2022 16:32:42 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:32:43 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:32:43 - INFO - __main__ - Printing 3 examples
03/19/2022 16:32:43 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:32:43 - INFO - __main__ - ['contradiction']
03/19/2022 16:32:43 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:32:43 - INFO - __main__ - ['entailment']
03/19/2022 16:32:43 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:32:43 - INFO - __main__ - ['contradiction']
03/19/2022 16:32:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:32:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:32:44 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:32:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:32:45 - INFO - __main__ - Starting training!
03/19/2022 16:33:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_21_0.0005_8_predictions.txt
03/19/2022 16:33:03 - INFO - __main__ - Classification-F1 on test data: 0.3113
03/19/2022 16:33:03 - INFO - __main__ - prefix=anli_16_21, lr=0.0005, bsz=8, dev_performance=0.41578947368421054, test_performance=0.31127501174145056
03/19/2022 16:33:03 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.0003, bsz=8 ...
03/19/2022 16:33:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:33:04 - INFO - __main__ - Printing 3 examples
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:33:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:33:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:33:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:33:04 - INFO - __main__ - Printing 3 examples
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:33:04 - INFO - __main__ - ['entailment']
03/19/2022 16:33:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:33:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:33:04 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:33:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:33:16 - INFO - __main__ - Starting training!
03/19/2022 16:33:22 - INFO - __main__ - Step 10 Global step 10 Train loss 22.897997 on epoch=3
03/19/2022 16:33:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.060844 on epoch=6
03/19/2022 16:33:32 - INFO - __main__ - Step 30 Global step 30 Train loss 14.520498 on epoch=9
03/19/2022 16:33:37 - INFO - __main__ - Step 40 Global step 40 Train loss 12.137450 on epoch=13
03/19/2022 16:33:42 - INFO - __main__ - Step 50 Global step 50 Train loss 10.990948 on epoch=16
03/19/2022 16:33:43 - INFO - __main__ - Global step 50 Train loss 15.921551 Classification-F1 0.0 on epoch=16
03/19/2022 16:33:49 - INFO - __main__ - Step 60 Global step 60 Train loss 10.753445 on epoch=19
03/19/2022 16:33:54 - INFO - __main__ - Step 70 Global step 70 Train loss 9.957214 on epoch=23
03/19/2022 16:33:59 - INFO - __main__ - Step 80 Global step 80 Train loss 9.618890 on epoch=26
03/19/2022 16:34:04 - INFO - __main__ - Step 90 Global step 90 Train loss 8.518789 on epoch=29
03/19/2022 16:34:09 - INFO - __main__ - Step 100 Global step 100 Train loss 7.029943 on epoch=33
03/19/2022 16:34:10 - INFO - __main__ - Global step 100 Train loss 9.175656 Classification-F1 0.06956521739130435 on epoch=33
03/19/2022 16:34:16 - INFO - __main__ - Step 110 Global step 110 Train loss 6.635575 on epoch=36
03/19/2022 16:34:21 - INFO - __main__ - Step 120 Global step 120 Train loss 6.364999 on epoch=39
03/19/2022 16:34:26 - INFO - __main__ - Step 130 Global step 130 Train loss 4.761820 on epoch=43
03/19/2022 16:34:31 - INFO - __main__ - Step 140 Global step 140 Train loss 3.532007 on epoch=46
03/19/2022 16:34:36 - INFO - __main__ - Step 150 Global step 150 Train loss 2.780027 on epoch=49
03/19/2022 16:34:37 - INFO - __main__ - Global step 150 Train loss 4.814885 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 16:34:42 - INFO - __main__ - Step 160 Global step 160 Train loss 3.169658 on epoch=53
03/19/2022 16:34:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.864606 on epoch=56
03/19/2022 16:34:53 - INFO - __main__ - Step 180 Global step 180 Train loss 2.917699 on epoch=59
03/19/2022 16:34:58 - INFO - __main__ - Step 190 Global step 190 Train loss 2.349543 on epoch=63
03/19/2022 16:35:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.692034 on epoch=66
03/19/2022 16:35:04 - INFO - __main__ - Global step 200 Train loss 2.398708 Classification-F1 0.1693121693121693 on epoch=66
03/19/2022 16:35:09 - INFO - __main__ - Step 210 Global step 210 Train loss 1.904992 on epoch=69
03/19/2022 16:35:14 - INFO - __main__ - Step 220 Global step 220 Train loss 2.393900 on epoch=73
03/19/2022 16:35:19 - INFO - __main__ - Step 230 Global step 230 Train loss 2.292665 on epoch=76
03/19/2022 16:35:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.703558 on epoch=79
03/19/2022 16:35:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.970285 on epoch=83
03/19/2022 16:35:30 - INFO - __main__ - Global step 250 Train loss 2.053080 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 16:35:35 - INFO - __main__ - Step 260 Global step 260 Train loss 1.599342 on epoch=86
03/19/2022 16:35:40 - INFO - __main__ - Step 270 Global step 270 Train loss 1.592016 on epoch=89
03/19/2022 16:35:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.448633 on epoch=93
03/19/2022 16:35:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.471282 on epoch=96
03/19/2022 16:35:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.672316 on epoch=99
03/19/2022 16:35:57 - INFO - __main__ - Global step 300 Train loss 1.556718 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 16:36:02 - INFO - __main__ - Step 310 Global step 310 Train loss 1.271257 on epoch=103
03/19/2022 16:36:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.380646 on epoch=106
03/19/2022 16:36:12 - INFO - __main__ - Step 330 Global step 330 Train loss 1.388999 on epoch=109
03/19/2022 16:36:17 - INFO - __main__ - Step 340 Global step 340 Train loss 1.151742 on epoch=113
03/19/2022 16:36:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.175205 on epoch=116
03/19/2022 16:36:23 - INFO - __main__ - Global step 350 Train loss 1.273570 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 16:36:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.049025 on epoch=119
03/19/2022 16:36:33 - INFO - __main__ - Step 370 Global step 370 Train loss 1.113866 on epoch=123
03/19/2022 16:36:38 - INFO - __main__ - Step 380 Global step 380 Train loss 1.112163 on epoch=126
03/19/2022 16:36:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.932189 on epoch=129
03/19/2022 16:36:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.987000 on epoch=133
03/19/2022 16:36:49 - INFO - __main__ - Global step 400 Train loss 1.038849 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 16:36:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.006693 on epoch=136
03/19/2022 16:36:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.751356 on epoch=139
03/19/2022 16:37:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.831150 on epoch=143
03/19/2022 16:37:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.898128 on epoch=146
03/19/2022 16:37:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.625105 on epoch=149
03/19/2022 16:37:15 - INFO - __main__ - Global step 450 Train loss 0.822486 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 16:37:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.615203 on epoch=153
03/19/2022 16:37:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.841064 on epoch=156
03/19/2022 16:37:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.694322 on epoch=159
03/19/2022 16:37:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.663210 on epoch=163
03/19/2022 16:37:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.738801 on epoch=166
03/19/2022 16:37:41 - INFO - __main__ - Global step 500 Train loss 0.710521 Classification-F1 0.20908004778972522 on epoch=166
03/19/2022 16:37:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.705904 on epoch=169
03/19/2022 16:37:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.677220 on epoch=173
03/19/2022 16:37:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.506368 on epoch=176
03/19/2022 16:38:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.821976 on epoch=179
03/19/2022 16:38:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.568852 on epoch=183
03/19/2022 16:38:08 - INFO - __main__ - Global step 550 Train loss 0.656064 Classification-F1 0.39354226020892685 on epoch=183
03/19/2022 16:38:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.489266 on epoch=186
03/19/2022 16:38:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.563533 on epoch=189
03/19/2022 16:38:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.553927 on epoch=193
03/19/2022 16:38:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.286329 on epoch=196
03/19/2022 16:38:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.130294 on epoch=199
03/19/2022 16:38:35 - INFO - __main__ - Global step 600 Train loss 0.404670 Classification-F1 0.34978354978354975 on epoch=199
03/19/2022 16:38:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.296792 on epoch=203
03/19/2022 16:38:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.113356 on epoch=206
03/19/2022 16:38:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.017998 on epoch=209
03/19/2022 16:38:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.023610 on epoch=213
03/19/2022 16:39:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.006231 on epoch=216
03/19/2022 16:39:02 - INFO - __main__ - Global step 650 Train loss 0.091597 Classification-F1 0.3406593406593407 on epoch=216
03/19/2022 16:39:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.008608 on epoch=219
03/19/2022 16:39:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.007119 on epoch=223
03/19/2022 16:39:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.008526 on epoch=226
03/19/2022 16:39:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001790 on epoch=229
03/19/2022 16:39:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.006524 on epoch=233
03/19/2022 16:39:28 - INFO - __main__ - Global step 700 Train loss 0.006513 Classification-F1 0.3120751452230874 on epoch=233
03/19/2022 16:39:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001546 on epoch=236
03/19/2022 16:39:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002771 on epoch=239
03/19/2022 16:39:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001095 on epoch=243
03/19/2022 16:39:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000763 on epoch=246
03/19/2022 16:39:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001640 on epoch=249
03/19/2022 16:39:54 - INFO - __main__ - Global step 750 Train loss 0.001563 Classification-F1 0.38589743589743586 on epoch=249
03/19/2022 16:39:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000956 on epoch=253
03/19/2022 16:40:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001063 on epoch=256
03/19/2022 16:40:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001258 on epoch=259
03/19/2022 16:40:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002818 on epoch=263
03/19/2022 16:40:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.005427 on epoch=266
03/19/2022 16:40:21 - INFO - __main__ - Global step 800 Train loss 0.002304 Classification-F1 0.3313131313131313 on epoch=266
03/19/2022 16:40:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.013560 on epoch=269
03/19/2022 16:40:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002595 on epoch=273
03/19/2022 16:40:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000549 on epoch=276
03/19/2022 16:40:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000424 on epoch=279
03/19/2022 16:40:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001017 on epoch=283
03/19/2022 16:40:47 - INFO - __main__ - Global step 850 Train loss 0.003629 Classification-F1 0.37575757575757573 on epoch=283
03/19/2022 16:40:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000301 on epoch=286
03/19/2022 16:40:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000166 on epoch=289
03/19/2022 16:41:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000662 on epoch=293
03/19/2022 16:41:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000530 on epoch=296
03/19/2022 16:41:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000146 on epoch=299
03/19/2022 16:41:13 - INFO - __main__ - Global step 900 Train loss 0.000361 Classification-F1 0.354411045943304 on epoch=299
03/19/2022 16:41:13 - INFO - __main__ - save last model!
03/19/2022 16:41:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:41:13 - INFO - __main__ - Printing 3 examples
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:41:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:41:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:41:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:41:13 - INFO - __main__ - Printing 3 examples
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:41:13 - INFO - __main__ - ['entailment']
03/19/2022 16:41:13 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:41:13 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:41:13 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:41:20 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:41:21 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:41:21 - INFO - __main__ - Printing 3 examples
03/19/2022 16:41:21 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:41:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:41:21 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:41:21 - INFO - __main__ - ['entailment']
03/19/2022 16:41:21 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:41:21 - INFO - __main__ - ['contradiction']
03/19/2022 16:41:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:41:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:41:22 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:41:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:41:24 - INFO - __main__ - Starting training!
03/19/2022 16:41:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_21_0.0003_8_predictions.txt
03/19/2022 16:41:40 - INFO - __main__ - Classification-F1 on test data: 0.3076
03/19/2022 16:41:41 - INFO - __main__ - prefix=anli_16_21, lr=0.0003, bsz=8, dev_performance=0.39354226020892685, test_performance=0.30759795869198586
03/19/2022 16:41:41 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.0002, bsz=8 ...
03/19/2022 16:41:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:41:42 - INFO - __main__ - Printing 3 examples
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:41:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:41:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:41:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:41:42 - INFO - __main__ - Printing 3 examples
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:41:42 - INFO - __main__ - ['entailment']
03/19/2022 16:41:42 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:41:42 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:41:42 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:41:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:41:53 - INFO - __main__ - Starting training!
03/19/2022 16:41:57 - INFO - __main__ - Step 10 Global step 10 Train loss 24.590420 on epoch=3
03/19/2022 16:42:01 - INFO - __main__ - Step 20 Global step 20 Train loss 21.875317 on epoch=6
03/19/2022 16:42:06 - INFO - __main__ - Step 30 Global step 30 Train loss 18.123684 on epoch=9
03/19/2022 16:42:11 - INFO - __main__ - Step 40 Global step 40 Train loss 14.001140 on epoch=13
03/19/2022 16:42:16 - INFO - __main__ - Step 50 Global step 50 Train loss 12.548157 on epoch=16
03/19/2022 16:42:18 - INFO - __main__ - Global step 50 Train loss 18.227743 Classification-F1 0.014035087719298244 on epoch=16
03/19/2022 16:42:24 - INFO - __main__ - Step 60 Global step 60 Train loss 12.332483 on epoch=19
03/19/2022 16:42:29 - INFO - __main__ - Step 70 Global step 70 Train loss 11.761472 on epoch=23
03/19/2022 16:42:34 - INFO - __main__ - Step 80 Global step 80 Train loss 11.266151 on epoch=26
03/19/2022 16:42:39 - INFO - __main__ - Step 90 Global step 90 Train loss 10.975451 on epoch=29
03/19/2022 16:42:44 - INFO - __main__ - Step 100 Global step 100 Train loss 9.994713 on epoch=33
03/19/2022 16:42:45 - INFO - __main__ - Global step 100 Train loss 11.266055 Classification-F1 0.0 on epoch=33
03/19/2022 16:42:50 - INFO - __main__ - Step 110 Global step 110 Train loss 9.243498 on epoch=36
03/19/2022 16:42:55 - INFO - __main__ - Step 120 Global step 120 Train loss 9.080826 on epoch=39
03/19/2022 16:43:00 - INFO - __main__ - Step 130 Global step 130 Train loss 8.136255 on epoch=43
03/19/2022 16:43:05 - INFO - __main__ - Step 140 Global step 140 Train loss 7.598430 on epoch=46
03/19/2022 16:43:10 - INFO - __main__ - Step 150 Global step 150 Train loss 7.298167 on epoch=49
03/19/2022 16:43:11 - INFO - __main__ - Global step 150 Train loss 8.271435 Classification-F1 0.0 on epoch=49
03/19/2022 16:43:16 - INFO - __main__ - Step 160 Global step 160 Train loss 6.672692 on epoch=53
03/19/2022 16:43:21 - INFO - __main__ - Step 170 Global step 170 Train loss 6.355964 on epoch=56
03/19/2022 16:43:26 - INFO - __main__ - Step 180 Global step 180 Train loss 5.088105 on epoch=59
03/19/2022 16:43:31 - INFO - __main__ - Step 190 Global step 190 Train loss 4.895848 on epoch=63
03/19/2022 16:43:36 - INFO - __main__ - Step 200 Global step 200 Train loss 4.675372 on epoch=66
03/19/2022 16:43:37 - INFO - __main__ - Global step 200 Train loss 5.537596 Classification-F1 0.0 on epoch=66
03/19/2022 16:43:42 - INFO - __main__ - Step 210 Global step 210 Train loss 3.748507 on epoch=69
03/19/2022 16:43:47 - INFO - __main__ - Step 220 Global step 220 Train loss 3.932217 on epoch=73
03/19/2022 16:43:52 - INFO - __main__ - Step 230 Global step 230 Train loss 2.633087 on epoch=76
03/19/2022 16:43:57 - INFO - __main__ - Step 240 Global step 240 Train loss 2.299182 on epoch=79
03/19/2022 16:44:02 - INFO - __main__ - Step 250 Global step 250 Train loss 2.238606 on epoch=83
03/19/2022 16:44:03 - INFO - __main__ - Global step 250 Train loss 2.970320 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 16:44:08 - INFO - __main__ - Step 260 Global step 260 Train loss 2.277109 on epoch=86
03/19/2022 16:44:13 - INFO - __main__ - Step 270 Global step 270 Train loss 2.322605 on epoch=89
03/19/2022 16:44:18 - INFO - __main__ - Step 280 Global step 280 Train loss 1.991678 on epoch=93
03/19/2022 16:44:23 - INFO - __main__ - Step 290 Global step 290 Train loss 2.517489 on epoch=96
03/19/2022 16:44:28 - INFO - __main__ - Step 300 Global step 300 Train loss 2.059877 on epoch=99
03/19/2022 16:44:29 - INFO - __main__ - Global step 300 Train loss 2.233752 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 16:44:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.049489 on epoch=103
03/19/2022 16:44:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.610271 on epoch=106
03/19/2022 16:44:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.896947 on epoch=109
03/19/2022 16:44:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.880523 on epoch=113
03/19/2022 16:44:53 - INFO - __main__ - Step 350 Global step 350 Train loss 2.060654 on epoch=116
03/19/2022 16:44:54 - INFO - __main__ - Global step 350 Train loss 2.099577 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 16:44:59 - INFO - __main__ - Step 360 Global step 360 Train loss 1.911479 on epoch=119
03/19/2022 16:45:04 - INFO - __main__ - Step 370 Global step 370 Train loss 2.283557 on epoch=123
03/19/2022 16:45:09 - INFO - __main__ - Step 380 Global step 380 Train loss 1.710401 on epoch=126
03/19/2022 16:45:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.453559 on epoch=129
03/19/2022 16:45:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.649082 on epoch=133
03/19/2022 16:45:20 - INFO - __main__ - Global step 400 Train loss 1.801616 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 16:45:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.538871 on epoch=136
03/19/2022 16:45:30 - INFO - __main__ - Step 420 Global step 420 Train loss 1.476716 on epoch=139
03/19/2022 16:45:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.458527 on epoch=143
03/19/2022 16:45:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.467981 on epoch=146
03/19/2022 16:45:44 - INFO - __main__ - Step 450 Global step 450 Train loss 1.444808 on epoch=149
03/19/2022 16:45:45 - INFO - __main__ - Global step 450 Train loss 1.477381 Classification-F1 0.3153439153439153 on epoch=149
03/19/2022 16:45:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.755814 on epoch=153
03/19/2022 16:45:56 - INFO - __main__ - Step 470 Global step 470 Train loss 1.676334 on epoch=156
03/19/2022 16:46:01 - INFO - __main__ - Step 480 Global step 480 Train loss 1.718369 on epoch=159
03/19/2022 16:46:06 - INFO - __main__ - Step 490 Global step 490 Train loss 1.111185 on epoch=163
03/19/2022 16:46:11 - INFO - __main__ - Step 500 Global step 500 Train loss 1.347613 on epoch=166
03/19/2022 16:46:11 - INFO - __main__ - Global step 500 Train loss 1.521863 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 16:46:16 - INFO - __main__ - Step 510 Global step 510 Train loss 1.073396 on epoch=169
03/19/2022 16:46:21 - INFO - __main__ - Step 520 Global step 520 Train loss 1.288410 on epoch=173
03/19/2022 16:46:26 - INFO - __main__ - Step 530 Global step 530 Train loss 1.272353 on epoch=176
03/19/2022 16:46:31 - INFO - __main__ - Step 540 Global step 540 Train loss 1.405720 on epoch=179
03/19/2022 16:46:36 - INFO - __main__ - Step 550 Global step 550 Train loss 1.026931 on epoch=183
03/19/2022 16:46:37 - INFO - __main__ - Global step 550 Train loss 1.213362 Classification-F1 0.16666666666666666 on epoch=183
03/19/2022 16:46:42 - INFO - __main__ - Step 560 Global step 560 Train loss 1.183810 on epoch=186
03/19/2022 16:46:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.922310 on epoch=189
03/19/2022 16:46:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.997861 on epoch=193
03/19/2022 16:46:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.993977 on epoch=196
03/19/2022 16:47:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.961436 on epoch=199
03/19/2022 16:47:02 - INFO - __main__ - Global step 600 Train loss 1.011879 Classification-F1 0.16666666666666666 on epoch=199
03/19/2022 16:47:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.987769 on epoch=203
03/19/2022 16:47:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.983213 on epoch=206
03/19/2022 16:47:17 - INFO - __main__ - Step 630 Global step 630 Train loss 1.061117 on epoch=209
03/19/2022 16:47:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.747438 on epoch=213
03/19/2022 16:47:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.866777 on epoch=216
03/19/2022 16:47:28 - INFO - __main__ - Global step 650 Train loss 0.929263 Classification-F1 0.2003261122757978 on epoch=216
03/19/2022 16:47:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.613959 on epoch=219
03/19/2022 16:47:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.687195 on epoch=223
03/19/2022 16:47:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.830640 on epoch=226
03/19/2022 16:47:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.615183 on epoch=229
03/19/2022 16:47:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.588078 on epoch=233
03/19/2022 16:47:54 - INFO - __main__ - Global step 700 Train loss 0.667011 Classification-F1 0.3036541655739138 on epoch=233
03/19/2022 16:47:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.713622 on epoch=236
03/19/2022 16:48:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.757064 on epoch=239
03/19/2022 16:48:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.678770 on epoch=243
03/19/2022 16:48:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.588715 on epoch=246
03/19/2022 16:48:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.630141 on epoch=249
03/19/2022 16:48:20 - INFO - __main__ - Global step 750 Train loss 0.673663 Classification-F1 0.3209876543209876 on epoch=249
03/19/2022 16:48:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.644897 on epoch=253
03/19/2022 16:48:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.509566 on epoch=256
03/19/2022 16:48:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.567137 on epoch=259
03/19/2022 16:48:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.544799 on epoch=263
03/19/2022 16:48:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.484667 on epoch=266
03/19/2022 16:48:47 - INFO - __main__ - Global step 800 Train loss 0.550213 Classification-F1 0.3439260761775975 on epoch=266
03/19/2022 16:48:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.625461 on epoch=269
03/19/2022 16:48:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.535720 on epoch=273
03/19/2022 16:49:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.536897 on epoch=276
03/19/2022 16:49:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.504434 on epoch=279
03/19/2022 16:49:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.496599 on epoch=283
03/19/2022 16:49:14 - INFO - __main__ - Global step 850 Train loss 0.539822 Classification-F1 0.38201058201058197 on epoch=283
03/19/2022 16:49:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.449155 on epoch=286
03/19/2022 16:49:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.658105 on epoch=289
03/19/2022 16:49:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.475519 on epoch=293
03/19/2022 16:49:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.482858 on epoch=296
03/19/2022 16:49:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.486647 on epoch=299
03/19/2022 16:49:41 - INFO - __main__ - Global step 900 Train loss 0.510457 Classification-F1 0.34491516146688567 on epoch=299
03/19/2022 16:49:41 - INFO - __main__ - save last model!
03/19/2022 16:49:41 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:49:41 - INFO - __main__ - Printing 3 examples
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:49:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:49:41 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:49:41 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:49:41 - INFO - __main__ - Printing 3 examples
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:49:41 - INFO - __main__ - ['entailment']
03/19/2022 16:49:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:49:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:49:41 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:49:47 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:49:48 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:49:48 - INFO - __main__ - Printing 3 examples
03/19/2022 16:49:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:49:48 - INFO - __main__ - ['contradiction']
03/19/2022 16:49:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:49:48 - INFO - __main__ - ['entailment']
03/19/2022 16:49:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:49:48 - INFO - __main__ - ['contradiction']
03/19/2022 16:49:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:49:48 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:49:49 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:49:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:49:52 - INFO - __main__ - Starting training!
03/19/2022 16:50:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_21_0.0002_8_predictions.txt
03/19/2022 16:50:08 - INFO - __main__ - Classification-F1 on test data: 0.2746
03/19/2022 16:50:08 - INFO - __main__ - prefix=anli_16_21, lr=0.0002, bsz=8, dev_performance=0.38201058201058197, test_performance=0.27455671891599653
03/19/2022 16:50:08 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.0001, bsz=8 ...
03/19/2022 16:50:09 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:50:09 - INFO - __main__ - Printing 3 examples
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:50:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:50:09 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:50:09 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:50:09 - INFO - __main__ - Printing 3 examples
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
03/19/2022 16:50:09 - INFO - __main__ - ['entailment']
03/19/2022 16:50:09 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:50:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:50:09 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:50:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:50:20 - INFO - __main__ - Starting training!
03/19/2022 16:50:24 - INFO - __main__ - Step 10 Global step 10 Train loss 25.720591 on epoch=3
03/19/2022 16:50:29 - INFO - __main__ - Step 20 Global step 20 Train loss 21.055838 on epoch=6
03/19/2022 16:50:34 - INFO - __main__ - Step 30 Global step 30 Train loss 15.889399 on epoch=9
03/19/2022 16:50:39 - INFO - __main__ - Step 40 Global step 40 Train loss 13.440527 on epoch=13
03/19/2022 16:50:43 - INFO - __main__ - Step 50 Global step 50 Train loss 12.632147 on epoch=16
03/19/2022 16:50:45 - INFO - __main__ - Global step 50 Train loss 17.747700 Classification-F1 0.013605442176870748 on epoch=16
03/19/2022 16:50:51 - INFO - __main__ - Step 60 Global step 60 Train loss 12.573006 on epoch=19
03/19/2022 16:50:55 - INFO - __main__ - Step 70 Global step 70 Train loss 11.910905 on epoch=23
03/19/2022 16:51:00 - INFO - __main__ - Step 80 Global step 80 Train loss 12.392238 on epoch=26
03/19/2022 16:51:05 - INFO - __main__ - Step 90 Global step 90 Train loss 11.951061 on epoch=29
03/19/2022 16:51:10 - INFO - __main__ - Step 100 Global step 100 Train loss 11.325963 on epoch=33
03/19/2022 16:51:11 - INFO - __main__ - Global step 100 Train loss 12.030634 Classification-F1 0.0 on epoch=33
03/19/2022 16:51:16 - INFO - __main__ - Step 110 Global step 110 Train loss 11.342072 on epoch=36
03/19/2022 16:51:21 - INFO - __main__ - Step 120 Global step 120 Train loss 11.199430 on epoch=39
03/19/2022 16:51:26 - INFO - __main__ - Step 130 Global step 130 Train loss 10.990248 on epoch=43
03/19/2022 16:51:30 - INFO - __main__ - Step 140 Global step 140 Train loss 10.699408 on epoch=46
03/19/2022 16:51:35 - INFO - __main__ - Step 150 Global step 150 Train loss 10.523046 on epoch=49
03/19/2022 16:51:36 - INFO - __main__ - Global step 150 Train loss 10.950840 Classification-F1 0.0 on epoch=49
03/19/2022 16:51:41 - INFO - __main__ - Step 160 Global step 160 Train loss 10.017070 on epoch=53
03/19/2022 16:51:46 - INFO - __main__ - Step 170 Global step 170 Train loss 9.910261 on epoch=56
03/19/2022 16:51:51 - INFO - __main__ - Step 180 Global step 180 Train loss 9.056372 on epoch=59
03/19/2022 16:51:56 - INFO - __main__ - Step 190 Global step 190 Train loss 10.003868 on epoch=63
03/19/2022 16:52:01 - INFO - __main__ - Step 200 Global step 200 Train loss 8.758698 on epoch=66
03/19/2022 16:52:02 - INFO - __main__ - Global step 200 Train loss 9.549254 Classification-F1 0.0 on epoch=66
03/19/2022 16:52:06 - INFO - __main__ - Step 210 Global step 210 Train loss 9.360788 on epoch=69
03/19/2022 16:52:11 - INFO - __main__ - Step 220 Global step 220 Train loss 8.512785 on epoch=73
03/19/2022 16:52:16 - INFO - __main__ - Step 230 Global step 230 Train loss 8.196853 on epoch=76
03/19/2022 16:52:21 - INFO - __main__ - Step 240 Global step 240 Train loss 7.755347 on epoch=79
03/19/2022 16:52:26 - INFO - __main__ - Step 250 Global step 250 Train loss 7.998508 on epoch=83
03/19/2022 16:52:27 - INFO - __main__ - Global step 250 Train loss 8.364856 Classification-F1 0.0 on epoch=83
03/19/2022 16:52:32 - INFO - __main__ - Step 260 Global step 260 Train loss 7.235326 on epoch=86
03/19/2022 16:52:37 - INFO - __main__ - Step 270 Global step 270 Train loss 6.910631 on epoch=89
03/19/2022 16:52:41 - INFO - __main__ - Step 280 Global step 280 Train loss 6.427886 on epoch=93
03/19/2022 16:52:46 - INFO - __main__ - Step 290 Global step 290 Train loss 6.478431 on epoch=96
03/19/2022 16:52:51 - INFO - __main__ - Step 300 Global step 300 Train loss 5.565928 on epoch=99
03/19/2022 16:52:52 - INFO - __main__ - Global step 300 Train loss 6.523640 Classification-F1 0.0 on epoch=99
03/19/2022 16:52:57 - INFO - __main__ - Step 310 Global step 310 Train loss 5.123237 on epoch=103
03/19/2022 16:53:02 - INFO - __main__ - Step 320 Global step 320 Train loss 4.976521 on epoch=106
03/19/2022 16:53:07 - INFO - __main__ - Step 330 Global step 330 Train loss 4.502331 on epoch=109
03/19/2022 16:53:12 - INFO - __main__ - Step 340 Global step 340 Train loss 3.589040 on epoch=113
03/19/2022 16:53:17 - INFO - __main__ - Step 350 Global step 350 Train loss 3.914216 on epoch=116
03/19/2022 16:53:17 - INFO - __main__ - Global step 350 Train loss 4.421069 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 16:53:23 - INFO - __main__ - Step 360 Global step 360 Train loss 3.155800 on epoch=119
03/19/2022 16:53:28 - INFO - __main__ - Step 370 Global step 370 Train loss 3.763760 on epoch=123
03/19/2022 16:53:33 - INFO - __main__ - Step 380 Global step 380 Train loss 3.303971 on epoch=126
03/19/2022 16:53:38 - INFO - __main__ - Step 390 Global step 390 Train loss 2.631807 on epoch=129
03/19/2022 16:53:43 - INFO - __main__ - Step 400 Global step 400 Train loss 3.188216 on epoch=133
03/19/2022 16:53:43 - INFO - __main__ - Global step 400 Train loss 3.208711 Classification-F1 0.09640522875816994 on epoch=133
03/19/2022 16:53:48 - INFO - __main__ - Step 410 Global step 410 Train loss 1.657959 on epoch=136
03/19/2022 16:53:53 - INFO - __main__ - Step 420 Global step 420 Train loss 1.060987 on epoch=139
03/19/2022 16:53:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.774648 on epoch=143
03/19/2022 16:54:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.752154 on epoch=146
03/19/2022 16:54:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.558982 on epoch=149
03/19/2022 16:54:08 - INFO - __main__ - Global step 450 Train loss 0.960946 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 16:54:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.551115 on epoch=153
03/19/2022 16:54:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.479150 on epoch=156
03/19/2022 16:54:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.483958 on epoch=159
03/19/2022 16:54:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.406964 on epoch=163
03/19/2022 16:54:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.398120 on epoch=166
03/19/2022 16:54:34 - INFO - __main__ - Global step 500 Train loss 0.463862 Classification-F1 0.28095238095238095 on epoch=166
03/19/2022 16:54:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.322555 on epoch=169
03/19/2022 16:54:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.318055 on epoch=173
03/19/2022 16:54:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.291268 on epoch=176
03/19/2022 16:54:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.197500 on epoch=179
03/19/2022 16:54:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.181848 on epoch=183
03/19/2022 16:54:59 - INFO - __main__ - Global step 550 Train loss 0.262245 Classification-F1 0.27486606102958305 on epoch=183
03/19/2022 16:55:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.156383 on epoch=186
03/19/2022 16:55:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.154862 on epoch=189
03/19/2022 16:55:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.073736 on epoch=193
03/19/2022 16:55:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.078176 on epoch=196
03/19/2022 16:55:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.070917 on epoch=199
03/19/2022 16:55:25 - INFO - __main__ - Global step 600 Train loss 0.106815 Classification-F1 0.34084809447128284 on epoch=199
03/19/2022 16:55:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.034020 on epoch=203
03/19/2022 16:55:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.035026 on epoch=206
03/19/2022 16:55:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.041209 on epoch=209
03/19/2022 16:55:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.032104 on epoch=213
03/19/2022 16:55:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.017505 on epoch=216
03/19/2022 16:55:50 - INFO - __main__ - Global step 650 Train loss 0.031973 Classification-F1 0.3423028470647518 on epoch=216
03/19/2022 16:55:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.018783 on epoch=219
03/19/2022 16:56:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.018111 on epoch=223
03/19/2022 16:56:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.022226 on epoch=226
03/19/2022 16:56:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.011255 on epoch=229
03/19/2022 16:56:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.017185 on epoch=233
03/19/2022 16:56:16 - INFO - __main__ - Global step 700 Train loss 0.017512 Classification-F1 0.27025641025641023 on epoch=233
03/19/2022 16:56:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015629 on epoch=236
03/19/2022 16:56:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.040660 on epoch=239
03/19/2022 16:56:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005648 on epoch=243
03/19/2022 16:56:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.011594 on epoch=246
03/19/2022 16:56:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.014683 on epoch=249
03/19/2022 16:56:42 - INFO - __main__ - Global step 750 Train loss 0.017643 Classification-F1 0.2934472934472935 on epoch=249
03/19/2022 16:56:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006360 on epoch=253
03/19/2022 16:56:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007868 on epoch=256
03/19/2022 16:56:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.005747 on epoch=259
03/19/2022 16:57:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.004521 on epoch=263
03/19/2022 16:57:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004909 on epoch=266
03/19/2022 16:57:07 - INFO - __main__ - Global step 800 Train loss 0.005881 Classification-F1 0.2909224602101568 on epoch=266
03/19/2022 16:57:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002167 on epoch=269
03/19/2022 16:57:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004701 on epoch=273
03/19/2022 16:57:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.022701 on epoch=276
03/19/2022 16:57:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010055 on epoch=279
03/19/2022 16:57:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003267 on epoch=283
03/19/2022 16:57:33 - INFO - __main__ - Global step 850 Train loss 0.008578 Classification-F1 0.4025642777436178 on epoch=283
03/19/2022 16:57:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001801 on epoch=286
03/19/2022 16:57:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002283 on epoch=289
03/19/2022 16:57:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001088 on epoch=293
03/19/2022 16:57:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001664 on epoch=296
03/19/2022 16:57:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.010589 on epoch=299
03/19/2022 16:57:59 - INFO - __main__ - Global step 900 Train loss 0.003485 Classification-F1 0.2980859010270775 on epoch=299
03/19/2022 16:57:59 - INFO - __main__ - save last model!
03/19/2022 16:57:59 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:57:59 - INFO - __main__ - Printing 3 examples
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 16:57:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:57:59 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:57:59 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:57:59 - INFO - __main__ - Printing 3 examples
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 16:57:59 - INFO - __main__ - ['neutral']
03/19/2022 16:57:59 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:57:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:57:59 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:58:06 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 16:58:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 16:58:06 - INFO - __main__ - Printing 3 examples
03/19/2022 16:58:06 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 16:58:06 - INFO - __main__ - ['contradiction']
03/19/2022 16:58:06 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 16:58:06 - INFO - __main__ - ['entailment']
03/19/2022 16:58:06 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 16:58:06 - INFO - __main__ - ['contradiction']
03/19/2022 16:58:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:58:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:58:08 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 16:58:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:58:10 - INFO - __main__ - Starting training!
03/19/2022 16:58:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_21_0.0001_8_predictions.txt
03/19/2022 16:58:27 - INFO - __main__ - Classification-F1 on test data: 0.2415
03/19/2022 16:58:27 - INFO - __main__ - prefix=anli_16_21, lr=0.0001, bsz=8, dev_performance=0.4025642777436178, test_performance=0.24150712510569294
03/19/2022 16:58:27 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.0005, bsz=8 ...
03/19/2022 16:58:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:58:28 - INFO - __main__ - Printing 3 examples
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 16:58:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:58:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 16:58:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 16:58:28 - INFO - __main__ - Printing 3 examples
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 16:58:28 - INFO - __main__ - ['neutral']
03/19/2022 16:58:28 - INFO - __main__ - Tokenizing Input ...
03/19/2022 16:58:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 16:58:28 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 16:58:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 16:58:39 - INFO - __main__ - Starting training!
03/19/2022 16:58:43 - INFO - __main__ - Step 10 Global step 10 Train loss 25.362541 on epoch=3
03/19/2022 16:58:47 - INFO - __main__ - Step 20 Global step 20 Train loss 19.447653 on epoch=6
03/19/2022 16:58:52 - INFO - __main__ - Step 30 Global step 30 Train loss 12.183698 on epoch=9
03/19/2022 16:58:57 - INFO - __main__ - Step 40 Global step 40 Train loss 10.889426 on epoch=13
03/19/2022 16:59:02 - INFO - __main__ - Step 50 Global step 50 Train loss 9.294656 on epoch=16
03/19/2022 16:59:04 - INFO - __main__ - Global step 50 Train loss 15.435595 Classification-F1 0.0634920634920635 on epoch=16
03/19/2022 16:59:10 - INFO - __main__ - Step 60 Global step 60 Train loss 8.331093 on epoch=19
03/19/2022 16:59:15 - INFO - __main__ - Step 70 Global step 70 Train loss 6.755313 on epoch=23
03/19/2022 16:59:20 - INFO - __main__ - Step 80 Global step 80 Train loss 5.716908 on epoch=26
03/19/2022 16:59:24 - INFO - __main__ - Step 90 Global step 90 Train loss 3.611925 on epoch=29
03/19/2022 16:59:29 - INFO - __main__ - Step 100 Global step 100 Train loss 2.904216 on epoch=33
03/19/2022 16:59:30 - INFO - __main__ - Global step 100 Train loss 5.463891 Classification-F1 0.16666666666666666 on epoch=33
03/19/2022 16:59:36 - INFO - __main__ - Step 110 Global step 110 Train loss 2.461171 on epoch=36
03/19/2022 16:59:41 - INFO - __main__ - Step 120 Global step 120 Train loss 2.509069 on epoch=39
03/19/2022 16:59:46 - INFO - __main__ - Step 130 Global step 130 Train loss 2.235746 on epoch=43
03/19/2022 16:59:50 - INFO - __main__ - Step 140 Global step 140 Train loss 2.407238 on epoch=46
03/19/2022 16:59:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.791125 on epoch=49
03/19/2022 16:59:56 - INFO - __main__ - Global step 150 Train loss 2.280870 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 17:00:01 - INFO - __main__ - Step 160 Global step 160 Train loss 2.368383 on epoch=53
03/19/2022 17:00:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.033586 on epoch=56
03/19/2022 17:00:11 - INFO - __main__ - Step 180 Global step 180 Train loss 1.593692 on epoch=59
03/19/2022 17:00:16 - INFO - __main__ - Step 190 Global step 190 Train loss 1.600314 on epoch=63
03/19/2022 17:00:21 - INFO - __main__ - Step 200 Global step 200 Train loss 1.451858 on epoch=66
03/19/2022 17:00:22 - INFO - __main__ - Global step 200 Train loss 1.809567 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 17:00:27 - INFO - __main__ - Step 210 Global step 210 Train loss 1.389224 on epoch=69
03/19/2022 17:00:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.301561 on epoch=73
03/19/2022 17:00:37 - INFO - __main__ - Step 230 Global step 230 Train loss 1.438159 on epoch=76
03/19/2022 17:00:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.016990 on epoch=79
03/19/2022 17:00:47 - INFO - __main__ - Step 250 Global step 250 Train loss 1.050783 on epoch=83
03/19/2022 17:00:48 - INFO - __main__ - Global step 250 Train loss 1.239343 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 17:00:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.220751 on epoch=86
03/19/2022 17:00:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.024717 on epoch=89
03/19/2022 17:01:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.803769 on epoch=93
03/19/2022 17:01:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.817118 on epoch=96
03/19/2022 17:01:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.986235 on epoch=99
03/19/2022 17:01:13 - INFO - __main__ - Global step 300 Train loss 0.970518 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 17:01:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.851966 on epoch=103
03/19/2022 17:01:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.792867 on epoch=106
03/19/2022 17:01:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.753935 on epoch=109
03/19/2022 17:01:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.863388 on epoch=113
03/19/2022 17:01:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.667158 on epoch=116
03/19/2022 17:01:39 - INFO - __main__ - Global step 350 Train loss 0.785863 Classification-F1 0.24761904761904763 on epoch=116
03/19/2022 17:01:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.644881 on epoch=119
03/19/2022 17:01:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.594962 on epoch=123
03/19/2022 17:01:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.654030 on epoch=126
03/19/2022 17:01:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.541031 on epoch=129
03/19/2022 17:02:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.521141 on epoch=133
03/19/2022 17:02:05 - INFO - __main__ - Global step 400 Train loss 0.591209 Classification-F1 0.28487716925850803 on epoch=133
03/19/2022 17:02:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.586199 on epoch=136
03/19/2022 17:02:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.562477 on epoch=139
03/19/2022 17:02:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.562453 on epoch=143
03/19/2022 17:02:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.548328 on epoch=146
03/19/2022 17:02:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.604638 on epoch=149
03/19/2022 17:02:31 - INFO - __main__ - Global step 450 Train loss 0.572819 Classification-F1 0.23590585659551178 on epoch=149
03/19/2022 17:02:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.521981 on epoch=153
03/19/2022 17:02:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.444457 on epoch=156
03/19/2022 17:02:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.494723 on epoch=159
03/19/2022 17:02:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.575174 on epoch=163
03/19/2022 17:02:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.490813 on epoch=166
03/19/2022 17:02:57 - INFO - __main__ - Global step 500 Train loss 0.505430 Classification-F1 0.2085278555866791 on epoch=166
03/19/2022 17:03:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.487095 on epoch=169
03/19/2022 17:03:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.567804 on epoch=173
03/19/2022 17:03:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.462777 on epoch=176
03/19/2022 17:03:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.572808 on epoch=179
03/19/2022 17:03:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.504584 on epoch=183
03/19/2022 17:03:23 - INFO - __main__ - Global step 550 Train loss 0.519014 Classification-F1 0.24611708482676223 on epoch=183
03/19/2022 17:03:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.478938 on epoch=186
03/19/2022 17:03:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.428025 on epoch=189
03/19/2022 17:03:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.481915 on epoch=193
03/19/2022 17:03:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.424684 on epoch=196
03/19/2022 17:03:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.479877 on epoch=199
03/19/2022 17:03:49 - INFO - __main__ - Global step 600 Train loss 0.458688 Classification-F1 0.1639344262295082 on epoch=199
03/19/2022 17:03:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.479869 on epoch=203
03/19/2022 17:03:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.428248 on epoch=206
03/19/2022 17:04:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.416647 on epoch=209
03/19/2022 17:04:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.467944 on epoch=213
03/19/2022 17:04:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.501541 on epoch=216
03/19/2022 17:04:14 - INFO - __main__ - Global step 650 Train loss 0.458850 Classification-F1 0.1983273596176822 on epoch=216
03/19/2022 17:04:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.456879 on epoch=219
03/19/2022 17:04:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.458835 on epoch=223
03/19/2022 17:04:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.444327 on epoch=226
03/19/2022 17:04:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.435804 on epoch=229
03/19/2022 17:04:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.436847 on epoch=233
03/19/2022 17:04:40 - INFO - __main__ - Global step 700 Train loss 0.446538 Classification-F1 0.21253699219800914 on epoch=233
03/19/2022 17:04:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.383342 on epoch=236
03/19/2022 17:04:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.398729 on epoch=239
03/19/2022 17:04:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.437588 on epoch=243
03/19/2022 17:05:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.412655 on epoch=246
03/19/2022 17:05:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.658396 on epoch=249
03/19/2022 17:05:06 - INFO - __main__ - Global step 750 Train loss 0.458142 Classification-F1 0.2085278555866791 on epoch=249
03/19/2022 17:05:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.458305 on epoch=253
03/19/2022 17:05:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.392955 on epoch=256
03/19/2022 17:05:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.443248 on epoch=259
03/19/2022 17:05:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.394398 on epoch=263
03/19/2022 17:05:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.401594 on epoch=266
03/19/2022 17:05:32 - INFO - __main__ - Global step 800 Train loss 0.418100 Classification-F1 0.22441160372194854 on epoch=266
03/19/2022 17:05:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.379471 on epoch=269
03/19/2022 17:05:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.432201 on epoch=273
03/19/2022 17:05:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.409748 on epoch=276
03/19/2022 17:05:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.387790 on epoch=279
03/19/2022 17:05:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.418376 on epoch=283
03/19/2022 17:05:58 - INFO - __main__ - Global step 850 Train loss 0.405517 Classification-F1 0.2619047619047619 on epoch=283
03/19/2022 17:06:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.398815 on epoch=286
03/19/2022 17:06:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.399945 on epoch=289
03/19/2022 17:06:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.403896 on epoch=293
03/19/2022 17:06:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.398150 on epoch=296
03/19/2022 17:06:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.395522 on epoch=299
03/19/2022 17:06:24 - INFO - __main__ - Global step 900 Train loss 0.399266 Classification-F1 0.16666666666666666 on epoch=299
03/19/2022 17:06:24 - INFO - __main__ - save last model!
03/19/2022 17:06:24 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:06:24 - INFO - __main__ - Printing 3 examples
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:06:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:06:24 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:06:24 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:06:24 - INFO - __main__ - Printing 3 examples
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:06:24 - INFO - __main__ - ['neutral']
03/19/2022 17:06:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:06:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:06:24 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:06:31 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:06:32 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:06:32 - INFO - __main__ - Printing 3 examples
03/19/2022 17:06:32 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:06:32 - INFO - __main__ - ['contradiction']
03/19/2022 17:06:32 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:06:32 - INFO - __main__ - ['entailment']
03/19/2022 17:06:32 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:06:32 - INFO - __main__ - ['contradiction']
03/19/2022 17:06:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:06:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:06:34 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:06:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:06:37 - INFO - __main__ - Starting training!
03/19/2022 17:06:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_42_0.0005_8_predictions.txt
03/19/2022 17:06:52 - INFO - __main__ - Classification-F1 on test data: 0.2394
03/19/2022 17:06:52 - INFO - __main__ - prefix=anli_16_42, lr=0.0005, bsz=8, dev_performance=0.28487716925850803, test_performance=0.23944786114527813
03/19/2022 17:06:52 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.0003, bsz=8 ...
03/19/2022 17:06:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:06:53 - INFO - __main__ - Printing 3 examples
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:06:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:06:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:06:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:06:53 - INFO - __main__ - Printing 3 examples
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:06:53 - INFO - __main__ - ['neutral']
03/19/2022 17:06:53 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:06:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:06:54 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:07:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:07:04 - INFO - __main__ - Starting training!
03/19/2022 17:07:09 - INFO - __main__ - Step 10 Global step 10 Train loss 23.587055 on epoch=3
03/19/2022 17:07:13 - INFO - __main__ - Step 20 Global step 20 Train loss 18.439999 on epoch=6
03/19/2022 17:07:18 - INFO - __main__ - Step 30 Global step 30 Train loss 13.644258 on epoch=9
03/19/2022 17:07:23 - INFO - __main__ - Step 40 Global step 40 Train loss 12.412903 on epoch=13
03/19/2022 17:07:28 - INFO - __main__ - Step 50 Global step 50 Train loss 10.814077 on epoch=16
03/19/2022 17:07:29 - INFO - __main__ - Global step 50 Train loss 15.779659 Classification-F1 0.0 on epoch=16
03/19/2022 17:07:35 - INFO - __main__ - Step 60 Global step 60 Train loss 10.383856 on epoch=19
03/19/2022 17:07:40 - INFO - __main__ - Step 70 Global step 70 Train loss 8.702791 on epoch=23
03/19/2022 17:07:45 - INFO - __main__ - Step 80 Global step 80 Train loss 8.020189 on epoch=26
03/19/2022 17:07:50 - INFO - __main__ - Step 90 Global step 90 Train loss 7.516444 on epoch=29
03/19/2022 17:07:55 - INFO - __main__ - Step 100 Global step 100 Train loss 7.185473 on epoch=33
03/19/2022 17:07:56 - INFO - __main__ - Global step 100 Train loss 8.361751 Classification-F1 0.0 on epoch=33
03/19/2022 17:08:01 - INFO - __main__ - Step 110 Global step 110 Train loss 6.383667 on epoch=36
03/19/2022 17:08:06 - INFO - __main__ - Step 120 Global step 120 Train loss 5.157013 on epoch=39
03/19/2022 17:08:10 - INFO - __main__ - Step 130 Global step 130 Train loss 3.933636 on epoch=43
03/19/2022 17:08:15 - INFO - __main__ - Step 140 Global step 140 Train loss 3.607663 on epoch=46
03/19/2022 17:08:20 - INFO - __main__ - Step 150 Global step 150 Train loss 3.054271 on epoch=49
03/19/2022 17:08:21 - INFO - __main__ - Global step 150 Train loss 4.427250 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 17:08:27 - INFO - __main__ - Step 160 Global step 160 Train loss 2.614147 on epoch=53
03/19/2022 17:08:32 - INFO - __main__ - Step 170 Global step 170 Train loss 2.366414 on epoch=56
03/19/2022 17:08:37 - INFO - __main__ - Step 180 Global step 180 Train loss 2.166507 on epoch=59
03/19/2022 17:08:42 - INFO - __main__ - Step 190 Global step 190 Train loss 2.181314 on epoch=63
03/19/2022 17:08:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.917010 on epoch=66
03/19/2022 17:08:48 - INFO - __main__ - Global step 200 Train loss 2.249078 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 17:08:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.899737 on epoch=69
03/19/2022 17:08:58 - INFO - __main__ - Step 220 Global step 220 Train loss 2.044388 on epoch=73
03/19/2022 17:09:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.957834 on epoch=76
03/19/2022 17:09:07 - INFO - __main__ - Step 240 Global step 240 Train loss 1.985507 on epoch=79
03/19/2022 17:09:12 - INFO - __main__ - Step 250 Global step 250 Train loss 2.108550 on epoch=83
03/19/2022 17:09:13 - INFO - __main__ - Global step 250 Train loss 1.999203 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 17:09:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.211119 on epoch=86
03/19/2022 17:09:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.486109 on epoch=89
03/19/2022 17:09:28 - INFO - __main__ - Step 280 Global step 280 Train loss 1.274066 on epoch=93
03/19/2022 17:09:33 - INFO - __main__ - Step 290 Global step 290 Train loss 1.324366 on epoch=96
03/19/2022 17:09:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.535637 on epoch=99
03/19/2022 17:09:39 - INFO - __main__ - Global step 300 Train loss 1.366259 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 17:09:44 - INFO - __main__ - Step 310 Global step 310 Train loss 1.384645 on epoch=103
03/19/2022 17:09:49 - INFO - __main__ - Step 320 Global step 320 Train loss 1.086800 on epoch=106
03/19/2022 17:09:54 - INFO - __main__ - Step 330 Global step 330 Train loss 1.203579 on epoch=109
03/19/2022 17:09:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.497889 on epoch=113
03/19/2022 17:10:04 - INFO - __main__ - Step 350 Global step 350 Train loss 1.414995 on epoch=116
03/19/2022 17:10:05 - INFO - __main__ - Global step 350 Train loss 1.317582 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 17:10:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.093155 on epoch=119
03/19/2022 17:10:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.216472 on epoch=123
03/19/2022 17:10:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.926550 on epoch=126
03/19/2022 17:10:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.805241 on epoch=129
03/19/2022 17:10:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.585323 on epoch=133
03/19/2022 17:10:31 - INFO - __main__ - Global step 400 Train loss 0.925348 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 17:10:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.940283 on epoch=136
03/19/2022 17:10:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.749355 on epoch=139
03/19/2022 17:10:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.782000 on epoch=143
03/19/2022 17:10:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.778768 on epoch=146
03/19/2022 17:10:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.571437 on epoch=149
03/19/2022 17:10:56 - INFO - __main__ - Global step 450 Train loss 0.764369 Classification-F1 0.15053763440860216 on epoch=149
03/19/2022 17:11:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.658086 on epoch=153
03/19/2022 17:11:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.741047 on epoch=156
03/19/2022 17:11:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.676877 on epoch=159
03/19/2022 17:11:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.691234 on epoch=163
03/19/2022 17:11:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.554914 on epoch=166
03/19/2022 17:11:22 - INFO - __main__ - Global step 500 Train loss 0.664431 Classification-F1 0.16129032258064516 on epoch=166
03/19/2022 17:11:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.503601 on epoch=169
03/19/2022 17:11:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.631475 on epoch=173
03/19/2022 17:11:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.464456 on epoch=176
03/19/2022 17:11:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.532768 on epoch=179
03/19/2022 17:11:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.467431 on epoch=183
03/19/2022 17:11:48 - INFO - __main__ - Global step 550 Train loss 0.519946 Classification-F1 0.1693121693121693 on epoch=183
03/19/2022 17:11:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.422182 on epoch=186
03/19/2022 17:11:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.400520 on epoch=189
03/19/2022 17:12:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.422814 on epoch=193
03/19/2022 17:12:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.256274 on epoch=196
03/19/2022 17:12:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.115426 on epoch=199
03/19/2022 17:12:15 - INFO - __main__ - Global step 600 Train loss 0.323443 Classification-F1 0.15300546448087435 on epoch=199
03/19/2022 17:12:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.075464 on epoch=203
03/19/2022 17:12:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.049932 on epoch=206
03/19/2022 17:12:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.015727 on epoch=209
03/19/2022 17:12:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.023027 on epoch=213
03/19/2022 17:12:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.008824 on epoch=216
03/19/2022 17:12:41 - INFO - __main__ - Global step 650 Train loss 0.034595 Classification-F1 0.2605820105820106 on epoch=216
03/19/2022 17:12:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.009713 on epoch=219
03/19/2022 17:12:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.003093 on epoch=223
03/19/2022 17:12:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001317 on epoch=226
03/19/2022 17:13:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.003137 on epoch=229
03/19/2022 17:13:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002011 on epoch=233
03/19/2022 17:13:08 - INFO - __main__ - Global step 700 Train loss 0.003854 Classification-F1 0.22895622895622894 on epoch=233
03/19/2022 17:13:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001699 on epoch=236
03/19/2022 17:13:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004913 on epoch=239
03/19/2022 17:13:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000372 on epoch=243
03/19/2022 17:13:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000677 on epoch=246
03/19/2022 17:13:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003727 on epoch=249
03/19/2022 17:13:34 - INFO - __main__ - Global step 750 Train loss 0.002278 Classification-F1 0.18102312480123572 on epoch=249
03/19/2022 17:13:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001000 on epoch=253
03/19/2022 17:13:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000786 on epoch=256
03/19/2022 17:13:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000362 on epoch=259
03/19/2022 17:13:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001647 on epoch=263
03/19/2022 17:13:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000253 on epoch=266
03/19/2022 17:14:00 - INFO - __main__ - Global step 800 Train loss 0.000809 Classification-F1 0.26750700280112044 on epoch=266
03/19/2022 17:14:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000241 on epoch=269
03/19/2022 17:14:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000509 on epoch=273
03/19/2022 17:14:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004718 on epoch=276
03/19/2022 17:14:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000314 on epoch=279
03/19/2022 17:14:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000635 on epoch=283
03/19/2022 17:14:27 - INFO - __main__ - Global step 850 Train loss 0.001283 Classification-F1 0.24844599844599843 on epoch=283
03/19/2022 17:14:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000907 on epoch=286
03/19/2022 17:14:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006843 on epoch=289
03/19/2022 17:14:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003835 on epoch=293
03/19/2022 17:14:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001140 on epoch=296
03/19/2022 17:14:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001285 on epoch=299
03/19/2022 17:14:53 - INFO - __main__ - Global step 900 Train loss 0.002802 Classification-F1 0.2389751170238975 on epoch=299
03/19/2022 17:14:53 - INFO - __main__ - save last model!
03/19/2022 17:14:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:14:53 - INFO - __main__ - Printing 3 examples
03/19/2022 17:14:53 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:14:53 - INFO - __main__ - ['neutral']
03/19/2022 17:14:53 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:14:53 - INFO - __main__ - ['neutral']
03/19/2022 17:14:53 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:14:53 - INFO - __main__ - ['neutral']
03/19/2022 17:14:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:14:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:14:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:14:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:14:54 - INFO - __main__ - Printing 3 examples
03/19/2022 17:14:54 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:14:54 - INFO - __main__ - ['neutral']
03/19/2022 17:14:54 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:14:54 - INFO - __main__ - ['neutral']
03/19/2022 17:14:54 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:14:54 - INFO - __main__ - ['neutral']
03/19/2022 17:14:54 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:14:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:14:54 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:15:00 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:15:01 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:15:01 - INFO - __main__ - Printing 3 examples
03/19/2022 17:15:01 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/19/2022 17:15:01 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:15:01 - INFO - __main__ - ['entailment']
03/19/2022 17:15:01 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:15:01 - INFO - __main__ - ['contradiction']
03/19/2022 17:15:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:15:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:15:02 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:15:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:15:05 - INFO - __main__ - Starting training!
03/19/2022 17:15:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_42_0.0003_8_predictions.txt
03/19/2022 17:15:20 - INFO - __main__ - Classification-F1 on test data: 0.1977
03/19/2022 17:15:20 - INFO - __main__ - prefix=anli_16_42, lr=0.0003, bsz=8, dev_performance=0.26750700280112044, test_performance=0.19769759993084318
03/19/2022 17:15:20 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.0002, bsz=8 ...
03/19/2022 17:15:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:15:21 - INFO - __main__ - Printing 3 examples
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:15:21 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:15:21 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:15:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:15:21 - INFO - __main__ - Printing 3 examples
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:15:21 - INFO - __main__ - ['neutral']
03/19/2022 17:15:21 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:15:22 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:15:22 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:15:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:15:32 - INFO - __main__ - Starting training!
03/19/2022 17:15:37 - INFO - __main__ - Step 10 Global step 10 Train loss 25.094940 on epoch=3
03/19/2022 17:15:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.987545 on epoch=6
03/19/2022 17:15:46 - INFO - __main__ - Step 30 Global step 30 Train loss 12.566924 on epoch=9
03/19/2022 17:15:51 - INFO - __main__ - Step 40 Global step 40 Train loss 12.373352 on epoch=13
03/19/2022 17:15:56 - INFO - __main__ - Step 50 Global step 50 Train loss 11.051953 on epoch=16
03/19/2022 17:15:57 - INFO - __main__ - Global step 50 Train loss 15.814942 Classification-F1 0.05555555555555555 on epoch=16
03/19/2022 17:16:03 - INFO - __main__ - Step 60 Global step 60 Train loss 11.022493 on epoch=19
03/19/2022 17:16:08 - INFO - __main__ - Step 70 Global step 70 Train loss 10.325509 on epoch=23
03/19/2022 17:16:13 - INFO - __main__ - Step 80 Global step 80 Train loss 9.552050 on epoch=26
03/19/2022 17:16:18 - INFO - __main__ - Step 90 Global step 90 Train loss 9.355709 on epoch=29
03/19/2022 17:16:22 - INFO - __main__ - Step 100 Global step 100 Train loss 8.754696 on epoch=33
03/19/2022 17:16:23 - INFO - __main__ - Global step 100 Train loss 9.802091 Classification-F1 0.0 on epoch=33
03/19/2022 17:16:28 - INFO - __main__ - Step 110 Global step 110 Train loss 8.095694 on epoch=36
03/19/2022 17:16:33 - INFO - __main__ - Step 120 Global step 120 Train loss 7.707001 on epoch=39
03/19/2022 17:16:38 - INFO - __main__ - Step 130 Global step 130 Train loss 7.679797 on epoch=43
03/19/2022 17:16:43 - INFO - __main__ - Step 140 Global step 140 Train loss 6.917607 on epoch=46
03/19/2022 17:16:48 - INFO - __main__ - Step 150 Global step 150 Train loss 5.485498 on epoch=49
03/19/2022 17:16:49 - INFO - __main__ - Global step 150 Train loss 7.177118 Classification-F1 0.0 on epoch=49
03/19/2022 17:16:54 - INFO - __main__ - Step 160 Global step 160 Train loss 5.287235 on epoch=53
03/19/2022 17:16:59 - INFO - __main__ - Step 170 Global step 170 Train loss 4.798252 on epoch=56
03/19/2022 17:17:04 - INFO - __main__ - Step 180 Global step 180 Train loss 3.534276 on epoch=59
03/19/2022 17:17:09 - INFO - __main__ - Step 190 Global step 190 Train loss 3.217759 on epoch=63
03/19/2022 17:17:14 - INFO - __main__ - Step 200 Global step 200 Train loss 2.737624 on epoch=66
03/19/2022 17:17:15 - INFO - __main__ - Global step 200 Train loss 3.915029 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 17:17:21 - INFO - __main__ - Step 210 Global step 210 Train loss 2.525274 on epoch=69
03/19/2022 17:17:26 - INFO - __main__ - Step 220 Global step 220 Train loss 3.148663 on epoch=73
03/19/2022 17:17:31 - INFO - __main__ - Step 230 Global step 230 Train loss 3.236626 on epoch=76
03/19/2022 17:17:36 - INFO - __main__ - Step 240 Global step 240 Train loss 2.668041 on epoch=79
03/19/2022 17:17:41 - INFO - __main__ - Step 250 Global step 250 Train loss 2.479629 on epoch=83
03/19/2022 17:17:42 - INFO - __main__ - Global step 250 Train loss 2.811647 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 17:17:47 - INFO - __main__ - Step 260 Global step 260 Train loss 2.869455 on epoch=86
03/19/2022 17:17:52 - INFO - __main__ - Step 270 Global step 270 Train loss 2.360694 on epoch=89
03/19/2022 17:17:57 - INFO - __main__ - Step 280 Global step 280 Train loss 2.019569 on epoch=93
03/19/2022 17:18:02 - INFO - __main__ - Step 290 Global step 290 Train loss 2.339689 on epoch=96
03/19/2022 17:18:06 - INFO - __main__ - Step 300 Global step 300 Train loss 1.766501 on epoch=99
03/19/2022 17:18:07 - INFO - __main__ - Global step 300 Train loss 2.271182 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 17:18:12 - INFO - __main__ - Step 310 Global step 310 Train loss 2.084535 on epoch=103
03/19/2022 17:18:17 - INFO - __main__ - Step 320 Global step 320 Train loss 1.998272 on epoch=106
03/19/2022 17:18:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.791852 on epoch=109
03/19/2022 17:18:27 - INFO - __main__ - Step 340 Global step 340 Train loss 2.224308 on epoch=113
03/19/2022 17:18:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.919755 on epoch=116
03/19/2022 17:18:33 - INFO - __main__ - Global step 350 Train loss 2.003745 Classification-F1 0.16666666666666666 on epoch=116
03/19/2022 17:18:38 - INFO - __main__ - Step 360 Global step 360 Train loss 1.765387 on epoch=119
03/19/2022 17:18:43 - INFO - __main__ - Step 370 Global step 370 Train loss 1.747074 on epoch=123
03/19/2022 17:18:48 - INFO - __main__ - Step 380 Global step 380 Train loss 1.486398 on epoch=126
03/19/2022 17:18:53 - INFO - __main__ - Step 390 Global step 390 Train loss 1.441105 on epoch=129
03/19/2022 17:18:58 - INFO - __main__ - Step 400 Global step 400 Train loss 1.825534 on epoch=133
03/19/2022 17:18:59 - INFO - __main__ - Global step 400 Train loss 1.653100 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 17:19:04 - INFO - __main__ - Step 410 Global step 410 Train loss 1.815702 on epoch=136
03/19/2022 17:19:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.399419 on epoch=139
03/19/2022 17:19:14 - INFO - __main__ - Step 430 Global step 430 Train loss 1.651692 on epoch=143
03/19/2022 17:19:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.278128 on epoch=146
03/19/2022 17:19:24 - INFO - __main__ - Step 450 Global step 450 Train loss 1.318216 on epoch=149
03/19/2022 17:19:25 - INFO - __main__ - Global step 450 Train loss 1.492632 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 17:19:30 - INFO - __main__ - Step 460 Global step 460 Train loss 1.118328 on epoch=153
03/19/2022 17:19:34 - INFO - __main__ - Step 470 Global step 470 Train loss 1.270577 on epoch=156
03/19/2022 17:19:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.983719 on epoch=159
03/19/2022 17:19:44 - INFO - __main__ - Step 490 Global step 490 Train loss 1.156446 on epoch=163
03/19/2022 17:19:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.994238 on epoch=166
03/19/2022 17:19:50 - INFO - __main__ - Global step 500 Train loss 1.104662 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 17:19:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.271767 on epoch=169
03/19/2022 17:20:00 - INFO - __main__ - Step 520 Global step 520 Train loss 1.103878 on epoch=173
03/19/2022 17:20:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.809490 on epoch=176
03/19/2022 17:20:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.923742 on epoch=179
03/19/2022 17:20:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.949739 on epoch=183
03/19/2022 17:20:16 - INFO - __main__ - Global step 550 Train loss 1.011723 Classification-F1 0.1693121693121693 on epoch=183
03/19/2022 17:20:21 - INFO - __main__ - Step 560 Global step 560 Train loss 1.051548 on epoch=186
03/19/2022 17:20:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.910709 on epoch=189
03/19/2022 17:20:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.906145 on epoch=193
03/19/2022 17:20:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.941115 on epoch=196
03/19/2022 17:20:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.697814 on epoch=199
03/19/2022 17:20:42 - INFO - __main__ - Global step 600 Train loss 0.901466 Classification-F1 0.2138271604938272 on epoch=199
03/19/2022 17:20:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.725665 on epoch=203
03/19/2022 17:20:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.524403 on epoch=206
03/19/2022 17:20:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.513463 on epoch=209
03/19/2022 17:21:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.526651 on epoch=213
03/19/2022 17:21:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.428089 on epoch=216
03/19/2022 17:21:09 - INFO - __main__ - Global step 650 Train loss 0.543654 Classification-F1 0.43840022947528023 on epoch=216
03/19/2022 17:21:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.480996 on epoch=219
03/19/2022 17:21:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.509689 on epoch=223
03/19/2022 17:21:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.474236 on epoch=226
03/19/2022 17:21:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.477181 on epoch=229
03/19/2022 17:21:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.413819 on epoch=233
03/19/2022 17:21:35 - INFO - __main__ - Global step 700 Train loss 0.471184 Classification-F1 0.26508667983322365 on epoch=233
03/19/2022 17:21:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.419034 on epoch=236
03/19/2022 17:21:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.477706 on epoch=239
03/19/2022 17:21:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.442366 on epoch=243
03/19/2022 17:21:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.463306 on epoch=246
03/19/2022 17:22:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.611376 on epoch=249
03/19/2022 17:22:01 - INFO - __main__ - Global step 750 Train loss 0.482758 Classification-F1 0.1680636148721255 on epoch=249
03/19/2022 17:22:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.560390 on epoch=253
03/19/2022 17:22:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.480875 on epoch=256
03/19/2022 17:22:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.477923 on epoch=259
03/19/2022 17:22:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.452691 on epoch=263
03/19/2022 17:22:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.403059 on epoch=266
03/19/2022 17:22:28 - INFO - __main__ - Global step 800 Train loss 0.474988 Classification-F1 0.2450388265746333 on epoch=266
03/19/2022 17:22:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.411727 on epoch=269
03/19/2022 17:22:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.431515 on epoch=273
03/19/2022 17:22:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.459400 on epoch=276
03/19/2022 17:22:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.447229 on epoch=279
03/19/2022 17:22:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.438619 on epoch=283
03/19/2022 17:22:54 - INFO - __main__ - Global step 850 Train loss 0.437698 Classification-F1 0.2933333333333333 on epoch=283
03/19/2022 17:22:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.422425 on epoch=286
03/19/2022 17:23:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.408536 on epoch=289
03/19/2022 17:23:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.419555 on epoch=293
03/19/2022 17:23:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.439258 on epoch=296
03/19/2022 17:23:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.469913 on epoch=299
03/19/2022 17:23:20 - INFO - __main__ - Global step 900 Train loss 0.431938 Classification-F1 0.2216841538875437 on epoch=299
03/19/2022 17:23:20 - INFO - __main__ - save last model!
03/19/2022 17:23:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:23:20 - INFO - __main__ - Printing 3 examples
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:23:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:23:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:23:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:23:20 - INFO - __main__ - Printing 3 examples
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:23:20 - INFO - __main__ - ['neutral']
03/19/2022 17:23:20 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:23:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:23:20 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:23:27 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:23:28 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:23:28 - INFO - __main__ - Printing 3 examples
03/19/2022 17:23:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:23:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:23:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:23:28 - INFO - __main__ - ['entailment']
03/19/2022 17:23:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:23:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:23:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:23:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:23:29 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:23:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:23:33 - INFO - __main__ - Starting training!
03/19/2022 17:23:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_42_0.0002_8_predictions.txt
03/19/2022 17:23:47 - INFO - __main__ - Classification-F1 on test data: 0.3047
03/19/2022 17:23:48 - INFO - __main__ - prefix=anli_16_42, lr=0.0002, bsz=8, dev_performance=0.43840022947528023, test_performance=0.30466756661314826
03/19/2022 17:23:48 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.0001, bsz=8 ...
03/19/2022 17:23:49 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:23:49 - INFO - __main__ - Printing 3 examples
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:23:49 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:23:49 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:23:49 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:23:49 - INFO - __main__ - Printing 3 examples
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: Charlotte Anley (17961893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 183638 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
03/19/2022 17:23:49 - INFO - __main__ - ['neutral']
03/19/2022 17:23:49 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:23:49 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:23:49 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:24:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:24:01 - INFO - __main__ - Starting training!
03/19/2022 17:24:05 - INFO - __main__ - Step 10 Global step 10 Train loss 23.797808 on epoch=3
03/19/2022 17:24:09 - INFO - __main__ - Step 20 Global step 20 Train loss 23.066107 on epoch=6
03/19/2022 17:24:14 - INFO - __main__ - Step 30 Global step 30 Train loss 18.425747 on epoch=9
03/19/2022 17:24:19 - INFO - __main__ - Step 40 Global step 40 Train loss 15.958446 on epoch=13
03/19/2022 17:24:25 - INFO - __main__ - Step 50 Global step 50 Train loss 14.052068 on epoch=16
03/19/2022 17:24:39 - INFO - __main__ - Global step 50 Train loss 19.060034 Classification-F1 0.009652509652509652 on epoch=16
03/19/2022 17:24:44 - INFO - __main__ - Step 60 Global step 60 Train loss 12.725376 on epoch=19
03/19/2022 17:24:49 - INFO - __main__ - Step 70 Global step 70 Train loss 12.285527 on epoch=23
03/19/2022 17:24:54 - INFO - __main__ - Step 80 Global step 80 Train loss 11.175241 on epoch=26
03/19/2022 17:24:59 - INFO - __main__ - Step 90 Global step 90 Train loss 11.109553 on epoch=29
03/19/2022 17:25:04 - INFO - __main__ - Step 100 Global step 100 Train loss 10.538363 on epoch=33
03/19/2022 17:25:07 - INFO - __main__ - Global step 100 Train loss 11.566813 Classification-F1 0.0 on epoch=33
03/19/2022 17:25:12 - INFO - __main__ - Step 110 Global step 110 Train loss 10.948021 on epoch=36
03/19/2022 17:25:17 - INFO - __main__ - Step 120 Global step 120 Train loss 10.230914 on epoch=39
03/19/2022 17:25:22 - INFO - __main__ - Step 130 Global step 130 Train loss 10.350718 on epoch=43
03/19/2022 17:25:27 - INFO - __main__ - Step 140 Global step 140 Train loss 9.835298 on epoch=46
03/19/2022 17:25:32 - INFO - __main__ - Step 150 Global step 150 Train loss 9.013653 on epoch=49
03/19/2022 17:25:33 - INFO - __main__ - Global step 150 Train loss 10.075722 Classification-F1 0.0 on epoch=49
03/19/2022 17:25:38 - INFO - __main__ - Step 160 Global step 160 Train loss 9.805588 on epoch=53
03/19/2022 17:25:43 - INFO - __main__ - Step 170 Global step 170 Train loss 9.107946 on epoch=56
03/19/2022 17:25:48 - INFO - __main__ - Step 180 Global step 180 Train loss 8.842974 on epoch=59
03/19/2022 17:25:53 - INFO - __main__ - Step 190 Global step 190 Train loss 8.847740 on epoch=63
03/19/2022 17:25:58 - INFO - __main__ - Step 200 Global step 200 Train loss 8.950968 on epoch=66
03/19/2022 17:25:59 - INFO - __main__ - Global step 200 Train loss 9.111043 Classification-F1 0.0 on epoch=66
03/19/2022 17:26:04 - INFO - __main__ - Step 210 Global step 210 Train loss 8.344836 on epoch=69
03/19/2022 17:26:09 - INFO - __main__ - Step 220 Global step 220 Train loss 8.643924 on epoch=73
03/19/2022 17:26:14 - INFO - __main__ - Step 230 Global step 230 Train loss 8.013191 on epoch=76
03/19/2022 17:26:19 - INFO - __main__ - Step 240 Global step 240 Train loss 8.010084 on epoch=79
03/19/2022 17:26:24 - INFO - __main__ - Step 250 Global step 250 Train loss 7.599621 on epoch=83
03/19/2022 17:26:25 - INFO - __main__ - Global step 250 Train loss 8.122332 Classification-F1 0.0 on epoch=83
03/19/2022 17:26:30 - INFO - __main__ - Step 260 Global step 260 Train loss 7.016668 on epoch=86
03/19/2022 17:26:35 - INFO - __main__ - Step 270 Global step 270 Train loss 6.342753 on epoch=89
03/19/2022 17:26:40 - INFO - __main__ - Step 280 Global step 280 Train loss 6.582316 on epoch=93
03/19/2022 17:26:45 - INFO - __main__ - Step 290 Global step 290 Train loss 6.006006 on epoch=96
03/19/2022 17:26:50 - INFO - __main__ - Step 300 Global step 300 Train loss 5.915186 on epoch=99
03/19/2022 17:26:51 - INFO - __main__ - Global step 300 Train loss 6.372586 Classification-F1 0.0 on epoch=99
03/19/2022 17:26:56 - INFO - __main__ - Step 310 Global step 310 Train loss 5.690104 on epoch=103
03/19/2022 17:27:01 - INFO - __main__ - Step 320 Global step 320 Train loss 5.122967 on epoch=106
03/19/2022 17:27:06 - INFO - __main__ - Step 330 Global step 330 Train loss 4.443747 on epoch=109
03/19/2022 17:27:11 - INFO - __main__ - Step 340 Global step 340 Train loss 4.228152 on epoch=113
03/19/2022 17:27:16 - INFO - __main__ - Step 350 Global step 350 Train loss 3.667871 on epoch=116
03/19/2022 17:27:17 - INFO - __main__ - Global step 350 Train loss 4.630569 Classification-F1 0.0 on epoch=116
03/19/2022 17:27:22 - INFO - __main__ - Step 360 Global step 360 Train loss 3.289160 on epoch=119
03/19/2022 17:27:27 - INFO - __main__ - Step 370 Global step 370 Train loss 3.054683 on epoch=123
03/19/2022 17:27:32 - INFO - __main__ - Step 380 Global step 380 Train loss 3.206535 on epoch=126
03/19/2022 17:27:37 - INFO - __main__ - Step 390 Global step 390 Train loss 2.815050 on epoch=129
03/19/2022 17:27:42 - INFO - __main__ - Step 400 Global step 400 Train loss 3.200811 on epoch=133
03/19/2022 17:27:43 - INFO - __main__ - Global step 400 Train loss 3.113248 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 17:27:49 - INFO - __main__ - Step 410 Global step 410 Train loss 2.457365 on epoch=136
03/19/2022 17:27:54 - INFO - __main__ - Step 420 Global step 420 Train loss 2.221331 on epoch=139
03/19/2022 17:27:59 - INFO - __main__ - Step 430 Global step 430 Train loss 2.792249 on epoch=143
03/19/2022 17:28:04 - INFO - __main__ - Step 440 Global step 440 Train loss 2.455440 on epoch=146
03/19/2022 17:28:09 - INFO - __main__ - Step 450 Global step 450 Train loss 2.294349 on epoch=149
03/19/2022 17:28:10 - INFO - __main__ - Global step 450 Train loss 2.444147 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 17:28:15 - INFO - __main__ - Step 460 Global step 460 Train loss 2.431659 on epoch=153
03/19/2022 17:28:20 - INFO - __main__ - Step 470 Global step 470 Train loss 2.409552 on epoch=156
03/19/2022 17:28:25 - INFO - __main__ - Step 480 Global step 480 Train loss 2.107232 on epoch=159
03/19/2022 17:28:30 - INFO - __main__ - Step 490 Global step 490 Train loss 2.685027 on epoch=163
03/19/2022 17:28:35 - INFO - __main__ - Step 500 Global step 500 Train loss 2.277025 on epoch=166
03/19/2022 17:28:36 - INFO - __main__ - Global step 500 Train loss 2.382099 Classification-F1 0.16666666666666666 on epoch=166
03/19/2022 17:28:41 - INFO - __main__ - Step 510 Global step 510 Train loss 2.457711 on epoch=169
03/19/2022 17:28:46 - INFO - __main__ - Step 520 Global step 520 Train loss 1.951581 on epoch=173
03/19/2022 17:28:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.858941 on epoch=176
03/19/2022 17:28:56 - INFO - __main__ - Step 540 Global step 540 Train loss 2.071586 on epoch=179
03/19/2022 17:29:01 - INFO - __main__ - Step 550 Global step 550 Train loss 2.119715 on epoch=183
03/19/2022 17:29:02 - INFO - __main__ - Global step 550 Train loss 2.091907 Classification-F1 0.22222222222222224 on epoch=183
03/19/2022 17:29:07 - INFO - __main__ - Step 560 Global step 560 Train loss 2.661582 on epoch=186
03/19/2022 17:29:12 - INFO - __main__ - Step 570 Global step 570 Train loss 1.877690 on epoch=189
03/19/2022 17:29:17 - INFO - __main__ - Step 580 Global step 580 Train loss 2.313023 on epoch=193
03/19/2022 17:29:22 - INFO - __main__ - Step 590 Global step 590 Train loss 2.457613 on epoch=196
03/19/2022 17:29:27 - INFO - __main__ - Step 600 Global step 600 Train loss 2.250853 on epoch=199
03/19/2022 17:29:28 - INFO - __main__ - Global step 600 Train loss 2.312152 Classification-F1 0.16666666666666666 on epoch=199
03/19/2022 17:29:33 - INFO - __main__ - Step 610 Global step 610 Train loss 2.331755 on epoch=203
03/19/2022 17:29:38 - INFO - __main__ - Step 620 Global step 620 Train loss 1.594420 on epoch=206
03/19/2022 17:29:43 - INFO - __main__ - Step 630 Global step 630 Train loss 1.965287 on epoch=209
03/19/2022 17:29:48 - INFO - __main__ - Step 640 Global step 640 Train loss 1.511550 on epoch=213
03/19/2022 17:29:53 - INFO - __main__ - Step 650 Global step 650 Train loss 1.873201 on epoch=216
03/19/2022 17:29:54 - INFO - __main__ - Global step 650 Train loss 1.855242 Classification-F1 0.16666666666666666 on epoch=216
03/19/2022 17:29:59 - INFO - __main__ - Step 660 Global step 660 Train loss 1.753141 on epoch=219
03/19/2022 17:30:04 - INFO - __main__ - Step 670 Global step 670 Train loss 2.372273 on epoch=223
03/19/2022 17:30:09 - INFO - __main__ - Step 680 Global step 680 Train loss 1.600365 on epoch=226
03/19/2022 17:30:14 - INFO - __main__ - Step 690 Global step 690 Train loss 1.676818 on epoch=229
03/19/2022 17:30:19 - INFO - __main__ - Step 700 Global step 700 Train loss 1.804146 on epoch=233
03/19/2022 17:30:20 - INFO - __main__ - Global step 700 Train loss 1.841349 Classification-F1 0.15053763440860216 on epoch=233
03/19/2022 17:30:25 - INFO - __main__ - Step 710 Global step 710 Train loss 1.547379 on epoch=236
03/19/2022 17:30:30 - INFO - __main__ - Step 720 Global step 720 Train loss 1.518364 on epoch=239
03/19/2022 17:30:35 - INFO - __main__ - Step 730 Global step 730 Train loss 1.660271 on epoch=243
03/19/2022 17:30:39 - INFO - __main__ - Step 740 Global step 740 Train loss 1.708466 on epoch=246
03/19/2022 17:30:44 - INFO - __main__ - Step 750 Global step 750 Train loss 1.361694 on epoch=249
03/19/2022 17:30:45 - INFO - __main__ - Global step 750 Train loss 1.559235 Classification-F1 0.2205387205387205 on epoch=249
03/19/2022 17:30:50 - INFO - __main__ - Step 760 Global step 760 Train loss 1.459273 on epoch=253
03/19/2022 17:30:55 - INFO - __main__ - Step 770 Global step 770 Train loss 1.074187 on epoch=256
03/19/2022 17:31:00 - INFO - __main__ - Step 780 Global step 780 Train loss 1.638770 on epoch=259
03/19/2022 17:31:05 - INFO - __main__ - Step 790 Global step 790 Train loss 1.455125 on epoch=263
03/19/2022 17:31:10 - INFO - __main__ - Step 800 Global step 800 Train loss 1.156611 on epoch=266
03/19/2022 17:31:11 - INFO - __main__ - Global step 800 Train loss 1.356793 Classification-F1 0.16666666666666666 on epoch=266
03/19/2022 17:31:16 - INFO - __main__ - Step 810 Global step 810 Train loss 1.317452 on epoch=269
03/19/2022 17:31:21 - INFO - __main__ - Step 820 Global step 820 Train loss 1.175414 on epoch=273
03/19/2022 17:31:26 - INFO - __main__ - Step 830 Global step 830 Train loss 1.317640 on epoch=276
03/19/2022 17:31:31 - INFO - __main__ - Step 840 Global step 840 Train loss 1.447412 on epoch=279
03/19/2022 17:31:36 - INFO - __main__ - Step 850 Global step 850 Train loss 1.135333 on epoch=283
03/19/2022 17:31:37 - INFO - __main__ - Global step 850 Train loss 1.278650 Classification-F1 0.24033437826541273 on epoch=283
03/19/2022 17:31:42 - INFO - __main__ - Step 860 Global step 860 Train loss 1.015092 on epoch=286
03/19/2022 17:31:47 - INFO - __main__ - Step 870 Global step 870 Train loss 1.069091 on epoch=289
03/19/2022 17:31:52 - INFO - __main__ - Step 880 Global step 880 Train loss 1.049479 on epoch=293
03/19/2022 17:31:57 - INFO - __main__ - Step 890 Global step 890 Train loss 1.163114 on epoch=296
03/19/2022 17:32:02 - INFO - __main__ - Step 900 Global step 900 Train loss 1.164929 on epoch=299
03/19/2022 17:32:03 - INFO - __main__ - Global step 900 Train loss 1.092341 Classification-F1 0.24888888888888885 on epoch=299
03/19/2022 17:32:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:32:03 - INFO - __main__ - Printing 3 examples
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:32:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:32:03 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:32:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:32:03 - INFO - __main__ - Printing 3 examples
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:32:03 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:32:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:32:03 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:32:04 - INFO - __main__ - save last model!
03/19/2022 17:32:10 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:32:11 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:32:11 - INFO - __main__ - Printing 3 examples
03/19/2022 17:32:11 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:32:11 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:11 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:32:11 - INFO - __main__ - ['entailment']
03/19/2022 17:32:11 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:32:11 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:32:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:32:13 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:32:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:32:15 - INFO - __main__ - Starting training!
03/19/2022 17:32:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_42_0.0001_8_predictions.txt
03/19/2022 17:32:29 - INFO - __main__ - Classification-F1 on test data: 0.2629
03/19/2022 17:32:30 - INFO - __main__ - prefix=anli_16_42, lr=0.0001, bsz=8, dev_performance=0.24888888888888885, test_performance=0.26293196365204125
03/19/2022 17:32:30 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.0005, bsz=8 ...
03/19/2022 17:32:31 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:32:31 - INFO - __main__ - Printing 3 examples
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:32:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:32:31 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:32:31 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:32:31 - INFO - __main__ - Printing 3 examples
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:32:31 - INFO - __main__ - ['contradiction']
03/19/2022 17:32:31 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:32:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:32:31 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:32:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:32:42 - INFO - __main__ - Starting training!
03/19/2022 17:32:46 - INFO - __main__ - Step 10 Global step 10 Train loss 23.749470 on epoch=3
03/19/2022 17:32:51 - INFO - __main__ - Step 20 Global step 20 Train loss 17.755142 on epoch=6
03/19/2022 17:32:56 - INFO - __main__ - Step 30 Global step 30 Train loss 11.677241 on epoch=9
03/19/2022 17:33:01 - INFO - __main__ - Step 40 Global step 40 Train loss 10.244864 on epoch=13
03/19/2022 17:33:06 - INFO - __main__ - Step 50 Global step 50 Train loss 9.130664 on epoch=16
03/19/2022 17:33:07 - INFO - __main__ - Global step 50 Train loss 14.511475 Classification-F1 0.06 on epoch=16
03/19/2022 17:33:13 - INFO - __main__ - Step 60 Global step 60 Train loss 7.995398 on epoch=19
03/19/2022 17:33:18 - INFO - __main__ - Step 70 Global step 70 Train loss 6.992171 on epoch=23
03/19/2022 17:33:23 - INFO - __main__ - Step 80 Global step 80 Train loss 5.563830 on epoch=26
03/19/2022 17:33:28 - INFO - __main__ - Step 90 Global step 90 Train loss 3.970490 on epoch=29
03/19/2022 17:33:33 - INFO - __main__ - Step 100 Global step 100 Train loss 2.405992 on epoch=33
03/19/2022 17:33:34 - INFO - __main__ - Global step 100 Train loss 5.385576 Classification-F1 0.16666666666666666 on epoch=33
03/19/2022 17:33:40 - INFO - __main__ - Step 110 Global step 110 Train loss 2.309383 on epoch=36
03/19/2022 17:33:44 - INFO - __main__ - Step 120 Global step 120 Train loss 2.409357 on epoch=39
03/19/2022 17:33:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.593721 on epoch=43
03/19/2022 17:33:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.911070 on epoch=46
03/19/2022 17:33:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.841798 on epoch=49
03/19/2022 17:34:00 - INFO - __main__ - Global step 150 Train loss 2.013066 Classification-F1 0.16666666666666666 on epoch=49
03/19/2022 17:34:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.513185 on epoch=53
03/19/2022 17:34:10 - INFO - __main__ - Step 170 Global step 170 Train loss 1.524410 on epoch=56
03/19/2022 17:34:15 - INFO - __main__ - Step 180 Global step 180 Train loss 1.786303 on epoch=59
03/19/2022 17:34:20 - INFO - __main__ - Step 190 Global step 190 Train loss 1.121041 on epoch=63
03/19/2022 17:34:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.186358 on epoch=66
03/19/2022 17:34:26 - INFO - __main__ - Global step 200 Train loss 1.426259 Classification-F1 0.3498019595580571 on epoch=66
03/19/2022 17:34:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.111064 on epoch=69
03/19/2022 17:34:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.819939 on epoch=73
03/19/2022 17:34:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.874242 on epoch=76
03/19/2022 17:34:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.785233 on epoch=79
03/19/2022 17:34:51 - INFO - __main__ - Step 250 Global step 250 Train loss 1.119631 on epoch=83
03/19/2022 17:34:52 - INFO - __main__ - Global step 250 Train loss 0.942022 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 17:34:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.870355 on epoch=86
03/19/2022 17:35:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.555646 on epoch=89
03/19/2022 17:35:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.538547 on epoch=93
03/19/2022 17:35:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.825497 on epoch=96
03/19/2022 17:35:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.424654 on epoch=99
03/19/2022 17:35:18 - INFO - __main__ - Global step 300 Train loss 0.642940 Classification-F1 0.16666666666666666 on epoch=99
03/19/2022 17:35:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.582755 on epoch=103
03/19/2022 17:35:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.434336 on epoch=106
03/19/2022 17:35:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.514639 on epoch=109
03/19/2022 17:35:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.446806 on epoch=113
03/19/2022 17:35:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.351099 on epoch=116
03/19/2022 17:35:43 - INFO - __main__ - Global step 350 Train loss 0.465927 Classification-F1 0.24970882832518052 on epoch=116
03/19/2022 17:35:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.342250 on epoch=119
03/19/2022 17:35:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.267427 on epoch=123
03/19/2022 17:35:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.147294 on epoch=126
03/19/2022 17:36:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.140085 on epoch=129
03/19/2022 17:36:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.083416 on epoch=133
03/19/2022 17:36:09 - INFO - __main__ - Global step 400 Train loss 0.196094 Classification-F1 0.23076923076923075 on epoch=133
03/19/2022 17:36:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.020806 on epoch=136
03/19/2022 17:36:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005653 on epoch=139
03/19/2022 17:36:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.176370 on epoch=143
03/19/2022 17:36:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.038479 on epoch=146
03/19/2022 17:36:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.031048 on epoch=149
03/19/2022 17:36:35 - INFO - __main__ - Global step 450 Train loss 0.054471 Classification-F1 0.29855873973521035 on epoch=149
03/19/2022 17:36:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.016892 on epoch=153
03/19/2022 17:36:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010001 on epoch=156
03/19/2022 17:36:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003935 on epoch=159
03/19/2022 17:36:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001584 on epoch=163
03/19/2022 17:37:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000786 on epoch=166
03/19/2022 17:37:01 - INFO - __main__ - Global step 500 Train loss 0.006640 Classification-F1 0.3238095238095238 on epoch=166
03/19/2022 17:37:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002124 on epoch=169
03/19/2022 17:37:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000927 on epoch=173
03/19/2022 17:37:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002452 on epoch=176
03/19/2022 17:37:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001906 on epoch=179
03/19/2022 17:37:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.017875 on epoch=183
03/19/2022 17:37:27 - INFO - __main__ - Global step 550 Train loss 0.005057 Classification-F1 0.2914653784219001 on epoch=183
03/19/2022 17:37:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018377 on epoch=186
03/19/2022 17:37:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003571 on epoch=189
03/19/2022 17:37:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000731 on epoch=193
03/19/2022 17:37:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000248 on epoch=196
03/19/2022 17:37:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000440 on epoch=199
03/19/2022 17:37:52 - INFO - __main__ - Global step 600 Train loss 0.004673 Classification-F1 0.2863488488488488 on epoch=199
03/19/2022 17:37:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000816 on epoch=203
03/19/2022 17:38:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000335 on epoch=206
03/19/2022 17:38:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000489 on epoch=209
03/19/2022 17:38:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001445 on epoch=213
03/19/2022 17:38:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000524 on epoch=216
03/19/2022 17:38:18 - INFO - __main__ - Global step 650 Train loss 0.000722 Classification-F1 0.32477477477477473 on epoch=216
03/19/2022 17:38:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000255 on epoch=219
03/19/2022 17:38:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000131 on epoch=223
03/19/2022 17:38:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000598 on epoch=226
03/19/2022 17:38:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.026093 on epoch=229
03/19/2022 17:38:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001084 on epoch=233
03/19/2022 17:38:43 - INFO - __main__ - Global step 700 Train loss 0.005632 Classification-F1 0.4147323794382618 on epoch=233
03/19/2022 17:38:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000416 on epoch=236
03/19/2022 17:38:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004583 on epoch=239
03/19/2022 17:38:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000794 on epoch=243
03/19/2022 17:39:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002044 on epoch=246
03/19/2022 17:39:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000215 on epoch=249
03/19/2022 17:39:10 - INFO - __main__ - Global step 750 Train loss 0.001610 Classification-F1 0.31203160638644506 on epoch=249
03/19/2022 17:39:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000611 on epoch=253
03/19/2022 17:39:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000099 on epoch=256
03/19/2022 17:39:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.010429 on epoch=259
03/19/2022 17:39:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002185 on epoch=263
03/19/2022 17:39:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002668 on epoch=266
03/19/2022 17:39:35 - INFO - __main__ - Global step 800 Train loss 0.003199 Classification-F1 0.32473009446693657 on epoch=266
03/19/2022 17:39:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000127 on epoch=269
03/19/2022 17:39:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000340 on epoch=273
03/19/2022 17:39:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000771 on epoch=276
03/19/2022 17:39:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000053 on epoch=279
03/19/2022 17:40:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000837 on epoch=283
03/19/2022 17:40:01 - INFO - __main__ - Global step 850 Train loss 0.000426 Classification-F1 0.35217976597286943 on epoch=283
03/19/2022 17:40:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000021 on epoch=286
03/19/2022 17:40:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000066 on epoch=289
03/19/2022 17:40:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000031 on epoch=293
03/19/2022 17:40:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000064 on epoch=296
03/19/2022 17:40:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000025 on epoch=299
03/19/2022 17:40:26 - INFO - __main__ - Global step 900 Train loss 0.000042 Classification-F1 0.3732758620689655 on epoch=299
03/19/2022 17:40:26 - INFO - __main__ - save last model!
03/19/2022 17:40:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:40:26 - INFO - __main__ - Printing 3 examples
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:40:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:40:26 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:40:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:40:26 - INFO - __main__ - Printing 3 examples
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:40:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:26 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:40:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:40:27 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:40:33 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:40:34 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:40:34 - INFO - __main__ - Printing 3 examples
03/19/2022 17:40:34 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:40:34 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:34 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:40:34 - INFO - __main__ - ['entailment']
03/19/2022 17:40:34 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:40:34 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:40:35 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:40:36 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:40:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:40:37 - INFO - __main__ - Starting training!
03/19/2022 17:40:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_87_0.0005_8_predictions.txt
03/19/2022 17:40:53 - INFO - __main__ - Classification-F1 on test data: 0.3372
03/19/2022 17:40:54 - INFO - __main__ - prefix=anli_16_87, lr=0.0005, bsz=8, dev_performance=0.4147323794382618, test_performance=0.33720094703020403
03/19/2022 17:40:54 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.0003, bsz=8 ...
03/19/2022 17:40:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:40:54 - INFO - __main__ - Printing 3 examples
03/19/2022 17:40:54 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:40:54 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:54 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:40:54 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:54 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:40:54 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:40:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:40:55 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:40:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:40:55 - INFO - __main__ - Printing 3 examples
03/19/2022 17:40:55 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:40:55 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:55 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:40:55 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:55 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:40:55 - INFO - __main__ - ['contradiction']
03/19/2022 17:40:55 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:40:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:40:55 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:41:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:41:07 - INFO - __main__ - Starting training!
03/19/2022 17:41:11 - INFO - __main__ - Step 10 Global step 10 Train loss 23.497852 on epoch=3
03/19/2022 17:41:16 - INFO - __main__ - Step 20 Global step 20 Train loss 18.676556 on epoch=6
03/19/2022 17:41:21 - INFO - __main__ - Step 30 Global step 30 Train loss 12.294573 on epoch=9
03/19/2022 17:41:25 - INFO - __main__ - Step 40 Global step 40 Train loss 11.208712 on epoch=13
03/19/2022 17:41:30 - INFO - __main__ - Step 50 Global step 50 Train loss 11.022385 on epoch=16
03/19/2022 17:41:31 - INFO - __main__ - Global step 50 Train loss 15.340017 Classification-F1 0.0 on epoch=16
03/19/2022 17:41:36 - INFO - __main__ - Step 60 Global step 60 Train loss 9.859918 on epoch=19
03/19/2022 17:41:41 - INFO - __main__ - Step 70 Global step 70 Train loss 8.774708 on epoch=23
03/19/2022 17:41:46 - INFO - __main__ - Step 80 Global step 80 Train loss 8.161219 on epoch=26
03/19/2022 17:41:51 - INFO - __main__ - Step 90 Global step 90 Train loss 8.410672 on epoch=29
03/19/2022 17:41:56 - INFO - __main__ - Step 100 Global step 100 Train loss 7.150359 on epoch=33
03/19/2022 17:41:57 - INFO - __main__ - Global step 100 Train loss 8.471375 Classification-F1 0.0660377358490566 on epoch=33
03/19/2022 17:42:03 - INFO - __main__ - Step 110 Global step 110 Train loss 6.654892 on epoch=36
03/19/2022 17:42:08 - INFO - __main__ - Step 120 Global step 120 Train loss 5.106317 on epoch=39
03/19/2022 17:42:12 - INFO - __main__ - Step 130 Global step 130 Train loss 3.200690 on epoch=43
03/19/2022 17:42:17 - INFO - __main__ - Step 140 Global step 140 Train loss 2.250573 on epoch=46
03/19/2022 17:42:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.665840 on epoch=49
03/19/2022 17:42:23 - INFO - __main__ - Global step 150 Train loss 3.575662 Classification-F1 0.2333333333333333 on epoch=49
03/19/2022 17:42:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.460810 on epoch=53
03/19/2022 17:42:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.444518 on epoch=56
03/19/2022 17:42:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.251476 on epoch=59
03/19/2022 17:42:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.148652 on epoch=63
03/19/2022 17:42:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.098463 on epoch=66
03/19/2022 17:42:49 - INFO - __main__ - Global step 200 Train loss 0.280784 Classification-F1 0.34708164642375167 on epoch=66
03/19/2022 17:42:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.075424 on epoch=69
03/19/2022 17:43:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.078905 on epoch=73
03/19/2022 17:43:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.026932 on epoch=76
03/19/2022 17:43:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.022593 on epoch=79
03/19/2022 17:43:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.094573 on epoch=83
03/19/2022 17:43:16 - INFO - __main__ - Global step 250 Train loss 0.059686 Classification-F1 0.29935483870967744 on epoch=83
03/19/2022 17:43:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.114757 on epoch=86
03/19/2022 17:43:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002356 on epoch=89
03/19/2022 17:43:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.131287 on epoch=93
03/19/2022 17:43:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.003055 on epoch=96
03/19/2022 17:43:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.007067 on epoch=99
03/19/2022 17:43:41 - INFO - __main__ - Global step 300 Train loss 0.051704 Classification-F1 0.3734095519085377 on epoch=99
03/19/2022 17:43:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.007996 on epoch=103
03/19/2022 17:43:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.016096 on epoch=106
03/19/2022 17:43:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.005954 on epoch=109
03/19/2022 17:44:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.020061 on epoch=113
03/19/2022 17:44:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002970 on epoch=116
03/19/2022 17:44:07 - INFO - __main__ - Global step 350 Train loss 0.010615 Classification-F1 0.35980922098569157 on epoch=116
03/19/2022 17:44:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001260 on epoch=119
03/19/2022 17:44:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.022341 on epoch=123
03/19/2022 17:44:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001142 on epoch=126
03/19/2022 17:44:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000562 on epoch=129
03/19/2022 17:44:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007503 on epoch=133
03/19/2022 17:44:33 - INFO - __main__ - Global step 400 Train loss 0.006561 Classification-F1 0.38665290677674574 on epoch=133
03/19/2022 17:44:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003027 on epoch=136
03/19/2022 17:44:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.015439 on epoch=139
03/19/2022 17:44:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001936 on epoch=143
03/19/2022 17:44:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001032 on epoch=146
03/19/2022 17:44:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000323 on epoch=149
03/19/2022 17:45:00 - INFO - __main__ - Global step 450 Train loss 0.004352 Classification-F1 0.38593838593838586 on epoch=149
03/19/2022 17:45:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000773 on epoch=153
03/19/2022 17:45:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000585 on epoch=156
03/19/2022 17:45:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000253 on epoch=159
03/19/2022 17:45:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000164 on epoch=163
03/19/2022 17:45:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000218 on epoch=166
03/19/2022 17:45:26 - INFO - __main__ - Global step 500 Train loss 0.000399 Classification-F1 0.36835930953578017 on epoch=166
03/19/2022 17:45:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000904 on epoch=169
03/19/2022 17:45:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000204 on epoch=173
03/19/2022 17:45:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000184 on epoch=176
03/19/2022 17:45:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000079 on epoch=179
03/19/2022 17:45:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002156 on epoch=183
03/19/2022 17:45:53 - INFO - __main__ - Global step 550 Train loss 0.000705 Classification-F1 0.37598204264870927 on epoch=183
03/19/2022 17:45:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000312 on epoch=186
03/19/2022 17:46:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001021 on epoch=189
03/19/2022 17:46:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000671 on epoch=193
03/19/2022 17:46:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000150 on epoch=196
03/19/2022 17:46:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000569 on epoch=199
03/19/2022 17:46:19 - INFO - __main__ - Global step 600 Train loss 0.000544 Classification-F1 0.27698032961190855 on epoch=199
03/19/2022 17:46:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000286 on epoch=203
03/19/2022 17:46:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000100 on epoch=206
03/19/2022 17:46:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000145 on epoch=209
03/19/2022 17:46:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000208 on epoch=213
03/19/2022 17:46:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000261 on epoch=216
03/19/2022 17:46:45 - INFO - __main__ - Global step 650 Train loss 0.000200 Classification-F1 0.29168973791066816 on epoch=216
03/19/2022 17:46:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000064 on epoch=219
03/19/2022 17:46:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000208 on epoch=223
03/19/2022 17:47:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000232 on epoch=226
03/19/2022 17:47:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000059 on epoch=229
03/19/2022 17:47:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000102 on epoch=233
03/19/2022 17:47:12 - INFO - __main__ - Global step 700 Train loss 0.000133 Classification-F1 0.2965675057208238 on epoch=233
03/19/2022 17:47:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000036 on epoch=236
03/19/2022 17:47:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000018 on epoch=239
03/19/2022 17:47:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000730 on epoch=243
03/19/2022 17:47:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000073 on epoch=246
03/19/2022 17:47:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000046 on epoch=249
03/19/2022 17:47:38 - INFO - __main__ - Global step 750 Train loss 0.000181 Classification-F1 0.32961190855927697 on epoch=249
03/19/2022 17:47:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000045 on epoch=253
03/19/2022 17:47:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000066 on epoch=256
03/19/2022 17:47:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000063 on epoch=259
03/19/2022 17:47:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000020 on epoch=263
03/19/2022 17:48:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000047 on epoch=266
03/19/2022 17:48:04 - INFO - __main__ - Global step 800 Train loss 0.000048 Classification-F1 0.3132262697480088 on epoch=266
03/19/2022 17:48:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000012 on epoch=269
03/19/2022 17:48:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000020 on epoch=273
03/19/2022 17:48:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000045 on epoch=276
03/19/2022 17:48:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000049 on epoch=279
03/19/2022 17:48:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000030 on epoch=283
03/19/2022 17:48:30 - INFO - __main__ - Global step 850 Train loss 0.000031 Classification-F1 0.34150197628458495 on epoch=283
03/19/2022 17:48:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000018 on epoch=286
03/19/2022 17:48:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.011551 on epoch=289
03/19/2022 17:48:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000007 on epoch=293
03/19/2022 17:48:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000032 on epoch=296
03/19/2022 17:48:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000017 on epoch=299
03/19/2022 17:48:56 - INFO - __main__ - Global step 900 Train loss 0.002325 Classification-F1 0.3700476947535771 on epoch=299
03/19/2022 17:48:56 - INFO - __main__ - save last model!
03/19/2022 17:48:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:48:57 - INFO - __main__ - Printing 3 examples
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:48:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:48:57 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:48:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:48:57 - INFO - __main__ - Printing 3 examples
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:48:57 - INFO - __main__ - ['contradiction']
03/19/2022 17:48:57 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:48:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:48:57 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:49:03 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:49:04 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:49:04 - INFO - __main__ - Printing 3 examples
03/19/2022 17:49:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:49:04 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:49:04 - INFO - __main__ - ['entailment']
03/19/2022 17:49:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:49:04 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:49:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:49:06 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:49:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:49:08 - INFO - __main__ - Starting training!
03/19/2022 17:49:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_87_0.0003_8_predictions.txt
03/19/2022 17:49:25 - INFO - __main__ - Classification-F1 on test data: 0.2394
03/19/2022 17:49:25 - INFO - __main__ - prefix=anli_16_87, lr=0.0003, bsz=8, dev_performance=0.38665290677674574, test_performance=0.23939726311239568
03/19/2022 17:49:25 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.0002, bsz=8 ...
03/19/2022 17:49:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:49:26 - INFO - __main__ - Printing 3 examples
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:49:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:49:26 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:49:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:49:26 - INFO - __main__ - Printing 3 examples
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:49:26 - INFO - __main__ - ['contradiction']
03/19/2022 17:49:26 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:49:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:49:26 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:49:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:49:37 - INFO - __main__ - Starting training!
03/19/2022 17:49:41 - INFO - __main__ - Step 10 Global step 10 Train loss 24.087618 on epoch=3
03/19/2022 17:49:46 - INFO - __main__ - Step 20 Global step 20 Train loss 21.165043 on epoch=6
03/19/2022 17:49:50 - INFO - __main__ - Step 30 Global step 30 Train loss 15.375630 on epoch=9
03/19/2022 17:49:55 - INFO - __main__ - Step 40 Global step 40 Train loss 12.934793 on epoch=13
03/19/2022 17:50:00 - INFO - __main__ - Step 50 Global step 50 Train loss 11.628603 on epoch=16
03/19/2022 17:50:04 - INFO - __main__ - Global step 50 Train loss 17.038338 Classification-F1 0.018575851393188854 on epoch=16
03/19/2022 17:50:10 - INFO - __main__ - Step 60 Global step 60 Train loss 11.192697 on epoch=19
03/19/2022 17:50:15 - INFO - __main__ - Step 70 Global step 70 Train loss 9.832637 on epoch=23
03/19/2022 17:50:20 - INFO - __main__ - Step 80 Global step 80 Train loss 9.504759 on epoch=26
03/19/2022 17:50:25 - INFO - __main__ - Step 90 Global step 90 Train loss 8.926267 on epoch=29
03/19/2022 17:50:30 - INFO - __main__ - Step 100 Global step 100 Train loss 8.433240 on epoch=33
03/19/2022 17:50:32 - INFO - __main__ - Global step 100 Train loss 9.577920 Classification-F1 0.017543859649122806 on epoch=33
03/19/2022 17:50:37 - INFO - __main__ - Step 110 Global step 110 Train loss 8.407430 on epoch=36
03/19/2022 17:50:42 - INFO - __main__ - Step 120 Global step 120 Train loss 7.830962 on epoch=39
03/19/2022 17:50:47 - INFO - __main__ - Step 130 Global step 130 Train loss 6.777797 on epoch=43
03/19/2022 17:50:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.689410 on epoch=46
03/19/2022 17:50:57 - INFO - __main__ - Step 150 Global step 150 Train loss 6.086340 on epoch=49
03/19/2022 17:50:58 - INFO - __main__ - Global step 150 Train loss 7.158388 Classification-F1 0.075 on epoch=49
03/19/2022 17:51:04 - INFO - __main__ - Step 160 Global step 160 Train loss 5.392296 on epoch=53
03/19/2022 17:51:09 - INFO - __main__ - Step 170 Global step 170 Train loss 4.743471 on epoch=56
03/19/2022 17:51:14 - INFO - __main__ - Step 180 Global step 180 Train loss 4.395630 on epoch=59
03/19/2022 17:51:19 - INFO - __main__ - Step 190 Global step 190 Train loss 3.198055 on epoch=63
03/19/2022 17:51:24 - INFO - __main__ - Step 200 Global step 200 Train loss 3.069843 on epoch=66
03/19/2022 17:51:25 - INFO - __main__ - Global step 200 Train loss 4.159859 Classification-F1 0.16666666666666666 on epoch=66
03/19/2022 17:51:30 - INFO - __main__ - Step 210 Global step 210 Train loss 2.616478 on epoch=69
03/19/2022 17:51:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.431530 on epoch=73
03/19/2022 17:51:40 - INFO - __main__ - Step 230 Global step 230 Train loss 2.437423 on epoch=76
03/19/2022 17:51:45 - INFO - __main__ - Step 240 Global step 240 Train loss 2.249785 on epoch=79
03/19/2022 17:51:50 - INFO - __main__ - Step 250 Global step 250 Train loss 2.680610 on epoch=83
03/19/2022 17:51:51 - INFO - __main__ - Global step 250 Train loss 2.483165 Classification-F1 0.16666666666666666 on epoch=83
03/19/2022 17:51:56 - INFO - __main__ - Step 260 Global step 260 Train loss 2.118394 on epoch=86
03/19/2022 17:52:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.097958 on epoch=89
03/19/2022 17:52:06 - INFO - __main__ - Step 280 Global step 280 Train loss 2.630104 on epoch=93
03/19/2022 17:52:11 - INFO - __main__ - Step 290 Global step 290 Train loss 2.122966 on epoch=96
03/19/2022 17:52:16 - INFO - __main__ - Step 300 Global step 300 Train loss 1.775627 on epoch=99
03/19/2022 17:52:16 - INFO - __main__ - Global step 300 Train loss 2.349010 Classification-F1 0.20908004778972522 on epoch=99
03/19/2022 17:52:22 - INFO - __main__ - Step 310 Global step 310 Train loss 2.025541 on epoch=103
03/19/2022 17:52:27 - INFO - __main__ - Step 320 Global step 320 Train loss 2.077715 on epoch=106
03/19/2022 17:52:32 - INFO - __main__ - Step 330 Global step 330 Train loss 2.015310 on epoch=109
03/19/2022 17:52:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.808077 on epoch=113
03/19/2022 17:52:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.787338 on epoch=116
03/19/2022 17:52:43 - INFO - __main__ - Global step 350 Train loss 1.942796 Classification-F1 0.24611708482676223 on epoch=116
03/19/2022 17:52:49 - INFO - __main__ - Step 360 Global step 360 Train loss 1.778231 on epoch=119
03/19/2022 17:52:53 - INFO - __main__ - Step 370 Global step 370 Train loss 1.831399 on epoch=123
03/19/2022 17:52:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.835880 on epoch=126
03/19/2022 17:53:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.846586 on epoch=129
03/19/2022 17:53:08 - INFO - __main__ - Step 400 Global step 400 Train loss 1.168447 on epoch=133
03/19/2022 17:53:09 - INFO - __main__ - Global step 400 Train loss 1.692108 Classification-F1 0.16666666666666666 on epoch=133
03/19/2022 17:53:14 - INFO - __main__ - Step 410 Global step 410 Train loss 1.472337 on epoch=136
03/19/2022 17:53:19 - INFO - __main__ - Step 420 Global step 420 Train loss 1.338114 on epoch=139
03/19/2022 17:53:24 - INFO - __main__ - Step 430 Global step 430 Train loss 1.371517 on epoch=143
03/19/2022 17:53:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.028220 on epoch=146
03/19/2022 17:53:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.127790 on epoch=149
03/19/2022 17:53:34 - INFO - __main__ - Global step 450 Train loss 1.267596 Classification-F1 0.16666666666666666 on epoch=149
03/19/2022 17:53:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.157029 on epoch=153
03/19/2022 17:53:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.875037 on epoch=156
03/19/2022 17:53:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.955268 on epoch=159
03/19/2022 17:53:54 - INFO - __main__ - Step 490 Global step 490 Train loss 1.171167 on epoch=163
03/19/2022 17:53:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.921976 on epoch=166
03/19/2022 17:54:00 - INFO - __main__ - Global step 500 Train loss 1.016095 Classification-F1 0.16129032258064516 on epoch=166
03/19/2022 17:54:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.941606 on epoch=169
03/19/2022 17:54:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.978607 on epoch=173
03/19/2022 17:54:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.989697 on epoch=176
03/19/2022 17:54:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.274085 on epoch=179
03/19/2022 17:54:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.173933 on epoch=183
03/19/2022 17:54:25 - INFO - __main__ - Global step 550 Train loss 0.671585 Classification-F1 0.21031746031746035 on epoch=183
03/19/2022 17:54:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.052207 on epoch=186
03/19/2022 17:54:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.029795 on epoch=189
03/19/2022 17:54:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018218 on epoch=193
03/19/2022 17:54:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.025236 on epoch=196
03/19/2022 17:54:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005734 on epoch=199
03/19/2022 17:54:51 - INFO - __main__ - Global step 600 Train loss 0.026238 Classification-F1 0.3628441275500099 on epoch=199
03/19/2022 17:54:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002385 on epoch=203
03/19/2022 17:55:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001593 on epoch=206
03/19/2022 17:55:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.003377 on epoch=209
03/19/2022 17:55:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001784 on epoch=213
03/19/2022 17:55:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001109 on epoch=216
03/19/2022 17:55:17 - INFO - __main__ - Global step 650 Train loss 0.002050 Classification-F1 0.25995670995670994 on epoch=216
03/19/2022 17:55:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000810 on epoch=219
03/19/2022 17:55:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001548 on epoch=223
03/19/2022 17:55:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.002065 on epoch=226
03/19/2022 17:55:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001654 on epoch=229
03/19/2022 17:55:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000680 on epoch=233
03/19/2022 17:55:43 - INFO - __main__ - Global step 700 Train loss 0.001351 Classification-F1 0.32954492954492953 on epoch=233
03/19/2022 17:55:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001203 on epoch=236
03/19/2022 17:55:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004846 on epoch=239
03/19/2022 17:55:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001816 on epoch=243
03/19/2022 17:56:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001284 on epoch=246
03/19/2022 17:56:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000526 on epoch=249
03/19/2022 17:56:09 - INFO - __main__ - Global step 750 Train loss 0.001935 Classification-F1 0.3738140417457306 on epoch=249
03/19/2022 17:56:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002145 on epoch=253
03/19/2022 17:56:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002194 on epoch=256
03/19/2022 17:56:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001074 on epoch=259
03/19/2022 17:56:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001332 on epoch=263
03/19/2022 17:56:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002299 on epoch=266
03/19/2022 17:56:36 - INFO - __main__ - Global step 800 Train loss 0.001809 Classification-F1 0.3455489930975592 on epoch=266
03/19/2022 17:56:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000383 on epoch=269
03/19/2022 17:56:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000213 on epoch=273
03/19/2022 17:56:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000376 on epoch=276
03/19/2022 17:56:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000905 on epoch=279
03/19/2022 17:57:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000364 on epoch=283
03/19/2022 17:57:02 - INFO - __main__ - Global step 850 Train loss 0.000448 Classification-F1 0.39455180834491177 on epoch=283
03/19/2022 17:57:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000348 on epoch=286
03/19/2022 17:57:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000230 on epoch=289
03/19/2022 17:57:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000136 on epoch=293
03/19/2022 17:57:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000142 on epoch=296
03/19/2022 17:57:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000132 on epoch=299
03/19/2022 17:57:28 - INFO - __main__ - Global step 900 Train loss 0.000198 Classification-F1 0.43301282051282053 on epoch=299
03/19/2022 17:57:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:57:28 - INFO - __main__ - Printing 3 examples
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 17:57:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:57:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:57:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:57:28 - INFO - __main__ - Printing 3 examples
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:57:28 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:28 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:57:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:57:28 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:57:29 - INFO - __main__ - save last model!
03/19/2022 17:57:36 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 17:57:37 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 17:57:37 - INFO - __main__ - Printing 3 examples
03/19/2022 17:57:37 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 17:57:37 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:37 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 17:57:37 - INFO - __main__ - ['entailment']
03/19/2022 17:57:37 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 17:57:37 - INFO - __main__ - ['contradiction']
03/19/2022 17:57:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:57:37 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:57:38 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 17:57:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:57:39 - INFO - __main__ - Starting training!
03/19/2022 17:57:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_87_0.0002_8_predictions.txt
03/19/2022 17:57:58 - INFO - __main__ - Classification-F1 on test data: 0.3662
03/19/2022 17:57:59 - INFO - __main__ - prefix=anli_16_87, lr=0.0002, bsz=8, dev_performance=0.43301282051282053, test_performance=0.36615888142853703
03/19/2022 17:57:59 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.0001, bsz=8 ...
03/19/2022 17:58:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:58:00 - INFO - __main__ - Printing 3 examples
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostednictvm psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Vclav Vorlek based on the story by Oldich Dank. Runtime 87 min. Mono. Produced by Filmov Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostednictvm psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 17:58:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:58:00 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/19/2022 17:58:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/19/2022 17:58:00 - INFO - __main__ - Printing 3 examples
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS,ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
03/19/2022 17:58:00 - INFO - __main__ - ['contradiction']
03/19/2022 17:58:00 - INFO - __main__ - Tokenizing Input ...
03/19/2022 17:58:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 17:58:00 - INFO - __main__ - Loaded 48 examples from dev data
03/19/2022 17:58:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 17:58:11 - INFO - __main__ - Starting training!
03/19/2022 17:58:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.578617 on epoch=3
03/19/2022 17:58:20 - INFO - __main__ - Step 20 Global step 20 Train loss 21.189367 on epoch=6
03/19/2022 17:58:24 - INFO - __main__ - Step 30 Global step 30 Train loss 15.687424 on epoch=9
03/19/2022 17:58:29 - INFO - __main__ - Step 40 Global step 40 Train loss 13.501410 on epoch=13
03/19/2022 17:58:34 - INFO - __main__ - Step 50 Global step 50 Train loss 12.768726 on epoch=16
03/19/2022 17:58:35 - INFO - __main__ - Global step 50 Train loss 17.345112 Classification-F1 0.0 on epoch=16
03/19/2022 17:58:41 - INFO - __main__ - Step 60 Global step 60 Train loss 11.354770 on epoch=19
03/19/2022 17:58:46 - INFO - __main__ - Step 70 Global step 70 Train loss 10.958899 on epoch=23
03/19/2022 17:58:51 - INFO - __main__ - Step 80 Global step 80 Train loss 11.257711 on epoch=26
03/19/2022 17:58:56 - INFO - __main__ - Step 90 Global step 90 Train loss 10.861517 on epoch=29
03/19/2022 17:59:01 - INFO - __main__ - Step 100 Global step 100 Train loss 10.221927 on epoch=33
03/19/2022 17:59:02 - INFO - __main__ - Global step 100 Train loss 10.930965 Classification-F1 0.0 on epoch=33
03/19/2022 17:59:07 - INFO - __main__ - Step 110 Global step 110 Train loss 9.927220 on epoch=36
03/19/2022 17:59:12 - INFO - __main__ - Step 120 Global step 120 Train loss 10.188841 on epoch=39
03/19/2022 17:59:17 - INFO - __main__ - Step 130 Global step 130 Train loss 9.514425 on epoch=43
03/19/2022 17:59:22 - INFO - __main__ - Step 140 Global step 140 Train loss 9.275878 on epoch=46
03/19/2022 17:59:27 - INFO - __main__ - Step 150 Global step 150 Train loss 9.559374 on epoch=49
03/19/2022 17:59:28 - INFO - __main__ - Global step 150 Train loss 9.693148 Classification-F1 0.0 on epoch=49
03/19/2022 17:59:33 - INFO - __main__ - Step 160 Global step 160 Train loss 8.533528 on epoch=53
03/19/2022 17:59:38 - INFO - __main__ - Step 170 Global step 170 Train loss 8.880190 on epoch=56
03/19/2022 17:59:43 - INFO - __main__ - Step 180 Global step 180 Train loss 8.897823 on epoch=59
03/19/2022 17:59:48 - INFO - __main__ - Step 190 Global step 190 Train loss 8.595016 on epoch=63
03/19/2022 17:59:53 - INFO - __main__ - Step 200 Global step 200 Train loss 8.849808 on epoch=66
03/19/2022 17:59:54 - INFO - __main__ - Global step 200 Train loss 8.751273 Classification-F1 0.0 on epoch=66
03/19/2022 17:59:59 - INFO - __main__ - Step 210 Global step 210 Train loss 8.663498 on epoch=69
03/19/2022 18:00:04 - INFO - __main__ - Step 220 Global step 220 Train loss 7.697310 on epoch=73
03/19/2022 18:00:09 - INFO - __main__ - Step 230 Global step 230 Train loss 7.757625 on epoch=76
03/19/2022 18:00:14 - INFO - __main__ - Step 240 Global step 240 Train loss 7.769436 on epoch=79
03/19/2022 18:00:19 - INFO - __main__ - Step 250 Global step 250 Train loss 7.014151 on epoch=83
03/19/2022 18:00:20 - INFO - __main__ - Global step 250 Train loss 7.780404 Classification-F1 0.0 on epoch=83
03/19/2022 18:00:25 - INFO - __main__ - Step 260 Global step 260 Train loss 6.993497 on epoch=86
03/19/2022 18:00:30 - INFO - __main__ - Step 270 Global step 270 Train loss 6.906087 on epoch=89
03/19/2022 18:00:35 - INFO - __main__ - Step 280 Global step 280 Train loss 6.309695 on epoch=93
03/19/2022 18:00:40 - INFO - __main__ - Step 290 Global step 290 Train loss 5.777933 on epoch=96
03/19/2022 18:00:46 - INFO - __main__ - Step 300 Global step 300 Train loss 5.928839 on epoch=99
03/19/2022 18:00:47 - INFO - __main__ - Global step 300 Train loss 6.383211 Classification-F1 0.0 on epoch=99
03/19/2022 18:00:52 - INFO - __main__ - Step 310 Global step 310 Train loss 5.318387 on epoch=103
03/19/2022 18:00:57 - INFO - __main__ - Step 320 Global step 320 Train loss 5.338280 on epoch=106
03/19/2022 18:01:02 - INFO - __main__ - Step 330 Global step 330 Train loss 4.106076 on epoch=109
03/19/2022 18:01:07 - INFO - __main__ - Step 340 Global step 340 Train loss 3.691027 on epoch=113
03/19/2022 18:01:12 - INFO - __main__ - Step 350 Global step 350 Train loss 3.322088 on epoch=116
03/19/2022 18:01:13 - INFO - __main__ - Global step 350 Train loss 4.355171 Classification-F1 0.11538461538461539 on epoch=116
03/19/2022 18:01:18 - INFO - __main__ - Step 360 Global step 360 Train loss 3.614198 on epoch=119
03/19/2022 18:01:23 - INFO - __main__ - Step 370 Global step 370 Train loss 3.491317 on epoch=123
03/19/2022 18:01:28 - INFO - __main__ - Step 380 Global step 380 Train loss 3.284130 on epoch=126
03/19/2022 18:01:33 - INFO - __main__ - Step 390 Global step 390 Train loss 3.428224 on epoch=129
03/19/2022 18:01:38 - INFO - __main__ - Step 400 Global step 400 Train loss 2.950260 on epoch=133
03/19/2022 18:01:39 - INFO - __main__ - Global step 400 Train loss 3.353626 Classification-F1 0.2623951182303585 on epoch=133
03/19/2022 18:01:44 - INFO - __main__ - Step 410 Global step 410 Train loss 2.203058 on epoch=136
03/19/2022 18:01:50 - INFO - __main__ - Step 420 Global step 420 Train loss 1.611194 on epoch=139
03/19/2022 18:01:55 - INFO - __main__ - Step 430 Global step 430 Train loss 3.514645 on epoch=143
03/19/2022 18:02:00 - INFO - __main__ - Step 440 Global step 440 Train loss 2.536903 on epoch=146
03/19/2022 18:02:05 - INFO - __main__ - Step 450 Global step 450 Train loss 2.941616 on epoch=149
03/19/2022 18:02:05 - INFO - __main__ - Global step 450 Train loss 2.561483 Classification-F1 0.15072859744990894 on epoch=149
03/19/2022 18:02:10 - INFO - __main__ - Step 460 Global step 460 Train loss 2.874837 on epoch=153
03/19/2022 18:02:15 - INFO - __main__ - Step 470 Global step 470 Train loss 2.440263 on epoch=156
03/19/2022 18:02:20 - INFO - __main__ - Step 480 Global step 480 Train loss 2.910829 on epoch=159
03/19/2022 18:02:25 - INFO - __main__ - Step 490 Global step 490 Train loss 2.158801 on epoch=163
03/19/2022 18:02:30 - INFO - __main__ - Step 500 Global step 500 Train loss 1.457660 on epoch=166
03/19/2022 18:02:31 - INFO - __main__ - Global step 500 Train loss 2.368478 Classification-F1 0.23015873015873012 on epoch=166
03/19/2022 18:02:36 - INFO - __main__ - Step 510 Global step 510 Train loss 1.302651 on epoch=169
03/19/2022 18:02:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.588363 on epoch=173
03/19/2022 18:02:46 - INFO - __main__ - Step 530 Global step 530 Train loss 1.256196 on epoch=176
03/19/2022 18:02:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.774553 on epoch=179
03/19/2022 18:02:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.866223 on epoch=183
03/19/2022 18:02:57 - INFO - __main__ - Global step 550 Train loss 1.157597 Classification-F1 0.323852560694666 on epoch=183
03/19/2022 18:03:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.762120 on epoch=186
03/19/2022 18:03:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.879231 on epoch=189
03/19/2022 18:03:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.671899 on epoch=193
03/19/2022 18:03:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.673193 on epoch=196
03/19/2022 18:03:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.783291 on epoch=199
03/19/2022 18:03:23 - INFO - __main__ - Global step 600 Train loss 0.753947 Classification-F1 0.20479302832244006 on epoch=199
03/19/2022 18:03:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.526814 on epoch=203
03/19/2022 18:03:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.559210 on epoch=206
03/19/2022 18:03:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.662317 on epoch=209
03/19/2022 18:03:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.973285 on epoch=213
03/19/2022 18:03:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.592806 on epoch=216
03/19/2022 18:03:50 - INFO - __main__ - Global step 650 Train loss 0.662886 Classification-F1 0.2922563661479917 on epoch=216
03/19/2022 18:03:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.384627 on epoch=219
03/19/2022 18:04:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.495407 on epoch=223
03/19/2022 18:04:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.483069 on epoch=226
03/19/2022 18:04:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.509492 on epoch=229
03/19/2022 18:04:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.493720 on epoch=233
03/19/2022 18:04:16 - INFO - __main__ - Global step 700 Train loss 0.473263 Classification-F1 0.2953045404208195 on epoch=233
03/19/2022 18:04:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.544080 on epoch=236
03/19/2022 18:04:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.433864 on epoch=239
03/19/2022 18:04:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.368643 on epoch=243
03/19/2022 18:04:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.618212 on epoch=246
03/19/2022 18:04:41 - INFO - __main__ - Step 750 Global step 750 Train loss 1.106874 on epoch=249
03/19/2022 18:04:42 - INFO - __main__ - Global step 750 Train loss 0.614335 Classification-F1 0.2778805120910384 on epoch=249
03/19/2022 18:04:47 - INFO - __main__ - Step 760 Global step 760 Train loss 1.642944 on epoch=253
03/19/2022 18:04:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.659661 on epoch=256
03/19/2022 18:04:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.707514 on epoch=259
03/19/2022 18:05:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.611316 on epoch=263
03/19/2022 18:05:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.270638 on epoch=266
03/19/2022 18:05:08 - INFO - __main__ - Global step 800 Train loss 0.778415 Classification-F1 0.27765419615773507 on epoch=266
03/19/2022 18:05:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.243572 on epoch=269
03/19/2022 18:05:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.376435 on epoch=273
03/19/2022 18:05:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.602552 on epoch=276
03/19/2022 18:05:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.209783 on epoch=279
03/19/2022 18:05:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.510358 on epoch=283
03/19/2022 18:05:34 - INFO - __main__ - Global step 850 Train loss 0.388540 Classification-F1 0.2871627283391989 on epoch=283
03/19/2022 18:05:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.189124 on epoch=286
03/19/2022 18:05:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.372463 on epoch=289
03/19/2022 18:05:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.696797 on epoch=293
03/19/2022 18:05:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.611142 on epoch=296
03/19/2022 18:05:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.202579 on epoch=299
03/19/2022 18:06:00 - INFO - __main__ - Global step 900 Train loss 0.414421 Classification-F1 0.27478632478632475 on epoch=299
03/19/2022 18:06:00 - INFO - __main__ - save last model!
03/19/2022 18:06:07 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:06:07 - INFO - __main__ - Start tokenizing ... 1000 instances
03/19/2022 18:06:07 - INFO - __main__ - Printing 3 examples
03/19/2022 18:06:07 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pini, who wrote a formal description of the Sanskrit language in his "Adhyy ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
03/19/2022 18:06:07 - INFO - __main__ - ['contradiction']
03/19/2022 18:06:07 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (19942001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
03/19/2022 18:06:07 - INFO - __main__ - ['entailment']
03/19/2022 18:06:07 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music Mxico, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
03/19/2022 18:06:07 - INFO - __main__ - ['contradiction']
03/19/2022 18:06:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:06:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:06:09 - INFO - __main__ - Loaded 1000 examples from test data
03/19/2022 18:06:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-anli/anli_16_87_0.0001_8_predictions.txt
03/19/2022 18:06:27 - INFO - __main__ - Classification-F1 on test data: 0.3160
03/19/2022 18:06:27 - INFO - __main__ - prefix=anli_16_87, lr=0.0001, bsz=8, dev_performance=0.323852560694666, test_performance=0.3160140440954394
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22464): No such process
Task: ethos-race, Checkpoint: None, Identifier: T5-large-ft-cls2cls
03/19/2022 18:06:33 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 18:06:33 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-ethos-race
Output directory () already exists and is not empty.
03/19/2022 18:06:33 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='4,5')
03/19/2022 18:06:33 - INFO - __main__ - models/T5-large-ft-cls2cls/singletask-ethos-race
03/19/2022 18:06:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/19/2022 18:06:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/19/2022 18:06:35 - INFO - __main__ - args.device: cuda:0
03/19/2022 18:06:35 - INFO - __main__ - Using 2 gpus
03/19/2022 18:06:35 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/19/2022 18:06:35 - INFO - __main__ - args.device: cuda:1
03/19/2022 18:06:35 - INFO - __main__ - Using 2 gpus
03/19/2022 18:06:35 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/19/2022 18:06:39 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.0005, bsz=8 ...
03/19/2022 18:06:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:06:40 - INFO - __main__ - Printing 3 examples
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:06:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:06:40 - INFO - __main__ - Printing 3 examples
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:06:40 - INFO - __main__ - ['false']
03/19/2022 18:06:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:06:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:06:40 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:06:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:06:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:06:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:06:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:06:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:06:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:06:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:06:41 - INFO - __main__ - ['false']
03/19/2022 18:06:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:06:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:06:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:06:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:06:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:06:53 - INFO - __main__ - Starting training!
03/19/2022 18:06:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:06:54 - INFO - __main__ - Starting training!
03/19/2022 18:06:58 - INFO - __main__ - Step 10 Global step 10 Train loss 23.651993 on epoch=4
03/19/2022 18:07:03 - INFO - __main__ - Step 20 Global step 20 Train loss 19.695713 on epoch=9
03/19/2022 18:07:08 - INFO - __main__ - Step 30 Global step 30 Train loss 20.206753 on epoch=14
03/19/2022 18:07:12 - INFO - __main__ - Step 40 Global step 40 Train loss 14.132365 on epoch=19
03/19/2022 18:07:17 - INFO - __main__ - Step 50 Global step 50 Train loss 12.431346 on epoch=24
03/19/2022 18:07:18 - INFO - __main__ - Global step 50 Train loss 18.023636 Classification-F1 0.0 on epoch=24
03/19/2022 18:07:24 - INFO - __main__ - Step 60 Global step 60 Train loss 9.224020 on epoch=29
03/19/2022 18:07:29 - INFO - __main__ - Step 70 Global step 70 Train loss 5.856874 on epoch=34
03/19/2022 18:07:34 - INFO - __main__ - Step 80 Global step 80 Train loss 1.265550 on epoch=39
03/19/2022 18:07:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.886613 on epoch=44
03/19/2022 18:07:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.576815 on epoch=49
03/19/2022 18:07:44 - INFO - __main__ - Global step 100 Train loss 3.561975 Classification-F1 0.3191489361702127 on epoch=49
03/19/2022 18:07:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.453112 on epoch=54
03/19/2022 18:07:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.391150 on epoch=59
03/19/2022 18:08:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.415799 on epoch=64
03/19/2022 18:08:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.396017 on epoch=69
03/19/2022 18:08:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.319751 on epoch=74
03/19/2022 18:08:11 - INFO - __main__ - Global step 150 Train loss 0.395166 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 18:08:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.349268 on epoch=79
03/19/2022 18:08:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.459212 on epoch=84
03/19/2022 18:08:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.292420 on epoch=89
03/19/2022 18:08:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.233040 on epoch=94
03/19/2022 18:08:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.169408 on epoch=99
03/19/2022 18:08:37 - INFO - __main__ - Global step 200 Train loss 0.300670 Classification-F1 0.4181818181818182 on epoch=99
03/19/2022 18:08:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.094711 on epoch=104
03/19/2022 18:08:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.090610 on epoch=109
03/19/2022 18:08:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.061710 on epoch=114
03/19/2022 18:08:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.026403 on epoch=119
03/19/2022 18:09:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.018686 on epoch=124
03/19/2022 18:09:02 - INFO - __main__ - Global step 250 Train loss 0.058424 Classification-F1 0.6825396825396826 on epoch=124
03/19/2022 18:09:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.035093 on epoch=129
03/19/2022 18:09:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.023576 on epoch=134
03/19/2022 18:09:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.179316 on epoch=139
03/19/2022 18:09:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.033164 on epoch=144
03/19/2022 18:09:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.042286 on epoch=149
03/19/2022 18:09:28 - INFO - __main__ - Global step 300 Train loss 0.062687 Classification-F1 0.6389743589743591 on epoch=149
03/19/2022 18:09:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.007335 on epoch=154
03/19/2022 18:09:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.019000 on epoch=159
03/19/2022 18:09:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.008847 on epoch=164
03/19/2022 18:09:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.050848 on epoch=169
03/19/2022 18:09:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020856 on epoch=174
03/19/2022 18:09:52 - INFO - __main__ - Global step 350 Train loss 0.021377 Classification-F1 0.7810361681329424 on epoch=174
03/19/2022 18:09:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006055 on epoch=179
03/19/2022 18:10:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009979 on epoch=184
03/19/2022 18:10:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002171 on epoch=189
03/19/2022 18:10:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.003620 on epoch=194
03/19/2022 18:10:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.008252 on epoch=199
03/19/2022 18:10:18 - INFO - __main__ - Global step 400 Train loss 0.006015 Classification-F1 0.7810361681329424 on epoch=199
03/19/2022 18:10:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003176 on epoch=204
03/19/2022 18:10:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006656 on epoch=209
03/19/2022 18:10:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001600 on epoch=214
03/19/2022 18:10:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002080 on epoch=219
03/19/2022 18:10:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000167 on epoch=224
03/19/2022 18:10:42 - INFO - __main__ - Global step 450 Train loss 0.002736 Classification-F1 0.8125 on epoch=224
03/19/2022 18:10:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.005404 on epoch=229
03/19/2022 18:10:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000508 on epoch=234
03/19/2022 18:10:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001020 on epoch=239
03/19/2022 18:11:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000341 on epoch=244
03/19/2022 18:11:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000638 on epoch=249
03/19/2022 18:11:08 - INFO - __main__ - Global step 500 Train loss 0.001582 Classification-F1 0.8125 on epoch=249
03/19/2022 18:11:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000418 on epoch=254
03/19/2022 18:11:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001053 on epoch=259
03/19/2022 18:11:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000249 on epoch=264
03/19/2022 18:11:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000293 on epoch=269
03/19/2022 18:11:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015296 on epoch=274
03/19/2022 18:11:32 - INFO - __main__ - Global step 550 Train loss 0.003462 Classification-F1 0.8745098039215686 on epoch=274
03/19/2022 18:11:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000491 on epoch=279
03/19/2022 18:11:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000757 on epoch=284
03/19/2022 18:11:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000437 on epoch=289
03/19/2022 18:11:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000350 on epoch=294
03/19/2022 18:11:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.087967 on epoch=299
03/19/2022 18:11:58 - INFO - __main__ - Global step 600 Train loss 0.018000 Classification-F1 0.3191489361702127 on epoch=299
03/19/2022 18:11:58 - INFO - __main__ - save last model!
03/19/2022 18:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:11:58 - INFO - __main__ - Printing 3 examples
03/19/2022 18:11:58 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:11:58 - INFO - __main__ - ['false']
03/19/2022 18:11:58 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:11:58 - INFO - __main__ - ['false']
03/19/2022 18:11:58 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:11:58 - INFO - __main__ - ['false']
03/19/2022 18:11:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:11:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:11:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:11:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:11:59 - INFO - __main__ - Printing 3 examples
03/19/2022 18:11:59 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:11:59 - INFO - __main__ - ['false']
03/19/2022 18:11:59 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:11:59 - INFO - __main__ - ['false']
03/19/2022 18:11:59 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:11:59 - INFO - __main__ - ['false']
03/19/2022 18:11:59 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:11:59 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:11:59 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:12:05 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:12:05 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:12:05 - INFO - __main__ - Printing 3 examples
03/19/2022 18:12:05 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:12:05 - INFO - __main__ - ['true']
03/19/2022 18:12:05 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:12:05 - INFO - __main__ - ['false']
03/19/2022 18:12:05 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:12:05 - INFO - __main__ - ['false']
03/19/2022 18:12:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:12:05 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:12:05 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:12:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_100_0.0005_8_predictions.txt
03/19/2022 18:12:07 - INFO - __main__ - Classification-F1 on test data: 0.8455
03/19/2022 18:12:07 - INFO - __main__ - prefix=ethos-race_16_100, lr=0.0005, bsz=8, dev_performance=0.8745098039215686, test_performance=0.8455316630499112
03/19/2022 18:12:07 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.0003, bsz=8 ...
03/19/2022 18:12:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:12:08 - INFO - __main__ - Printing 3 examples
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:12:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:12:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:12:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:12:08 - INFO - __main__ - Printing 3 examples
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:12:08 - INFO - __main__ - ['false']
03/19/2022 18:12:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:12:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:12:08 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:12:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:12:09 - INFO - __main__ - Starting training!
03/19/2022 18:12:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:12:21 - INFO - __main__ - Starting training!
03/19/2022 18:12:26 - INFO - __main__ - Step 10 Global step 10 Train loss 23.884777 on epoch=4
03/19/2022 18:12:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.022457 on epoch=9
03/19/2022 18:12:35 - INFO - __main__ - Step 30 Global step 30 Train loss 17.175129 on epoch=14
03/19/2022 18:12:40 - INFO - __main__ - Step 40 Global step 40 Train loss 15.912112 on epoch=19
03/19/2022 18:12:45 - INFO - __main__ - Step 50 Global step 50 Train loss 14.633654 on epoch=24
03/19/2022 18:12:47 - INFO - __main__ - Global step 50 Train loss 17.925625 Classification-F1 0.0 on epoch=24
03/19/2022 18:12:52 - INFO - __main__ - Step 60 Global step 60 Train loss 13.101999 on epoch=29
03/19/2022 18:12:57 - INFO - __main__ - Step 70 Global step 70 Train loss 12.254320 on epoch=34
03/19/2022 18:13:02 - INFO - __main__ - Step 80 Global step 80 Train loss 9.852283 on epoch=39
03/19/2022 18:13:06 - INFO - __main__ - Step 90 Global step 90 Train loss 6.057209 on epoch=44
03/19/2022 18:13:11 - INFO - __main__ - Step 100 Global step 100 Train loss 3.757920 on epoch=49
03/19/2022 18:13:12 - INFO - __main__ - Global step 100 Train loss 9.004746 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 18:13:19 - INFO - __main__ - Step 110 Global step 110 Train loss 2.990204 on epoch=54
03/19/2022 18:13:24 - INFO - __main__ - Step 120 Global step 120 Train loss 1.720528 on epoch=59
03/19/2022 18:13:29 - INFO - __main__ - Step 130 Global step 130 Train loss 1.238034 on epoch=64
03/19/2022 18:13:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.613361 on epoch=69
03/19/2022 18:13:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.977979 on epoch=74
03/19/2022 18:13:39 - INFO - __main__ - Global step 150 Train loss 1.508021 Classification-F1 0.4920634920634921 on epoch=74
03/19/2022 18:13:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.233605 on epoch=79
03/19/2022 18:13:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.316000 on epoch=84
03/19/2022 18:13:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.144694 on epoch=89
03/19/2022 18:14:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.928829 on epoch=94
03/19/2022 18:14:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.562584 on epoch=99
03/19/2022 18:14:05 - INFO - __main__ - Global step 200 Train loss 1.037143 Classification-F1 0.3333333333333333 on epoch=99
03/19/2022 18:14:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.637736 on epoch=104
03/19/2022 18:14:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.424227 on epoch=109
03/19/2022 18:14:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.429314 on epoch=114
03/19/2022 18:14:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.405555 on epoch=119
03/19/2022 18:14:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.542191 on epoch=124
03/19/2022 18:14:31 - INFO - __main__ - Global step 250 Train loss 0.487804 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 18:14:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.341541 on epoch=129
03/19/2022 18:14:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.387073 on epoch=134
03/19/2022 18:14:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.458793 on epoch=139
03/19/2022 18:14:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.362702 on epoch=144
03/19/2022 18:14:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.378768 on epoch=149
03/19/2022 18:14:56 - INFO - __main__ - Global step 300 Train loss 0.385775 Classification-F1 0.3333333333333333 on epoch=149
03/19/2022 18:15:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.482690 on epoch=154
03/19/2022 18:15:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.391874 on epoch=159
03/19/2022 18:15:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.371394 on epoch=164
03/19/2022 18:15:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.351038 on epoch=169
03/19/2022 18:15:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.450156 on epoch=174
03/19/2022 18:15:22 - INFO - __main__ - Global step 350 Train loss 0.409430 Classification-F1 0.3333333333333333 on epoch=174
03/19/2022 18:15:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.350663 on epoch=179
03/19/2022 18:15:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.489499 on epoch=184
03/19/2022 18:15:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.382421 on epoch=189
03/19/2022 18:15:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.333754 on epoch=194
03/19/2022 18:15:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.407213 on epoch=199
03/19/2022 18:15:47 - INFO - __main__ - Global step 400 Train loss 0.392710 Classification-F1 0.3333333333333333 on epoch=199
03/19/2022 18:15:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.417125 on epoch=204
03/19/2022 18:15:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.334148 on epoch=209
03/19/2022 18:16:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.319276 on epoch=214
03/19/2022 18:16:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.313249 on epoch=219
03/19/2022 18:16:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.359465 on epoch=224
03/19/2022 18:16:13 - INFO - __main__ - Global step 450 Train loss 0.348652 Classification-F1 0.4589371980676329 on epoch=224
03/19/2022 18:16:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.348331 on epoch=229
03/19/2022 18:16:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.354657 on epoch=234
03/19/2022 18:16:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.354644 on epoch=239
03/19/2022 18:16:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.340265 on epoch=244
03/19/2022 18:16:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.353184 on epoch=249
03/19/2022 18:16:38 - INFO - __main__ - Global step 500 Train loss 0.350216 Classification-F1 0.3333333333333333 on epoch=249
03/19/2022 18:16:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.323023 on epoch=254
03/19/2022 18:16:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.341269 on epoch=259
03/19/2022 18:16:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.359251 on epoch=264
03/19/2022 18:16:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.314562 on epoch=269
03/19/2022 18:17:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.351848 on epoch=274
03/19/2022 18:17:04 - INFO - __main__ - Global step 550 Train loss 0.337991 Classification-F1 0.5333333333333333 on epoch=274
03/19/2022 18:17:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.324250 on epoch=279
03/19/2022 18:17:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.328451 on epoch=284
03/19/2022 18:17:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.304457 on epoch=289
03/19/2022 18:17:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.355579 on epoch=294
03/19/2022 18:17:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.308255 on epoch=299
03/19/2022 18:17:30 - INFO - __main__ - Global step 600 Train loss 0.324198 Classification-F1 0.6875 on epoch=299
03/19/2022 18:17:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:17:30 - INFO - __main__ - Printing 3 examples
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:17:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:17:30 - INFO - __main__ - save last model!
03/19/2022 18:17:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:17:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:17:30 - INFO - __main__ - Printing 3 examples
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:17:30 - INFO - __main__ - ['false']
03/19/2022 18:17:30 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:17:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:17:31 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:17:37 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:17:38 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:17:38 - INFO - __main__ - Printing 3 examples
03/19/2022 18:17:38 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:17:38 - INFO - __main__ - ['true']
03/19/2022 18:17:38 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:17:38 - INFO - __main__ - ['false']
03/19/2022 18:17:38 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:17:38 - INFO - __main__ - ['false']
03/19/2022 18:17:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:17:38 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:17:38 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:17:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_100_0.0003_8_predictions.txt
03/19/2022 18:17:39 - INFO - __main__ - Classification-F1 on test data: 0.6065
03/19/2022 18:17:40 - INFO - __main__ - prefix=ethos-race_16_100, lr=0.0003, bsz=8, dev_performance=0.6875, test_performance=0.6064667448483834
03/19/2022 18:17:40 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.0002, bsz=8 ...
03/19/2022 18:17:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:17:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:17:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:17:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:17:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:17:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:17:41 - INFO - __main__ - ['false']
03/19/2022 18:17:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:17:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:17:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:17:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:17:43 - INFO - __main__ - Starting training!
03/19/2022 18:17:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:17:52 - INFO - __main__ - Starting training!
03/19/2022 18:17:56 - INFO - __main__ - Step 10 Global step 10 Train loss 24.371454 on epoch=4
03/19/2022 18:18:01 - INFO - __main__ - Step 20 Global step 20 Train loss 18.856045 on epoch=9
03/19/2022 18:18:06 - INFO - __main__ - Step 30 Global step 30 Train loss 18.358896 on epoch=14
03/19/2022 18:18:11 - INFO - __main__ - Step 40 Global step 40 Train loss 16.791468 on epoch=19
03/19/2022 18:18:16 - INFO - __main__ - Step 50 Global step 50 Train loss 16.387436 on epoch=24
03/19/2022 18:18:17 - INFO - __main__ - Global step 50 Train loss 18.953060 Classification-F1 0.0 on epoch=24
03/19/2022 18:18:23 - INFO - __main__ - Step 60 Global step 60 Train loss 16.862988 on epoch=29
03/19/2022 18:18:28 - INFO - __main__ - Step 70 Global step 70 Train loss 14.299484 on epoch=34
03/19/2022 18:18:33 - INFO - __main__ - Step 80 Global step 80 Train loss 15.038706 on epoch=39
03/19/2022 18:18:38 - INFO - __main__ - Step 90 Global step 90 Train loss 13.371783 on epoch=44
03/19/2022 18:18:43 - INFO - __main__ - Step 100 Global step 100 Train loss 12.246881 on epoch=49
03/19/2022 18:18:44 - INFO - __main__ - Global step 100 Train loss 14.363969 Classification-F1 0.0 on epoch=49
03/19/2022 18:18:49 - INFO - __main__ - Step 110 Global step 110 Train loss 12.593477 on epoch=54
03/19/2022 18:18:54 - INFO - __main__ - Step 120 Global step 120 Train loss 10.861132 on epoch=59
03/19/2022 18:18:59 - INFO - __main__ - Step 130 Global step 130 Train loss 8.854227 on epoch=64
03/19/2022 18:19:04 - INFO - __main__ - Step 140 Global step 140 Train loss 6.367476 on epoch=69
03/19/2022 18:19:09 - INFO - __main__ - Step 150 Global step 150 Train loss 3.802238 on epoch=74
03/19/2022 18:19:09 - INFO - __main__ - Global step 150 Train loss 8.495709 Classification-F1 0.13938618925831203 on epoch=74
03/19/2022 18:19:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.607894 on epoch=79
03/19/2022 18:19:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.877702 on epoch=84
03/19/2022 18:19:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.471928 on epoch=89
03/19/2022 18:19:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.303680 on epoch=94
03/19/2022 18:19:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.290970 on epoch=99
03/19/2022 18:19:36 - INFO - __main__ - Global step 200 Train loss 0.710435 Classification-F1 0.7046153846153846 on epoch=99
03/19/2022 18:19:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.267877 on epoch=104
03/19/2022 18:19:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.271610 on epoch=109
03/19/2022 18:19:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.200114 on epoch=114
03/19/2022 18:19:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.144974 on epoch=119
03/19/2022 18:20:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.237156 on epoch=124
03/19/2022 18:20:03 - INFO - __main__ - Global step 250 Train loss 0.224346 Classification-F1 0.7046153846153846 on epoch=124
03/19/2022 18:20:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.158932 on epoch=129
03/19/2022 18:20:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.124165 on epoch=134
03/19/2022 18:20:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100560 on epoch=139
03/19/2022 18:20:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.122793 on epoch=144
03/19/2022 18:20:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.119393 on epoch=149
03/19/2022 18:20:28 - INFO - __main__ - Global step 300 Train loss 0.125169 Classification-F1 0.8117647058823529 on epoch=149
03/19/2022 18:20:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.080087 on epoch=154
03/19/2022 18:20:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.082776 on epoch=159
03/19/2022 18:20:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.062067 on epoch=164
03/19/2022 18:20:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.031905 on epoch=169
03/19/2022 18:20:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.041859 on epoch=174
03/19/2022 18:20:55 - INFO - __main__ - Global step 350 Train loss 0.059739 Classification-F1 0.8117647058823529 on epoch=174
03/19/2022 18:21:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.032981 on epoch=179
03/19/2022 18:21:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.021873 on epoch=184
03/19/2022 18:21:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025844 on epoch=189
03/19/2022 18:21:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.012289 on epoch=194
03/19/2022 18:21:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.032903 on epoch=199
03/19/2022 18:21:20 - INFO - __main__ - Global step 400 Train loss 0.025178 Classification-F1 0.906158357771261 on epoch=199
03/19/2022 18:21:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015474 on epoch=204
03/19/2022 18:21:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005753 on epoch=209
03/19/2022 18:21:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.018604 on epoch=214
03/19/2022 18:21:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.007287 on epoch=219
03/19/2022 18:21:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004012 on epoch=224
03/19/2022 18:21:46 - INFO - __main__ - Global step 450 Train loss 0.010226 Classification-F1 0.8117647058823529 on epoch=224
03/19/2022 18:21:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002097 on epoch=229
03/19/2022 18:21:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003273 on epoch=234
03/19/2022 18:22:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012204 on epoch=239
03/19/2022 18:22:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004012 on epoch=244
03/19/2022 18:22:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002407 on epoch=249
03/19/2022 18:22:10 - INFO - __main__ - Global step 500 Train loss 0.004798 Classification-F1 0.8435972629521017 on epoch=249
03/19/2022 18:22:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002812 on epoch=254
03/19/2022 18:22:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000853 on epoch=259
03/19/2022 18:22:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001508 on epoch=264
03/19/2022 18:22:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001432 on epoch=269
03/19/2022 18:22:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003187 on epoch=274
03/19/2022 18:22:36 - INFO - __main__ - Global step 550 Train loss 0.001959 Classification-F1 0.8423645320197044 on epoch=274
03/19/2022 18:22:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002401 on epoch=279
03/19/2022 18:22:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.007744 on epoch=284
03/19/2022 18:22:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.015238 on epoch=289
03/19/2022 18:22:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.063262 on epoch=294
03/19/2022 18:23:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003151 on epoch=299
03/19/2022 18:23:00 - INFO - __main__ - Global step 600 Train loss 0.018359 Classification-F1 0.8117647058823529 on epoch=299
03/19/2022 18:23:00 - INFO - __main__ - save last model!
03/19/2022 18:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:23:01 - INFO - __main__ - Printing 3 examples
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:23:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:23:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:23:01 - INFO - __main__ - Printing 3 examples
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:23:01 - INFO - __main__ - ['false']
03/19/2022 18:23:01 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:23:01 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:23:01 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:23:08 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:23:08 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:23:08 - INFO - __main__ - Printing 3 examples
03/19/2022 18:23:08 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:23:08 - INFO - __main__ - ['true']
03/19/2022 18:23:08 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:23:08 - INFO - __main__ - ['false']
03/19/2022 18:23:08 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:23:08 - INFO - __main__ - ['false']
03/19/2022 18:23:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:23:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:23:09 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:23:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_100_0.0002_8_predictions.txt
03/19/2022 18:23:10 - INFO - __main__ - Classification-F1 on test data: 0.8653
03/19/2022 18:23:10 - INFO - __main__ - prefix=ethos-race_16_100, lr=0.0002, bsz=8, dev_performance=0.906158357771261, test_performance=0.8653250773993808
03/19/2022 18:23:10 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.0001, bsz=8 ...
03/19/2022 18:23:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:23:11 - INFO - __main__ - Printing 3 examples
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:23:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:23:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:23:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:23:11 - INFO - __main__ - Printing 3 examples
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] Islam is a horrible religion and does no good for society as a whole........ Horrible! Horrible! Horrible!
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] I dont care about your mental illness. I dont want to pay tax for your damage brain
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 18:23:11 - INFO - __main__ - ['false']
03/19/2022 18:23:11 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:23:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:23:11 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:23:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:23:12 - INFO - __main__ - Starting training!
03/19/2022 18:23:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:23:24 - INFO - __main__ - Starting training!
03/19/2022 18:23:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.060268 on epoch=4
03/19/2022 18:23:33 - INFO - __main__ - Step 20 Global step 20 Train loss 22.671732 on epoch=9
03/19/2022 18:23:38 - INFO - __main__ - Step 30 Global step 30 Train loss 18.499155 on epoch=14
03/19/2022 18:23:43 - INFO - __main__ - Step 40 Global step 40 Train loss 18.093235 on epoch=19
03/19/2022 18:23:48 - INFO - __main__ - Step 50 Global step 50 Train loss 17.581146 on epoch=24
03/19/2022 18:23:59 - INFO - __main__ - Global step 50 Train loss 19.981106 Classification-F1 0.0 on epoch=24
03/19/2022 18:24:04 - INFO - __main__ - Step 60 Global step 60 Train loss 16.872150 on epoch=29
03/19/2022 18:24:09 - INFO - __main__ - Step 70 Global step 70 Train loss 16.622513 on epoch=34
03/19/2022 18:24:14 - INFO - __main__ - Step 80 Global step 80 Train loss 16.878712 on epoch=39
03/19/2022 18:24:19 - INFO - __main__ - Step 90 Global step 90 Train loss 15.112043 on epoch=44
03/19/2022 18:24:24 - INFO - __main__ - Step 100 Global step 100 Train loss 14.993402 on epoch=49
03/19/2022 18:24:35 - INFO - __main__ - Global step 100 Train loss 16.095766 Classification-F1 0.0 on epoch=49
03/19/2022 18:24:40 - INFO - __main__ - Step 110 Global step 110 Train loss 14.522757 on epoch=54
03/19/2022 18:24:45 - INFO - __main__ - Step 120 Global step 120 Train loss 13.991544 on epoch=59
03/19/2022 18:24:50 - INFO - __main__ - Step 130 Global step 130 Train loss 14.557012 on epoch=64
03/19/2022 18:24:54 - INFO - __main__ - Step 140 Global step 140 Train loss 14.026289 on epoch=69
03/19/2022 18:24:59 - INFO - __main__ - Step 150 Global step 150 Train loss 13.431070 on epoch=74
03/19/2022 18:25:10 - INFO - __main__ - Global step 150 Train loss 14.105734 Classification-F1 0.0 on epoch=74
03/19/2022 18:25:15 - INFO - __main__ - Step 160 Global step 160 Train loss 13.259295 on epoch=79
03/19/2022 18:25:20 - INFO - __main__ - Step 170 Global step 170 Train loss 12.233609 on epoch=84
03/19/2022 18:25:25 - INFO - __main__ - Step 180 Global step 180 Train loss 11.605153 on epoch=89
03/19/2022 18:25:30 - INFO - __main__ - Step 190 Global step 190 Train loss 10.501926 on epoch=94
03/19/2022 18:25:34 - INFO - __main__ - Step 200 Global step 200 Train loss 7.860994 on epoch=99
03/19/2022 18:25:36 - INFO - __main__ - Global step 200 Train loss 11.092196 Classification-F1 0.07368421052631578 on epoch=99
03/19/2022 18:25:42 - INFO - __main__ - Step 210 Global step 210 Train loss 3.935274 on epoch=104
03/19/2022 18:25:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.007071 on epoch=109
03/19/2022 18:25:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.696768 on epoch=114
03/19/2022 18:25:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.711942 on epoch=119
03/19/2022 18:26:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.414675 on epoch=124
03/19/2022 18:26:02 - INFO - __main__ - Global step 250 Train loss 1.353146 Classification-F1 0.539313399778516 on epoch=124
03/19/2022 18:26:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.483502 on epoch=129
03/19/2022 18:26:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.398445 on epoch=134
03/19/2022 18:26:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.280044 on epoch=139
03/19/2022 18:26:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.277504 on epoch=144
03/19/2022 18:26:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.262456 on epoch=149
03/19/2022 18:26:28 - INFO - __main__ - Global step 300 Train loss 0.340390 Classification-F1 0.7793103448275862 on epoch=149
03/19/2022 18:26:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.299595 on epoch=154
03/19/2022 18:26:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.181841 on epoch=159
03/19/2022 18:26:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.409328 on epoch=164
03/19/2022 18:26:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.209032 on epoch=169
03/19/2022 18:26:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.142142 on epoch=174
03/19/2022 18:26:54 - INFO - __main__ - Global step 350 Train loss 0.248388 Classification-F1 0.6101882613510521 on epoch=174
03/19/2022 18:26:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.107758 on epoch=179
03/19/2022 18:27:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.149104 on epoch=184
03/19/2022 18:27:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.123162 on epoch=189
03/19/2022 18:27:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.095061 on epoch=194
03/19/2022 18:27:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.087117 on epoch=199
03/19/2022 18:27:20 - INFO - __main__ - Global step 400 Train loss 0.112441 Classification-F1 0.8423645320197044 on epoch=199
03/19/2022 18:27:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.079263 on epoch=204
03/19/2022 18:27:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.039419 on epoch=209
03/19/2022 18:27:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.083554 on epoch=214
03/19/2022 18:27:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.033624 on epoch=219
03/19/2022 18:27:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.026133 on epoch=224
03/19/2022 18:27:46 - INFO - __main__ - Global step 450 Train loss 0.052399 Classification-F1 0.6945917285259808 on epoch=224
03/19/2022 18:27:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.043006 on epoch=229
03/19/2022 18:27:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034152 on epoch=234
03/19/2022 18:28:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.058421 on epoch=239
03/19/2022 18:28:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.014251 on epoch=244
03/19/2022 18:28:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.031252 on epoch=249
03/19/2022 18:28:11 - INFO - __main__ - Global step 500 Train loss 0.036216 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 18:28:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.159362 on epoch=254
03/19/2022 18:28:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.279104 on epoch=259
03/19/2022 18:28:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.066994 on epoch=264
03/19/2022 18:28:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.057611 on epoch=269
03/19/2022 18:28:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.007064 on epoch=274
03/19/2022 18:28:37 - INFO - __main__ - Global step 550 Train loss 0.114027 Classification-F1 0.7333333333333334 on epoch=274
03/19/2022 18:28:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.036653 on epoch=279
03/19/2022 18:28:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.020043 on epoch=284
03/19/2022 18:28:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021639 on epoch=289
03/19/2022 18:28:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021630 on epoch=294
03/19/2022 18:29:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004285 on epoch=299
03/19/2022 18:29:02 - INFO - __main__ - Global step 600 Train loss 0.020850 Classification-F1 0.7757757757757757 on epoch=299
03/19/2022 18:29:02 - INFO - __main__ - save last model!
03/19/2022 18:29:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:29:02 - INFO - __main__ - Printing 3 examples
03/19/2022 18:29:02 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:29:02 - INFO - __main__ - ['false']
03/19/2022 18:29:02 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:29:02 - INFO - __main__ - ['false']
03/19/2022 18:29:02 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:29:02 - INFO - __main__ - ['false']
03/19/2022 18:29:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:29:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:29:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:29:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:29:03 - INFO - __main__ - Printing 3 examples
03/19/2022 18:29:03 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:29:03 - INFO - __main__ - ['false']
03/19/2022 18:29:03 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:29:03 - INFO - __main__ - ['false']
03/19/2022 18:29:03 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:29:03 - INFO - __main__ - ['false']
03/19/2022 18:29:03 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:29:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:29:03 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:29:09 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:29:09 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:29:09 - INFO - __main__ - Printing 3 examples
03/19/2022 18:29:09 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:29:09 - INFO - __main__ - ['true']
03/19/2022 18:29:09 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:29:09 - INFO - __main__ - ['false']
03/19/2022 18:29:09 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:29:09 - INFO - __main__ - ['false']
03/19/2022 18:29:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:29:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:29:09 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:29:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_100_0.0001_8_predictions.txt
03/19/2022 18:29:11 - INFO - __main__ - Classification-F1 on test data: 0.7231
03/19/2022 18:29:11 - INFO - __main__ - prefix=ethos-race_16_100, lr=0.0001, bsz=8, dev_performance=0.8423645320197044, test_performance=0.7230691908192326
03/19/2022 18:29:11 - INFO - __main__ - Running ... prefix=ethos-race_16_13, lr=0.0005, bsz=8 ...
03/19/2022 18:29:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:29:12 - INFO - __main__ - Printing 3 examples
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:29:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:29:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:29:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:29:12 - INFO - __main__ - Printing 3 examples
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:29:12 - INFO - __main__ - ['false']
03/19/2022 18:29:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:29:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:29:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:29:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:29:13 - INFO - __main__ - Starting training!
03/19/2022 18:29:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:29:23 - INFO - __main__ - Starting training!
03/19/2022 18:29:27 - INFO - __main__ - Step 10 Global step 10 Train loss 24.921276 on epoch=4
03/19/2022 18:29:32 - INFO - __main__ - Step 20 Global step 20 Train loss 19.790237 on epoch=9
03/19/2022 18:29:38 - INFO - __main__ - Step 30 Global step 30 Train loss 16.587156 on epoch=14
03/19/2022 18:29:43 - INFO - __main__ - Step 40 Global step 40 Train loss 14.561041 on epoch=19
03/19/2022 18:29:48 - INFO - __main__ - Step 50 Global step 50 Train loss 12.708891 on epoch=24
03/19/2022 18:29:48 - INFO - __main__ - Global step 50 Train loss 17.713720 Classification-F1 0.0 on epoch=24
03/19/2022 18:29:55 - INFO - __main__ - Step 60 Global step 60 Train loss 10.316935 on epoch=29
03/19/2022 18:29:59 - INFO - __main__ - Step 70 Global step 70 Train loss 7.040132 on epoch=34
03/19/2022 18:30:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.450913 on epoch=39
03/19/2022 18:30:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.124971 on epoch=44
03/19/2022 18:30:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.507338 on epoch=49
03/19/2022 18:30:15 - INFO - __main__ - Global step 100 Train loss 4.088058 Classification-F1 0.3992490613266583 on epoch=49
03/19/2022 18:30:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.378224 on epoch=54
03/19/2022 18:30:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.416926 on epoch=59
03/19/2022 18:30:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.365005 on epoch=64
03/19/2022 18:30:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.466351 on epoch=69
03/19/2022 18:30:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.366057 on epoch=74
03/19/2022 18:30:43 - INFO - __main__ - Global step 150 Train loss 0.398512 Classification-F1 0.3333333333333333 on epoch=74
03/19/2022 18:30:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.333734 on epoch=79
03/19/2022 18:30:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.335085 on epoch=84
03/19/2022 18:30:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.287483 on epoch=89
03/19/2022 18:31:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.324606 on epoch=94
03/19/2022 18:31:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.288330 on epoch=99
03/19/2022 18:31:10 - INFO - __main__ - Global step 200 Train loss 0.313848 Classification-F1 0.3992490613266583 on epoch=99
03/19/2022 18:31:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.253371 on epoch=104
03/19/2022 18:31:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.331898 on epoch=109
03/19/2022 18:31:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.281919 on epoch=114
03/19/2022 18:31:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.299294 on epoch=119
03/19/2022 18:31:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.314195 on epoch=124
03/19/2022 18:31:36 - INFO - __main__ - Global step 250 Train loss 0.296135 Classification-F1 0.3333333333333333 on epoch=124
03/19/2022 18:31:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.269184 on epoch=129
03/19/2022 18:31:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.275436 on epoch=134
03/19/2022 18:31:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.314193 on epoch=139
03/19/2022 18:31:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.304118 on epoch=144
03/19/2022 18:32:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.260075 on epoch=149
03/19/2022 18:32:03 - INFO - __main__ - Global step 300 Train loss 0.284601 Classification-F1 0.3992490613266583 on epoch=149
03/19/2022 18:32:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.223006 on epoch=154
03/19/2022 18:32:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.369295 on epoch=159
03/19/2022 18:32:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.334399 on epoch=164
03/19/2022 18:32:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.270569 on epoch=169
03/19/2022 18:32:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.237171 on epoch=174
03/19/2022 18:32:30 - INFO - __main__ - Global step 350 Train loss 0.286888 Classification-F1 0.6389743589743591 on epoch=174
03/19/2022 18:32:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.266360 on epoch=179
03/19/2022 18:32:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.244777 on epoch=184
03/19/2022 18:32:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.239677 on epoch=189
03/19/2022 18:32:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.171248 on epoch=194
03/19/2022 18:32:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.282228 on epoch=199
03/19/2022 18:32:58 - INFO - __main__ - Global step 400 Train loss 0.240858 Classification-F1 0.6190476190476191 on epoch=199
03/19/2022 18:33:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.200542 on epoch=204
03/19/2022 18:33:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.262216 on epoch=209
03/19/2022 18:33:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.144451 on epoch=214
03/19/2022 18:33:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.123102 on epoch=219
03/19/2022 18:33:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.279954 on epoch=224
03/19/2022 18:33:25 - INFO - __main__ - Global step 450 Train loss 0.202053 Classification-F1 0.5333333333333333 on epoch=224
03/19/2022 18:33:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.115887 on epoch=229
03/19/2022 18:33:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.080355 on epoch=234
03/19/2022 18:33:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.110693 on epoch=239
03/19/2022 18:33:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.286132 on epoch=244
03/19/2022 18:33:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.083376 on epoch=249
03/19/2022 18:33:52 - INFO - __main__ - Global step 500 Train loss 0.135288 Classification-F1 0.906158357771261 on epoch=249
03/19/2022 18:33:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.107325 on epoch=254
03/19/2022 18:34:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.055987 on epoch=259
03/19/2022 18:34:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.098751 on epoch=264
03/19/2022 18:34:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.054999 on epoch=269
03/19/2022 18:34:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.013601 on epoch=274
03/19/2022 18:34:20 - INFO - __main__ - Global step 550 Train loss 0.066133 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 18:34:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.047113 on epoch=279
03/19/2022 18:34:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.113710 on epoch=284
03/19/2022 18:34:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.038231 on epoch=289
03/19/2022 18:34:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.635863 on epoch=294
03/19/2022 18:34:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.018715 on epoch=299
03/19/2022 18:34:47 - INFO - __main__ - Global step 600 Train loss 0.170726 Classification-F1 0.8423645320197044 on epoch=299
03/19/2022 18:34:47 - INFO - __main__ - save last model!
03/19/2022 18:34:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:34:50 - INFO - __main__ - Printing 3 examples
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:34:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:34:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:34:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:34:50 - INFO - __main__ - Printing 3 examples
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:34:50 - INFO - __main__ - ['false']
03/19/2022 18:34:50 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:34:50 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:34:50 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:34:54 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:34:55 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:34:55 - INFO - __main__ - Printing 3 examples
03/19/2022 18:34:55 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:34:55 - INFO - __main__ - ['true']
03/19/2022 18:34:55 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:34:55 - INFO - __main__ - ['false']
03/19/2022 18:34:55 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:34:55 - INFO - __main__ - ['false']
03/19/2022 18:34:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:34:55 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:34:55 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:34:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_13_0.0005_8_predictions.txt
03/19/2022 18:34:57 - INFO - __main__ - Classification-F1 on test data: 0.7807
03/19/2022 18:34:57 - INFO - __main__ - prefix=ethos-race_16_13, lr=0.0005, bsz=8, dev_performance=0.906158357771261, test_performance=0.7806722689075631
03/19/2022 18:34:57 - INFO - __main__ - Running ... prefix=ethos-race_16_13, lr=0.0003, bsz=8 ...
03/19/2022 18:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:34:58 - INFO - __main__ - Printing 3 examples
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:34:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:34:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:34:58 - INFO - __main__ - Printing 3 examples
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:34:58 - INFO - __main__ - ['false']
03/19/2022 18:34:58 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:34:58 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:34:58 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:35:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:35:01 - INFO - __main__ - Starting training!
03/19/2022 18:35:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:35:11 - INFO - __main__ - Starting training!
03/19/2022 18:35:15 - INFO - __main__ - Step 10 Global step 10 Train loss 24.219219 on epoch=4
03/19/2022 18:35:21 - INFO - __main__ - Step 20 Global step 20 Train loss 20.140003 on epoch=9
03/19/2022 18:35:26 - INFO - __main__ - Step 30 Global step 30 Train loss 17.094395 on epoch=14
03/19/2022 18:35:31 - INFO - __main__ - Step 40 Global step 40 Train loss 16.504572 on epoch=19
03/19/2022 18:35:36 - INFO - __main__ - Step 50 Global step 50 Train loss 14.322889 on epoch=24
03/19/2022 18:35:37 - INFO - __main__ - Global step 50 Train loss 18.456215 Classification-F1 0.0 on epoch=24
03/19/2022 18:35:42 - INFO - __main__ - Step 60 Global step 60 Train loss 13.948477 on epoch=29
03/19/2022 18:35:47 - INFO - __main__ - Step 70 Global step 70 Train loss 12.636735 on epoch=34
03/19/2022 18:35:52 - INFO - __main__ - Step 80 Global step 80 Train loss 11.354333 on epoch=39
03/19/2022 18:35:57 - INFO - __main__ - Step 90 Global step 90 Train loss 7.446322 on epoch=44
03/19/2022 18:36:02 - INFO - __main__ - Step 100 Global step 100 Train loss 4.918843 on epoch=49
03/19/2022 18:36:03 - INFO - __main__ - Global step 100 Train loss 10.060943 Classification-F1 0.19703703703703704 on epoch=49
03/19/2022 18:36:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.795919 on epoch=54
03/19/2022 18:36:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.564383 on epoch=59
03/19/2022 18:36:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.432637 on epoch=64
03/19/2022 18:36:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.356618 on epoch=69
03/19/2022 18:36:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.313666 on epoch=74
03/19/2022 18:36:30 - INFO - __main__ - Global step 150 Train loss 0.692645 Classification-F1 0.3378787878787879 on epoch=74
03/19/2022 18:36:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.272728 on epoch=79
03/19/2022 18:36:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.339016 on epoch=84
03/19/2022 18:36:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.269955 on epoch=89
03/19/2022 18:36:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.257086 on epoch=94
03/19/2022 18:36:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.247292 on epoch=99
03/19/2022 18:36:57 - INFO - __main__ - Global step 200 Train loss 0.277216 Classification-F1 0.3992490613266583 on epoch=99
03/19/2022 18:37:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.239835 on epoch=104
03/19/2022 18:37:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.185459 on epoch=109
03/19/2022 18:37:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.139253 on epoch=114
03/19/2022 18:37:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.130524 on epoch=119
03/19/2022 18:37:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.157377 on epoch=124
03/19/2022 18:37:24 - INFO - __main__ - Global step 250 Train loss 0.170490 Classification-F1 0.5134502923976608 on epoch=124
03/19/2022 18:37:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.114849 on epoch=129
03/19/2022 18:37:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.219558 on epoch=134
03/19/2022 18:37:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100578 on epoch=139
03/19/2022 18:37:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.103490 on epoch=144
03/19/2022 18:37:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.104502 on epoch=149
03/19/2022 18:37:51 - INFO - __main__ - Global step 300 Train loss 0.128595 Classification-F1 0.5588547189819725 on epoch=149
03/19/2022 18:37:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.119477 on epoch=154
03/19/2022 18:38:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.072179 on epoch=159
03/19/2022 18:38:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.061714 on epoch=164
03/19/2022 18:38:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.083323 on epoch=169
03/19/2022 18:38:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.044479 on epoch=174
03/19/2022 18:38:18 - INFO - __main__ - Global step 350 Train loss 0.076234 Classification-F1 0.539313399778516 on epoch=174
03/19/2022 18:38:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.089325 on epoch=179
03/19/2022 18:38:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.050492 on epoch=184
03/19/2022 18:38:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.095611 on epoch=189
03/19/2022 18:38:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.059063 on epoch=194
03/19/2022 18:38:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.065098 on epoch=199
03/19/2022 18:38:44 - INFO - __main__ - Global step 400 Train loss 0.071918 Classification-F1 0.6389743589743591 on epoch=199
03/19/2022 18:38:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.023687 on epoch=204
03/19/2022 18:38:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.014766 on epoch=209
03/19/2022 18:39:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.057908 on epoch=214
03/19/2022 18:39:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.040686 on epoch=219
03/19/2022 18:39:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.091567 on epoch=224
03/19/2022 18:39:11 - INFO - __main__ - Global step 450 Train loss 0.045723 Classification-F1 0.746031746031746 on epoch=224
03/19/2022 18:39:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.059816 on epoch=229
03/19/2022 18:39:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.060611 on epoch=234
03/19/2022 18:39:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.100384 on epoch=239
03/19/2022 18:39:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.045444 on epoch=244
03/19/2022 18:39:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.153690 on epoch=249
03/19/2022 18:39:38 - INFO - __main__ - Global step 500 Train loss 0.083989 Classification-F1 0.6389743589743591 on epoch=249
03/19/2022 18:39:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.065456 on epoch=254
03/19/2022 18:39:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.071222 on epoch=259
03/19/2022 18:39:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.083263 on epoch=264
03/19/2022 18:39:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.073437 on epoch=269
03/19/2022 18:40:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.101226 on epoch=274
03/19/2022 18:40:04 - INFO - __main__ - Global step 550 Train loss 0.078921 Classification-F1 0.5134502923976608 on epoch=274
03/19/2022 18:40:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.065256 on epoch=279
03/19/2022 18:40:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.056534 on epoch=284
03/19/2022 18:40:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.081794 on epoch=289
03/19/2022 18:40:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.083069 on epoch=294
03/19/2022 18:40:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.053310 on epoch=299
03/19/2022 18:40:31 - INFO - __main__ - Global step 600 Train loss 0.067993 Classification-F1 0.6267232237539766 on epoch=299
03/19/2022 18:40:31 - INFO - __main__ - save last model!
03/19/2022 18:40:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:40:32 - INFO - __main__ - Printing 3 examples
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:40:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:40:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:40:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:40:32 - INFO - __main__ - Printing 3 examples
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:40:32 - INFO - __main__ - ['false']
03/19/2022 18:40:32 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:40:32 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:40:32 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:40:38 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:40:39 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:40:39 - INFO - __main__ - Printing 3 examples
03/19/2022 18:40:39 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:40:39 - INFO - __main__ - ['true']
03/19/2022 18:40:39 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:40:39 - INFO - __main__ - ['false']
03/19/2022 18:40:39 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:40:39 - INFO - __main__ - ['false']
03/19/2022 18:40:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:40:39 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:40:39 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:40:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_13_0.0003_8_predictions.txt
03/19/2022 18:40:40 - INFO - __main__ - Classification-F1 on test data: 0.6853
03/19/2022 18:40:40 - INFO - __main__ - prefix=ethos-race_16_13, lr=0.0003, bsz=8, dev_performance=0.746031746031746, test_performance=0.6852713178294574
03/19/2022 18:40:40 - INFO - __main__ - Running ... prefix=ethos-race_16_13, lr=0.0002, bsz=8 ...
03/19/2022 18:40:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:40:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:40:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:40:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:40:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:40:41 - INFO - __main__ - Printing 3 examples
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:40:41 - INFO - __main__ - ['false']
03/19/2022 18:40:41 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:40:41 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:40:41 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:40:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:40:43 - INFO - __main__ - Starting training!
03/19/2022 18:40:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:40:54 - INFO - __main__ - Starting training!
03/19/2022 18:40:59 - INFO - __main__ - Step 10 Global step 10 Train loss 24.113773 on epoch=4
03/19/2022 18:41:04 - INFO - __main__ - Step 20 Global step 20 Train loss 19.249426 on epoch=9
03/19/2022 18:41:09 - INFO - __main__ - Step 30 Global step 30 Train loss 17.343311 on epoch=14
03/19/2022 18:41:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.676920 on epoch=19
03/19/2022 18:41:20 - INFO - __main__ - Step 50 Global step 50 Train loss 16.257191 on epoch=24
03/19/2022 18:41:21 - INFO - __main__ - Global step 50 Train loss 18.728125 Classification-F1 0.0 on epoch=24
03/19/2022 18:41:26 - INFO - __main__ - Step 60 Global step 60 Train loss 15.548330 on epoch=29
03/19/2022 18:41:32 - INFO - __main__ - Step 70 Global step 70 Train loss 14.565462 on epoch=34
03/19/2022 18:41:37 - INFO - __main__ - Step 80 Global step 80 Train loss 13.687338 on epoch=39
03/19/2022 18:41:42 - INFO - __main__ - Step 90 Global step 90 Train loss 13.585991 on epoch=44
03/19/2022 18:41:47 - INFO - __main__ - Step 100 Global step 100 Train loss 12.969905 on epoch=49
03/19/2022 18:41:48 - INFO - __main__ - Global step 100 Train loss 14.071405 Classification-F1 0.0 on epoch=49
03/19/2022 18:41:53 - INFO - __main__ - Step 110 Global step 110 Train loss 10.727774 on epoch=54
03/19/2022 18:41:59 - INFO - __main__ - Step 120 Global step 120 Train loss 8.844698 on epoch=59
03/19/2022 18:42:04 - INFO - __main__ - Step 130 Global step 130 Train loss 1.920918 on epoch=64
03/19/2022 18:42:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.676436 on epoch=69
03/19/2022 18:42:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.479976 on epoch=74
03/19/2022 18:42:15 - INFO - __main__ - Global step 150 Train loss 4.529960 Classification-F1 0.4909862142099682 on epoch=74
03/19/2022 18:42:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.309835 on epoch=79
03/19/2022 18:42:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.301047 on epoch=84
03/19/2022 18:42:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.216771 on epoch=89
03/19/2022 18:42:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.202678 on epoch=94
03/19/2022 18:42:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.174455 on epoch=99
03/19/2022 18:42:42 - INFO - __main__ - Global step 200 Train loss 0.240957 Classification-F1 0.5151515151515151 on epoch=99
03/19/2022 18:42:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.179981 on epoch=104
03/19/2022 18:42:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.168867 on epoch=109
03/19/2022 18:42:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.083670 on epoch=114
03/19/2022 18:43:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.167214 on epoch=119
03/19/2022 18:43:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.278712 on epoch=124
03/19/2022 18:43:10 - INFO - __main__ - Global step 250 Train loss 0.175689 Classification-F1 0.805668016194332 on epoch=124
03/19/2022 18:43:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.165549 on epoch=129
03/19/2022 18:43:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.097301 on epoch=134
03/19/2022 18:43:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.138404 on epoch=139
03/19/2022 18:43:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.112029 on epoch=144
03/19/2022 18:43:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.089212 on epoch=149
03/19/2022 18:43:37 - INFO - __main__ - Global step 300 Train loss 0.120499 Classification-F1 0.716256157635468 on epoch=149
03/19/2022 18:43:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.063799 on epoch=154
03/19/2022 18:43:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.059644 on epoch=159
03/19/2022 18:43:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.030713 on epoch=164
03/19/2022 18:43:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.058424 on epoch=169
03/19/2022 18:44:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.039735 on epoch=174
03/19/2022 18:44:04 - INFO - __main__ - Global step 350 Train loss 0.050463 Classification-F1 0.6389743589743591 on epoch=174
03/19/2022 18:44:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.038902 on epoch=179
03/19/2022 18:44:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.122180 on epoch=184
03/19/2022 18:44:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.102062 on epoch=189
03/19/2022 18:44:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.064303 on epoch=194
03/19/2022 18:44:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.122647 on epoch=199
03/19/2022 18:44:31 - INFO - __main__ - Global step 400 Train loss 0.090019 Classification-F1 0.7757757757757757 on epoch=199
03/19/2022 18:44:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.098038 on epoch=204
03/19/2022 18:44:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.061832 on epoch=209
03/19/2022 18:44:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.073548 on epoch=214
03/19/2022 18:44:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.017163 on epoch=219
03/19/2022 18:44:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.112975 on epoch=224
03/19/2022 18:44:57 - INFO - __main__ - Global step 450 Train loss 0.072711 Classification-F1 0.6190476190476191 on epoch=224
03/19/2022 18:45:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.051672 on epoch=229
03/19/2022 18:45:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.033842 on epoch=234
03/19/2022 18:45:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.017608 on epoch=239
03/19/2022 18:45:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.040436 on epoch=244
03/19/2022 18:45:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027332 on epoch=249
03/19/2022 18:45:24 - INFO - __main__ - Global step 500 Train loss 0.034178 Classification-F1 0.7490196078431373 on epoch=249
03/19/2022 18:45:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.019629 on epoch=254
03/19/2022 18:45:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.015821 on epoch=259
03/19/2022 18:45:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.026144 on epoch=264
03/19/2022 18:45:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.016598 on epoch=269
03/19/2022 18:45:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.047458 on epoch=274
03/19/2022 18:45:51 - INFO - __main__ - Global step 550 Train loss 0.025130 Classification-F1 0.8117647058823529 on epoch=274
03/19/2022 18:45:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003054 on epoch=279
03/19/2022 18:46:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006756 on epoch=284
03/19/2022 18:46:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.029085 on epoch=289
03/19/2022 18:46:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032498 on epoch=294
03/19/2022 18:46:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006901 on epoch=299
03/19/2022 18:46:18 - INFO - __main__ - Global step 600 Train loss 0.015659 Classification-F1 0.6113360323886641 on epoch=299
03/19/2022 18:46:18 - INFO - __main__ - save last model!
03/19/2022 18:46:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:46:20 - INFO - __main__ - Printing 3 examples
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:46:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:46:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:46:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:46:20 - INFO - __main__ - Printing 3 examples
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:46:20 - INFO - __main__ - ['false']
03/19/2022 18:46:20 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:46:20 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:46:20 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:46:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:46:25 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:46:25 - INFO - __main__ - Printing 3 examples
03/19/2022 18:46:25 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:46:25 - INFO - __main__ - ['true']
03/19/2022 18:46:25 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:46:25 - INFO - __main__ - ['false']
03/19/2022 18:46:25 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:46:25 - INFO - __main__ - ['false']
03/19/2022 18:46:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:46:25 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:46:25 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:46:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_13_0.0002_8_predictions.txt
03/19/2022 18:46:27 - INFO - __main__ - Classification-F1 on test data: 0.6448
03/19/2022 18:46:27 - INFO - __main__ - prefix=ethos-race_16_13, lr=0.0002, bsz=8, dev_performance=0.8117647058823529, test_performance=0.644771879992899
03/19/2022 18:46:27 - INFO - __main__ - Running ... prefix=ethos-race_16_13, lr=0.0001, bsz=8 ...
03/19/2022 18:46:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:46:28 - INFO - __main__ - Printing 3 examples
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] Disability centres all around the world. Thats were our money goes. Lets take em down
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:46:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:46:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:46:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:46:28 - INFO - __main__ - Printing 3 examples
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] Arrest that fat ass bitch!
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ -  [ethos-race] This is just crazy liberal satanic crap. Don't fall for it. There are only two genders and these people are just cofused psychos
03/19/2022 18:46:28 - INFO - __main__ - ['false']
03/19/2022 18:46:28 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:46:28 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:46:28 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:46:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:46:31 - INFO - __main__ - Starting training!
03/19/2022 18:46:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:46:39 - INFO - __main__ - Starting training!
03/19/2022 18:46:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.857613 on epoch=4
03/19/2022 18:46:48 - INFO - __main__ - Step 20 Global step 20 Train loss 22.179167 on epoch=9
03/19/2022 18:46:54 - INFO - __main__ - Step 30 Global step 30 Train loss 18.244940 on epoch=14
03/19/2022 18:46:59 - INFO - __main__ - Step 40 Global step 40 Train loss 17.774139 on epoch=19
03/19/2022 18:47:04 - INFO - __main__ - Step 50 Global step 50 Train loss 17.598185 on epoch=24
03/19/2022 18:47:14 - INFO - __main__ - Global step 50 Train loss 19.930809 Classification-F1 0.0 on epoch=24
03/19/2022 18:47:20 - INFO - __main__ - Step 60 Global step 60 Train loss 17.035450 on epoch=29
03/19/2022 18:47:25 - INFO - __main__ - Step 70 Global step 70 Train loss 16.862110 on epoch=34
03/19/2022 18:47:30 - INFO - __main__ - Step 80 Global step 80 Train loss 16.420776 on epoch=39
03/19/2022 18:47:36 - INFO - __main__ - Step 90 Global step 90 Train loss 15.263593 on epoch=44
03/19/2022 18:47:41 - INFO - __main__ - Step 100 Global step 100 Train loss 15.713739 on epoch=49
03/19/2022 18:47:50 - INFO - __main__ - Global step 100 Train loss 16.259134 Classification-F1 0.0 on epoch=49
03/19/2022 18:47:55 - INFO - __main__ - Step 110 Global step 110 Train loss 14.746155 on epoch=54
03/19/2022 18:48:00 - INFO - __main__ - Step 120 Global step 120 Train loss 15.416235 on epoch=59
03/19/2022 18:48:05 - INFO - __main__ - Step 130 Global step 130 Train loss 13.942645 on epoch=64
03/19/2022 18:48:11 - INFO - __main__ - Step 140 Global step 140 Train loss 14.030782 on epoch=69
03/19/2022 18:48:16 - INFO - __main__ - Step 150 Global step 150 Train loss 13.673831 on epoch=74
03/19/2022 18:48:25 - INFO - __main__ - Global step 150 Train loss 14.361929 Classification-F1 0.0 on epoch=74
03/19/2022 18:48:30 - INFO - __main__ - Step 160 Global step 160 Train loss 13.868256 on epoch=79
03/19/2022 18:48:35 - INFO - __main__ - Step 170 Global step 170 Train loss 13.198758 on epoch=84
03/19/2022 18:48:41 - INFO - __main__ - Step 180 Global step 180 Train loss 12.345396 on epoch=89
03/19/2022 18:48:46 - INFO - __main__ - Step 190 Global step 190 Train loss 11.780294 on epoch=94
03/19/2022 18:48:51 - INFO - __main__ - Step 200 Global step 200 Train loss 10.832048 on epoch=99
03/19/2022 18:49:00 - INFO - __main__ - Global step 200 Train loss 12.404950 Classification-F1 0.0 on epoch=99
03/19/2022 18:49:05 - INFO - __main__ - Step 210 Global step 210 Train loss 8.674786 on epoch=104
03/19/2022 18:49:10 - INFO - __main__ - Step 220 Global step 220 Train loss 6.571424 on epoch=109
03/19/2022 18:49:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.480237 on epoch=114
03/19/2022 18:49:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.795906 on epoch=119
03/19/2022 18:49:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.634477 on epoch=124
03/19/2022 18:49:26 - INFO - __main__ - Global step 250 Train loss 3.631366 Classification-F1 0.5933528836754642 on epoch=124
03/19/2022 18:49:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.549500 on epoch=129
03/19/2022 18:49:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.491438 on epoch=134
03/19/2022 18:49:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.370656 on epoch=139
03/19/2022 18:49:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.267027 on epoch=144
03/19/2022 18:49:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.274381 on epoch=149
03/19/2022 18:49:54 - INFO - __main__ - Global step 300 Train loss 0.390600 Classification-F1 0.4666666666666667 on epoch=149
03/19/2022 18:49:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.262043 on epoch=154
03/19/2022 18:50:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.220987 on epoch=159
03/19/2022 18:50:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.190240 on epoch=164
03/19/2022 18:50:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.143964 on epoch=169
03/19/2022 18:50:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.145286 on epoch=174
03/19/2022 18:50:20 - INFO - __main__ - Global step 350 Train loss 0.192504 Classification-F1 0.5835835835835835 on epoch=174
03/19/2022 18:50:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.120886 on epoch=179
03/19/2022 18:50:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.131535 on epoch=184
03/19/2022 18:50:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.119019 on epoch=189
03/19/2022 18:50:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.080619 on epoch=194
03/19/2022 18:50:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.088067 on epoch=199
03/19/2022 18:50:47 - INFO - __main__ - Global step 400 Train loss 0.108025 Classification-F1 0.5465587044534412 on epoch=199
03/19/2022 18:50:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.085031 on epoch=204
03/19/2022 18:50:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.064153 on epoch=209
03/19/2022 18:51:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.074575 on epoch=214
03/19/2022 18:51:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.043851 on epoch=219
03/19/2022 18:51:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.155357 on epoch=224
03/19/2022 18:51:14 - INFO - __main__ - Global step 450 Train loss 0.084593 Classification-F1 0.5733333333333335 on epoch=224
03/19/2022 18:51:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.117623 on epoch=229
03/19/2022 18:51:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.031780 on epoch=234
03/19/2022 18:51:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.043431 on epoch=239
03/19/2022 18:51:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.036616 on epoch=244
03/19/2022 18:51:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.055497 on epoch=249
03/19/2022 18:51:41 - INFO - __main__ - Global step 500 Train loss 0.056989 Classification-F1 0.5733333333333335 on epoch=249
03/19/2022 18:51:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.018066 on epoch=254
03/19/2022 18:51:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.049341 on epoch=259
03/19/2022 18:51:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.096685 on epoch=264
03/19/2022 18:52:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.029120 on epoch=269
03/19/2022 18:52:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016727 on epoch=274
03/19/2022 18:52:07 - INFO - __main__ - Global step 550 Train loss 0.041988 Classification-F1 0.6862745098039216 on epoch=274
03/19/2022 18:52:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.025129 on epoch=279
03/19/2022 18:52:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024524 on epoch=284
03/19/2022 18:52:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026854 on epoch=289
03/19/2022 18:52:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.064849 on epoch=294
03/19/2022 18:52:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.033885 on epoch=299
03/19/2022 18:52:35 - INFO - __main__ - Global step 600 Train loss 0.035048 Classification-F1 0.6389743589743591 on epoch=299
03/19/2022 18:52:35 - INFO - __main__ - save last model!
03/19/2022 18:52:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:52:36 - INFO - __main__ - Printing 3 examples
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:52:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:52:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:52:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:52:36 - INFO - __main__ - Printing 3 examples
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 18:52:36 - INFO - __main__ - ['false']
03/19/2022 18:52:36 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:52:36 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:52:36 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:52:42 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:52:43 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:52:43 - INFO - __main__ - Printing 3 examples
03/19/2022 18:52:43 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:52:43 - INFO - __main__ - ['true']
03/19/2022 18:52:43 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:52:43 - INFO - __main__ - ['false']
03/19/2022 18:52:43 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:52:43 - INFO - __main__ - ['false']
03/19/2022 18:52:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:52:43 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:52:43 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:52:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_13_0.0001_8_predictions.txt
03/19/2022 18:52:44 - INFO - __main__ - Classification-F1 on test data: 0.6160
03/19/2022 18:52:45 - INFO - __main__ - prefix=ethos-race_16_13, lr=0.0001, bsz=8, dev_performance=0.6862745098039216, test_performance=0.616015132408575
03/19/2022 18:52:45 - INFO - __main__ - Running ... prefix=ethos-race_16_21, lr=0.0005, bsz=8 ...
03/19/2022 18:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:52:46 - INFO - __main__ - Printing 3 examples
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:52:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:52:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:52:46 - INFO - __main__ - Printing 3 examples
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 18:52:46 - INFO - __main__ - ['false']
03/19/2022 18:52:46 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:52:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:52:46 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:52:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:52:47 - INFO - __main__ - Starting training!
03/19/2022 18:52:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:52:56 - INFO - __main__ - Starting training!
03/19/2022 18:53:01 - INFO - __main__ - Step 10 Global step 10 Train loss 22.701069 on epoch=4
03/19/2022 18:53:06 - INFO - __main__ - Step 20 Global step 20 Train loss 17.557333 on epoch=9
03/19/2022 18:53:11 - INFO - __main__ - Step 30 Global step 30 Train loss 14.968165 on epoch=14
03/19/2022 18:53:16 - INFO - __main__ - Step 40 Global step 40 Train loss 14.090668 on epoch=19
03/19/2022 18:53:22 - INFO - __main__ - Step 50 Global step 50 Train loss 10.781642 on epoch=24
03/19/2022 18:53:32 - INFO - __main__ - Global step 50 Train loss 16.019775 Classification-F1 0.0 on epoch=24
03/19/2022 18:53:38 - INFO - __main__ - Step 60 Global step 60 Train loss 6.355067 on epoch=29
03/19/2022 18:53:43 - INFO - __main__ - Step 70 Global step 70 Train loss 2.270910 on epoch=34
03/19/2022 18:53:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.747292 on epoch=39
03/19/2022 18:53:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.465662 on epoch=44
03/19/2022 18:53:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.513403 on epoch=49
03/19/2022 18:53:58 - INFO - __main__ - Global step 100 Train loss 2.070467 Classification-F1 0.3816425120772947 on epoch=49
03/19/2022 18:54:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.382818 on epoch=54
03/19/2022 18:54:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.444256 on epoch=59
03/19/2022 18:54:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.427944 on epoch=64
03/19/2022 18:54:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.350925 on epoch=69
03/19/2022 18:54:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.344938 on epoch=74
03/19/2022 18:54:25 - INFO - __main__ - Global step 150 Train loss 0.390176 Classification-F1 0.3992490613266583 on epoch=74
03/19/2022 18:54:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.344746 on epoch=79
03/19/2022 18:54:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.361529 on epoch=84
03/19/2022 18:54:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.360276 on epoch=89
03/19/2022 18:54:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.306651 on epoch=94
03/19/2022 18:54:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.251522 on epoch=99
03/19/2022 18:54:52 - INFO - __main__ - Global step 200 Train loss 0.324945 Classification-F1 0.39756367663344405 on epoch=99
03/19/2022 18:54:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.378554 on epoch=104
03/19/2022 18:55:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.259236 on epoch=109
03/19/2022 18:55:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.267790 on epoch=114
03/19/2022 18:55:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.220819 on epoch=119
03/19/2022 18:55:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.233667 on epoch=124
03/19/2022 18:55:18 - INFO - __main__ - Global step 250 Train loss 0.272013 Classification-F1 0.39756367663344405 on epoch=124
03/19/2022 18:55:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.305553 on epoch=129
03/19/2022 18:55:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.252868 on epoch=134
03/19/2022 18:55:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.134395 on epoch=139
03/19/2022 18:55:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.210651 on epoch=144
03/19/2022 18:55:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.158941 on epoch=149
03/19/2022 18:55:44 - INFO - __main__ - Global step 300 Train loss 0.212481 Classification-F1 0.6267232237539766 on epoch=149
03/19/2022 18:55:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.112348 on epoch=154
03/19/2022 18:55:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.183449 on epoch=159
03/19/2022 18:56:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.218576 on epoch=164
03/19/2022 18:56:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.164527 on epoch=169
03/19/2022 18:56:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.082121 on epoch=174
03/19/2022 18:56:11 - INFO - __main__ - Global step 350 Train loss 0.152204 Classification-F1 0.6113360323886641 on epoch=174
03/19/2022 18:56:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.107892 on epoch=179
03/19/2022 18:56:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.088046 on epoch=184
03/19/2022 18:56:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034761 on epoch=189
03/19/2022 18:56:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.069001 on epoch=194
03/19/2022 18:56:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.055720 on epoch=199
03/19/2022 18:56:37 - INFO - __main__ - Global step 400 Train loss 0.071084 Classification-F1 0.7046153846153846 on epoch=199
03/19/2022 18:56:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.043295 on epoch=204
03/19/2022 18:56:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.025562 on epoch=209
03/19/2022 18:56:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.050950 on epoch=214
03/19/2022 18:56:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.007543 on epoch=219
03/19/2022 18:57:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.026475 on epoch=224
03/19/2022 18:57:04 - INFO - __main__ - Global step 450 Train loss 0.030765 Classification-F1 0.6761133603238867 on epoch=224
03/19/2022 18:57:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.071499 on epoch=229
03/19/2022 18:57:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.029780 on epoch=234
03/19/2022 18:57:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.031245 on epoch=239
03/19/2022 18:57:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.072923 on epoch=244
03/19/2022 18:57:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005950 on epoch=249
03/19/2022 18:57:30 - INFO - __main__ - Global step 500 Train loss 0.042280 Classification-F1 0.7046153846153846 on epoch=249
03/19/2022 18:57:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.016167 on epoch=254
03/19/2022 18:57:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012508 on epoch=259
03/19/2022 18:57:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003164 on epoch=264
03/19/2022 18:57:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.014951 on epoch=269
03/19/2022 18:57:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.008047 on epoch=274
03/19/2022 18:57:56 - INFO - __main__ - Global step 550 Train loss 0.010967 Classification-F1 0.7490196078431373 on epoch=274
03/19/2022 18:58:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.182287 on epoch=279
03/19/2022 18:58:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.034011 on epoch=284
03/19/2022 18:58:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011829 on epoch=289
03/19/2022 18:58:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014504 on epoch=294
03/19/2022 18:58:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.017220 on epoch=299
03/19/2022 18:58:23 - INFO - __main__ - Global step 600 Train loss 0.051970 Classification-F1 0.7184750733137829 on epoch=299
03/19/2022 18:58:23 - INFO - __main__ - save last model!
03/19/2022 18:58:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:58:24 - INFO - __main__ - Printing 3 examples
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 18:58:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:58:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:58:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:58:24 - INFO - __main__ - Printing 3 examples
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 18:58:24 - INFO - __main__ - ['false']
03/19/2022 18:58:24 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:58:24 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:58:24 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:58:30 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 18:58:31 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 18:58:31 - INFO - __main__ - Printing 3 examples
03/19/2022 18:58:31 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 18:58:31 - INFO - __main__ - ['true']
03/19/2022 18:58:31 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 18:58:31 - INFO - __main__ - ['false']
03/19/2022 18:58:31 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 18:58:31 - INFO - __main__ - ['false']
03/19/2022 18:58:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:58:31 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:58:31 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 18:58:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_21_0.0005_8_predictions.txt
03/19/2022 18:58:33 - INFO - __main__ - Classification-F1 on test data: 0.6345
03/19/2022 18:58:33 - INFO - __main__ - prefix=ethos-race_16_21, lr=0.0005, bsz=8, dev_performance=0.7490196078431373, test_performance=0.634453781512605
03/19/2022 18:58:33 - INFO - __main__ - Running ... prefix=ethos-race_16_21, lr=0.0003, bsz=8 ...
03/19/2022 18:58:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:58:34 - INFO - __main__ - Printing 3 examples
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 18:58:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:58:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 18:58:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 18:58:34 - INFO - __main__ - Printing 3 examples
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 18:58:34 - INFO - __main__ - ['false']
03/19/2022 18:58:34 - INFO - __main__ - Tokenizing Input ...
03/19/2022 18:58:34 - INFO - __main__ - Tokenizing Output ...
03/19/2022 18:58:34 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 18:58:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:58:35 - INFO - __main__ - Starting training!
03/19/2022 18:58:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 18:58:47 - INFO - __main__ - Starting training!
03/19/2022 18:58:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.720888 on epoch=4
03/19/2022 18:58:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.784794 on epoch=9
03/19/2022 18:59:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.741787 on epoch=14
03/19/2022 18:59:08 - INFO - __main__ - Step 40 Global step 40 Train loss 16.104671 on epoch=19
03/19/2022 18:59:14 - INFO - __main__ - Step 50 Global step 50 Train loss 14.305423 on epoch=24
03/19/2022 18:59:14 - INFO - __main__ - Global step 50 Train loss 18.131512 Classification-F1 0.0 on epoch=24
03/19/2022 18:59:20 - INFO - __main__ - Step 60 Global step 60 Train loss 13.286692 on epoch=29
03/19/2022 18:59:25 - INFO - __main__ - Step 70 Global step 70 Train loss 12.084376 on epoch=34
03/19/2022 18:59:31 - INFO - __main__ - Step 80 Global step 80 Train loss 10.268255 on epoch=39
03/19/2022 18:59:36 - INFO - __main__ - Step 90 Global step 90 Train loss 6.130181 on epoch=44
03/19/2022 18:59:41 - INFO - __main__ - Step 100 Global step 100 Train loss 2.848535 on epoch=49
03/19/2022 18:59:41 - INFO - __main__ - Global step 100 Train loss 8.923607 Classification-F1 0.4980392156862745 on epoch=49
03/19/2022 18:59:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.736925 on epoch=54
03/19/2022 18:59:53 - INFO - __main__ - Step 120 Global step 120 Train loss 3.718236 on epoch=59
03/19/2022 18:59:58 - INFO - __main__ - Step 130 Global step 130 Train loss 3.333537 on epoch=64
03/19/2022 19:00:03 - INFO - __main__ - Step 140 Global step 140 Train loss 2.787392 on epoch=69
03/19/2022 19:00:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.351399 on epoch=74
03/19/2022 19:00:09 - INFO - __main__ - Global step 150 Train loss 2.785498 Classification-F1 0.39756367663344405 on epoch=74
03/19/2022 19:00:14 - INFO - __main__ - Step 160 Global step 160 Train loss 1.086050 on epoch=79
03/19/2022 19:00:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.293489 on epoch=84
03/19/2022 19:00:25 - INFO - __main__ - Step 180 Global step 180 Train loss 1.032400 on epoch=89
03/19/2022 19:00:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.752723 on epoch=94
03/19/2022 19:00:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.717280 on epoch=99
03/19/2022 19:00:35 - INFO - __main__ - Global step 200 Train loss 0.976388 Classification-F1 0.5901477832512315 on epoch=99
03/19/2022 19:00:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.522724 on epoch=104
03/19/2022 19:00:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.518660 on epoch=109
03/19/2022 19:00:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.579910 on epoch=114
03/19/2022 19:00:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.535236 on epoch=119
03/19/2022 19:01:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.380112 on epoch=124
03/19/2022 19:01:03 - INFO - __main__ - Global step 250 Train loss 0.507328 Classification-F1 0.5901477832512315 on epoch=124
03/19/2022 19:01:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.489804 on epoch=129
03/19/2022 19:01:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.909368 on epoch=134
03/19/2022 19:01:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.367199 on epoch=139
03/19/2022 19:01:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.337863 on epoch=144
03/19/2022 19:01:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.237026 on epoch=149
03/19/2022 19:01:29 - INFO - __main__ - Global step 300 Train loss 0.468252 Classification-F1 0.5307917888563051 on epoch=149
03/19/2022 19:01:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.296817 on epoch=154
03/19/2022 19:01:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.396144 on epoch=159
03/19/2022 19:01:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.559608 on epoch=164
03/19/2022 19:01:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.314738 on epoch=169
03/19/2022 19:01:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.336874 on epoch=174
03/19/2022 19:01:56 - INFO - __main__ - Global step 350 Train loss 0.380836 Classification-F1 0.5134502923976608 on epoch=174
03/19/2022 19:02:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.394968 on epoch=179
03/19/2022 19:02:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.294674 on epoch=184
03/19/2022 19:02:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.280249 on epoch=189
03/19/2022 19:02:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.236887 on epoch=194
03/19/2022 19:02:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.247485 on epoch=199
03/19/2022 19:02:22 - INFO - __main__ - Global step 400 Train loss 0.290853 Classification-F1 0.3992490613266583 on epoch=199
03/19/2022 19:02:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.260903 on epoch=204
03/19/2022 19:02:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.273275 on epoch=209
03/19/2022 19:02:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.148311 on epoch=214
03/19/2022 19:02:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.142819 on epoch=219
03/19/2022 19:02:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.180727 on epoch=224
03/19/2022 19:02:49 - INFO - __main__ - Global step 450 Train loss 0.201207 Classification-F1 0.6235294117647059 on epoch=224
03/19/2022 19:02:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.102495 on epoch=229
03/19/2022 19:03:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116582 on epoch=234
03/19/2022 19:03:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.103001 on epoch=239
03/19/2022 19:03:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.081564 on epoch=244
03/19/2022 19:03:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.090108 on epoch=249
03/19/2022 19:03:16 - INFO - __main__ - Global step 500 Train loss 0.098750 Classification-F1 0.6559139784946237 on epoch=249
03/19/2022 19:03:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.064002 on epoch=254
03/19/2022 19:03:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.067736 on epoch=259
03/19/2022 19:03:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.081301 on epoch=264
03/19/2022 19:03:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.069156 on epoch=269
03/19/2022 19:03:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.044713 on epoch=274
03/19/2022 19:03:43 - INFO - __main__ - Global step 550 Train loss 0.065382 Classification-F1 0.7757757757757757 on epoch=274
03/19/2022 19:03:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.120307 on epoch=279
03/19/2022 19:03:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024063 on epoch=284
03/19/2022 19:04:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.019909 on epoch=289
03/19/2022 19:04:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.033748 on epoch=294
03/19/2022 19:04:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008439 on epoch=299
03/19/2022 19:04:11 - INFO - __main__ - Global step 600 Train loss 0.041293 Classification-F1 0.8117647058823529 on epoch=299
03/19/2022 19:04:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:04:12 - INFO - __main__ - Printing 3 examples
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:04:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:04:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:04:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:04:12 - INFO - __main__ - Printing 3 examples
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 19:04:12 - INFO - __main__ - ['false']
03/19/2022 19:04:12 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:04:12 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:04:12 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:04:12 - INFO - __main__ - save last model!
03/19/2022 19:04:19 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:04:19 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:04:19 - INFO - __main__ - Printing 3 examples
03/19/2022 19:04:19 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:04:19 - INFO - __main__ - ['true']
03/19/2022 19:04:19 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:04:19 - INFO - __main__ - ['false']
03/19/2022 19:04:19 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:04:19 - INFO - __main__ - ['false']
03/19/2022 19:04:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:04:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:04:19 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:04:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_21_0.0003_8_predictions.txt
03/19/2022 19:04:21 - INFO - __main__ - Classification-F1 on test data: 0.5704
03/19/2022 19:04:22 - INFO - __main__ - prefix=ethos-race_16_21, lr=0.0003, bsz=8, dev_performance=0.8117647058823529, test_performance=0.5703703703703703
03/19/2022 19:04:22 - INFO - __main__ - Running ... prefix=ethos-race_16_21, lr=0.0002, bsz=8 ...
03/19/2022 19:04:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:04:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:04:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:04:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:04:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:04:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 19:04:23 - INFO - __main__ - ['false']
03/19/2022 19:04:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:04:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:04:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:04:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:04:24 - INFO - __main__ - Starting training!
03/19/2022 19:04:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:04:33 - INFO - __main__ - Starting training!
03/19/2022 19:04:38 - INFO - __main__ - Step 10 Global step 10 Train loss 23.970861 on epoch=4
03/19/2022 19:04:43 - INFO - __main__ - Step 20 Global step 20 Train loss 20.507919 on epoch=9
03/19/2022 19:04:48 - INFO - __main__ - Step 30 Global step 30 Train loss 19.042385 on epoch=14
03/19/2022 19:04:53 - INFO - __main__ - Step 40 Global step 40 Train loss 17.163647 on epoch=19
03/19/2022 19:04:59 - INFO - __main__ - Step 50 Global step 50 Train loss 16.382397 on epoch=24
03/19/2022 19:05:08 - INFO - __main__ - Global step 50 Train loss 19.413441 Classification-F1 0.0 on epoch=24
03/19/2022 19:05:14 - INFO - __main__ - Step 60 Global step 60 Train loss 15.786552 on epoch=29
03/19/2022 19:05:19 - INFO - __main__ - Step 70 Global step 70 Train loss 15.088931 on epoch=34
03/19/2022 19:05:25 - INFO - __main__ - Step 80 Global step 80 Train loss 14.233858 on epoch=39
03/19/2022 19:05:30 - INFO - __main__ - Step 90 Global step 90 Train loss 14.345134 on epoch=44
03/19/2022 19:05:35 - INFO - __main__ - Step 100 Global step 100 Train loss 12.619282 on epoch=49
03/19/2022 19:05:40 - INFO - __main__ - Global step 100 Train loss 14.414751 Classification-F1 0.0 on epoch=49
03/19/2022 19:05:45 - INFO - __main__ - Step 110 Global step 110 Train loss 12.708747 on epoch=54
03/19/2022 19:05:50 - INFO - __main__ - Step 120 Global step 120 Train loss 11.766138 on epoch=59
03/19/2022 19:05:56 - INFO - __main__ - Step 130 Global step 130 Train loss 10.514986 on epoch=64
03/19/2022 19:06:01 - INFO - __main__ - Step 140 Global step 140 Train loss 9.207296 on epoch=69
03/19/2022 19:06:06 - INFO - __main__ - Step 150 Global step 150 Train loss 6.424767 on epoch=74
03/19/2022 19:06:07 - INFO - __main__ - Global step 150 Train loss 10.124387 Classification-F1 0.09090909090909091 on epoch=74
03/19/2022 19:06:13 - INFO - __main__ - Step 160 Global step 160 Train loss 1.931681 on epoch=79
03/19/2022 19:06:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.922514 on epoch=84
03/19/2022 19:06:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.582804 on epoch=89
03/19/2022 19:06:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.321230 on epoch=94
03/19/2022 19:06:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.370943 on epoch=99
03/19/2022 19:06:34 - INFO - __main__ - Global step 200 Train loss 0.825834 Classification-F1 0.6761133603238867 on epoch=99
03/19/2022 19:06:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.328262 on epoch=104
03/19/2022 19:06:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.250382 on epoch=109
03/19/2022 19:06:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.200579 on epoch=114
03/19/2022 19:06:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.122957 on epoch=119
03/19/2022 19:07:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.121277 on epoch=124
03/19/2022 19:07:02 - INFO - __main__ - Global step 250 Train loss 0.204691 Classification-F1 0.6113360323886641 on epoch=124
03/19/2022 19:07:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.134257 on epoch=129
03/19/2022 19:07:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.133188 on epoch=134
03/19/2022 19:07:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.093586 on epoch=139
03/19/2022 19:07:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.065131 on epoch=144
03/19/2022 19:07:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.065687 on epoch=149
03/19/2022 19:07:28 - INFO - __main__ - Global step 300 Train loss 0.098370 Classification-F1 0.6559139784946237 on epoch=149
03/19/2022 19:07:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.034988 on epoch=154
03/19/2022 19:07:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.340771 on epoch=159
03/19/2022 19:07:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.044200 on epoch=164
03/19/2022 19:07:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.039983 on epoch=169
03/19/2022 19:07:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.044487 on epoch=174
03/19/2022 19:07:55 - INFO - __main__ - Global step 350 Train loss 0.100886 Classification-F1 0.6113360323886641 on epoch=174
03/19/2022 19:08:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.031365 on epoch=179
03/19/2022 19:08:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.031463 on epoch=184
03/19/2022 19:08:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008989 on epoch=189
03/19/2022 19:08:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.041279 on epoch=194
03/19/2022 19:08:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.040229 on epoch=199
03/19/2022 19:08:22 - INFO - __main__ - Global step 400 Train loss 0.030665 Classification-F1 0.6862745098039216 on epoch=199
03/19/2022 19:08:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.006848 on epoch=204
03/19/2022 19:08:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005180 on epoch=209
03/19/2022 19:08:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008805 on epoch=214
03/19/2022 19:08:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.031713 on epoch=219
03/19/2022 19:08:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008734 on epoch=224
03/19/2022 19:08:49 - INFO - __main__ - Global step 450 Train loss 0.012256 Classification-F1 0.6113360323886641 on epoch=224
03/19/2022 19:08:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.023444 on epoch=229
03/19/2022 19:09:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.014953 on epoch=234
03/19/2022 19:09:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002106 on epoch=239
03/19/2022 19:09:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009383 on epoch=244
03/19/2022 19:09:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.014791 on epoch=249
03/19/2022 19:09:16 - INFO - __main__ - Global step 500 Train loss 0.012935 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 19:09:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001264 on epoch=254
03/19/2022 19:09:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003722 on epoch=259
03/19/2022 19:09:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005739 on epoch=264
03/19/2022 19:09:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.013002 on epoch=269
03/19/2022 19:09:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.018139 on epoch=274
03/19/2022 19:09:43 - INFO - __main__ - Global step 550 Train loss 0.008373 Classification-F1 0.6666666666666667 on epoch=274
03/19/2022 19:09:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001783 on epoch=279
03/19/2022 19:09:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001347 on epoch=284
03/19/2022 19:09:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003102 on epoch=289
03/19/2022 19:10:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010754 on epoch=294
03/19/2022 19:10:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009614 on epoch=299
03/19/2022 19:10:10 - INFO - __main__ - Global step 600 Train loss 0.005320 Classification-F1 0.6666666666666667 on epoch=299
03/19/2022 19:10:10 - INFO - __main__ - save last model!
03/19/2022 19:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:10:11 - INFO - __main__ - Printing 3 examples
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:10:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:10:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:10:11 - INFO - __main__ - Printing 3 examples
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 19:10:11 - INFO - __main__ - ['false']
03/19/2022 19:10:11 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:10:11 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:10:11 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:10:17 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:10:18 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:10:18 - INFO - __main__ - Printing 3 examples
03/19/2022 19:10:18 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:10:18 - INFO - __main__ - ['true']
03/19/2022 19:10:18 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:10:18 - INFO - __main__ - ['false']
03/19/2022 19:10:18 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:10:18 - INFO - __main__ - ['false']
03/19/2022 19:10:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:10:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:10:18 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:10:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_21_0.0002_8_predictions.txt
03/19/2022 19:10:21 - INFO - __main__ - Classification-F1 on test data: 0.5930
03/19/2022 19:10:22 - INFO - __main__ - prefix=ethos-race_16_21, lr=0.0002, bsz=8, dev_performance=0.7117117117117117, test_performance=0.5929824561403509
03/19/2022 19:10:22 - INFO - __main__ - Running ... prefix=ethos-race_16_21, lr=0.0001, bsz=8 ...
03/19/2022 19:10:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:10:22 - INFO - __main__ - Starting training!
03/19/2022 19:10:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:10:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:10:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:10:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:10:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:10:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] He is the weirdest autistic Ive seen in my life
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] You look magically retarded.
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ -  [ethos-race] What about a global massacre of men? Pussiezzz only girls
03/19/2022 19:10:23 - INFO - __main__ - ['false']
03/19/2022 19:10:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:10:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:10:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:10:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:10:33 - INFO - __main__ - Starting training!
03/19/2022 19:10:38 - INFO - __main__ - Step 10 Global step 10 Train loss 23.550671 on epoch=4
03/19/2022 19:10:43 - INFO - __main__ - Step 20 Global step 20 Train loss 21.089508 on epoch=9
03/19/2022 19:10:48 - INFO - __main__ - Step 30 Global step 30 Train loss 19.826511 on epoch=14
03/19/2022 19:10:53 - INFO - __main__ - Step 40 Global step 40 Train loss 18.306431 on epoch=19
03/19/2022 19:10:58 - INFO - __main__ - Step 50 Global step 50 Train loss 18.769535 on epoch=24
03/19/2022 19:11:08 - INFO - __main__ - Global step 50 Train loss 20.308531 Classification-F1 0.0 on epoch=24
03/19/2022 19:11:14 - INFO - __main__ - Step 60 Global step 60 Train loss 16.434870 on epoch=29
03/19/2022 19:11:20 - INFO - __main__ - Step 70 Global step 70 Train loss 17.086979 on epoch=34
03/19/2022 19:11:25 - INFO - __main__ - Step 80 Global step 80 Train loss 17.241440 on epoch=39
03/19/2022 19:11:30 - INFO - __main__ - Step 90 Global step 90 Train loss 17.076702 on epoch=44
03/19/2022 19:11:35 - INFO - __main__ - Step 100 Global step 100 Train loss 15.635310 on epoch=49
03/19/2022 19:11:41 - INFO - __main__ - Global step 100 Train loss 16.695059 Classification-F1 0.0 on epoch=49
03/19/2022 19:11:46 - INFO - __main__ - Step 110 Global step 110 Train loss 15.487508 on epoch=54
03/19/2022 19:11:52 - INFO - __main__ - Step 120 Global step 120 Train loss 14.679932 on epoch=59
03/19/2022 19:11:57 - INFO - __main__ - Step 130 Global step 130 Train loss 15.049791 on epoch=64
03/19/2022 19:12:02 - INFO - __main__ - Step 140 Global step 140 Train loss 14.752722 on epoch=69
03/19/2022 19:12:07 - INFO - __main__ - Step 150 Global step 150 Train loss 14.233554 on epoch=74
03/19/2022 19:12:13 - INFO - __main__ - Global step 150 Train loss 14.840702 Classification-F1 0.0 on epoch=74
03/19/2022 19:12:18 - INFO - __main__ - Step 160 Global step 160 Train loss 13.129478 on epoch=79
03/19/2022 19:12:24 - INFO - __main__ - Step 170 Global step 170 Train loss 13.497103 on epoch=84
03/19/2022 19:12:29 - INFO - __main__ - Step 180 Global step 180 Train loss 12.464771 on epoch=89
03/19/2022 19:12:34 - INFO - __main__ - Step 190 Global step 190 Train loss 12.860471 on epoch=94
03/19/2022 19:12:39 - INFO - __main__ - Step 200 Global step 200 Train loss 12.449136 on epoch=99
03/19/2022 19:12:45 - INFO - __main__ - Global step 200 Train loss 12.880191 Classification-F1 0.0 on epoch=99
03/19/2022 19:12:50 - INFO - __main__ - Step 210 Global step 210 Train loss 11.378977 on epoch=104
03/19/2022 19:12:55 - INFO - __main__ - Step 220 Global step 220 Train loss 10.491857 on epoch=109
03/19/2022 19:13:01 - INFO - __main__ - Step 230 Global step 230 Train loss 9.893726 on epoch=114
03/19/2022 19:13:06 - INFO - __main__ - Step 240 Global step 240 Train loss 8.285572 on epoch=119
03/19/2022 19:13:11 - INFO - __main__ - Step 250 Global step 250 Train loss 7.139074 on epoch=124
03/19/2022 19:13:13 - INFO - __main__ - Global step 250 Train loss 9.437840 Classification-F1 0.09090909090909093 on epoch=124
03/19/2022 19:13:19 - INFO - __main__ - Step 260 Global step 260 Train loss 5.589122 on epoch=129
03/19/2022 19:13:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.856472 on epoch=134
03/19/2022 19:13:29 - INFO - __main__ - Step 280 Global step 280 Train loss 1.043939 on epoch=139
03/19/2022 19:13:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.873211 on epoch=144
03/19/2022 19:13:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.946455 on epoch=149
03/19/2022 19:13:40 - INFO - __main__ - Global step 300 Train loss 2.061840 Classification-F1 0.4458874458874459 on epoch=149
03/19/2022 19:13:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.584437 on epoch=154
03/19/2022 19:13:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.337666 on epoch=159
03/19/2022 19:13:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.598870 on epoch=164
03/19/2022 19:14:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.220980 on epoch=169
03/19/2022 19:14:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.256301 on epoch=174
03/19/2022 19:14:07 - INFO - __main__ - Global step 350 Train loss 0.399651 Classification-F1 0.6559139784946237 on epoch=174
03/19/2022 19:14:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.360316 on epoch=179
03/19/2022 19:14:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.224971 on epoch=184
03/19/2022 19:14:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.270172 on epoch=189
03/19/2022 19:14:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.133115 on epoch=194
03/19/2022 19:14:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.183479 on epoch=199
03/19/2022 19:14:34 - INFO - __main__ - Global step 400 Train loss 0.234411 Classification-F1 0.5933528836754642 on epoch=199
03/19/2022 19:14:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.177546 on epoch=204
03/19/2022 19:14:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.233375 on epoch=209
03/19/2022 19:14:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.092485 on epoch=214
03/19/2022 19:14:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.065302 on epoch=219
03/19/2022 19:15:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.098927 on epoch=224
03/19/2022 19:15:00 - INFO - __main__ - Global step 450 Train loss 0.133527 Classification-F1 0.6235294117647059 on epoch=224
03/19/2022 19:15:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.193410 on epoch=229
03/19/2022 19:15:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.052136 on epoch=234
03/19/2022 19:15:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.075663 on epoch=239
03/19/2022 19:15:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.046300 on epoch=244
03/19/2022 19:15:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.047112 on epoch=249
03/19/2022 19:15:27 - INFO - __main__ - Global step 500 Train loss 0.082924 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 19:15:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.086998 on epoch=254
03/19/2022 19:15:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.048541 on epoch=259
03/19/2022 19:15:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.020148 on epoch=264
03/19/2022 19:15:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.030446 on epoch=269
03/19/2022 19:15:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.027836 on epoch=274
03/19/2022 19:15:54 - INFO - __main__ - Global step 550 Train loss 0.042794 Classification-F1 0.7490196078431373 on epoch=274
03/19/2022 19:16:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.034076 on epoch=279
03/19/2022 19:16:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.091106 on epoch=284
03/19/2022 19:16:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012223 on epoch=289
03/19/2022 19:16:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.038268 on epoch=294
03/19/2022 19:16:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009050 on epoch=299
03/19/2022 19:16:22 - INFO - __main__ - Global step 600 Train loss 0.036945 Classification-F1 0.8125 on epoch=299
03/19/2022 19:16:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:16:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:16:23 - INFO - __main__ - save last model!
03/19/2022 19:16:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:16:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:16:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:16:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:16:23 - INFO - __main__ - ['false']
03/19/2022 19:16:23 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:16:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:16:23 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:16:29 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:16:30 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:16:30 - INFO - __main__ - Printing 3 examples
03/19/2022 19:16:30 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:16:30 - INFO - __main__ - ['true']
03/19/2022 19:16:30 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:16:30 - INFO - __main__ - ['false']
03/19/2022 19:16:30 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:16:30 - INFO - __main__ - ['false']
03/19/2022 19:16:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:16:30 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:16:30 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:16:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_21_0.0001_8_predictions.txt
03/19/2022 19:16:31 - INFO - __main__ - Classification-F1 on test data: 0.5613
03/19/2022 19:16:32 - INFO - __main__ - prefix=ethos-race_16_21, lr=0.0001, bsz=8, dev_performance=0.8125, test_performance=0.561344537815126
03/19/2022 19:16:32 - INFO - __main__ - Running ... prefix=ethos-race_16_42, lr=0.0005, bsz=8 ...
03/19/2022 19:16:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:16:33 - INFO - __main__ - Printing 3 examples
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:16:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:16:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:16:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:16:33 - INFO - __main__ - Printing 3 examples
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:16:33 - INFO - __main__ - ['false']
03/19/2022 19:16:33 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:16:33 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:16:33 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:16:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:16:34 - INFO - __main__ - Starting training!
03/19/2022 19:16:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:16:46 - INFO - __main__ - Starting training!
03/19/2022 19:16:52 - INFO - __main__ - Step 10 Global step 10 Train loss 25.011133 on epoch=4
03/19/2022 19:16:57 - INFO - __main__ - Step 20 Global step 20 Train loss 19.472166 on epoch=9
03/19/2022 19:17:02 - INFO - __main__ - Step 30 Global step 30 Train loss 16.432823 on epoch=14
03/19/2022 19:17:07 - INFO - __main__ - Step 40 Global step 40 Train loss 15.536139 on epoch=19
03/19/2022 19:17:12 - INFO - __main__ - Step 50 Global step 50 Train loss 13.787933 on epoch=24
03/19/2022 19:17:13 - INFO - __main__ - Global step 50 Train loss 18.048040 Classification-F1 0.0 on epoch=24
03/19/2022 19:17:19 - INFO - __main__ - Step 60 Global step 60 Train loss 11.943534 on epoch=29
03/19/2022 19:17:24 - INFO - __main__ - Step 70 Global step 70 Train loss 7.636947 on epoch=34
03/19/2022 19:17:29 - INFO - __main__ - Step 80 Global step 80 Train loss 3.867105 on epoch=39
03/19/2022 19:17:34 - INFO - __main__ - Step 90 Global step 90 Train loss 2.531537 on epoch=44
03/19/2022 19:17:39 - INFO - __main__ - Step 100 Global step 100 Train loss 3.206256 on epoch=49
03/19/2022 19:17:40 - INFO - __main__ - Global step 100 Train loss 5.837076 Classification-F1 0.3333333333333333 on epoch=49
03/19/2022 19:17:46 - INFO - __main__ - Step 110 Global step 110 Train loss 2.176039 on epoch=54
03/19/2022 19:17:51 - INFO - __main__ - Step 120 Global step 120 Train loss 2.182472 on epoch=59
03/19/2022 19:17:56 - INFO - __main__ - Step 130 Global step 130 Train loss 2.174291 on epoch=64
03/19/2022 19:18:01 - INFO - __main__ - Step 140 Global step 140 Train loss 1.641548 on epoch=69
03/19/2022 19:18:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.810495 on epoch=74
03/19/2022 19:18:07 - INFO - __main__ - Global step 150 Train loss 1.996969 Classification-F1 0.4385964912280702 on epoch=74
03/19/2022 19:18:13 - INFO - __main__ - Step 160 Global step 160 Train loss 1.793873 on epoch=79
03/19/2022 19:18:18 - INFO - __main__ - Step 170 Global step 170 Train loss 1.754579 on epoch=84
03/19/2022 19:18:23 - INFO - __main__ - Step 180 Global step 180 Train loss 1.605986 on epoch=89
03/19/2022 19:18:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.187244 on epoch=94
03/19/2022 19:18:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.805115 on epoch=99
03/19/2022 19:18:34 - INFO - __main__ - Global step 200 Train loss 1.629359 Classification-F1 0.6536796536796536 on epoch=99
03/19/2022 19:18:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.216150 on epoch=104
03/19/2022 19:18:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.354048 on epoch=109
03/19/2022 19:18:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.355540 on epoch=114
03/19/2022 19:18:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.237421 on epoch=119
03/19/2022 19:19:01 - INFO - __main__ - Step 250 Global step 250 Train loss 1.187247 on epoch=124
03/19/2022 19:19:01 - INFO - __main__ - Global step 250 Train loss 1.270081 Classification-F1 0.7046153846153846 on epoch=124
03/19/2022 19:19:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.048924 on epoch=129
03/19/2022 19:19:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.218939 on epoch=134
03/19/2022 19:19:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.692627 on epoch=139
03/19/2022 19:19:22 - INFO - __main__ - Step 290 Global step 290 Train loss 1.099364 on epoch=144
03/19/2022 19:19:27 - INFO - __main__ - Step 300 Global step 300 Train loss 1.014012 on epoch=149
03/19/2022 19:19:28 - INFO - __main__ - Global step 300 Train loss 1.214773 Classification-F1 0.5901477832512315 on epoch=149
03/19/2022 19:19:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.293364 on epoch=154
03/19/2022 19:19:38 - INFO - __main__ - Step 320 Global step 320 Train loss 1.083135 on epoch=159
03/19/2022 19:19:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.988204 on epoch=164
03/19/2022 19:19:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.902297 on epoch=169
03/19/2022 19:19:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.938775 on epoch=174
03/19/2022 19:19:54 - INFO - __main__ - Global step 350 Train loss 1.041155 Classification-F1 0.6536796536796536 on epoch=174
03/19/2022 19:19:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.553306 on epoch=179
03/19/2022 19:20:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.753998 on epoch=184
03/19/2022 19:20:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.899625 on epoch=189
03/19/2022 19:20:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.869046 on epoch=194
03/19/2022 19:20:20 - INFO - __main__ - Step 400 Global step 400 Train loss 2.318212 on epoch=199
03/19/2022 19:20:21 - INFO - __main__ - Global step 400 Train loss 1.078837 Classification-F1 0.5282882882882882 on epoch=199
03/19/2022 19:20:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.477049 on epoch=204
03/19/2022 19:20:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.203643 on epoch=209
03/19/2022 19:20:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110844 on epoch=214
03/19/2022 19:20:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.174514 on epoch=219
03/19/2022 19:20:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.094109 on epoch=224
03/19/2022 19:20:47 - INFO - __main__ - Global step 450 Train loss 0.212032 Classification-F1 0.873015873015873 on epoch=224
03/19/2022 19:20:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.140117 on epoch=229
03/19/2022 19:20:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.098704 on epoch=234
03/19/2022 19:21:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.046464 on epoch=239
03/19/2022 19:21:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.040799 on epoch=244
03/19/2022 19:21:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053231 on epoch=249
03/19/2022 19:21:14 - INFO - __main__ - Global step 500 Train loss 0.075863 Classification-F1 0.8435972629521017 on epoch=249
03/19/2022 19:21:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.048723 on epoch=254
03/19/2022 19:21:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.023969 on epoch=259
03/19/2022 19:21:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009568 on epoch=264
03/19/2022 19:21:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.015816 on epoch=269
03/19/2022 19:21:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.011816 on epoch=274
03/19/2022 19:21:40 - INFO - __main__ - Global step 550 Train loss 0.021978 Classification-F1 0.9372549019607843 on epoch=274
03/19/2022 19:21:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.010416 on epoch=279
03/19/2022 19:21:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024444 on epoch=284
03/19/2022 19:21:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017630 on epoch=289
03/19/2022 19:22:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.024819 on epoch=294
03/19/2022 19:22:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.014699 on epoch=299
03/19/2022 19:22:07 - INFO - __main__ - Global step 600 Train loss 0.018401 Classification-F1 0.8435972629521017 on epoch=299
03/19/2022 19:22:07 - INFO - __main__ - save last model!
03/19/2022 19:22:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:22:08 - INFO - __main__ - Printing 3 examples
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:22:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:22:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:22:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:22:08 - INFO - __main__ - Printing 3 examples
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:22:08 - INFO - __main__ - ['false']
03/19/2022 19:22:08 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:22:08 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:22:08 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:22:14 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:22:14 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:22:14 - INFO - __main__ - Printing 3 examples
03/19/2022 19:22:14 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:22:14 - INFO - __main__ - ['true']
03/19/2022 19:22:14 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:22:14 - INFO - __main__ - ['false']
03/19/2022 19:22:14 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:22:14 - INFO - __main__ - ['false']
03/19/2022 19:22:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:22:14 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:22:14 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:22:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_42_0.0005_8_predictions.txt
03/19/2022 19:22:16 - INFO - __main__ - Classification-F1 on test data: 0.7566
03/19/2022 19:22:18 - INFO - __main__ - prefix=ethos-race_16_42, lr=0.0005, bsz=8, dev_performance=0.9372549019607843, test_performance=0.7566433566433567
03/19/2022 19:22:18 - INFO - __main__ - Running ... prefix=ethos-race_16_42, lr=0.0003, bsz=8 ...
03/19/2022 19:22:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:22:18 - INFO - __main__ - Printing 3 examples
03/19/2022 19:22:18 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:22:18 - INFO - __main__ - ['false']
03/19/2022 19:22:18 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:22:18 - INFO - __main__ - ['false']
03/19/2022 19:22:18 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:22:18 - INFO - __main__ - ['false']
03/19/2022 19:22:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:22:18 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:22:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:22:19 - INFO - __main__ - Starting training!
03/19/2022 19:22:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:22:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:22:19 - INFO - __main__ - Printing 3 examples
03/19/2022 19:22:19 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:22:19 - INFO - __main__ - ['false']
03/19/2022 19:22:19 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:22:19 - INFO - __main__ - ['false']
03/19/2022 19:22:19 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:22:19 - INFO - __main__ - ['false']
03/19/2022 19:22:19 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:22:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:22:19 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:22:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:22:29 - INFO - __main__ - Starting training!
03/19/2022 19:22:34 - INFO - __main__ - Step 10 Global step 10 Train loss 24.022802 on epoch=4
03/19/2022 19:22:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.149433 on epoch=9
03/19/2022 19:22:44 - INFO - __main__ - Step 30 Global step 30 Train loss 17.776218 on epoch=14
03/19/2022 19:22:49 - INFO - __main__ - Step 40 Global step 40 Train loss 15.842360 on epoch=19
03/19/2022 19:22:55 - INFO - __main__ - Step 50 Global step 50 Train loss 14.729673 on epoch=24
03/19/2022 19:22:59 - INFO - __main__ - Global step 50 Train loss 18.104097 Classification-F1 0.0 on epoch=24
03/19/2022 19:23:05 - INFO - __main__ - Step 60 Global step 60 Train loss 13.367612 on epoch=29
03/19/2022 19:23:10 - INFO - __main__ - Step 70 Global step 70 Train loss 12.380962 on epoch=34
03/19/2022 19:23:15 - INFO - __main__ - Step 80 Global step 80 Train loss 9.751086 on epoch=39
03/19/2022 19:23:21 - INFO - __main__ - Step 90 Global step 90 Train loss 7.989831 on epoch=44
03/19/2022 19:23:26 - INFO - __main__ - Step 100 Global step 100 Train loss 3.264829 on epoch=49
03/19/2022 19:23:26 - INFO - __main__ - Global step 100 Train loss 9.350864 Classification-F1 0.22807017543859653 on epoch=49
03/19/2022 19:23:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.944569 on epoch=54
03/19/2022 19:23:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.249048 on epoch=59
03/19/2022 19:23:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.704118 on epoch=64
03/19/2022 19:23:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.815393 on epoch=69
03/19/2022 19:23:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.408335 on epoch=74
03/19/2022 19:23:53 - INFO - __main__ - Global step 150 Train loss 1.024293 Classification-F1 0.6945917285259808 on epoch=74
03/19/2022 19:24:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.412723 on epoch=79
03/19/2022 19:24:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.189701 on epoch=84
03/19/2022 19:24:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.223716 on epoch=89
03/19/2022 19:24:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.235224 on epoch=94
03/19/2022 19:24:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.136039 on epoch=99
03/19/2022 19:24:21 - INFO - __main__ - Global step 200 Train loss 0.239480 Classification-F1 0.7184750733137829 on epoch=99
03/19/2022 19:24:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.110924 on epoch=104
03/19/2022 19:24:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.152045 on epoch=109
03/19/2022 19:24:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.048244 on epoch=114
03/19/2022 19:24:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.106107 on epoch=119
03/19/2022 19:24:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.193266 on epoch=124
03/19/2022 19:24:48 - INFO - __main__ - Global step 250 Train loss 0.122117 Classification-F1 0.8745098039215686 on epoch=124
03/19/2022 19:24:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.030009 on epoch=129
03/19/2022 19:25:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.068982 on epoch=134
03/19/2022 19:25:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.199287 on epoch=139
03/19/2022 19:25:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.273014 on epoch=144
03/19/2022 19:25:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.164410 on epoch=149
03/19/2022 19:25:16 - INFO - __main__ - Global step 300 Train loss 0.147140 Classification-F1 0.6945917285259808 on epoch=149
03/19/2022 19:25:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.097311 on epoch=154
03/19/2022 19:25:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.049325 on epoch=159
03/19/2022 19:25:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.040323 on epoch=164
03/19/2022 19:25:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.160085 on epoch=169
03/19/2022 19:25:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.186666 on epoch=174
03/19/2022 19:25:42 - INFO - __main__ - Global step 350 Train loss 0.106742 Classification-F1 0.906158357771261 on epoch=174
03/19/2022 19:25:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.202846 on epoch=179
03/19/2022 19:25:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.096902 on epoch=184
03/19/2022 19:25:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.192083 on epoch=189
03/19/2022 19:26:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.074913 on epoch=194
03/19/2022 19:26:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.243348 on epoch=199
03/19/2022 19:26:10 - INFO - __main__ - Global step 400 Train loss 0.162018 Classification-F1 0.6536796536796536 on epoch=199
03/19/2022 19:26:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.152764 on epoch=204
03/19/2022 19:26:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.139246 on epoch=209
03/19/2022 19:26:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.074886 on epoch=214
03/19/2022 19:26:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.041703 on epoch=219
03/19/2022 19:26:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.044845 on epoch=224
03/19/2022 19:26:36 - INFO - __main__ - Global step 450 Train loss 0.090689 Classification-F1 0.8125 on epoch=224
03/19/2022 19:26:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.021369 on epoch=229
03/19/2022 19:26:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.052174 on epoch=234
03/19/2022 19:26:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.032367 on epoch=239
03/19/2022 19:26:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.025046 on epoch=244
03/19/2022 19:27:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.010209 on epoch=249
03/19/2022 19:27:03 - INFO - __main__ - Global step 500 Train loss 0.028233 Classification-F1 0.7810361681329424 on epoch=249
03/19/2022 19:27:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027267 on epoch=254
03/19/2022 19:27:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.026990 on epoch=259
03/19/2022 19:27:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016748 on epoch=264
03/19/2022 19:27:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002322 on epoch=269
03/19/2022 19:27:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005869 on epoch=274
03/19/2022 19:27:29 - INFO - __main__ - Global step 550 Train loss 0.015839 Classification-F1 0.8125 on epoch=274
03/19/2022 19:27:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.050786 on epoch=279
03/19/2022 19:27:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004591 on epoch=284
03/19/2022 19:27:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005866 on epoch=289
03/19/2022 19:27:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001810 on epoch=294
03/19/2022 19:27:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.014912 on epoch=299
03/19/2022 19:27:56 - INFO - __main__ - Global step 600 Train loss 0.015593 Classification-F1 0.7793103448275862 on epoch=299
03/19/2022 19:27:56 - INFO - __main__ - save last model!
03/19/2022 19:27:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:27:56 - INFO - __main__ - Printing 3 examples
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:27:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:27:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:27:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:27:56 - INFO - __main__ - Printing 3 examples
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:27:56 - INFO - __main__ - ['false']
03/19/2022 19:27:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:27:57 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:27:57 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:28:02 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:28:03 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:28:03 - INFO - __main__ - Printing 3 examples
03/19/2022 19:28:03 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:28:03 - INFO - __main__ - ['true']
03/19/2022 19:28:03 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:28:03 - INFO - __main__ - ['false']
03/19/2022 19:28:03 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:28:03 - INFO - __main__ - ['false']
03/19/2022 19:28:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:28:03 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:28:03 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:28:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_42_0.0003_8_predictions.txt
03/19/2022 19:28:05 - INFO - __main__ - Classification-F1 on test data: 0.5991
03/19/2022 19:28:05 - INFO - __main__ - prefix=ethos-race_16_42, lr=0.0003, bsz=8, dev_performance=0.906158357771261, test_performance=0.5990783410138248
03/19/2022 19:28:05 - INFO - __main__ - Running ... prefix=ethos-race_16_42, lr=0.0002, bsz=8 ...
03/19/2022 19:28:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:28:06 - INFO - __main__ - Printing 3 examples
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:28:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:28:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:28:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:28:06 - INFO - __main__ - Printing 3 examples
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:28:06 - INFO - __main__ - ['false']
03/19/2022 19:28:06 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:28:06 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:28:06 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:28:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:28:09 - INFO - __main__ - Starting training!
03/19/2022 19:28:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:28:16 - INFO - __main__ - Starting training!
03/19/2022 19:28:23 - INFO - __main__ - Step 10 Global step 10 Train loss 24.421988 on epoch=4
03/19/2022 19:28:28 - INFO - __main__ - Step 20 Global step 20 Train loss 20.994553 on epoch=9
03/19/2022 19:28:33 - INFO - __main__ - Step 30 Global step 30 Train loss 18.554710 on epoch=14
03/19/2022 19:28:39 - INFO - __main__ - Step 40 Global step 40 Train loss 16.597414 on epoch=19
03/19/2022 19:28:44 - INFO - __main__ - Step 50 Global step 50 Train loss 16.423788 on epoch=24
03/19/2022 19:28:53 - INFO - __main__ - Global step 50 Train loss 19.398493 Classification-F1 0.0 on epoch=24
03/19/2022 19:29:00 - INFO - __main__ - Step 60 Global step 60 Train loss 15.566991 on epoch=29
03/19/2022 19:29:05 - INFO - __main__ - Step 70 Global step 70 Train loss 14.931921 on epoch=34
03/19/2022 19:29:10 - INFO - __main__ - Step 80 Global step 80 Train loss 14.232660 on epoch=39
03/19/2022 19:29:15 - INFO - __main__ - Step 90 Global step 90 Train loss 12.907309 on epoch=44
03/19/2022 19:29:20 - INFO - __main__ - Step 100 Global step 100 Train loss 11.917889 on epoch=49
03/19/2022 19:29:29 - INFO - __main__ - Global step 100 Train loss 13.911354 Classification-F1 0.0 on epoch=49
03/19/2022 19:29:34 - INFO - __main__ - Step 110 Global step 110 Train loss 11.260965 on epoch=54
03/19/2022 19:29:40 - INFO - __main__ - Step 120 Global step 120 Train loss 8.821528 on epoch=59
03/19/2022 19:29:45 - INFO - __main__ - Step 130 Global step 130 Train loss 4.848628 on epoch=64
03/19/2022 19:29:50 - INFO - __main__ - Step 140 Global step 140 Train loss 1.145362 on epoch=69
03/19/2022 19:29:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.642380 on epoch=74
03/19/2022 19:29:55 - INFO - __main__ - Global step 150 Train loss 5.343772 Classification-F1 0.6761133603238867 on epoch=74
03/19/2022 19:30:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.479678 on epoch=79
03/19/2022 19:30:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.312119 on epoch=84
03/19/2022 19:30:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.233843 on epoch=89
03/19/2022 19:30:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.212408 on epoch=94
03/19/2022 19:30:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.182405 on epoch=99
03/19/2022 19:30:23 - INFO - __main__ - Global step 200 Train loss 0.284091 Classification-F1 0.7810361681329424 on epoch=99
03/19/2022 19:30:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.155883 on epoch=104
03/19/2022 19:30:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.094951 on epoch=109
03/19/2022 19:30:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.126302 on epoch=114
03/19/2022 19:30:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.081674 on epoch=119
03/19/2022 19:30:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.042581 on epoch=124
03/19/2022 19:30:51 - INFO - __main__ - Global step 250 Train loss 0.100278 Classification-F1 0.7757757757757757 on epoch=124
03/19/2022 19:30:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.091223 on epoch=129
03/19/2022 19:31:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.037917 on epoch=134
03/19/2022 19:31:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.016934 on epoch=139
03/19/2022 19:31:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.026664 on epoch=144
03/19/2022 19:31:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.029928 on epoch=149
03/19/2022 19:31:17 - INFO - __main__ - Global step 300 Train loss 0.040533 Classification-F1 0.7757757757757757 on epoch=149
03/19/2022 19:31:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.019613 on epoch=154
03/19/2022 19:31:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.017915 on epoch=159
03/19/2022 19:31:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.024502 on epoch=164
03/19/2022 19:31:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.023947 on epoch=169
03/19/2022 19:31:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020544 on epoch=174
03/19/2022 19:31:44 - INFO - __main__ - Global step 350 Train loss 0.021304 Classification-F1 0.7793103448275862 on epoch=174
03/19/2022 19:31:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.007159 on epoch=179
03/19/2022 19:31:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.020591 on epoch=184
03/19/2022 19:32:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008490 on epoch=189
03/19/2022 19:32:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000789 on epoch=194
03/19/2022 19:32:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.143587 on epoch=199
03/19/2022 19:32:11 - INFO - __main__ - Global step 400 Train loss 0.036123 Classification-F1 0.8117647058823529 on epoch=199
03/19/2022 19:32:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000821 on epoch=204
03/19/2022 19:32:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.008041 on epoch=209
03/19/2022 19:32:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001977 on epoch=214
03/19/2022 19:32:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000723 on epoch=219
03/19/2022 19:32:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001707 on epoch=224
03/19/2022 19:32:38 - INFO - __main__ - Global step 450 Train loss 0.002654 Classification-F1 0.875 on epoch=224
03/19/2022 19:32:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000567 on epoch=229
03/19/2022 19:32:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.011872 on epoch=234
03/19/2022 19:32:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.016503 on epoch=239
03/19/2022 19:33:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001812 on epoch=244
03/19/2022 19:33:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000257 on epoch=249
03/19/2022 19:33:06 - INFO - __main__ - Global step 500 Train loss 0.006202 Classification-F1 0.8117647058823529 on epoch=249
03/19/2022 19:33:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000319 on epoch=254
03/19/2022 19:33:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000602 on epoch=259
03/19/2022 19:33:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000797 on epoch=264
03/19/2022 19:33:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000207 on epoch=269
03/19/2022 19:33:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000364 on epoch=274
03/19/2022 19:33:32 - INFO - __main__ - Global step 550 Train loss 0.000458 Classification-F1 0.8435972629521017 on epoch=274
03/19/2022 19:33:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000096 on epoch=279
03/19/2022 19:33:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000091 on epoch=284
03/19/2022 19:33:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002660 on epoch=289
03/19/2022 19:33:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000430 on epoch=294
03/19/2022 19:33:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000347 on epoch=299
03/19/2022 19:33:59 - INFO - __main__ - Global step 600 Train loss 0.000725 Classification-F1 0.8435972629521017 on epoch=299
03/19/2022 19:33:59 - INFO - __main__ - save last model!
03/19/2022 19:34:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:34:00 - INFO - __main__ - Printing 3 examples
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:34:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:34:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:34:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:34:00 - INFO - __main__ - Printing 3 examples
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:34:00 - INFO - __main__ - ['false']
03/19/2022 19:34:00 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:34:00 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:34:00 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:34:06 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:34:07 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:34:07 - INFO - __main__ - Printing 3 examples
03/19/2022 19:34:07 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:34:07 - INFO - __main__ - ['true']
03/19/2022 19:34:07 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:34:07 - INFO - __main__ - ['false']
03/19/2022 19:34:07 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:34:07 - INFO - __main__ - ['false']
03/19/2022 19:34:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:34:07 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:34:07 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:34:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_42_0.0002_8_predictions.txt
03/19/2022 19:34:08 - INFO - __main__ - Classification-F1 on test data: 0.6733
03/19/2022 19:34:09 - INFO - __main__ - prefix=ethos-race_16_42, lr=0.0002, bsz=8, dev_performance=0.875, test_performance=0.6733061224489796
03/19/2022 19:34:09 - INFO - __main__ - Running ... prefix=ethos-race_16_42, lr=0.0001, bsz=8 ...
03/19/2022 19:34:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:34:09 - INFO - __main__ - Printing 3 examples
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:34:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:34:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:34:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:34:09 - INFO - __main__ - Printing 3 examples
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] Really, I hope she get raped in the future. Raping women should be legal I think
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ -  [ethos-race] Christians are all demonic
03/19/2022 19:34:09 - INFO - __main__ - ['false']
03/19/2022 19:34:09 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:34:09 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:34:10 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:34:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:34:11 - INFO - __main__ - Starting training!
03/19/2022 19:34:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:34:20 - INFO - __main__ - Starting training!
03/19/2022 19:34:25 - INFO - __main__ - Step 10 Global step 10 Train loss 24.821167 on epoch=4
03/19/2022 19:34:30 - INFO - __main__ - Step 20 Global step 20 Train loss 20.573069 on epoch=9
03/19/2022 19:34:35 - INFO - __main__ - Step 30 Global step 30 Train loss 18.575708 on epoch=14
03/19/2022 19:34:40 - INFO - __main__ - Step 40 Global step 40 Train loss 17.771227 on epoch=19
03/19/2022 19:34:45 - INFO - __main__ - Step 50 Global step 50 Train loss 17.084232 on epoch=24
03/19/2022 19:34:56 - INFO - __main__ - Global step 50 Train loss 19.765079 Classification-F1 0.0 on epoch=24
03/19/2022 19:35:02 - INFO - __main__ - Step 60 Global step 60 Train loss 16.959873 on epoch=29
03/19/2022 19:35:07 - INFO - __main__ - Step 70 Global step 70 Train loss 16.966045 on epoch=34
03/19/2022 19:35:12 - INFO - __main__ - Step 80 Global step 80 Train loss 16.275253 on epoch=39
03/19/2022 19:35:17 - INFO - __main__ - Step 90 Global step 90 Train loss 15.428284 on epoch=44
03/19/2022 19:35:22 - INFO - __main__ - Step 100 Global step 100 Train loss 15.700769 on epoch=49
03/19/2022 19:35:32 - INFO - __main__ - Global step 100 Train loss 16.266045 Classification-F1 0.0 on epoch=49
03/19/2022 19:35:37 - INFO - __main__ - Step 110 Global step 110 Train loss 15.061322 on epoch=54
03/19/2022 19:35:42 - INFO - __main__ - Step 120 Global step 120 Train loss 14.515951 on epoch=59
03/19/2022 19:35:47 - INFO - __main__ - Step 130 Global step 130 Train loss 14.329390 on epoch=64
03/19/2022 19:35:52 - INFO - __main__ - Step 140 Global step 140 Train loss 14.119113 on epoch=69
03/19/2022 19:35:57 - INFO - __main__ - Step 150 Global step 150 Train loss 13.126462 on epoch=74
03/19/2022 19:36:07 - INFO - __main__ - Global step 150 Train loss 14.230450 Classification-F1 0.0 on epoch=74
03/19/2022 19:36:12 - INFO - __main__ - Step 160 Global step 160 Train loss 13.736697 on epoch=79
03/19/2022 19:36:17 - INFO - __main__ - Step 170 Global step 170 Train loss 12.984716 on epoch=84
03/19/2022 19:36:22 - INFO - __main__ - Step 180 Global step 180 Train loss 11.836516 on epoch=89
03/19/2022 19:36:27 - INFO - __main__ - Step 190 Global step 190 Train loss 12.363886 on epoch=94
03/19/2022 19:36:33 - INFO - __main__ - Step 200 Global step 200 Train loss 11.147825 on epoch=99
03/19/2022 19:36:40 - INFO - __main__ - Global step 200 Train loss 12.413928 Classification-F1 0.0 on epoch=99
03/19/2022 19:36:45 - INFO - __main__ - Step 210 Global step 210 Train loss 10.070355 on epoch=104
03/19/2022 19:36:50 - INFO - __main__ - Step 220 Global step 220 Train loss 8.946134 on epoch=109
03/19/2022 19:36:56 - INFO - __main__ - Step 230 Global step 230 Train loss 8.111758 on epoch=114
03/19/2022 19:37:01 - INFO - __main__ - Step 240 Global step 240 Train loss 5.868124 on epoch=119
03/19/2022 19:37:06 - INFO - __main__ - Step 250 Global step 250 Train loss 2.313873 on epoch=124
03/19/2022 19:37:06 - INFO - __main__ - Global step 250 Train loss 7.062048 Classification-F1 0.3595430107526882 on epoch=124
03/19/2022 19:37:12 - INFO - __main__ - Step 260 Global step 260 Train loss 1.674605 on epoch=129
03/19/2022 19:37:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.590744 on epoch=134
03/19/2022 19:37:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.397226 on epoch=139
03/19/2022 19:37:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.857099 on epoch=144
03/19/2022 19:37:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.525553 on epoch=149
03/19/2022 19:37:33 - INFO - __main__ - Global step 300 Train loss 1.209046 Classification-F1 0.5333333333333333 on epoch=149
03/19/2022 19:37:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.398823 on epoch=154
03/19/2022 19:37:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.467611 on epoch=159
03/19/2022 19:37:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.554967 on epoch=164
03/19/2022 19:37:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.365698 on epoch=169
03/19/2022 19:38:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.313382 on epoch=174
03/19/2022 19:38:00 - INFO - __main__ - Global step 350 Train loss 0.420096 Classification-F1 0.6113360323886641 on epoch=174
03/19/2022 19:38:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.315706 on epoch=179
03/19/2022 19:38:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.197347 on epoch=184
03/19/2022 19:38:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.283066 on epoch=189
03/19/2022 19:38:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.620482 on epoch=194
03/19/2022 19:38:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.232409 on epoch=199
03/19/2022 19:38:27 - INFO - __main__ - Global step 400 Train loss 0.329802 Classification-F1 0.8095238095238095 on epoch=199
03/19/2022 19:38:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.210555 on epoch=204
03/19/2022 19:38:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.132517 on epoch=209
03/19/2022 19:38:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.177233 on epoch=214
03/19/2022 19:38:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.166200 on epoch=219
03/19/2022 19:38:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.125726 on epoch=224
03/19/2022 19:38:54 - INFO - __main__ - Global step 450 Train loss 0.162446 Classification-F1 0.8435972629521017 on epoch=224
03/19/2022 19:39:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.207182 on epoch=229
03/19/2022 19:39:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.173561 on epoch=234
03/19/2022 19:39:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.155552 on epoch=239
03/19/2022 19:39:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.233408 on epoch=244
03/19/2022 19:39:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.098887 on epoch=249
03/19/2022 19:39:21 - INFO - __main__ - Global step 500 Train loss 0.173718 Classification-F1 0.8745098039215686 on epoch=249
03/19/2022 19:39:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.059198 on epoch=254
03/19/2022 19:39:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.085072 on epoch=259
03/19/2022 19:39:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.053125 on epoch=264
03/19/2022 19:39:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.045072 on epoch=269
03/19/2022 19:39:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.131235 on epoch=274
03/19/2022 19:39:48 - INFO - __main__ - Global step 550 Train loss 0.074740 Classification-F1 0.906158357771261 on epoch=274
03/19/2022 19:39:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.078274 on epoch=279
03/19/2022 19:40:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.197784 on epoch=284
03/19/2022 19:40:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.076128 on epoch=289
03/19/2022 19:40:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.036089 on epoch=294
03/19/2022 19:40:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.054603 on epoch=299
03/19/2022 19:40:15 - INFO - __main__ - Global step 600 Train loss 0.088576 Classification-F1 0.906158357771261 on epoch=299
03/19/2022 19:40:15 - INFO - __main__ - save last model!
03/19/2022 19:40:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:40:16 - INFO - __main__ - Printing 3 examples
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:40:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:40:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:40:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:40:16 - INFO - __main__ - Printing 3 examples
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:40:16 - INFO - __main__ - ['false']
03/19/2022 19:40:16 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:40:16 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:40:16 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:40:22 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:40:23 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:40:23 - INFO - __main__ - Printing 3 examples
03/19/2022 19:40:23 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:40:23 - INFO - __main__ - ['true']
03/19/2022 19:40:23 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:40:23 - INFO - __main__ - ['false']
03/19/2022 19:40:23 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:40:23 - INFO - __main__ - ['false']
03/19/2022 19:40:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:40:23 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:40:23 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:40:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_42_0.0001_8_predictions.txt
03/19/2022 19:40:25 - INFO - __main__ - Classification-F1 on test data: 0.7643
03/19/2022 19:40:25 - INFO - __main__ - prefix=ethos-race_16_42, lr=0.0001, bsz=8, dev_performance=0.906158357771261, test_performance=0.7643188854489165
03/19/2022 19:40:25 - INFO - __main__ - Running ... prefix=ethos-race_16_87, lr=0.0005, bsz=8 ...
03/19/2022 19:40:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:40:26 - INFO - __main__ - Printing 3 examples
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:40:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:40:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:40:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:40:26 - INFO - __main__ - Printing 3 examples
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:40:26 - INFO - __main__ - ['false']
03/19/2022 19:40:26 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:40:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:40:26 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:40:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:40:29 - INFO - __main__ - Starting training!
03/19/2022 19:40:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:40:38 - INFO - __main__ - Starting training!
03/19/2022 19:40:42 - INFO - __main__ - Step 10 Global step 10 Train loss 22.682005 on epoch=4
03/19/2022 19:40:47 - INFO - __main__ - Step 20 Global step 20 Train loss 18.103216 on epoch=9
03/19/2022 19:40:52 - INFO - __main__ - Step 30 Global step 30 Train loss 16.243393 on epoch=14
03/19/2022 19:40:57 - INFO - __main__ - Step 40 Global step 40 Train loss 13.830904 on epoch=19
03/19/2022 19:41:02 - INFO - __main__ - Step 50 Global step 50 Train loss 11.281730 on epoch=24
03/19/2022 19:41:03 - INFO - __main__ - Global step 50 Train loss 16.428247 Classification-F1 0.0 on epoch=24
03/19/2022 19:41:09 - INFO - __main__ - Step 60 Global step 60 Train loss 7.910726 on epoch=29
03/19/2022 19:41:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.774251 on epoch=34
03/19/2022 19:41:19 - INFO - __main__ - Step 80 Global step 80 Train loss 1.066901 on epoch=39
03/19/2022 19:41:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.476190 on epoch=44
03/19/2022 19:41:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.346801 on epoch=49
03/19/2022 19:41:29 - INFO - __main__ - Global step 100 Train loss 2.514974 Classification-F1 0.5195195195195195 on epoch=49
03/19/2022 19:41:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.234537 on epoch=54
03/19/2022 19:41:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.148974 on epoch=59
03/19/2022 19:41:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.110303 on epoch=64
03/19/2022 19:41:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.168827 on epoch=69
03/19/2022 19:41:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.133340 on epoch=74
03/19/2022 19:41:56 - INFO - __main__ - Global step 150 Train loss 0.159196 Classification-F1 0.6875 on epoch=74
03/19/2022 19:42:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.113553 on epoch=79
03/19/2022 19:42:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.088254 on epoch=84
03/19/2022 19:42:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.261227 on epoch=89
03/19/2022 19:42:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.216619 on epoch=94
03/19/2022 19:42:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.089044 on epoch=99
03/19/2022 19:42:22 - INFO - __main__ - Global step 200 Train loss 0.153739 Classification-F1 0.5307917888563051 on epoch=99
03/19/2022 19:42:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.125788 on epoch=104
03/19/2022 19:42:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.025098 on epoch=109
03/19/2022 19:42:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.014355 on epoch=114
03/19/2022 19:42:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.088750 on epoch=119
03/19/2022 19:42:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.065493 on epoch=124
03/19/2022 19:42:47 - INFO - __main__ - Global step 250 Train loss 0.063897 Classification-F1 0.6476476476476476 on epoch=124
03/19/2022 19:42:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.037069 on epoch=129
03/19/2022 19:42:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.075552 on epoch=134
03/19/2022 19:43:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.028067 on epoch=139
03/19/2022 19:43:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.039941 on epoch=144
03/19/2022 19:43:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.029769 on epoch=149
03/19/2022 19:43:12 - INFO - __main__ - Global step 300 Train loss 0.042080 Classification-F1 0.5625 on epoch=149
03/19/2022 19:43:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003392 on epoch=154
03/19/2022 19:43:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002636 on epoch=159
03/19/2022 19:43:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.015295 on epoch=164
03/19/2022 19:43:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017126 on epoch=169
03/19/2022 19:43:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.014138 on epoch=174
03/19/2022 19:43:38 - INFO - __main__ - Global step 350 Train loss 0.010517 Classification-F1 0.6532019704433498 on epoch=174
03/19/2022 19:43:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.004859 on epoch=179
03/19/2022 19:43:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001522 on epoch=184
03/19/2022 19:43:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.017127 on epoch=189
03/19/2022 19:43:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.009149 on epoch=194
03/19/2022 19:44:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000743 on epoch=199
03/19/2022 19:44:03 - INFO - __main__ - Global step 400 Train loss 0.006680 Classification-F1 0.6235294117647059 on epoch=199
03/19/2022 19:44:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001608 on epoch=204
03/19/2022 19:44:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000266 on epoch=209
03/19/2022 19:44:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.010129 on epoch=214
03/19/2022 19:44:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005838 on epoch=219
03/19/2022 19:44:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.019099 on epoch=224
03/19/2022 19:44:28 - INFO - __main__ - Global step 450 Train loss 0.007388 Classification-F1 0.6532019704433498 on epoch=224
03/19/2022 19:44:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001072 on epoch=229
03/19/2022 19:44:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001492 on epoch=234
03/19/2022 19:44:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002548 on epoch=239
03/19/2022 19:44:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000281 on epoch=244
03/19/2022 19:44:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002230 on epoch=249
03/19/2022 19:44:53 - INFO - __main__ - Global step 500 Train loss 0.001524 Classification-F1 0.7046153846153846 on epoch=249
03/19/2022 19:45:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006012 on epoch=254
03/19/2022 19:45:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001237 on epoch=259
03/19/2022 19:45:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.066391 on epoch=264
03/19/2022 19:45:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000973 on epoch=269
03/19/2022 19:45:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.013091 on epoch=274
03/19/2022 19:45:20 - INFO - __main__ - Global step 550 Train loss 0.017541 Classification-F1 0.6235294117647059 on epoch=274
03/19/2022 19:45:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003162 on epoch=279
03/19/2022 19:45:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.027214 on epoch=284
03/19/2022 19:45:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001522 on epoch=289
03/19/2022 19:45:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000242 on epoch=294
03/19/2022 19:45:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000348 on epoch=299
03/19/2022 19:45:46 - INFO - __main__ - Global step 600 Train loss 0.006497 Classification-F1 0.6862745098039216 on epoch=299
03/19/2022 19:45:46 - INFO - __main__ - save last model!
03/19/2022 19:45:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:45:46 - INFO - __main__ - Printing 3 examples
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:45:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:45:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:45:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:45:46 - INFO - __main__ - Printing 3 examples
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:45:46 - INFO - __main__ - ['false']
03/19/2022 19:45:46 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:45:46 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:45:46 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:45:52 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:45:53 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:45:53 - INFO - __main__ - Printing 3 examples
03/19/2022 19:45:53 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:45:53 - INFO - __main__ - ['true']
03/19/2022 19:45:53 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:45:53 - INFO - __main__ - ['false']
03/19/2022 19:45:53 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:45:53 - INFO - __main__ - ['false']
03/19/2022 19:45:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:45:53 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:45:53 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:45:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_87_0.0005_8_predictions.txt
03/19/2022 19:45:55 - INFO - __main__ - Classification-F1 on test data: 0.9019
03/19/2022 19:45:55 - INFO - __main__ - prefix=ethos-race_16_87, lr=0.0005, bsz=8, dev_performance=0.7046153846153846, test_performance=0.90187232122716
03/19/2022 19:45:55 - INFO - __main__ - Running ... prefix=ethos-race_16_87, lr=0.0003, bsz=8 ...
03/19/2022 19:45:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:45:56 - INFO - __main__ - Printing 3 examples
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:45:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:45:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:45:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:45:56 - INFO - __main__ - Printing 3 examples
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:45:56 - INFO - __main__ - ['false']
03/19/2022 19:45:56 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:45:56 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:45:56 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:45:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:45:59 - INFO - __main__ - Starting training!
03/19/2022 19:46:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:46:06 - INFO - __main__ - Starting training!
03/19/2022 19:46:11 - INFO - __main__ - Step 10 Global step 10 Train loss 24.604698 on epoch=4
03/19/2022 19:46:16 - INFO - __main__ - Step 20 Global step 20 Train loss 18.929754 on epoch=9
03/19/2022 19:46:21 - INFO - __main__ - Step 30 Global step 30 Train loss 16.475729 on epoch=14
03/19/2022 19:46:26 - INFO - __main__ - Step 40 Global step 40 Train loss 15.575315 on epoch=19
03/19/2022 19:46:31 - INFO - __main__ - Step 50 Global step 50 Train loss 14.631518 on epoch=24
03/19/2022 19:46:32 - INFO - __main__ - Global step 50 Train loss 18.043402 Classification-F1 0.0 on epoch=24
03/19/2022 19:46:38 - INFO - __main__ - Step 60 Global step 60 Train loss 13.962008 on epoch=29
03/19/2022 19:46:43 - INFO - __main__ - Step 70 Global step 70 Train loss 12.579907 on epoch=34
03/19/2022 19:46:48 - INFO - __main__ - Step 80 Global step 80 Train loss 10.467970 on epoch=39
03/19/2022 19:46:53 - INFO - __main__ - Step 90 Global step 90 Train loss 5.100449 on epoch=44
03/19/2022 19:46:57 - INFO - __main__ - Step 100 Global step 100 Train loss 3.785321 on epoch=49
03/19/2022 19:46:58 - INFO - __main__ - Global step 100 Train loss 9.179131 Classification-F1 0.3992490613266583 on epoch=49
03/19/2022 19:47:04 - INFO - __main__ - Step 110 Global step 110 Train loss 2.156398 on epoch=54
03/19/2022 19:47:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.770543 on epoch=59
03/19/2022 19:47:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.604002 on epoch=64
03/19/2022 19:47:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.508344 on epoch=69
03/19/2022 19:47:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.519584 on epoch=74
03/19/2022 19:47:25 - INFO - __main__ - Global step 150 Train loss 0.911774 Classification-F1 0.4458874458874459 on epoch=74
03/19/2022 19:47:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.443307 on epoch=79
03/19/2022 19:47:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.382440 on epoch=84
03/19/2022 19:47:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.354482 on epoch=89
03/19/2022 19:47:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.253891 on epoch=94
03/19/2022 19:47:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.294954 on epoch=99
03/19/2022 19:47:51 - INFO - __main__ - Global step 200 Train loss 0.345815 Classification-F1 0.6190476190476191 on epoch=99
03/19/2022 19:47:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.269907 on epoch=104
03/19/2022 19:48:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.273754 on epoch=109
03/19/2022 19:48:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.243524 on epoch=114
03/19/2022 19:48:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.218121 on epoch=119
03/19/2022 19:48:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.226852 on epoch=124
03/19/2022 19:48:18 - INFO - __main__ - Global step 250 Train loss 0.246432 Classification-F1 0.6862745098039216 on epoch=124
03/19/2022 19:48:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.263601 on epoch=129
03/19/2022 19:48:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.162691 on epoch=134
03/19/2022 19:48:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.153173 on epoch=139
03/19/2022 19:48:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.192236 on epoch=144
03/19/2022 19:48:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.159416 on epoch=149
03/19/2022 19:48:45 - INFO - __main__ - Global step 300 Train loss 0.186223 Classification-F1 0.4909862142099682 on epoch=149
03/19/2022 19:48:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.132643 on epoch=154
03/19/2022 19:48:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.165900 on epoch=159
03/19/2022 19:48:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.159458 on epoch=164
03/19/2022 19:49:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.112177 on epoch=169
03/19/2022 19:49:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.150214 on epoch=174
03/19/2022 19:49:10 - INFO - __main__ - Global step 350 Train loss 0.144078 Classification-F1 0.6190476190476191 on epoch=174
03/19/2022 19:49:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.058173 on epoch=179
03/19/2022 19:49:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.073036 on epoch=184
03/19/2022 19:49:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054262 on epoch=189
03/19/2022 19:49:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.106485 on epoch=194
03/19/2022 19:49:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.063323 on epoch=199
03/19/2022 19:49:35 - INFO - __main__ - Global step 400 Train loss 0.071056 Classification-F1 0.6190476190476191 on epoch=199
03/19/2022 19:49:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.052193 on epoch=204
03/19/2022 19:49:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.050428 on epoch=209
03/19/2022 19:49:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046103 on epoch=214
03/19/2022 19:49:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.022549 on epoch=219
03/19/2022 19:50:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.016158 on epoch=224
03/19/2022 19:50:01 - INFO - __main__ - Global step 450 Train loss 0.037486 Classification-F1 0.6235294117647059 on epoch=224
03/19/2022 19:50:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.013732 on epoch=229
03/19/2022 19:50:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.020591 on epoch=234
03/19/2022 19:50:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008225 on epoch=239
03/19/2022 19:50:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026878 on epoch=244
03/19/2022 19:50:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011328 on epoch=249
03/19/2022 19:50:26 - INFO - __main__ - Global step 500 Train loss 0.016151 Classification-F1 0.6761133603238867 on epoch=249
03/19/2022 19:50:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.048193 on epoch=254
03/19/2022 19:50:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.013774 on epoch=259
03/19/2022 19:50:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003470 on epoch=264
03/19/2022 19:50:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.015367 on epoch=269
03/19/2022 19:50:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002107 on epoch=274
03/19/2022 19:50:52 - INFO - __main__ - Global step 550 Train loss 0.016582 Classification-F1 0.7117117117117117 on epoch=274
03/19/2022 19:50:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001727 on epoch=279
03/19/2022 19:51:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.012741 on epoch=284
03/19/2022 19:51:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007898 on epoch=289
03/19/2022 19:51:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002477 on epoch=294
03/19/2022 19:51:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002568 on epoch=299
03/19/2022 19:51:18 - INFO - __main__ - Global step 600 Train loss 0.005482 Classification-F1 0.6825396825396826 on epoch=299
03/19/2022 19:51:18 - INFO - __main__ - save last model!
03/19/2022 19:51:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:51:19 - INFO - __main__ - Printing 3 examples
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:51:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:51:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:51:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:51:19 - INFO - __main__ - Printing 3 examples
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:51:19 - INFO - __main__ - ['false']
03/19/2022 19:51:19 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:51:19 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:51:19 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:51:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:51:26 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:51:26 - INFO - __main__ - Printing 3 examples
03/19/2022 19:51:26 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:51:26 - INFO - __main__ - ['true']
03/19/2022 19:51:26 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:51:26 - INFO - __main__ - ['false']
03/19/2022 19:51:26 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:51:26 - INFO - __main__ - ['false']
03/19/2022 19:51:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:51:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:51:26 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:51:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_87_0.0003_8_predictions.txt
03/19/2022 19:51:27 - INFO - __main__ - Classification-F1 on test data: 0.7728
03/19/2022 19:51:28 - INFO - __main__ - prefix=ethos-race_16_87, lr=0.0003, bsz=8, dev_performance=0.7117117117117117, test_performance=0.7727611940298507
03/19/2022 19:51:28 - INFO - __main__ - Running ... prefix=ethos-race_16_87, lr=0.0002, bsz=8 ...
03/19/2022 19:51:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:51:29 - INFO - __main__ - Printing 3 examples
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:51:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:51:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:51:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:51:29 - INFO - __main__ - Printing 3 examples
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:51:29 - INFO - __main__ - ['false']
03/19/2022 19:51:29 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:51:29 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:51:29 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:51:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:51:32 - INFO - __main__ - Starting training!
03/19/2022 19:51:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:51:39 - INFO - __main__ - Starting training!
03/19/2022 19:51:43 - INFO - __main__ - Step 10 Global step 10 Train loss 23.927189 on epoch=4
03/19/2022 19:51:48 - INFO - __main__ - Step 20 Global step 20 Train loss 20.554277 on epoch=9
03/19/2022 19:51:53 - INFO - __main__ - Step 30 Global step 30 Train loss 18.265289 on epoch=14
03/19/2022 19:51:58 - INFO - __main__ - Step 40 Global step 40 Train loss 17.245411 on epoch=19
03/19/2022 19:52:03 - INFO - __main__ - Step 50 Global step 50 Train loss 16.091835 on epoch=24
03/19/2022 19:52:04 - INFO - __main__ - Global step 50 Train loss 19.216799 Classification-F1 0.0 on epoch=24
03/19/2022 19:52:10 - INFO - __main__ - Step 60 Global step 60 Train loss 15.799284 on epoch=29
03/19/2022 19:52:15 - INFO - __main__ - Step 70 Global step 70 Train loss 14.716701 on epoch=34
03/19/2022 19:52:20 - INFO - __main__ - Step 80 Global step 80 Train loss 14.464749 on epoch=39
03/19/2022 19:52:25 - INFO - __main__ - Step 90 Global step 90 Train loss 13.931908 on epoch=44
03/19/2022 19:52:30 - INFO - __main__ - Step 100 Global step 100 Train loss 12.976175 on epoch=49
03/19/2022 19:52:32 - INFO - __main__ - Global step 100 Train loss 14.377764 Classification-F1 0.0 on epoch=49
03/19/2022 19:52:37 - INFO - __main__ - Step 110 Global step 110 Train loss 11.953133 on epoch=54
03/19/2022 19:52:42 - INFO - __main__ - Step 120 Global step 120 Train loss 10.704648 on epoch=59
03/19/2022 19:52:47 - INFO - __main__ - Step 130 Global step 130 Train loss 8.865924 on epoch=64
03/19/2022 19:52:52 - INFO - __main__ - Step 140 Global step 140 Train loss 5.674221 on epoch=69
03/19/2022 19:52:57 - INFO - __main__ - Step 150 Global step 150 Train loss 3.744175 on epoch=74
03/19/2022 19:52:58 - INFO - __main__ - Global step 150 Train loss 8.188419 Classification-F1 0.19259259259259257 on epoch=74
03/19/2022 19:53:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.643288 on epoch=79
03/19/2022 19:53:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.416686 on epoch=84
03/19/2022 19:53:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.331704 on epoch=89
03/19/2022 19:53:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.227776 on epoch=94
03/19/2022 19:53:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.171270 on epoch=99
03/19/2022 19:53:25 - INFO - __main__ - Global step 200 Train loss 0.358145 Classification-F1 0.5933528836754642 on epoch=99
03/19/2022 19:53:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.175489 on epoch=104
03/19/2022 19:53:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.154843 on epoch=109
03/19/2022 19:53:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.101240 on epoch=114
03/19/2022 19:53:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.096740 on epoch=119
03/19/2022 19:53:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.086142 on epoch=124
03/19/2022 19:53:51 - INFO - __main__ - Global step 250 Train loss 0.122891 Classification-F1 0.7117117117117117 on epoch=124
03/19/2022 19:53:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.074728 on epoch=129
03/19/2022 19:54:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.056731 on epoch=134
03/19/2022 19:54:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.088134 on epoch=139
03/19/2022 19:54:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.072369 on epoch=144
03/19/2022 19:54:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.018195 on epoch=149
03/19/2022 19:54:18 - INFO - __main__ - Global step 300 Train loss 0.062031 Classification-F1 0.716256157635468 on epoch=149
03/19/2022 19:54:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.020681 on epoch=154
03/19/2022 19:54:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.025620 on epoch=159
03/19/2022 19:54:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.091194 on epoch=164
03/19/2022 19:54:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.007369 on epoch=169
03/19/2022 19:54:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.012855 on epoch=174
03/19/2022 19:54:45 - INFO - __main__ - Global step 350 Train loss 0.031544 Classification-F1 0.716256157635468 on epoch=174
03/19/2022 19:54:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.004479 on epoch=179
03/19/2022 19:54:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.018401 on epoch=184
03/19/2022 19:55:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008251 on epoch=189
03/19/2022 19:55:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.011483 on epoch=194
03/19/2022 19:55:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009483 on epoch=199
03/19/2022 19:55:10 - INFO - __main__ - Global step 400 Train loss 0.010419 Classification-F1 0.746031746031746 on epoch=199
03/19/2022 19:55:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010088 on epoch=204
03/19/2022 19:55:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000415 on epoch=209
03/19/2022 19:55:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004055 on epoch=214
03/19/2022 19:55:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005256 on epoch=219
03/19/2022 19:55:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001101 on epoch=224
03/19/2022 19:55:37 - INFO - __main__ - Global step 450 Train loss 0.004183 Classification-F1 0.716256157635468 on epoch=224
03/19/2022 19:55:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.011900 on epoch=229
03/19/2022 19:55:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001314 on epoch=234
03/19/2022 19:55:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.024648 on epoch=239
03/19/2022 19:55:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004298 on epoch=244
03/19/2022 19:56:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000416 on epoch=249
03/19/2022 19:56:02 - INFO - __main__ - Global step 500 Train loss 0.008515 Classification-F1 0.7117117117117117 on epoch=249
03/19/2022 19:56:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001236 on epoch=254
03/19/2022 19:56:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001948 on epoch=259
03/19/2022 19:56:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002494 on epoch=264
03/19/2022 19:56:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001334 on epoch=269
03/19/2022 19:56:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000992 on epoch=274
03/19/2022 19:56:28 - INFO - __main__ - Global step 550 Train loss 0.001601 Classification-F1 0.7117117117117117 on epoch=274
03/19/2022 19:56:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.033776 on epoch=279
03/19/2022 19:56:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004378 on epoch=284
03/19/2022 19:56:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000291 on epoch=289
03/19/2022 19:56:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032974 on epoch=294
03/19/2022 19:56:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000304 on epoch=299
03/19/2022 19:56:53 - INFO - __main__ - Global step 600 Train loss 0.014344 Classification-F1 0.7490196078431373 on epoch=299
03/19/2022 19:56:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:56:54 - INFO - __main__ - Printing 3 examples
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/19/2022 19:56:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:56:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:56:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:56:54 - INFO - __main__ - Printing 3 examples
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:56:54 - INFO - __main__ - ['false']
03/19/2022 19:56:54 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:56:54 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:56:54 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:56:54 - INFO - __main__ - save last model!
03/19/2022 19:57:01 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 19:57:02 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 19:57:02 - INFO - __main__ - Printing 3 examples
03/19/2022 19:57:02 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 19:57:02 - INFO - __main__ - ['true']
03/19/2022 19:57:02 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 19:57:02 - INFO - __main__ - ['false']
03/19/2022 19:57:02 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 19:57:02 - INFO - __main__ - ['false']
03/19/2022 19:57:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:57:02 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:57:02 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 19:57:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_87_0.0002_8_predictions.txt
03/19/2022 19:57:03 - INFO - __main__ - Classification-F1 on test data: 0.7411
03/19/2022 19:57:03 - INFO - __main__ - prefix=ethos-race_16_87, lr=0.0002, bsz=8, dev_performance=0.7490196078431373, test_performance=0.7410714285714286
03/19/2022 19:57:03 - INFO - __main__ - Running ... prefix=ethos-race_16_87, lr=0.0001, bsz=8 ...
03/19/2022 19:57:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:57:04 - INFO - __main__ - Printing 3 examples
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 19:57:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:57:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/19/2022 19:57:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/19/2022 19:57:04 - INFO - __main__ - Printing 3 examples
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] If your fucking with a cool female is all good but if your fucking with a bitch you need to break her ass and shake her ass
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ -  [ethos-race] Why are we not allowed to kill christians?
03/19/2022 19:57:04 - INFO - __main__ - ['false']
03/19/2022 19:57:04 - INFO - __main__ - Tokenizing Input ...
03/19/2022 19:57:04 - INFO - __main__ - Tokenizing Output ...
03/19/2022 19:57:04 - INFO - __main__ - Loaded 32 examples from dev data
03/19/2022 19:57:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:57:06 - INFO - __main__ - Starting training!
03/19/2022 19:57:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/19/2022 19:57:15 - INFO - __main__ - Starting training!
03/19/2022 19:57:20 - INFO - __main__ - Step 10 Global step 10 Train loss 23.963806 on epoch=4
03/19/2022 19:57:24 - INFO - __main__ - Step 20 Global step 20 Train loss 21.533613 on epoch=9
03/19/2022 19:57:29 - INFO - __main__ - Step 30 Global step 30 Train loss 18.413795 on epoch=14
03/19/2022 19:57:34 - INFO - __main__ - Step 40 Global step 40 Train loss 18.510906 on epoch=19
03/19/2022 19:57:39 - INFO - __main__ - Step 50 Global step 50 Train loss 17.563475 on epoch=24
03/19/2022 19:57:40 - INFO - __main__ - Global step 50 Train loss 19.997120 Classification-F1 0.0 on epoch=24
03/19/2022 19:57:46 - INFO - __main__ - Step 60 Global step 60 Train loss 17.633465 on epoch=29
03/19/2022 19:57:50 - INFO - __main__ - Step 70 Global step 70 Train loss 16.430622 on epoch=34
03/19/2022 19:57:55 - INFO - __main__ - Step 80 Global step 80 Train loss 16.847658 on epoch=39
03/19/2022 19:58:00 - INFO - __main__ - Step 90 Global step 90 Train loss 15.886116 on epoch=44
03/19/2022 19:58:05 - INFO - __main__ - Step 100 Global step 100 Train loss 15.186111 on epoch=49
03/19/2022 19:58:06 - INFO - __main__ - Global step 100 Train loss 16.396795 Classification-F1 0.0 on epoch=49
03/19/2022 19:58:11 - INFO - __main__ - Step 110 Global step 110 Train loss 15.272257 on epoch=54
03/19/2022 19:58:15 - INFO - __main__ - Step 120 Global step 120 Train loss 14.622211 on epoch=59
03/19/2022 19:58:20 - INFO - __main__ - Step 130 Global step 130 Train loss 14.430262 on epoch=64
03/19/2022 19:58:25 - INFO - __main__ - Step 140 Global step 140 Train loss 13.633150 on epoch=69
03/19/2022 19:58:30 - INFO - __main__ - Step 150 Global step 150 Train loss 13.373179 on epoch=74
03/19/2022 19:58:31 - INFO - __main__ - Global step 150 Train loss 14.266212 Classification-F1 0.0 on epoch=74
03/19/2022 19:58:36 - INFO - __main__ - Step 160 Global step 160 Train loss 12.247252 on epoch=79
03/19/2022 19:58:40 - INFO - __main__ - Step 170 Global step 170 Train loss 12.376930 on epoch=84
03/19/2022 19:58:45 - INFO - __main__ - Step 180 Global step 180 Train loss 12.401276 on epoch=89
03/19/2022 19:58:50 - INFO - __main__ - Step 190 Global step 190 Train loss 12.011738 on epoch=94
03/19/2022 19:58:55 - INFO - __main__ - Step 200 Global step 200 Train loss 10.512115 on epoch=99
03/19/2022 19:58:56 - INFO - __main__ - Global step 200 Train loss 11.909863 Classification-F1 0.0 on epoch=99
03/19/2022 19:59:01 - INFO - __main__ - Step 210 Global step 210 Train loss 10.095789 on epoch=104
03/19/2022 19:59:05 - INFO - __main__ - Step 220 Global step 220 Train loss 7.348761 on epoch=109
03/19/2022 19:59:10 - INFO - __main__ - Step 230 Global step 230 Train loss 5.912667 on epoch=114
03/19/2022 19:59:15 - INFO - __main__ - Step 240 Global step 240 Train loss 3.394616 on epoch=119
03/19/2022 19:59:20 - INFO - __main__ - Step 250 Global step 250 Train loss 1.573542 on epoch=124
03/19/2022 19:59:21 - INFO - __main__ - Global step 250 Train loss 5.665074 Classification-F1 0.4181818181818182 on epoch=124
03/19/2022 19:59:26 - INFO - __main__ - Step 260 Global step 260 Train loss 1.133980 on epoch=129
03/19/2022 19:59:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.572722 on epoch=134
03/19/2022 19:59:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.480848 on epoch=139
03/19/2022 19:59:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.488519 on epoch=144
03/19/2022 19:59:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.451819 on epoch=149
03/19/2022 19:59:46 - INFO - __main__ - Global step 300 Train loss 0.625578 Classification-F1 0.4554554554554554 on epoch=149
03/19/2022 19:59:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.511984 on epoch=154
03/19/2022 19:59:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.393626 on epoch=159
03/19/2022 20:00:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.366709 on epoch=164
03/19/2022 20:00:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.342327 on epoch=169
03/19/2022 20:00:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.262124 on epoch=174
03/19/2022 20:00:12 - INFO - __main__ - Global step 350 Train loss 0.375354 Classification-F1 0.5733333333333335 on epoch=174
03/19/2022 20:00:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.269546 on epoch=179
03/19/2022 20:00:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.231640 on epoch=184
03/19/2022 20:00:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.192029 on epoch=189
03/19/2022 20:00:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.262338 on epoch=194
03/19/2022 20:00:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.158840 on epoch=199
03/19/2022 20:00:37 - INFO - __main__ - Global step 400 Train loss 0.222879 Classification-F1 0.5835835835835835 on epoch=199
03/19/2022 20:00:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.160814 on epoch=204
03/19/2022 20:00:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.122025 on epoch=209
03/19/2022 20:00:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.088078 on epoch=214
03/19/2022 20:00:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.071480 on epoch=219
03/19/2022 20:01:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.108037 on epoch=224
03/19/2022 20:01:03 - INFO - __main__ - Global step 450 Train loss 0.110087 Classification-F1 0.5933528836754642 on epoch=224
03/19/2022 20:01:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.142517 on epoch=229
03/19/2022 20:01:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.074627 on epoch=234
03/19/2022 20:01:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.077589 on epoch=239
03/19/2022 20:01:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.085315 on epoch=244
03/19/2022 20:01:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.110594 on epoch=249
03/19/2022 20:01:28 - INFO - __main__ - Global step 500 Train loss 0.098128 Classification-F1 0.6559139784946237 on epoch=249
03/19/2022 20:01:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.052986 on epoch=254
03/19/2022 20:01:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.079799 on epoch=259
03/19/2022 20:01:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.074936 on epoch=264
03/19/2022 20:01:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.080221 on epoch=269
03/19/2022 20:01:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.059386 on epoch=274
03/19/2022 20:01:53 - INFO - __main__ - Global step 550 Train loss 0.069466 Classification-F1 0.625 on epoch=274
03/19/2022 20:01:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.060418 on epoch=279
03/19/2022 20:02:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.087470 on epoch=284
03/19/2022 20:02:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.184353 on epoch=289
03/19/2022 20:02:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.060281 on epoch=294
03/19/2022 20:02:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.057556 on epoch=299
03/19/2022 20:02:18 - INFO - __main__ - Global step 600 Train loss 0.090016 Classification-F1 0.716256157635468 on epoch=299
03/19/2022 20:02:19 - INFO - __main__ - save last model!
03/19/2022 20:02:25 - INFO - __main__ - Loading checkpoint on the fly
03/19/2022 20:02:26 - INFO - __main__ - Start tokenizing ... 87 instances
03/19/2022 20:02:26 - INFO - __main__ - Printing 3 examples
03/19/2022 20:02:26 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/19/2022 20:02:26 - INFO - __main__ - ['true']
03/19/2022 20:02:26 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
03/19/2022 20:02:26 - INFO - __main__ - ['false']
03/19/2022 20:02:26 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
03/19/2022 20:02:26 - INFO - __main__ - ['false']
03/19/2022 20:02:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/19/2022 20:02:26 - INFO - __main__ - Tokenizing Output ...
03/19/2022 20:02:26 - INFO - __main__ - Loaded 87 examples from test data
03/19/2022 20:02:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls/singletask-ethos-race/ethos-race_16_87_0.0001_8_predictions.txt
03/19/2022 20:02:27 - INFO - __main__ - Classification-F1 on test data: 0.8112
03/19/2022 20:02:27 - INFO - __main__ - prefix=ethos-race_16_87, lr=0.0001, bsz=8, dev_performance=0.716256157635468, test_performance=0.8112053659498915
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (22550): No such process
