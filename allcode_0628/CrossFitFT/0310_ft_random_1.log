nohup: ignoring input
Task: blimp-sentential_negation_npi_licensor_present, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_8dni_elt/none_1u9wf745
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_8dni_elt/none_1u9wf745/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_8dni_elt/none_1u9wf745/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/10/2022 15:41:19 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 15:41:19 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present
03/10/2022 15:41:19 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 15:41:19 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present
03/10/2022 15:41:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 15:41:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 15:41:20 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 15:41:20 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 15:41:20 - INFO - __main__ - args.device: cuda:0
03/10/2022 15:41:20 - INFO - __main__ - args.device: cuda:1
03/10/2022 15:41:20 - INFO - __main__ - Using 2 gpus
03/10/2022 15:41:20 - INFO - __main__ - Using 2 gpus
03/10/2022 15:41:20 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
03/10/2022 15:41:20 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 15:41:26 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0005, bsz=8 ...
03/10/2022 15:41:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:41:27 - INFO - __main__ - Printing 3 examples
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:41:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:41:27 - INFO - __main__ - Printing 3 examples
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:41:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:41:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:41:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:41:27 - INFO - __main__ - Printing 3 examples
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:41:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:41:27 - INFO - __main__ - Printing 3 examples
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:41:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:41:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:41:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:41:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:41:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:41:39 - INFO - __main__ - Starting training!
03/10/2022 15:41:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:41:39 - INFO - __main__ - Starting training!
03/10/2022 15:41:43 - INFO - __main__ - Step 10 Global step 10 Train loss 22.138430 on epoch=4
03/10/2022 15:41:48 - INFO - __main__ - Step 20 Global step 20 Train loss 22.915096 on epoch=9
03/10/2022 15:41:52 - INFO - __main__ - Step 30 Global step 30 Train loss 15.590918 on epoch=14
03/10/2022 15:41:57 - INFO - __main__ - Step 40 Global step 40 Train loss 12.431821 on epoch=19
03/10/2022 15:42:02 - INFO - __main__ - Step 50 Global step 50 Train loss 11.689927 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 15:42:02 - INFO - __main__ - Global step 50 Train loss 16.953238 ACC 0.0 on epoch=24
03/10/2022 15:42:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.127098 on epoch=29
03/10/2022 15:42:13 - INFO - __main__ - Step 70 Global step 70 Train loss 8.544050 on epoch=34
03/10/2022 15:42:18 - INFO - __main__ - Step 80 Global step 80 Train loss 5.240985 on epoch=39
03/10/2022 15:42:23 - INFO - __main__ - Step 90 Global step 90 Train loss 2.728260 on epoch=44
03/10/2022 15:42:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.769987 on epoch=49
03/10/2022 15:42:28 - INFO - __main__ - Global step 100 Train loss 5.482077 ACC 0.65625 on epoch=49
03/10/2022 15:42:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.224050 on epoch=54
03/10/2022 15:42:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.203558 on epoch=59
03/10/2022 15:42:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.203899 on epoch=64
03/10/2022 15:42:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.199520 on epoch=69
03/10/2022 15:42:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.289257 on epoch=74
03/10/2022 15:42:54 - INFO - __main__ - Global step 150 Train loss 0.224057 ACC 0.59375 on epoch=74
03/10/2022 15:42:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.160802 on epoch=79
03/10/2022 15:43:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.121175 on epoch=84
03/10/2022 15:43:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.121560 on epoch=89
03/10/2022 15:43:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.097055 on epoch=94
03/10/2022 15:43:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.140430 on epoch=99
03/10/2022 15:43:19 - INFO - __main__ - Global step 200 Train loss 0.128205 ACC 0.71875 on epoch=99
03/10/2022 15:43:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.064905 on epoch=104
03/10/2022 15:43:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.064830 on epoch=109
03/10/2022 15:43:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.029363 on epoch=114
03/10/2022 15:43:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.147385 on epoch=119
03/10/2022 15:43:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.374356 on epoch=124
03/10/2022 15:43:44 - INFO - __main__ - Global step 250 Train loss 0.136168 ACC 0.5 on epoch=124
03/10/2022 15:43:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.133412 on epoch=129
03/10/2022 15:43:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.212144 on epoch=134
03/10/2022 15:43:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.234260 on epoch=139
03/10/2022 15:44:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.176362 on epoch=144
03/10/2022 15:44:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.096246 on epoch=149
03/10/2022 15:44:09 - INFO - __main__ - Global step 300 Train loss 0.170485 ACC 0.59375 on epoch=149
03/10/2022 15:44:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.103893 on epoch=154
03/10/2022 15:44:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.065093 on epoch=159
03/10/2022 15:44:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.087371 on epoch=164
03/10/2022 15:44:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.023196 on epoch=169
03/10/2022 15:44:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009912 on epoch=174
03/10/2022 15:44:34 - INFO - __main__ - Global step 350 Train loss 0.057893 ACC 0.65625 on epoch=174
03/10/2022 15:44:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.010400 on epoch=179
03/10/2022 15:44:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.003867 on epoch=184
03/10/2022 15:44:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001067 on epoch=189
03/10/2022 15:44:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001379 on epoch=194
03/10/2022 15:44:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000749 on epoch=199
03/10/2022 15:44:58 - INFO - __main__ - Global step 400 Train loss 0.003492 ACC 0.65625 on epoch=199
03/10/2022 15:45:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001663 on epoch=204
03/10/2022 15:45:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001233 on epoch=209
03/10/2022 15:45:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000344 on epoch=214
03/10/2022 15:45:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.017484 on epoch=219
03/10/2022 15:45:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.093331 on epoch=224
03/10/2022 15:45:23 - INFO - __main__ - Global step 450 Train loss 0.022811 ACC 0.46875 on epoch=224
03/10/2022 15:45:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.083305 on epoch=229
03/10/2022 15:45:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.017764 on epoch=234
03/10/2022 15:45:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004324 on epoch=239
03/10/2022 15:45:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002084 on epoch=244
03/10/2022 15:45:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001491 on epoch=249
03/10/2022 15:45:48 - INFO - __main__ - Global step 500 Train loss 0.021793 ACC 0.53125 on epoch=249
03/10/2022 15:45:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001640 on epoch=254
03/10/2022 15:45:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004234 on epoch=259
03/10/2022 15:46:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.037460 on epoch=264
03/10/2022 15:46:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000593 on epoch=269
03/10/2022 15:46:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022379 on epoch=274
03/10/2022 15:46:13 - INFO - __main__ - Global step 550 Train loss 0.013261 ACC 0.6875 on epoch=274
03/10/2022 15:46:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.010868 on epoch=279
03/10/2022 15:46:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.080903 on epoch=284
03/10/2022 15:46:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.107147 on epoch=289
03/10/2022 15:46:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.123925 on epoch=294
03/10/2022 15:46:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.022955 on epoch=299
03/10/2022 15:46:38 - INFO - __main__ - Global step 600 Train loss 0.069160 ACC 0.625 on epoch=299
03/10/2022 15:46:38 - INFO - __main__ - save last model!
03/10/2022 15:46:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:46:38 - INFO - __main__ - Printing 3 examples
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:46:38 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:46:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:46:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:46:38 - INFO - __main__ - Printing 3 examples
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:46:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:38 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:46:38 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:46:38 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:46:45 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 15:46:46 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 15:46:46 - INFO - __main__ - Printing 3 examples
03/10/2022 15:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 15:46:46 - INFO - __main__ - ['sentence 2']
03/10/2022 15:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 15:46:46 - INFO - __main__ - ['sentence 2']
03/10/2022 15:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 15:46:46 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:46 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:46:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:46:46 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 15:46:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:46:49 - INFO - __main__ - Starting training!
03/10/2022 15:46:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.0005_8_predictions.txt
03/10/2022 15:46:50 - INFO - __main__ - ACC on test data: 0.7450
03/10/2022 15:46:51 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0005, bsz=8, dev_performance=0.71875, test_performance=0.745
03/10/2022 15:46:51 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0003, bsz=8 ...
03/10/2022 15:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:46:52 - INFO - __main__ - Printing 3 examples
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:46:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:46:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:46:52 - INFO - __main__ - Printing 3 examples
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:46:52 - INFO - __main__ - ['sentence 1']
03/10/2022 15:46:52 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:46:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:46:52 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:47:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:47:03 - INFO - __main__ - Starting training!
03/10/2022 15:47:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.616144 on epoch=4
03/10/2022 15:47:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.149357 on epoch=9
03/10/2022 15:47:16 - INFO - __main__ - Step 30 Global step 30 Train loss 14.154810 on epoch=14
03/10/2022 15:47:21 - INFO - __main__ - Step 40 Global step 40 Train loss 13.385585 on epoch=19
03/10/2022 15:47:26 - INFO - __main__ - Step 50 Global step 50 Train loss 12.529467 on epoch=24
03/10/2022 15:47:26 - INFO - __main__ - Global step 50 Train loss 15.967072 ACC 0.0 on epoch=24
03/10/2022 15:47:31 - INFO - __main__ - Step 60 Global step 60 Train loss 11.494421 on epoch=29
03/10/2022 15:47:36 - INFO - __main__ - Step 70 Global step 70 Train loss 10.720563 on epoch=34
03/10/2022 15:47:41 - INFO - __main__ - Step 80 Global step 80 Train loss 10.034289 on epoch=39
03/10/2022 15:47:46 - INFO - __main__ - Step 90 Global step 90 Train loss 8.536927 on epoch=44
03/10/2022 15:47:51 - INFO - __main__ - Step 100 Global step 100 Train loss 6.472810 on epoch=49
03/10/2022 15:47:51 - INFO - __main__ - Global step 100 Train loss 9.451801 ACC 0.0 on epoch=49
03/10/2022 15:47:56 - INFO - __main__ - Step 110 Global step 110 Train loss 3.809133 on epoch=54
03/10/2022 15:48:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.777696 on epoch=59
03/10/2022 15:48:05 - INFO - __main__ - Step 130 Global step 130 Train loss 1.754093 on epoch=64
03/10/2022 15:48:10 - INFO - __main__ - Step 140 Global step 140 Train loss 1.085530 on epoch=69
03/10/2022 15:48:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.605581 on epoch=74
03/10/2022 15:48:15 - INFO - __main__ - Global step 150 Train loss 1.806406 ACC 0.59375 on epoch=74
03/10/2022 15:48:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.401737 on epoch=79
03/10/2022 15:48:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.526662 on epoch=84
03/10/2022 15:48:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.161527 on epoch=89
03/10/2022 15:48:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.244307 on epoch=94
03/10/2022 15:48:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.158914 on epoch=99
03/10/2022 15:48:41 - INFO - __main__ - Global step 200 Train loss 0.298629 ACC 0.71875 on epoch=99
03/10/2022 15:48:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.323560 on epoch=104
03/10/2022 15:48:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.187789 on epoch=109
03/10/2022 15:48:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.175513 on epoch=114
03/10/2022 15:49:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.132142 on epoch=119
03/10/2022 15:49:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.133712 on epoch=124
03/10/2022 15:49:05 - INFO - __main__ - Global step 250 Train loss 0.190543 ACC 0.78125 on epoch=124
03/10/2022 15:49:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.140928 on epoch=129
03/10/2022 15:49:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.135443 on epoch=134
03/10/2022 15:49:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.164143 on epoch=139
03/10/2022 15:49:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.129834 on epoch=144
03/10/2022 15:49:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.082758 on epoch=149
03/10/2022 15:49:30 - INFO - __main__ - Global step 300 Train loss 0.130621 ACC 0.875 on epoch=149
03/10/2022 15:49:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.097195 on epoch=154
03/10/2022 15:49:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070514 on epoch=159
03/10/2022 15:49:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.065505 on epoch=164
03/10/2022 15:49:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.095762 on epoch=169
03/10/2022 15:49:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.039018 on epoch=174
03/10/2022 15:49:55 - INFO - __main__ - Global step 350 Train loss 0.073599 ACC 0.84375 on epoch=174
03/10/2022 15:49:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.038047 on epoch=179
03/10/2022 15:50:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.036767 on epoch=184
03/10/2022 15:50:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.020612 on epoch=189
03/10/2022 15:50:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.083320 on epoch=194
03/10/2022 15:50:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.085420 on epoch=199
03/10/2022 15:50:19 - INFO - __main__ - Global step 400 Train loss 0.052833 ACC 0.96875 on epoch=199
03/10/2022 15:50:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015415 on epoch=204
03/10/2022 15:50:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.010611 on epoch=209
03/10/2022 15:50:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004540 on epoch=214
03/10/2022 15:50:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004023 on epoch=219
03/10/2022 15:50:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002512 on epoch=224
03/10/2022 15:50:43 - INFO - __main__ - Global step 450 Train loss 0.007420 ACC 0.9375 on epoch=224
03/10/2022 15:50:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.005982 on epoch=229
03/10/2022 15:50:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.007770 on epoch=234
03/10/2022 15:50:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000976 on epoch=239
03/10/2022 15:51:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003144 on epoch=244
03/10/2022 15:51:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000452 on epoch=249
03/10/2022 15:51:08 - INFO - __main__ - Global step 500 Train loss 0.003665 ACC 0.9375 on epoch=249
03/10/2022 15:51:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001065 on epoch=254
03/10/2022 15:51:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000394 on epoch=259
03/10/2022 15:51:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001787 on epoch=264
03/10/2022 15:51:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002003 on epoch=269
03/10/2022 15:51:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000393 on epoch=274
03/10/2022 15:51:32 - INFO - __main__ - Global step 550 Train loss 0.001129 ACC 0.875 on epoch=274
03/10/2022 15:51:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001929 on epoch=279
03/10/2022 15:51:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000091 on epoch=284
03/10/2022 15:51:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000140 on epoch=289
03/10/2022 15:51:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.022271 on epoch=294
03/10/2022 15:51:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007943 on epoch=299
03/10/2022 15:51:56 - INFO - __main__ - Global step 600 Train loss 0.006475 ACC 0.90625 on epoch=299
03/10/2022 15:51:56 - INFO - __main__ - save last model!
03/10/2022 15:51:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:51:57 - INFO - __main__ - Printing 3 examples
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:51:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:51:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:51:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:51:57 - INFO - __main__ - Printing 3 examples
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:51:57 - INFO - __main__ - ['sentence 1']
03/10/2022 15:51:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:51:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:51:57 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:52:03 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 15:52:04 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 15:52:04 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 15:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 15:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 15:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 15:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 15:52:04 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:52:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:52:04 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 15:52:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:52:06 - INFO - __main__ - Starting training!
03/10/2022 15:52:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.0003_8_predictions.txt
03/10/2022 15:52:06 - INFO - __main__ - ACC on test data: 0.8800
03/10/2022 15:52:07 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0003, bsz=8, dev_performance=0.96875, test_performance=0.88
03/10/2022 15:52:07 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0002, bsz=8 ...
03/10/2022 15:52:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:52:07 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:52:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:52:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:52:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:52:07 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:52:07 - INFO - __main__ - ['sentence 1']
03/10/2022 15:52:07 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:52:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:52:08 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:52:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:52:17 - INFO - __main__ - Starting training!
03/10/2022 15:52:22 - INFO - __main__ - Step 10 Global step 10 Train loss 21.838778 on epoch=4
03/10/2022 15:52:26 - INFO - __main__ - Step 20 Global step 20 Train loss 18.790268 on epoch=9
03/10/2022 15:52:31 - INFO - __main__ - Step 30 Global step 30 Train loss 15.837433 on epoch=14
03/10/2022 15:52:36 - INFO - __main__ - Step 40 Global step 40 Train loss 13.995961 on epoch=19
03/10/2022 15:52:41 - INFO - __main__ - Step 50 Global step 50 Train loss 13.492511 on epoch=24
03/10/2022 15:52:51 - INFO - __main__ - Global step 50 Train loss 16.790991 ACC 0.0 on epoch=24
03/10/2022 15:52:57 - INFO - __main__ - Step 60 Global step 60 Train loss 13.124301 on epoch=29
03/10/2022 15:53:02 - INFO - __main__ - Step 70 Global step 70 Train loss 11.925812 on epoch=34
03/10/2022 15:53:07 - INFO - __main__ - Step 80 Global step 80 Train loss 11.962407 on epoch=39
03/10/2022 15:53:12 - INFO - __main__ - Step 90 Global step 90 Train loss 11.651152 on epoch=44
03/10/2022 15:53:16 - INFO - __main__ - Step 100 Global step 100 Train loss 10.598227 on epoch=49
03/10/2022 15:53:18 - INFO - __main__ - Global step 100 Train loss 11.852379 ACC 0.1875 on epoch=49
03/10/2022 15:53:23 - INFO - __main__ - Step 110 Global step 110 Train loss 10.360807 on epoch=54
03/10/2022 15:53:28 - INFO - __main__ - Step 120 Global step 120 Train loss 9.521576 on epoch=59
03/10/2022 15:53:33 - INFO - __main__ - Step 130 Global step 130 Train loss 8.722663 on epoch=64
03/10/2022 15:53:38 - INFO - __main__ - Step 140 Global step 140 Train loss 8.225465 on epoch=69
03/10/2022 15:53:43 - INFO - __main__ - Step 150 Global step 150 Train loss 6.582223 on epoch=74
03/10/2022 15:53:43 - INFO - __main__ - Global step 150 Train loss 8.682547 ACC 0.0 on epoch=74
03/10/2022 15:53:48 - INFO - __main__ - Step 160 Global step 160 Train loss 4.639923 on epoch=79
03/10/2022 15:53:53 - INFO - __main__ - Step 170 Global step 170 Train loss 2.299718 on epoch=84
03/10/2022 15:53:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.885475 on epoch=89
03/10/2022 15:54:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.262786 on epoch=94
03/10/2022 15:54:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.234135 on epoch=99
03/10/2022 15:54:08 - INFO - __main__ - Global step 200 Train loss 1.664408 ACC 0.59375 on epoch=99
03/10/2022 15:54:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.208091 on epoch=104
03/10/2022 15:54:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.214196 on epoch=109
03/10/2022 15:54:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.208023 on epoch=114
03/10/2022 15:54:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.203985 on epoch=119
03/10/2022 15:54:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.195305 on epoch=124
03/10/2022 15:54:33 - INFO - __main__ - Global step 250 Train loss 0.205920 ACC 0.75 on epoch=124
03/10/2022 15:54:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.157203 on epoch=129
03/10/2022 15:54:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.173172 on epoch=134
03/10/2022 15:54:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.166284 on epoch=139
03/10/2022 15:54:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.211299 on epoch=144
03/10/2022 15:54:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.164887 on epoch=149
03/10/2022 15:54:58 - INFO - __main__ - Global step 300 Train loss 0.174569 ACC 0.78125 on epoch=149
03/10/2022 15:55:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.150988 on epoch=154
03/10/2022 15:55:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.165729 on epoch=159
03/10/2022 15:55:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.146625 on epoch=164
03/10/2022 15:55:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.133502 on epoch=169
03/10/2022 15:55:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.128439 on epoch=174
03/10/2022 15:55:23 - INFO - __main__ - Global step 350 Train loss 0.145056 ACC 0.78125 on epoch=174
03/10/2022 15:55:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.171691 on epoch=179
03/10/2022 15:55:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.121079 on epoch=184
03/10/2022 15:55:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.137040 on epoch=189
03/10/2022 15:55:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.119854 on epoch=194
03/10/2022 15:55:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.140634 on epoch=199
03/10/2022 15:55:48 - INFO - __main__ - Global step 400 Train loss 0.138060 ACC 0.78125 on epoch=199
03/10/2022 15:55:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.110043 on epoch=204
03/10/2022 15:55:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.122768 on epoch=209
03/10/2022 15:56:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.123257 on epoch=214
03/10/2022 15:56:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.093883 on epoch=219
03/10/2022 15:56:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.128009 on epoch=224
03/10/2022 15:56:12 - INFO - __main__ - Global step 450 Train loss 0.115592 ACC 0.71875 on epoch=224
03/10/2022 15:56:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.125996 on epoch=229
03/10/2022 15:56:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.133760 on epoch=234
03/10/2022 15:56:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.111811 on epoch=239
03/10/2022 15:56:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.129185 on epoch=244
03/10/2022 15:56:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.090414 on epoch=249
03/10/2022 15:56:37 - INFO - __main__ - Global step 500 Train loss 0.118233 ACC 0.90625 on epoch=249
03/10/2022 15:56:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.098729 on epoch=254
03/10/2022 15:56:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.091036 on epoch=259
03/10/2022 15:56:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.078686 on epoch=264
03/10/2022 15:56:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.060751 on epoch=269
03/10/2022 15:57:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.064121 on epoch=274
03/10/2022 15:57:02 - INFO - __main__ - Global step 550 Train loss 0.078665 ACC 0.75 on epoch=274
03/10/2022 15:57:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.048539 on epoch=279
03/10/2022 15:57:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.069022 on epoch=284
03/10/2022 15:57:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.033146 on epoch=289
03/10/2022 15:57:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.018076 on epoch=294
03/10/2022 15:57:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.017140 on epoch=299
03/10/2022 15:57:27 - INFO - __main__ - Global step 600 Train loss 0.037185 ACC 0.84375 on epoch=299
03/10/2022 15:57:27 - INFO - __main__ - save last model!
03/10/2022 15:57:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:57:27 - INFO - __main__ - Printing 3 examples
03/10/2022 15:57:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:57:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:57:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:57:27 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:57:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:57:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:57:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:57:28 - INFO - __main__ - Printing 3 examples
03/10/2022 15:57:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:57:28 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:57:28 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:57:28 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:57:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:57:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:57:34 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 15:57:34 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 15:57:34 - INFO - __main__ - Printing 3 examples
03/10/2022 15:57:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 15:57:34 - INFO - __main__ - ['sentence 2']
03/10/2022 15:57:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 15:57:34 - INFO - __main__ - ['sentence 2']
03/10/2022 15:57:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 15:57:34 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:34 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:57:34 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:57:35 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 15:57:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.0002_8_predictions.txt
03/10/2022 15:57:37 - INFO - __main__ - ACC on test data: 0.7450
03/10/2022 15:57:37 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0002, bsz=8, dev_performance=0.90625, test_performance=0.745
03/10/2022 15:57:37 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0001, bsz=8 ...
03/10/2022 15:57:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:57:38 - INFO - __main__ - Starting training!
03/10/2022 15:57:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:57:38 - INFO - __main__ - Printing 3 examples
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:57:38 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:57:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:57:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:57:38 - INFO - __main__ - Printing 3 examples
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/10/2022 15:57:38 - INFO - __main__ - ['sentence 1']
03/10/2022 15:57:38 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:57:38 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:57:38 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:57:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 15:57:49 - INFO - __main__ - Starting training!
03/10/2022 15:57:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.094450 on epoch=4
03/10/2022 15:57:58 - INFO - __main__ - Step 20 Global step 20 Train loss 20.521503 on epoch=9
03/10/2022 15:58:02 - INFO - __main__ - Step 30 Global step 30 Train loss 16.245016 on epoch=14
03/10/2022 15:58:07 - INFO - __main__ - Step 40 Global step 40 Train loss 15.365517 on epoch=19
03/10/2022 15:58:12 - INFO - __main__ - Step 50 Global step 50 Train loss 14.599022 on epoch=24
03/10/2022 15:58:13 - INFO - __main__ - Global step 50 Train loss 17.765100 ACC 0.0 on epoch=24
03/10/2022 15:58:18 - INFO - __main__ - Step 60 Global step 60 Train loss 13.821764 on epoch=29
03/10/2022 15:58:23 - INFO - __main__ - Step 70 Global step 70 Train loss 13.574941 on epoch=34
03/10/2022 15:58:27 - INFO - __main__ - Step 80 Global step 80 Train loss 13.828700 on epoch=39
03/10/2022 15:58:32 - INFO - __main__ - Step 90 Global step 90 Train loss 13.113828 on epoch=44
03/10/2022 15:58:37 - INFO - __main__ - Step 100 Global step 100 Train loss 12.354929 on epoch=49
03/10/2022 15:58:38 - INFO - __main__ - Global step 100 Train loss 13.338832 ACC 0.0 on epoch=49
03/10/2022 15:58:42 - INFO - __main__ - Step 110 Global step 110 Train loss 12.402066 on epoch=54
03/10/2022 15:58:47 - INFO - __main__ - Step 120 Global step 120 Train loss 11.817460 on epoch=59
03/10/2022 15:58:52 - INFO - __main__ - Step 130 Global step 130 Train loss 12.113165 on epoch=64
03/10/2022 15:58:57 - INFO - __main__ - Step 140 Global step 140 Train loss 11.391304 on epoch=69
03/10/2022 15:59:01 - INFO - __main__ - Step 150 Global step 150 Train loss 11.579552 on epoch=74
03/10/2022 15:59:02 - INFO - __main__ - Global step 150 Train loss 11.860709 ACC 0.0 on epoch=74
03/10/2022 15:59:06 - INFO - __main__ - Step 160 Global step 160 Train loss 11.037390 on epoch=79
03/10/2022 15:59:11 - INFO - __main__ - Step 170 Global step 170 Train loss 10.557548 on epoch=84
03/10/2022 15:59:16 - INFO - __main__ - Step 180 Global step 180 Train loss 10.540902 on epoch=89
03/10/2022 15:59:21 - INFO - __main__ - Step 190 Global step 190 Train loss 10.729035 on epoch=94
03/10/2022 15:59:26 - INFO - __main__ - Step 200 Global step 200 Train loss 10.389704 on epoch=99
03/10/2022 15:59:26 - INFO - __main__ - Global step 200 Train loss 10.650917 ACC 0.0 on epoch=99
03/10/2022 15:59:31 - INFO - __main__ - Step 210 Global step 210 Train loss 9.190042 on epoch=104
03/10/2022 15:59:36 - INFO - __main__ - Step 220 Global step 220 Train loss 9.431752 on epoch=109
03/10/2022 15:59:40 - INFO - __main__ - Step 230 Global step 230 Train loss 8.701734 on epoch=114
03/10/2022 15:59:45 - INFO - __main__ - Step 240 Global step 240 Train loss 8.447420 on epoch=119
03/10/2022 15:59:50 - INFO - __main__ - Step 250 Global step 250 Train loss 7.521989 on epoch=124
03/10/2022 15:59:51 - INFO - __main__ - Global step 250 Train loss 8.658587 ACC 0.0 on epoch=124
03/10/2022 15:59:56 - INFO - __main__ - Step 260 Global step 260 Train loss 7.209131 on epoch=129
03/10/2022 16:00:00 - INFO - __main__ - Step 270 Global step 270 Train loss 4.953506 on epoch=134
03/10/2022 16:00:05 - INFO - __main__ - Step 280 Global step 280 Train loss 2.191665 on epoch=139
03/10/2022 16:00:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.872534 on epoch=144
03/10/2022 16:00:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.632376 on epoch=149
03/10/2022 16:00:15 - INFO - __main__ - Global step 300 Train loss 3.171842 ACC 0.75 on epoch=149
03/10/2022 16:00:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.366141 on epoch=154
03/10/2022 16:00:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.206264 on epoch=159
03/10/2022 16:00:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.157117 on epoch=164
03/10/2022 16:00:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.128843 on epoch=169
03/10/2022 16:00:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.142990 on epoch=174
03/10/2022 16:00:40 - INFO - __main__ - Global step 350 Train loss 0.200271 ACC 0.6875 on epoch=174
03/10/2022 16:00:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.102884 on epoch=179
03/10/2022 16:00:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.153115 on epoch=184
03/10/2022 16:00:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.101766 on epoch=189
03/10/2022 16:00:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.080537 on epoch=194
03/10/2022 16:01:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.049772 on epoch=199
03/10/2022 16:01:04 - INFO - __main__ - Global step 400 Train loss 0.097615 ACC 0.78125 on epoch=199
03/10/2022 16:01:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.194386 on epoch=204
03/10/2022 16:01:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.053829 on epoch=209
03/10/2022 16:01:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.100783 on epoch=214
03/10/2022 16:01:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.043103 on epoch=219
03/10/2022 16:01:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.030883 on epoch=224
03/10/2022 16:01:28 - INFO - __main__ - Global step 450 Train loss 0.084597 ACC 0.75 on epoch=224
03/10/2022 16:01:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.017972 on epoch=229
03/10/2022 16:01:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.042275 on epoch=234
03/10/2022 16:01:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.037746 on epoch=239
03/10/2022 16:01:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.025759 on epoch=244
03/10/2022 16:01:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.023529 on epoch=249
03/10/2022 16:01:53 - INFO - __main__ - Global step 500 Train loss 0.029456 ACC 0.78125 on epoch=249
03/10/2022 16:01:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.029922 on epoch=254
03/10/2022 16:02:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.018820 on epoch=259
03/10/2022 16:02:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.018143 on epoch=264
03/10/2022 16:02:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.013564 on epoch=269
03/10/2022 16:02:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.011808 on epoch=274
03/10/2022 16:02:26 - INFO - __main__ - Global step 550 Train loss 0.018451 ACC 0.71875 on epoch=274
03/10/2022 16:02:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006827 on epoch=279
03/10/2022 16:02:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002240 on epoch=284
03/10/2022 16:02:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003353 on epoch=289
03/10/2022 16:02:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006152 on epoch=294
03/10/2022 16:02:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003031 on epoch=299
03/10/2022 16:02:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:02:51 - INFO - __main__ - Printing 3 examples
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:02:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:02:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:02:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:02:51 - INFO - __main__ - Printing 3 examples
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:02:51 - INFO - __main__ - ['sentence 1']
03/10/2022 16:02:51 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:02:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:02:51 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:02:57 - INFO - __main__ - Global step 600 Train loss 0.004321 ACC 0.8125 on epoch=299
03/10/2022 16:02:57 - INFO - __main__ - save last model!
03/10/2022 16:03:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:03:00 - INFO - __main__ - Starting training!
03/10/2022 16:03:04 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:03:05 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:03:05 - INFO - __main__ - Printing 3 examples
03/10/2022 16:03:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:03:05 - INFO - __main__ - ['sentence 2']
03/10/2022 16:03:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:03:05 - INFO - __main__ - ['sentence 2']
03/10/2022 16:03:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:03:05 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:03:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:03:05 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:03:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.0001_8_predictions.txt
03/10/2022 16:03:08 - INFO - __main__ - ACC on test data: 0.7600
03/10/2022 16:03:09 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0001, bsz=8, dev_performance=0.8125, test_performance=0.76
03/10/2022 16:03:09 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0005, bsz=8 ...
03/10/2022 16:03:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:03:10 - INFO - __main__ - Printing 3 examples
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:03:10 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:03:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:03:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:03:10 - INFO - __main__ - Printing 3 examples
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:03:10 - INFO - __main__ - ['sentence 1']
03/10/2022 16:03:10 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:03:10 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:03:10 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:03:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:03:19 - INFO - __main__ - Starting training!
03/10/2022 16:03:25 - INFO - __main__ - Step 10 Global step 10 Train loss 21.562822 on epoch=4
03/10/2022 16:03:29 - INFO - __main__ - Step 20 Global step 20 Train loss 15.381891 on epoch=9
03/10/2022 16:03:34 - INFO - __main__ - Step 30 Global step 30 Train loss 13.913111 on epoch=14
03/10/2022 16:03:39 - INFO - __main__ - Step 40 Global step 40 Train loss 11.925192 on epoch=19
03/10/2022 16:03:44 - INFO - __main__ - Step 50 Global step 50 Train loss 10.118068 on epoch=24
03/10/2022 16:03:45 - INFO - __main__ - Global step 50 Train loss 14.580216 ACC 0.0 on epoch=24
03/10/2022 16:03:50 - INFO - __main__ - Step 60 Global step 60 Train loss 9.453541 on epoch=29
03/10/2022 16:03:55 - INFO - __main__ - Step 70 Global step 70 Train loss 5.991973 on epoch=34
03/10/2022 16:04:00 - INFO - __main__ - Step 80 Global step 80 Train loss 1.575247 on epoch=39
03/10/2022 16:04:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.285298 on epoch=44
03/10/2022 16:04:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.231665 on epoch=49
03/10/2022 16:04:10 - INFO - __main__ - Global step 100 Train loss 3.507544 ACC 0.6875 on epoch=49
03/10/2022 16:04:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.203121 on epoch=54
03/10/2022 16:04:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.158412 on epoch=59
03/10/2022 16:04:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.158521 on epoch=64
03/10/2022 16:04:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.155329 on epoch=69
03/10/2022 16:04:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.145413 on epoch=74
03/10/2022 16:04:37 - INFO - __main__ - Global step 150 Train loss 0.164159 ACC 0.71875 on epoch=74
03/10/2022 16:04:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.083430 on epoch=79
03/10/2022 16:04:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.126929 on epoch=84
03/10/2022 16:04:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.064034 on epoch=89
03/10/2022 16:04:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.085628 on epoch=94
03/10/2022 16:05:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.024416 on epoch=99
03/10/2022 16:05:02 - INFO - __main__ - Global step 200 Train loss 0.076887 ACC 0.6875 on epoch=99
03/10/2022 16:05:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.113688 on epoch=104
03/10/2022 16:05:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.168615 on epoch=109
03/10/2022 16:05:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.036948 on epoch=114
03/10/2022 16:05:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.017009 on epoch=119
03/10/2022 16:05:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.013493 on epoch=124
03/10/2022 16:05:27 - INFO - __main__ - Global step 250 Train loss 0.069950 ACC 0.625 on epoch=124
03/10/2022 16:05:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.009663 on epoch=129
03/10/2022 16:05:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007301 on epoch=134
03/10/2022 16:05:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.075609 on epoch=139
03/10/2022 16:05:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.020679 on epoch=144
03/10/2022 16:05:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.006242 on epoch=149
03/10/2022 16:05:53 - INFO - __main__ - Global step 300 Train loss 0.023899 ACC 0.5 on epoch=149
03/10/2022 16:05:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001415 on epoch=154
03/10/2022 16:06:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001562 on epoch=159
03/10/2022 16:06:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.043170 on epoch=164
03/10/2022 16:06:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.011471 on epoch=169
03/10/2022 16:06:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.006298 on epoch=174
03/10/2022 16:06:17 - INFO - __main__ - Global step 350 Train loss 0.012783 ACC 0.625 on epoch=174
03/10/2022 16:06:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.003079 on epoch=179
03/10/2022 16:06:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000272 on epoch=184
03/10/2022 16:06:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000316 on epoch=189
03/10/2022 16:06:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000205 on epoch=194
03/10/2022 16:06:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000279 on epoch=199
03/10/2022 16:06:42 - INFO - __main__ - Global step 400 Train loss 0.000830 ACC 0.625 on epoch=199
03/10/2022 16:06:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000361 on epoch=204
03/10/2022 16:06:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000250 on epoch=209
03/10/2022 16:06:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000140 on epoch=214
03/10/2022 16:07:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000174 on epoch=219
03/10/2022 16:07:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000115 on epoch=224
03/10/2022 16:07:07 - INFO - __main__ - Global step 450 Train loss 0.000208 ACC 0.625 on epoch=224
03/10/2022 16:07:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000103 on epoch=229
03/10/2022 16:07:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000124 on epoch=234
03/10/2022 16:07:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000111 on epoch=239
03/10/2022 16:07:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000333 on epoch=244
03/10/2022 16:07:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000048 on epoch=249
03/10/2022 16:07:32 - INFO - __main__ - Global step 500 Train loss 0.000144 ACC 0.625 on epoch=249
03/10/2022 16:07:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000082 on epoch=254
03/10/2022 16:07:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000039 on epoch=259
03/10/2022 16:07:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001206 on epoch=264
03/10/2022 16:07:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.142316 on epoch=269
03/10/2022 16:07:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.259858 on epoch=274
03/10/2022 16:07:57 - INFO - __main__ - Global step 550 Train loss 0.080700 ACC 0.5625 on epoch=274
03/10/2022 16:08:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009799 on epoch=279
03/10/2022 16:08:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001208 on epoch=284
03/10/2022 16:08:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000953 on epoch=289
03/10/2022 16:08:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000564 on epoch=294
03/10/2022 16:08:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000990 on epoch=299
03/10/2022 16:08:22 - INFO - __main__ - Global step 600 Train loss 0.002703 ACC 0.625 on epoch=299
03/10/2022 16:08:22 - INFO - __main__ - save last model!
03/10/2022 16:08:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:08:23 - INFO - __main__ - Printing 3 examples
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:08:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:08:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:08:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:08:23 - INFO - __main__ - Printing 3 examples
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:08:23 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:23 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:08:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:08:23 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:08:29 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:08:30 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:08:30 - INFO - __main__ - Printing 3 examples
03/10/2022 16:08:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:08:30 - INFO - __main__ - ['sentence 2']
03/10/2022 16:08:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:08:30 - INFO - __main__ - ['sentence 2']
03/10/2022 16:08:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:08:30 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:30 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:08:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:08:30 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:08:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.0005_8_predictions.txt
03/10/2022 16:08:33 - INFO - __main__ - ACC on test data: 0.5700
03/10/2022 16:08:34 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0005, bsz=8, dev_performance=0.71875, test_performance=0.57
03/10/2022 16:08:34 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0003, bsz=8 ...
03/10/2022 16:08:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:08:34 - INFO - __main__ - Starting training!
03/10/2022 16:08:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:08:34 - INFO - __main__ - Printing 3 examples
03/10/2022 16:08:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:08:34 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:08:34 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:08:34 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:08:34 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:08:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:08:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:08:34 - INFO - __main__ - Printing 3 examples
03/10/2022 16:08:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:08:34 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:08:35 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:08:35 - INFO - __main__ - ['sentence 1']
03/10/2022 16:08:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:08:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:08:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:08:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:08:44 - INFO - __main__ - Starting training!
03/10/2022 16:08:48 - INFO - __main__ - Step 10 Global step 10 Train loss 21.471058 on epoch=4
03/10/2022 16:08:53 - INFO - __main__ - Step 20 Global step 20 Train loss 16.425022 on epoch=9
03/10/2022 16:08:58 - INFO - __main__ - Step 30 Global step 30 Train loss 14.645213 on epoch=14
03/10/2022 16:09:02 - INFO - __main__ - Step 40 Global step 40 Train loss 13.833600 on epoch=19
03/10/2022 16:09:07 - INFO - __main__ - Step 50 Global step 50 Train loss 12.498660 on epoch=24
03/10/2022 16:09:08 - INFO - __main__ - Global step 50 Train loss 15.774712 ACC 0.0 on epoch=24
03/10/2022 16:09:13 - INFO - __main__ - Step 60 Global step 60 Train loss 11.533583 on epoch=29
03/10/2022 16:09:18 - INFO - __main__ - Step 70 Global step 70 Train loss 10.619653 on epoch=34
03/10/2022 16:09:23 - INFO - __main__ - Step 80 Global step 80 Train loss 9.917706 on epoch=39
03/10/2022 16:09:28 - INFO - __main__ - Step 90 Global step 90 Train loss 9.145333 on epoch=44
03/10/2022 16:09:33 - INFO - __main__ - Step 100 Global step 100 Train loss 7.463931 on epoch=49
03/10/2022 16:09:43 - INFO - __main__ - Global step 100 Train loss 9.736041 ACC 0.0 on epoch=49
03/10/2022 16:09:48 - INFO - __main__ - Step 110 Global step 110 Train loss 1.832999 on epoch=54
03/10/2022 16:09:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.250229 on epoch=59
03/10/2022 16:09:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.245593 on epoch=64
03/10/2022 16:10:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.221962 on epoch=69
03/10/2022 16:10:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.201559 on epoch=74
03/10/2022 16:10:08 - INFO - __main__ - Global step 150 Train loss 0.550469 ACC 0.53125 on epoch=74
03/10/2022 16:10:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.168778 on epoch=79
03/10/2022 16:10:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.173482 on epoch=84
03/10/2022 16:10:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.134662 on epoch=89
03/10/2022 16:10:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.129315 on epoch=94
03/10/2022 16:10:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.140044 on epoch=99
03/10/2022 16:10:34 - INFO - __main__ - Global step 200 Train loss 0.149256 ACC 0.71875 on epoch=99
03/10/2022 16:10:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.117214 on epoch=104
03/10/2022 16:10:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.089172 on epoch=109
03/10/2022 16:10:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.158490 on epoch=114
03/10/2022 16:10:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.091173 on epoch=119
03/10/2022 16:11:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.112281 on epoch=124
03/10/2022 16:11:00 - INFO - __main__ - Global step 250 Train loss 0.113666 ACC 0.6875 on epoch=124
03/10/2022 16:11:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.059667 on epoch=129
03/10/2022 16:11:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.029573 on epoch=134
03/10/2022 16:11:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.107020 on epoch=139
03/10/2022 16:11:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.023964 on epoch=144
03/10/2022 16:11:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.017655 on epoch=149
03/10/2022 16:11:25 - INFO - __main__ - Global step 300 Train loss 0.047576 ACC 0.71875 on epoch=149
03/10/2022 16:11:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.007990 on epoch=154
03/10/2022 16:11:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.050174 on epoch=159
03/10/2022 16:11:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.008318 on epoch=164
03/10/2022 16:11:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012767 on epoch=169
03/10/2022 16:11:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001659 on epoch=174
03/10/2022 16:11:50 - INFO - __main__ - Global step 350 Train loss 0.016181 ACC 0.75 on epoch=174
03/10/2022 16:11:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006520 on epoch=179
03/10/2022 16:12:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001370 on epoch=184
03/10/2022 16:12:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000833 on epoch=189
03/10/2022 16:12:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000871 on epoch=194
03/10/2022 16:12:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.014444 on epoch=199
03/10/2022 16:12:16 - INFO - __main__ - Global step 400 Train loss 0.004808 ACC 0.59375 on epoch=199
03/10/2022 16:12:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002606 on epoch=204
03/10/2022 16:12:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003482 on epoch=209
03/10/2022 16:12:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001140 on epoch=214
03/10/2022 16:12:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004502 on epoch=219
03/10/2022 16:12:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009144 on epoch=224
03/10/2022 16:12:40 - INFO - __main__ - Global step 450 Train loss 0.004175 ACC 0.78125 on epoch=224
03/10/2022 16:12:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000176 on epoch=229
03/10/2022 16:12:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003995 on epoch=234
03/10/2022 16:12:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000983 on epoch=239
03/10/2022 16:13:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000022 on epoch=244
03/10/2022 16:13:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000851 on epoch=249
03/10/2022 16:13:06 - INFO - __main__ - Global step 500 Train loss 0.001206 ACC 0.75 on epoch=249
03/10/2022 16:13:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000213 on epoch=254
03/10/2022 16:13:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000045 on epoch=259
03/10/2022 16:13:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000182 on epoch=264
03/10/2022 16:13:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000111 on epoch=269
03/10/2022 16:13:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000070 on epoch=274
03/10/2022 16:13:31 - INFO - __main__ - Global step 550 Train loss 0.000124 ACC 0.875 on epoch=274
03/10/2022 16:13:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008162 on epoch=279
03/10/2022 16:13:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000078 on epoch=284
03/10/2022 16:13:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020568 on epoch=289
03/10/2022 16:13:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.081201 on epoch=294
03/10/2022 16:13:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.193821 on epoch=299
03/10/2022 16:13:56 - INFO - __main__ - Global step 600 Train loss 0.060766 ACC 0.78125 on epoch=299
03/10/2022 16:13:56 - INFO - __main__ - save last model!
03/10/2022 16:13:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:13:57 - INFO - __main__ - Printing 3 examples
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:13:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:13:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:13:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:13:57 - INFO - __main__ - Printing 3 examples
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:13:57 - INFO - __main__ - ['sentence 1']
03/10/2022 16:13:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:13:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:13:57 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:14:03 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:14:04 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:14:04 - INFO - __main__ - Printing 3 examples
03/10/2022 16:14:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:14:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:14:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:14:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:14:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:14:04 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:14:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:14:05 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:14:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:14:07 - INFO - __main__ - Starting training!
03/10/2022 16:14:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.0003_8_predictions.txt
03/10/2022 16:14:07 - INFO - __main__ - ACC on test data: 0.7500
03/10/2022 16:14:08 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0003, bsz=8, dev_performance=0.875, test_performance=0.75
03/10/2022 16:14:08 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0002, bsz=8 ...
03/10/2022 16:14:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:14:09 - INFO - __main__ - Printing 3 examples
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:14:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:14:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:14:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:14:09 - INFO - __main__ - Printing 3 examples
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:14:09 - INFO - __main__ - ['sentence 1']
03/10/2022 16:14:09 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:14:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:14:09 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:14:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:14:19 - INFO - __main__ - Starting training!
03/10/2022 16:14:25 - INFO - __main__ - Step 10 Global step 10 Train loss 21.688438 on epoch=4
03/10/2022 16:14:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.328276 on epoch=9
03/10/2022 16:14:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.153805 on epoch=14
03/10/2022 16:14:39 - INFO - __main__ - Step 40 Global step 40 Train loss 14.594000 on epoch=19
03/10/2022 16:14:44 - INFO - __main__ - Step 50 Global step 50 Train loss 13.363269 on epoch=24
03/10/2022 16:14:47 - INFO - __main__ - Global step 50 Train loss 17.025558 ACC 0.0 on epoch=24
03/10/2022 16:14:53 - INFO - __main__ - Step 60 Global step 60 Train loss 12.811867 on epoch=29
03/10/2022 16:14:57 - INFO - __main__ - Step 70 Global step 70 Train loss 11.455091 on epoch=34
03/10/2022 16:15:02 - INFO - __main__ - Step 80 Global step 80 Train loss 11.640368 on epoch=39
03/10/2022 16:15:07 - INFO - __main__ - Step 90 Global step 90 Train loss 11.252552 on epoch=44
03/10/2022 16:15:12 - INFO - __main__ - Step 100 Global step 100 Train loss 10.122052 on epoch=49
03/10/2022 16:15:14 - INFO - __main__ - Global step 100 Train loss 11.456386 ACC 0.0 on epoch=49
03/10/2022 16:15:19 - INFO - __main__ - Step 110 Global step 110 Train loss 9.933270 on epoch=54
03/10/2022 16:15:24 - INFO - __main__ - Step 120 Global step 120 Train loss 9.397828 on epoch=59
03/10/2022 16:15:29 - INFO - __main__ - Step 130 Global step 130 Train loss 8.610632 on epoch=64
03/10/2022 16:15:34 - INFO - __main__ - Step 140 Global step 140 Train loss 7.342851 on epoch=69
03/10/2022 16:15:38 - INFO - __main__ - Step 150 Global step 150 Train loss 5.168255 on epoch=74
03/10/2022 16:15:50 - INFO - __main__ - Global step 150 Train loss 8.090568 ACC 0.1875 on epoch=74
03/10/2022 16:15:55 - INFO - __main__ - Step 160 Global step 160 Train loss 2.294826 on epoch=79
03/10/2022 16:16:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.773373 on epoch=84
03/10/2022 16:16:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.414031 on epoch=89
03/10/2022 16:16:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.178193 on epoch=94
03/10/2022 16:16:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.217295 on epoch=99
03/10/2022 16:16:15 - INFO - __main__ - Global step 200 Train loss 0.775544 ACC 0.6875 on epoch=99
03/10/2022 16:16:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.163465 on epoch=104
03/10/2022 16:16:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.159991 on epoch=109
03/10/2022 16:16:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.156979 on epoch=114
03/10/2022 16:16:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.111174 on epoch=119
03/10/2022 16:16:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.131763 on epoch=124
03/10/2022 16:16:40 - INFO - __main__ - Global step 250 Train loss 0.144674 ACC 0.8125 on epoch=124
03/10/2022 16:16:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.100603 on epoch=129
03/10/2022 16:16:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.126544 on epoch=134
03/10/2022 16:16:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.067830 on epoch=139
03/10/2022 16:17:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.086940 on epoch=144
03/10/2022 16:17:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.079416 on epoch=149
03/10/2022 16:17:05 - INFO - __main__ - Global step 300 Train loss 0.092267 ACC 0.78125 on epoch=149
03/10/2022 16:17:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.045982 on epoch=154
03/10/2022 16:17:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.057917 on epoch=159
03/10/2022 16:17:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.058392 on epoch=164
03/10/2022 16:17:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.037865 on epoch=169
03/10/2022 16:17:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.019619 on epoch=174
03/10/2022 16:17:29 - INFO - __main__ - Global step 350 Train loss 0.043955 ACC 0.8125 on epoch=174
03/10/2022 16:17:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.015937 on epoch=179
03/10/2022 16:17:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.010191 on epoch=184
03/10/2022 16:17:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008807 on epoch=189
03/10/2022 16:17:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.034166 on epoch=194
03/10/2022 16:17:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.010793 on epoch=199
03/10/2022 16:17:53 - INFO - __main__ - Global step 400 Train loss 0.015979 ACC 0.9375 on epoch=199
03/10/2022 16:17:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.021139 on epoch=204
03/10/2022 16:18:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005962 on epoch=209
03/10/2022 16:18:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.025764 on epoch=214
03/10/2022 16:18:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002714 on epoch=219
03/10/2022 16:18:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000945 on epoch=224
03/10/2022 16:18:18 - INFO - __main__ - Global step 450 Train loss 0.011305 ACC 0.9375 on epoch=224
03/10/2022 16:18:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.009860 on epoch=229
03/10/2022 16:18:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003476 on epoch=234
03/10/2022 16:18:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001469 on epoch=239
03/10/2022 16:18:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005406 on epoch=244
03/10/2022 16:18:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000602 on epoch=249
03/10/2022 16:18:43 - INFO - __main__ - Global step 500 Train loss 0.004163 ACC 0.9375 on epoch=249
03/10/2022 16:18:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000863 on epoch=254
03/10/2022 16:18:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003144 on epoch=259
03/10/2022 16:18:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002419 on epoch=264
03/10/2022 16:19:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000457 on epoch=269
03/10/2022 16:19:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000368 on epoch=274
03/10/2022 16:19:07 - INFO - __main__ - Global step 550 Train loss 0.001450 ACC 0.96875 on epoch=274
03/10/2022 16:19:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000183 on epoch=279
03/10/2022 16:19:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003546 on epoch=284
03/10/2022 16:19:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000651 on epoch=289
03/10/2022 16:19:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000285 on epoch=294
03/10/2022 16:19:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000519 on epoch=299
03/10/2022 16:19:32 - INFO - __main__ - Global step 600 Train loss 0.001037 ACC 1.0 on epoch=299
03/10/2022 16:19:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:19:33 - INFO - __main__ - Printing 3 examples
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:19:33 - INFO - __main__ - save last model!
03/10/2022 16:19:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:19:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:19:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:19:33 - INFO - __main__ - Printing 3 examples
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:19:33 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:33 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:19:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:19:33 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:19:40 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:19:40 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:19:40 - INFO - __main__ - Printing 3 examples
03/10/2022 16:19:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:19:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:19:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:19:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:19:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:19:40 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:19:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:19:41 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:19:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.0002_8_predictions.txt
03/10/2022 16:19:43 - INFO - __main__ - ACC on test data: 0.9750
03/10/2022 16:19:43 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.975
03/10/2022 16:19:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:19:43 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0001, bsz=8 ...
03/10/2022 16:19:43 - INFO - __main__ - Starting training!
03/10/2022 16:19:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:19:44 - INFO - __main__ - Printing 3 examples
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:19:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:19:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:19:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:19:44 - INFO - __main__ - Printing 3 examples
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/10/2022 16:19:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:19:44 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:19:45 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:19:45 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:19:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:19:55 - INFO - __main__ - Starting training!
03/10/2022 16:19:59 - INFO - __main__ - Step 10 Global step 10 Train loss 21.520414 on epoch=4
03/10/2022 16:20:03 - INFO - __main__ - Step 20 Global step 20 Train loss 20.693867 on epoch=9
03/10/2022 16:20:08 - INFO - __main__ - Step 30 Global step 30 Train loss 16.376715 on epoch=14
03/10/2022 16:20:12 - INFO - __main__ - Step 40 Global step 40 Train loss 15.717196 on epoch=19
03/10/2022 16:20:17 - INFO - __main__ - Step 50 Global step 50 Train loss 14.985621 on epoch=24
03/10/2022 16:20:22 - INFO - __main__ - Global step 50 Train loss 17.858763 ACC 0.03125 on epoch=24
03/10/2022 16:20:28 - INFO - __main__ - Step 60 Global step 60 Train loss 14.007070 on epoch=29
03/10/2022 16:20:32 - INFO - __main__ - Step 70 Global step 70 Train loss 14.700803 on epoch=34
03/10/2022 16:20:37 - INFO - __main__ - Step 80 Global step 80 Train loss 13.437834 on epoch=39
03/10/2022 16:20:42 - INFO - __main__ - Step 90 Global step 90 Train loss 12.964938 on epoch=44
03/10/2022 16:20:47 - INFO - __main__ - Step 100 Global step 100 Train loss 13.196177 on epoch=49
03/10/2022 16:20:54 - INFO - __main__ - Global step 100 Train loss 13.661365 ACC 0.0625 on epoch=49
03/10/2022 16:21:00 - INFO - __main__ - Step 110 Global step 110 Train loss 13.196882 on epoch=54
03/10/2022 16:21:05 - INFO - __main__ - Step 120 Global step 120 Train loss 12.166459 on epoch=59
03/10/2022 16:21:10 - INFO - __main__ - Step 130 Global step 130 Train loss 11.749885 on epoch=64
03/10/2022 16:21:15 - INFO - __main__ - Step 140 Global step 140 Train loss 12.036438 on epoch=69
03/10/2022 16:21:20 - INFO - __main__ - Step 150 Global step 150 Train loss 11.352624 on epoch=74
03/10/2022 16:21:20 - INFO - __main__ - Global step 150 Train loss 12.100459 ACC 0.0 on epoch=74
03/10/2022 16:21:25 - INFO - __main__ - Step 160 Global step 160 Train loss 11.173084 on epoch=79
03/10/2022 16:21:30 - INFO - __main__ - Step 170 Global step 170 Train loss 10.875451 on epoch=84
03/10/2022 16:21:35 - INFO - __main__ - Step 180 Global step 180 Train loss 10.868365 on epoch=89
03/10/2022 16:21:40 - INFO - __main__ - Step 190 Global step 190 Train loss 10.859062 on epoch=94
03/10/2022 16:21:45 - INFO - __main__ - Step 200 Global step 200 Train loss 10.428431 on epoch=99
03/10/2022 16:21:45 - INFO - __main__ - Global step 200 Train loss 10.840878 ACC 0.03125 on epoch=99
03/10/2022 16:21:50 - INFO - __main__ - Step 210 Global step 210 Train loss 10.137499 on epoch=104
03/10/2022 16:21:55 - INFO - __main__ - Step 220 Global step 220 Train loss 10.138449 on epoch=109
03/10/2022 16:22:00 - INFO - __main__ - Step 230 Global step 230 Train loss 9.379320 on epoch=114
03/10/2022 16:22:05 - INFO - __main__ - Step 240 Global step 240 Train loss 9.375284 on epoch=119
03/10/2022 16:22:10 - INFO - __main__ - Step 250 Global step 250 Train loss 8.278818 on epoch=124
03/10/2022 16:22:11 - INFO - __main__ - Global step 250 Train loss 9.461874 ACC 0.0625 on epoch=124
03/10/2022 16:22:16 - INFO - __main__ - Step 260 Global step 260 Train loss 8.806042 on epoch=129
03/10/2022 16:22:21 - INFO - __main__ - Step 270 Global step 270 Train loss 8.202775 on epoch=134
03/10/2022 16:22:26 - INFO - __main__ - Step 280 Global step 280 Train loss 7.559317 on epoch=139
03/10/2022 16:22:31 - INFO - __main__ - Step 290 Global step 290 Train loss 5.791509 on epoch=144
03/10/2022 16:22:35 - INFO - __main__ - Step 300 Global step 300 Train loss 4.570179 on epoch=149
03/10/2022 16:22:36 - INFO - __main__ - Global step 300 Train loss 6.985964 ACC 0.65625 on epoch=149
03/10/2022 16:22:42 - INFO - __main__ - Step 310 Global step 310 Train loss 3.379651 on epoch=154
03/10/2022 16:22:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.158758 on epoch=159
03/10/2022 16:22:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.690978 on epoch=164
03/10/2022 16:22:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.357889 on epoch=169
03/10/2022 16:23:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.516303 on epoch=174
03/10/2022 16:23:01 - INFO - __main__ - Global step 350 Train loss 1.220716 ACC 0.46875 on epoch=174
03/10/2022 16:23:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.257542 on epoch=179
03/10/2022 16:23:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.200330 on epoch=184
03/10/2022 16:23:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.261484 on epoch=189
03/10/2022 16:23:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.129590 on epoch=194
03/10/2022 16:23:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.268198 on epoch=199
03/10/2022 16:23:26 - INFO - __main__ - Global step 400 Train loss 0.223429 ACC 0.59375 on epoch=199
03/10/2022 16:23:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.171823 on epoch=204
03/10/2022 16:23:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.167912 on epoch=209
03/10/2022 16:23:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.154455 on epoch=214
03/10/2022 16:23:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.144218 on epoch=219
03/10/2022 16:23:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.151575 on epoch=224
03/10/2022 16:23:51 - INFO - __main__ - Global step 450 Train loss 0.157996 ACC 0.59375 on epoch=224
03/10/2022 16:23:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.113960 on epoch=229
03/10/2022 16:24:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.126721 on epoch=234
03/10/2022 16:24:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.205860 on epoch=239
03/10/2022 16:24:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.152876 on epoch=244
03/10/2022 16:24:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.142546 on epoch=249
03/10/2022 16:24:16 - INFO - __main__ - Global step 500 Train loss 0.148393 ACC 0.53125 on epoch=249
03/10/2022 16:24:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.181361 on epoch=254
03/10/2022 16:24:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181695 on epoch=259
03/10/2022 16:24:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.191425 on epoch=264
03/10/2022 16:24:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.151567 on epoch=269
03/10/2022 16:24:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.131834 on epoch=274
03/10/2022 16:24:41 - INFO - __main__ - Global step 550 Train loss 0.167576 ACC 0.5625 on epoch=274
03/10/2022 16:24:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.126839 on epoch=279
03/10/2022 16:24:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.130693 on epoch=284
03/10/2022 16:24:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.121691 on epoch=289
03/10/2022 16:25:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.134057 on epoch=294
03/10/2022 16:25:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.151110 on epoch=299
03/10/2022 16:25:05 - INFO - __main__ - Global step 600 Train loss 0.132878 ACC 0.625 on epoch=299
03/10/2022 16:25:05 - INFO - __main__ - save last model!
03/10/2022 16:25:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:25:06 - INFO - __main__ - Printing 3 examples
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:25:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:25:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:25:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:25:06 - INFO - __main__ - Printing 3 examples
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:25:06 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:06 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:25:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:25:06 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:25:12 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:25:13 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:25:13 - INFO - __main__ - Printing 3 examples
03/10/2022 16:25:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:25:13 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:25:13 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:25:13 - INFO - __main__ - ['sentence 1']
03/10/2022 16:25:13 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:25:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:25:13 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:25:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.0001_8_predictions.txt
03/10/2022 16:25:16 - INFO - __main__ - ACC on test data: 0.5150
03/10/2022 16:25:17 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.0001, bsz=8, dev_performance=0.65625, test_performance=0.515
03/10/2022 16:25:17 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0005, bsz=8 ...
03/10/2022 16:25:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:25:17 - INFO - __main__ - Starting training!
03/10/2022 16:25:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:25:17 - INFO - __main__ - Printing 3 examples
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:25:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:25:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:25:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:25:17 - INFO - __main__ - Printing 3 examples
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:25:17 - INFO - __main__ - ['sentence 2']
03/10/2022 16:25:17 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:25:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:25:18 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:25:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:25:28 - INFO - __main__ - Starting training!
03/10/2022 16:25:32 - INFO - __main__ - Step 10 Global step 10 Train loss 20.540125 on epoch=4
03/10/2022 16:25:36 - INFO - __main__ - Step 20 Global step 20 Train loss 16.485550 on epoch=9
03/10/2022 16:25:41 - INFO - __main__ - Step 30 Global step 30 Train loss 16.041565 on epoch=14
03/10/2022 16:25:46 - INFO - __main__ - Step 40 Global step 40 Train loss 11.519268 on epoch=19
03/10/2022 16:25:51 - INFO - __main__ - Step 50 Global step 50 Train loss 10.925953 on epoch=24
03/10/2022 16:25:51 - INFO - __main__ - Global step 50 Train loss 15.102494 ACC 0.0 on epoch=24
03/10/2022 16:25:56 - INFO - __main__ - Step 60 Global step 60 Train loss 9.735122 on epoch=29
03/10/2022 16:26:01 - INFO - __main__ - Step 70 Global step 70 Train loss 7.073199 on epoch=34
03/10/2022 16:26:06 - INFO - __main__ - Step 80 Global step 80 Train loss 3.271719 on epoch=39
03/10/2022 16:26:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.265892 on epoch=44
03/10/2022 16:26:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.238770 on epoch=49
03/10/2022 16:26:16 - INFO - __main__ - Global step 100 Train loss 4.116940 ACC 0.5 on epoch=49
03/10/2022 16:26:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.266778 on epoch=54
03/10/2022 16:26:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.269384 on epoch=59
03/10/2022 16:26:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.243390 on epoch=64
03/10/2022 16:26:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.231830 on epoch=69
03/10/2022 16:26:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.235171 on epoch=74
03/10/2022 16:26:41 - INFO - __main__ - Global step 150 Train loss 0.249311 ACC 0.5 on epoch=74
03/10/2022 16:26:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.211137 on epoch=79
03/10/2022 16:26:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.220787 on epoch=84
03/10/2022 16:26:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.224402 on epoch=89
03/10/2022 16:27:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.194087 on epoch=94
03/10/2022 16:27:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.239312 on epoch=99
03/10/2022 16:27:06 - INFO - __main__ - Global step 200 Train loss 0.217945 ACC 0.53125 on epoch=99
03/10/2022 16:27:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.191629 on epoch=104
03/10/2022 16:27:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.191165 on epoch=109
03/10/2022 16:27:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.191186 on epoch=114
03/10/2022 16:27:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.182922 on epoch=119
03/10/2022 16:27:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.179327 on epoch=124
03/10/2022 16:27:30 - INFO - __main__ - Global step 250 Train loss 0.187246 ACC 0.46875 on epoch=124
03/10/2022 16:27:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.221542 on epoch=129
03/10/2022 16:27:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.160902 on epoch=134
03/10/2022 16:27:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.131381 on epoch=139
03/10/2022 16:27:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.172063 on epoch=144
03/10/2022 16:27:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.148160 on epoch=149
03/10/2022 16:27:55 - INFO - __main__ - Global step 300 Train loss 0.166810 ACC 0.625 on epoch=149
03/10/2022 16:28:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.124737 on epoch=154
03/10/2022 16:28:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.121642 on epoch=159
03/10/2022 16:28:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.102708 on epoch=164
03/10/2022 16:28:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.096306 on epoch=169
03/10/2022 16:28:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.094199 on epoch=174
03/10/2022 16:28:19 - INFO - __main__ - Global step 350 Train loss 0.107918 ACC 0.59375 on epoch=174
03/10/2022 16:28:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.038348 on epoch=179
03/10/2022 16:28:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.131647 on epoch=184
03/10/2022 16:28:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.150262 on epoch=189
03/10/2022 16:28:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.130401 on epoch=194
03/10/2022 16:28:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.021858 on epoch=199
03/10/2022 16:28:44 - INFO - __main__ - Global step 400 Train loss 0.094503 ACC 0.65625 on epoch=199
03/10/2022 16:28:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.065831 on epoch=204
03/10/2022 16:28:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.037785 on epoch=209
03/10/2022 16:28:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003428 on epoch=214
03/10/2022 16:29:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002564 on epoch=219
03/10/2022 16:29:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005235 on epoch=224
03/10/2022 16:29:08 - INFO - __main__ - Global step 450 Train loss 0.022968 ACC 0.625 on epoch=224
03/10/2022 16:29:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.224494 on epoch=229
03/10/2022 16:29:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.020114 on epoch=234
03/10/2022 16:29:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004540 on epoch=239
03/10/2022 16:29:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002482 on epoch=244
03/10/2022 16:29:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001723 on epoch=249
03/10/2022 16:29:33 - INFO - __main__ - Global step 500 Train loss 0.050671 ACC 0.65625 on epoch=249
03/10/2022 16:29:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001502 on epoch=254
03/10/2022 16:29:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.058365 on epoch=259
03/10/2022 16:29:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.012963 on epoch=264
03/10/2022 16:29:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004390 on epoch=269
03/10/2022 16:29:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002591 on epoch=274
03/10/2022 16:29:56 - INFO - __main__ - Global step 550 Train loss 0.015962 ACC 0.59375 on epoch=274
03/10/2022 16:30:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000713 on epoch=279
03/10/2022 16:30:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000443 on epoch=284
03/10/2022 16:30:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000573 on epoch=289
03/10/2022 16:30:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000214 on epoch=294
03/10/2022 16:30:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000322 on epoch=299
03/10/2022 16:30:21 - INFO - __main__ - Global step 600 Train loss 0.000453 ACC 0.59375 on epoch=299
03/10/2022 16:30:21 - INFO - __main__ - save last model!
03/10/2022 16:30:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:30:21 - INFO - __main__ - Printing 3 examples
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:30:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:30:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:30:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:30:21 - INFO - __main__ - Printing 3 examples
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:30:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:30:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:30:21 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:30:28 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:30:28 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:30:28 - INFO - __main__ - Printing 3 examples
03/10/2022 16:30:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:30:28 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:30:28 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:30:28 - INFO - __main__ - ['sentence 1']
03/10/2022 16:30:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:30:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:30:29 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:30:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:30:31 - INFO - __main__ - Starting training!
03/10/2022 16:30:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.0005_8_predictions.txt
03/10/2022 16:30:31 - INFO - __main__ - ACC on test data: 0.6350
03/10/2022 16:30:31 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.635
03/10/2022 16:30:31 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0003, bsz=8 ...
03/10/2022 16:30:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:30:32 - INFO - __main__ - Printing 3 examples
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:30:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:30:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:30:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:30:32 - INFO - __main__ - Printing 3 examples
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:30:32 - INFO - __main__ - ['sentence 2']
03/10/2022 16:30:32 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:30:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:30:32 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:30:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:30:42 - INFO - __main__ - Starting training!
03/10/2022 16:30:46 - INFO - __main__ - Step 10 Global step 10 Train loss 23.255066 on epoch=4
03/10/2022 16:30:50 - INFO - __main__ - Step 20 Global step 20 Train loss 19.085194 on epoch=9
03/10/2022 16:30:55 - INFO - __main__ - Step 30 Global step 30 Train loss 14.846288 on epoch=14
03/10/2022 16:30:59 - INFO - __main__ - Step 40 Global step 40 Train loss 13.658075 on epoch=19
03/10/2022 16:31:04 - INFO - __main__ - Step 50 Global step 50 Train loss 12.371351 on epoch=24
03/10/2022 16:31:05 - INFO - __main__ - Global step 50 Train loss 16.643194 ACC 0.15625 on epoch=24
03/10/2022 16:31:11 - INFO - __main__ - Step 60 Global step 60 Train loss 11.735617 on epoch=29
03/10/2022 16:31:15 - INFO - __main__ - Step 70 Global step 70 Train loss 11.133292 on epoch=34
03/10/2022 16:31:20 - INFO - __main__ - Step 80 Global step 80 Train loss 9.679526 on epoch=39
03/10/2022 16:31:25 - INFO - __main__ - Step 90 Global step 90 Train loss 9.044146 on epoch=44
03/10/2022 16:31:30 - INFO - __main__ - Step 100 Global step 100 Train loss 8.313853 on epoch=49
03/10/2022 16:31:30 - INFO - __main__ - Global step 100 Train loss 9.981287 ACC 0.03125 on epoch=49
03/10/2022 16:31:35 - INFO - __main__ - Step 110 Global step 110 Train loss 4.228814 on epoch=54
03/10/2022 16:31:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.278270 on epoch=59
03/10/2022 16:31:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.214957 on epoch=64
03/10/2022 16:31:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.253623 on epoch=69
03/10/2022 16:31:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.172312 on epoch=74
03/10/2022 16:31:54 - INFO - __main__ - Global step 150 Train loss 1.029595 ACC 0.71875 on epoch=74
03/10/2022 16:32:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.115130 on epoch=79
03/10/2022 16:32:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.142186 on epoch=84
03/10/2022 16:32:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.115169 on epoch=89
03/10/2022 16:32:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.104799 on epoch=94
03/10/2022 16:32:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.083393 on epoch=99
03/10/2022 16:32:20 - INFO - __main__ - Global step 200 Train loss 0.112136 ACC 0.8125 on epoch=99
03/10/2022 16:32:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.087844 on epoch=104
03/10/2022 16:32:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.060426 on epoch=109
03/10/2022 16:32:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.046400 on epoch=114
03/10/2022 16:32:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.027861 on epoch=119
03/10/2022 16:32:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.034696 on epoch=124
03/10/2022 16:32:45 - INFO - __main__ - Global step 250 Train loss 0.051445 ACC 0.8125 on epoch=124
03/10/2022 16:32:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.020598 on epoch=129
03/10/2022 16:32:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.015845 on epoch=134
03/10/2022 16:32:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.015338 on epoch=139
03/10/2022 16:33:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002535 on epoch=144
03/10/2022 16:33:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005850 on epoch=149
03/10/2022 16:33:09 - INFO - __main__ - Global step 300 Train loss 0.012033 ACC 0.8125 on epoch=149
03/10/2022 16:33:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003683 on epoch=154
03/10/2022 16:33:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001420 on epoch=159
03/10/2022 16:33:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002126 on epoch=164
03/10/2022 16:33:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001492 on epoch=169
03/10/2022 16:33:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020495 on epoch=174
03/10/2022 16:33:34 - INFO - __main__ - Global step 350 Train loss 0.005843 ACC 0.84375 on epoch=174
03/10/2022 16:33:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001585 on epoch=179
03/10/2022 16:33:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000875 on epoch=184
03/10/2022 16:33:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001080 on epoch=189
03/10/2022 16:33:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000825 on epoch=194
03/10/2022 16:33:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.048378 on epoch=199
03/10/2022 16:34:00 - INFO - __main__ - Global step 400 Train loss 0.010549 ACC 1.0 on epoch=199
03/10/2022 16:34:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003946 on epoch=204
03/10/2022 16:34:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001725 on epoch=209
03/10/2022 16:34:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000559 on epoch=214
03/10/2022 16:34:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001055 on epoch=219
03/10/2022 16:34:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000335 on epoch=224
03/10/2022 16:34:25 - INFO - __main__ - Global step 450 Train loss 0.001524 ACC 0.96875 on epoch=224
03/10/2022 16:34:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000269 on epoch=229
03/10/2022 16:34:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000220 on epoch=234
03/10/2022 16:34:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000161 on epoch=239
03/10/2022 16:34:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001265 on epoch=244
03/10/2022 16:34:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000440 on epoch=249
03/10/2022 16:34:50 - INFO - __main__ - Global step 500 Train loss 0.000471 ACC 1.0 on epoch=249
03/10/2022 16:34:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000147 on epoch=254
03/10/2022 16:35:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000093 on epoch=259
03/10/2022 16:35:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000479 on epoch=264
03/10/2022 16:35:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.019185 on epoch=269
03/10/2022 16:35:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001641 on epoch=274
03/10/2022 16:35:15 - INFO - __main__ - Global step 550 Train loss 0.004309 ACC 1.0 on epoch=274
03/10/2022 16:35:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000361 on epoch=279
03/10/2022 16:35:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000514 on epoch=284
03/10/2022 16:35:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000209 on epoch=289
03/10/2022 16:35:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000185 on epoch=294
03/10/2022 16:35:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000398 on epoch=299
03/10/2022 16:35:39 - INFO - __main__ - Global step 600 Train loss 0.000333 ACC 1.0 on epoch=299
03/10/2022 16:35:39 - INFO - __main__ - save last model!
03/10/2022 16:35:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:35:40 - INFO - __main__ - Printing 3 examples
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:35:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:35:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:35:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:35:40 - INFO - __main__ - Printing 3 examples
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:35:40 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:35:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:35:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:35:47 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:35:47 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:35:47 - INFO - __main__ - Printing 3 examples
03/10/2022 16:35:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:35:47 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:35:47 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:35:47 - INFO - __main__ - ['sentence 1']
03/10/2022 16:35:47 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:35:47 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:35:48 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:35:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:35:51 - INFO - __main__ - Starting training!
03/10/2022 16:35:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.0003_8_predictions.txt
03/10/2022 16:35:51 - INFO - __main__ - ACC on test data: 0.9850
03/10/2022 16:35:52 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.985
03/10/2022 16:35:52 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0002, bsz=8 ...
03/10/2022 16:35:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:35:53 - INFO - __main__ - Printing 3 examples
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:35:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:35:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:35:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:35:53 - INFO - __main__ - Printing 3 examples
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:35:53 - INFO - __main__ - ['sentence 2']
03/10/2022 16:35:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:35:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:35:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:36:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:36:02 - INFO - __main__ - Starting training!
03/10/2022 16:36:06 - INFO - __main__ - Step 10 Global step 10 Train loss 21.696014 on epoch=4
03/10/2022 16:36:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.894796 on epoch=9
03/10/2022 16:36:16 - INFO - __main__ - Step 30 Global step 30 Train loss 15.965030 on epoch=14
03/10/2022 16:36:21 - INFO - __main__ - Step 40 Global step 40 Train loss 14.245520 on epoch=19
03/10/2022 16:36:26 - INFO - __main__ - Step 50 Global step 50 Train loss 12.798762 on epoch=24
03/10/2022 16:36:26 - INFO - __main__ - Global step 50 Train loss 16.720026 ACC 0.0 on epoch=24
03/10/2022 16:36:32 - INFO - __main__ - Step 60 Global step 60 Train loss 12.446040 on epoch=29
03/10/2022 16:36:36 - INFO - __main__ - Step 70 Global step 70 Train loss 12.099894 on epoch=34
03/10/2022 16:36:41 - INFO - __main__ - Step 80 Global step 80 Train loss 11.082358 on epoch=39
03/10/2022 16:36:46 - INFO - __main__ - Step 90 Global step 90 Train loss 10.630258 on epoch=44
03/10/2022 16:36:51 - INFO - __main__ - Step 100 Global step 100 Train loss 10.271360 on epoch=49
03/10/2022 16:36:51 - INFO - __main__ - Global step 100 Train loss 11.305982 ACC 0.0 on epoch=49
03/10/2022 16:36:56 - INFO - __main__ - Step 110 Global step 110 Train loss 9.761762 on epoch=54
03/10/2022 16:37:01 - INFO - __main__ - Step 120 Global step 120 Train loss 9.182843 on epoch=59
03/10/2022 16:37:06 - INFO - __main__ - Step 130 Global step 130 Train loss 8.425337 on epoch=64
03/10/2022 16:37:10 - INFO - __main__ - Step 140 Global step 140 Train loss 7.463411 on epoch=69
03/10/2022 16:37:15 - INFO - __main__ - Step 150 Global step 150 Train loss 6.273029 on epoch=74
03/10/2022 16:37:16 - INFO - __main__ - Global step 150 Train loss 8.221276 ACC 0.4375 on epoch=74
03/10/2022 16:37:22 - INFO - __main__ - Step 160 Global step 160 Train loss 4.817022 on epoch=79
03/10/2022 16:37:26 - INFO - __main__ - Step 170 Global step 170 Train loss 2.274068 on epoch=84
03/10/2022 16:37:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.616404 on epoch=89
03/10/2022 16:37:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.234042 on epoch=94
03/10/2022 16:37:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.265628 on epoch=99
03/10/2022 16:37:41 - INFO - __main__ - Global step 200 Train loss 1.841433 ACC 0.8125 on epoch=99
03/10/2022 16:37:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.187042 on epoch=104
03/10/2022 16:37:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.195057 on epoch=109
03/10/2022 16:37:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.233763 on epoch=114
03/10/2022 16:38:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.189521 on epoch=119
03/10/2022 16:38:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.185239 on epoch=124
03/10/2022 16:38:06 - INFO - __main__ - Global step 250 Train loss 0.198124 ACC 0.6875 on epoch=124
03/10/2022 16:38:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.153505 on epoch=129
03/10/2022 16:38:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.189513 on epoch=134
03/10/2022 16:38:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.183061 on epoch=139
03/10/2022 16:38:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.163730 on epoch=144
03/10/2022 16:38:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.132207 on epoch=149
03/10/2022 16:38:30 - INFO - __main__ - Global step 300 Train loss 0.164403 ACC 0.8125 on epoch=149
03/10/2022 16:38:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.096277 on epoch=154
03/10/2022 16:38:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.082019 on epoch=159
03/10/2022 16:38:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.073504 on epoch=164
03/10/2022 16:38:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.094557 on epoch=169
03/10/2022 16:38:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.140649 on epoch=174
03/10/2022 16:38:55 - INFO - __main__ - Global step 350 Train loss 0.097401 ACC 0.625 on epoch=174
03/10/2022 16:39:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.130123 on epoch=179
03/10/2022 16:39:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.120557 on epoch=184
03/10/2022 16:39:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.090925 on epoch=189
03/10/2022 16:39:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.062090 on epoch=194
03/10/2022 16:39:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.064394 on epoch=199
03/10/2022 16:39:19 - INFO - __main__ - Global step 400 Train loss 0.093618 ACC 0.71875 on epoch=199
03/10/2022 16:39:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.105630 on epoch=204
03/10/2022 16:39:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.062553 on epoch=209
03/10/2022 16:39:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.044516 on epoch=214
03/10/2022 16:39:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.044189 on epoch=219
03/10/2022 16:39:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.028847 on epoch=224
03/10/2022 16:39:43 - INFO - __main__ - Global step 450 Train loss 0.057147 ACC 0.8125 on epoch=224
03/10/2022 16:39:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.046846 on epoch=229
03/10/2022 16:39:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.024470 on epoch=234
03/10/2022 16:39:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.015935 on epoch=239
03/10/2022 16:40:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.008793 on epoch=244
03/10/2022 16:40:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.015607 on epoch=249
03/10/2022 16:40:08 - INFO - __main__ - Global step 500 Train loss 0.022330 ACC 0.78125 on epoch=249
03/10/2022 16:40:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.020848 on epoch=254
03/10/2022 16:40:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.018861 on epoch=259
03/10/2022 16:40:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.022989 on epoch=264
03/10/2022 16:40:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003637 on epoch=269
03/10/2022 16:40:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002279 on epoch=274
03/10/2022 16:40:32 - INFO - __main__ - Global step 550 Train loss 0.013723 ACC 0.9375 on epoch=274
03/10/2022 16:40:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018849 on epoch=279
03/10/2022 16:40:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004773 on epoch=284
03/10/2022 16:40:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003435 on epoch=289
03/10/2022 16:40:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001823 on epoch=294
03/10/2022 16:40:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003040 on epoch=299
03/10/2022 16:40:57 - INFO - __main__ - Global step 600 Train loss 0.006384 ACC 0.8125 on epoch=299
03/10/2022 16:40:57 - INFO - __main__ - save last model!
03/10/2022 16:40:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:40:58 - INFO - __main__ - Printing 3 examples
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:40:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:40:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:40:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:40:58 - INFO - __main__ - Printing 3 examples
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:40:58 - INFO - __main__ - ['sentence 2']
03/10/2022 16:40:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:40:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:40:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:41:04 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:41:05 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:41:05 - INFO - __main__ - Printing 3 examples
03/10/2022 16:41:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:41:05 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:41:05 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:41:05 - INFO - __main__ - ['sentence 1']
03/10/2022 16:41:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:41:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:41:05 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:41:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.0002_8_predictions.txt
03/10/2022 16:41:07 - INFO - __main__ - ACC on test data: 0.8350
03/10/2022 16:41:08 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.835
03/10/2022 16:41:08 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0001, bsz=8 ...
03/10/2022 16:41:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:41:08 - INFO - __main__ - Starting training!
03/10/2022 16:41:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:41:09 - INFO - __main__ - Printing 3 examples
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:41:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:41:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:41:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:41:09 - INFO - __main__ - Printing 3 examples
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/10/2022 16:41:09 - INFO - __main__ - ['sentence 2']
03/10/2022 16:41:09 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:41:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:41:09 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:41:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:41:19 - INFO - __main__ - Starting training!
03/10/2022 16:41:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.246031 on epoch=4
03/10/2022 16:41:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.046362 on epoch=9
03/10/2022 16:41:34 - INFO - __main__ - Step 30 Global step 30 Train loss 16.379324 on epoch=14
03/10/2022 16:41:39 - INFO - __main__ - Step 40 Global step 40 Train loss 15.314173 on epoch=19
03/10/2022 16:41:44 - INFO - __main__ - Step 50 Global step 50 Train loss 14.250662 on epoch=24
03/10/2022 16:41:56 - INFO - __main__ - Global step 50 Train loss 17.447309 ACC 0.0 on epoch=24
03/10/2022 16:42:01 - INFO - __main__ - Step 60 Global step 60 Train loss 14.288646 on epoch=29
03/10/2022 16:42:06 - INFO - __main__ - Step 70 Global step 70 Train loss 13.871509 on epoch=34
03/10/2022 16:42:11 - INFO - __main__ - Step 80 Global step 80 Train loss 13.263733 on epoch=39
03/10/2022 16:42:16 - INFO - __main__ - Step 90 Global step 90 Train loss 13.695287 on epoch=44
03/10/2022 16:42:20 - INFO - __main__ - Step 100 Global step 100 Train loss 12.848608 on epoch=49
03/10/2022 16:42:26 - INFO - __main__ - Global step 100 Train loss 13.593557 ACC 0.0 on epoch=49
03/10/2022 16:42:31 - INFO - __main__ - Step 110 Global step 110 Train loss 11.933740 on epoch=54
03/10/2022 16:42:36 - INFO - __main__ - Step 120 Global step 120 Train loss 12.326658 on epoch=59
03/10/2022 16:42:41 - INFO - __main__ - Step 130 Global step 130 Train loss 12.073673 on epoch=64
03/10/2022 16:42:45 - INFO - __main__ - Step 140 Global step 140 Train loss 12.108442 on epoch=69
03/10/2022 16:42:50 - INFO - __main__ - Step 150 Global step 150 Train loss 11.644347 on epoch=74
03/10/2022 16:42:51 - INFO - __main__ - Global step 150 Train loss 12.017372 ACC 0.03125 on epoch=74
03/10/2022 16:42:57 - INFO - __main__ - Step 160 Global step 160 Train loss 11.141673 on epoch=79
03/10/2022 16:43:02 - INFO - __main__ - Step 170 Global step 170 Train loss 11.437210 on epoch=84
03/10/2022 16:43:07 - INFO - __main__ - Step 180 Global step 180 Train loss 10.955274 on epoch=89
03/10/2022 16:43:11 - INFO - __main__ - Step 190 Global step 190 Train loss 10.388656 on epoch=94
03/10/2022 16:43:16 - INFO - __main__ - Step 200 Global step 200 Train loss 9.955687 on epoch=99
03/10/2022 16:43:17 - INFO - __main__ - Global step 200 Train loss 10.775700 ACC 0.125 on epoch=99
03/10/2022 16:43:23 - INFO - __main__ - Step 210 Global step 210 Train loss 10.118002 on epoch=104
03/10/2022 16:43:27 - INFO - __main__ - Step 220 Global step 220 Train loss 10.042079 on epoch=109
03/10/2022 16:43:32 - INFO - __main__ - Step 230 Global step 230 Train loss 9.451454 on epoch=114
03/10/2022 16:43:37 - INFO - __main__ - Step 240 Global step 240 Train loss 9.303553 on epoch=119
03/10/2022 16:43:42 - INFO - __main__ - Step 250 Global step 250 Train loss 8.940513 on epoch=124
03/10/2022 16:43:43 - INFO - __main__ - Global step 250 Train loss 9.571120 ACC 0.0625 on epoch=124
03/10/2022 16:43:48 - INFO - __main__ - Step 260 Global step 260 Train loss 8.551973 on epoch=129
03/10/2022 16:43:52 - INFO - __main__ - Step 270 Global step 270 Train loss 8.396875 on epoch=134
03/10/2022 16:43:57 - INFO - __main__ - Step 280 Global step 280 Train loss 7.462799 on epoch=139
03/10/2022 16:44:02 - INFO - __main__ - Step 290 Global step 290 Train loss 6.817893 on epoch=144
03/10/2022 16:44:07 - INFO - __main__ - Step 300 Global step 300 Train loss 5.275382 on epoch=149
03/10/2022 16:44:07 - INFO - __main__ - Global step 300 Train loss 7.300984 ACC 0.59375 on epoch=149
03/10/2022 16:44:13 - INFO - __main__ - Step 310 Global step 310 Train loss 2.255585 on epoch=154
03/10/2022 16:44:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.526026 on epoch=159
03/10/2022 16:44:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.360822 on epoch=164
03/10/2022 16:44:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.132821 on epoch=169
03/10/2022 16:44:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.163337 on epoch=174
03/10/2022 16:44:32 - INFO - __main__ - Global step 350 Train loss 0.687718 ACC 0.5625 on epoch=174
03/10/2022 16:44:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.138662 on epoch=179
03/10/2022 16:44:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.186690 on epoch=184
03/10/2022 16:44:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.237313 on epoch=189
03/10/2022 16:44:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.126340 on epoch=194
03/10/2022 16:44:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.245756 on epoch=199
03/10/2022 16:44:57 - INFO - __main__ - Global step 400 Train loss 0.186952 ACC 0.625 on epoch=199
03/10/2022 16:45:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.103095 on epoch=204
03/10/2022 16:45:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.079256 on epoch=209
03/10/2022 16:45:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.145978 on epoch=214
03/10/2022 16:45:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.068357 on epoch=219
03/10/2022 16:45:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.057715 on epoch=224
03/10/2022 16:45:22 - INFO - __main__ - Global step 450 Train loss 0.090880 ACC 0.65625 on epoch=224
03/10/2022 16:45:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.071408 on epoch=229
03/10/2022 16:45:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.063025 on epoch=234
03/10/2022 16:45:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.057781 on epoch=239
03/10/2022 16:45:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.041274 on epoch=244
03/10/2022 16:45:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.068717 on epoch=249
03/10/2022 16:45:47 - INFO - __main__ - Global step 500 Train loss 0.060441 ACC 0.71875 on epoch=249
03/10/2022 16:45:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.240361 on epoch=254
03/10/2022 16:45:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.031969 on epoch=259
03/10/2022 16:46:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.040614 on epoch=264
03/10/2022 16:46:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.042003 on epoch=269
03/10/2022 16:46:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.043218 on epoch=274
03/10/2022 16:46:12 - INFO - __main__ - Global step 550 Train loss 0.079633 ACC 0.625 on epoch=274
03/10/2022 16:46:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.027699 on epoch=279
03/10/2022 16:46:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015918 on epoch=284
03/10/2022 16:46:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.035582 on epoch=289
03/10/2022 16:46:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.024103 on epoch=294
03/10/2022 16:46:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.031089 on epoch=299
03/10/2022 16:46:36 - INFO - __main__ - Global step 600 Train loss 0.026878 ACC 0.6875 on epoch=299
03/10/2022 16:46:36 - INFO - __main__ - save last model!
03/10/2022 16:46:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:37 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:46:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:46:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:37 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:46:37 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:37 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:46:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:37 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:46:43 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:46:44 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:46:44 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:46:44 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:46:44 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:44 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:46:44 - INFO - __main__ - ['sentence 1']
03/10/2022 16:46:44 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:46:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:44 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:46:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:46:46 - INFO - __main__ - Starting training!
03/10/2022 16:46:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.0001_8_predictions.txt
03/10/2022 16:46:47 - INFO - __main__ - ACC on test data: 0.6600
03/10/2022 16:46:48 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.0001, bsz=8, dev_performance=0.71875, test_performance=0.66
03/10/2022 16:46:48 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0005, bsz=8 ...
03/10/2022 16:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:49 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:46:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:49 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:46:49 - INFO - __main__ - ['sentence 2']
03/10/2022 16:46:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:46:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:46:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:46:58 - INFO - __main__ - Starting training!
03/10/2022 16:47:03 - INFO - __main__ - Step 10 Global step 10 Train loss 21.341066 on epoch=4
03/10/2022 16:47:07 - INFO - __main__ - Step 20 Global step 20 Train loss 15.703560 on epoch=9
03/10/2022 16:47:12 - INFO - __main__ - Step 30 Global step 30 Train loss 13.368204 on epoch=14
03/10/2022 16:47:17 - INFO - __main__ - Step 40 Global step 40 Train loss 11.562447 on epoch=19
03/10/2022 16:47:22 - INFO - __main__ - Step 50 Global step 50 Train loss 9.849377 on epoch=24
03/10/2022 16:47:23 - INFO - __main__ - Global step 50 Train loss 14.364930 ACC 0.0 on epoch=24
03/10/2022 16:47:28 - INFO - __main__ - Step 60 Global step 60 Train loss 8.736064 on epoch=29
03/10/2022 16:47:33 - INFO - __main__ - Step 70 Global step 70 Train loss 5.135440 on epoch=34
03/10/2022 16:47:38 - INFO - __main__ - Step 80 Global step 80 Train loss 2.572262 on epoch=39
03/10/2022 16:47:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.292864 on epoch=44
03/10/2022 16:47:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.286153 on epoch=49
03/10/2022 16:47:48 - INFO - __main__ - Global step 100 Train loss 3.404557 ACC 0.46875 on epoch=49
03/10/2022 16:47:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.262134 on epoch=54
03/10/2022 16:47:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.247482 on epoch=59
03/10/2022 16:48:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.240224 on epoch=64
03/10/2022 16:48:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.241070 on epoch=69
03/10/2022 16:48:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.241516 on epoch=74
03/10/2022 16:48:13 - INFO - __main__ - Global step 150 Train loss 0.246485 ACC 0.5 on epoch=74
03/10/2022 16:48:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.241746 on epoch=79
03/10/2022 16:48:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.254578 on epoch=84
03/10/2022 16:48:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.246130 on epoch=89
03/10/2022 16:48:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.233744 on epoch=94
03/10/2022 16:48:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.259565 on epoch=99
03/10/2022 16:48:38 - INFO - __main__ - Global step 200 Train loss 0.247153 ACC 0.5 on epoch=99
03/10/2022 16:48:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.244234 on epoch=104
03/10/2022 16:48:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.238065 on epoch=109
03/10/2022 16:48:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.236181 on epoch=114
03/10/2022 16:48:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.247241 on epoch=119
03/10/2022 16:49:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.241779 on epoch=124
03/10/2022 16:49:03 - INFO - __main__ - Global step 250 Train loss 0.241500 ACC 0.5 on epoch=124
03/10/2022 16:49:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.243913 on epoch=129
03/10/2022 16:49:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.229791 on epoch=134
03/10/2022 16:49:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.242102 on epoch=139
03/10/2022 16:49:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.236546 on epoch=144
03/10/2022 16:49:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.238228 on epoch=149
03/10/2022 16:49:27 - INFO - __main__ - Global step 300 Train loss 0.238116 ACC 0.5 on epoch=149
03/10/2022 16:49:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.243063 on epoch=154
03/10/2022 16:49:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.222035 on epoch=159
03/10/2022 16:49:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.243276 on epoch=164
03/10/2022 16:49:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.230977 on epoch=169
03/10/2022 16:49:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.224921 on epoch=174
03/10/2022 16:49:51 - INFO - __main__ - Global step 350 Train loss 0.232854 ACC 0.5 on epoch=174
03/10/2022 16:49:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.208543 on epoch=179
03/10/2022 16:50:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.199436 on epoch=184
03/10/2022 16:50:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.137364 on epoch=189
03/10/2022 16:50:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.166880 on epoch=194
03/10/2022 16:50:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.106808 on epoch=199
03/10/2022 16:50:15 - INFO - __main__ - Global step 400 Train loss 0.163806 ACC 0.53125 on epoch=199
03/10/2022 16:50:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.184773 on epoch=204
03/10/2022 16:50:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.073837 on epoch=209
03/10/2022 16:50:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.048928 on epoch=214
03/10/2022 16:50:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.019904 on epoch=219
03/10/2022 16:50:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.036195 on epoch=224
03/10/2022 16:50:40 - INFO - __main__ - Global step 450 Train loss 0.072727 ACC 0.5 on epoch=224
03/10/2022 16:50:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.104823 on epoch=229
03/10/2022 16:50:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.005456 on epoch=234
03/10/2022 16:50:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003588 on epoch=239
03/10/2022 16:50:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.014691 on epoch=244
03/10/2022 16:51:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.188426 on epoch=249
03/10/2022 16:51:04 - INFO - __main__ - Global step 500 Train loss 0.063397 ACC 0.5 on epoch=249
03/10/2022 16:51:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.074585 on epoch=254
03/10/2022 16:51:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016318 on epoch=259
03/10/2022 16:51:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008478 on epoch=264
03/10/2022 16:51:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000747 on epoch=269
03/10/2022 16:51:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001091 on epoch=274
03/10/2022 16:51:29 - INFO - __main__ - Global step 550 Train loss 0.020244 ACC 0.40625 on epoch=274
03/10/2022 16:51:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001584 on epoch=279
03/10/2022 16:51:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.111325 on epoch=284
03/10/2022 16:51:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002432 on epoch=289
03/10/2022 16:51:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003030 on epoch=294
03/10/2022 16:51:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000985 on epoch=299
03/10/2022 16:51:53 - INFO - __main__ - Global step 600 Train loss 0.023871 ACC 0.4375 on epoch=299
03/10/2022 16:51:53 - INFO - __main__ - save last model!
03/10/2022 16:51:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:51:54 - INFO - __main__ - Printing 3 examples
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:51:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:51:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:51:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:51:54 - INFO - __main__ - Printing 3 examples
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:51:54 - INFO - __main__ - ['sentence 2']
03/10/2022 16:51:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:51:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:51:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:52:00 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:52:01 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:52:01 - INFO - __main__ - Printing 3 examples
03/10/2022 16:52:01 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:52:01 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:01 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:52:01 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:01 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:52:01 - INFO - __main__ - ['sentence 1']
03/10/2022 16:52:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:52:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:52:01 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:52:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:52:03 - INFO - __main__ - Starting training!
03/10/2022 16:52:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.0005_8_predictions.txt
03/10/2022 16:52:03 - INFO - __main__ - ACC on test data: 0.4850
03/10/2022 16:52:03 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.485
03/10/2022 16:52:03 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0003, bsz=8 ...
03/10/2022 16:52:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:52:04 - INFO - __main__ - Printing 3 examples
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:52:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:52:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:52:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:52:04 - INFO - __main__ - Printing 3 examples
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:52:04 - INFO - __main__ - ['sentence 2']
03/10/2022 16:52:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:52:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:52:04 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:52:14 - INFO - __main__ - Starting training!
03/10/2022 16:52:18 - INFO - __main__ - Step 10 Global step 10 Train loss 21.476349 on epoch=4
03/10/2022 16:52:23 - INFO - __main__ - Step 20 Global step 20 Train loss 18.122730 on epoch=9
03/10/2022 16:52:27 - INFO - __main__ - Step 30 Global step 30 Train loss 15.164665 on epoch=14
03/10/2022 16:52:32 - INFO - __main__ - Step 40 Global step 40 Train loss 12.952278 on epoch=19
03/10/2022 16:52:37 - INFO - __main__ - Step 50 Global step 50 Train loss 11.790056 on epoch=24
03/10/2022 16:52:38 - INFO - __main__ - Global step 50 Train loss 15.901216 ACC 0.0625 on epoch=24
03/10/2022 16:52:43 - INFO - __main__ - Step 60 Global step 60 Train loss 11.303529 on epoch=29
03/10/2022 16:52:48 - INFO - __main__ - Step 70 Global step 70 Train loss 10.402487 on epoch=34
03/10/2022 16:52:53 - INFO - __main__ - Step 80 Global step 80 Train loss 9.314753 on epoch=39
03/10/2022 16:52:58 - INFO - __main__ - Step 90 Global step 90 Train loss 8.476855 on epoch=44
03/10/2022 16:53:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.247907 on epoch=49
03/10/2022 16:53:03 - INFO - __main__ - Global step 100 Train loss 9.149106 ACC 0.0 on epoch=49
03/10/2022 16:53:08 - INFO - __main__ - Step 110 Global step 110 Train loss 3.606747 on epoch=54
03/10/2022 16:53:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.007540 on epoch=59
03/10/2022 16:53:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.252045 on epoch=64
03/10/2022 16:53:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.184398 on epoch=69
03/10/2022 16:53:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.188707 on epoch=74
03/10/2022 16:53:28 - INFO - __main__ - Global step 150 Train loss 1.047887 ACC 0.5625 on epoch=74
03/10/2022 16:53:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.150179 on epoch=79
03/10/2022 16:53:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.184737 on epoch=84
03/10/2022 16:53:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.122557 on epoch=89
03/10/2022 16:53:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.126285 on epoch=94
03/10/2022 16:53:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.130572 on epoch=99
03/10/2022 16:53:54 - INFO - __main__ - Global step 200 Train loss 0.142866 ACC 0.75 on epoch=99
03/10/2022 16:53:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.135208 on epoch=104
03/10/2022 16:54:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.097823 on epoch=109
03/10/2022 16:54:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.083687 on epoch=114
03/10/2022 16:54:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.091069 on epoch=119
03/10/2022 16:54:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.052390 on epoch=124
03/10/2022 16:54:19 - INFO - __main__ - Global step 250 Train loss 0.092035 ACC 1.0 on epoch=124
03/10/2022 16:54:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.056204 on epoch=129
03/10/2022 16:54:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.032996 on epoch=134
03/10/2022 16:54:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.017989 on epoch=139
03/10/2022 16:54:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.012724 on epoch=144
03/10/2022 16:54:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.003810 on epoch=149
03/10/2022 16:54:45 - INFO - __main__ - Global step 300 Train loss 0.024744 ACC 1.0 on epoch=149
03/10/2022 16:54:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.011435 on epoch=154
03/10/2022 16:54:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.168293 on epoch=159
03/10/2022 16:55:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.380045 on epoch=164
03/10/2022 16:55:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.278473 on epoch=169
03/10/2022 16:55:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.193123 on epoch=174
03/10/2022 16:55:10 - INFO - __main__ - Global step 350 Train loss 0.206274 ACC 1.0 on epoch=174
03/10/2022 16:55:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.212470 on epoch=179
03/10/2022 16:55:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.253483 on epoch=184
03/10/2022 16:55:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.204678 on epoch=189
03/10/2022 16:55:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.246972 on epoch=194
03/10/2022 16:55:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.239392 on epoch=199
03/10/2022 16:55:34 - INFO - __main__ - Global step 400 Train loss 0.231399 ACC 0.5 on epoch=199
03/10/2022 16:55:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.238875 on epoch=204
03/10/2022 16:55:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.205263 on epoch=209
03/10/2022 16:55:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.191544 on epoch=214
03/10/2022 16:55:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.225364 on epoch=219
03/10/2022 16:55:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.195508 on epoch=224
03/10/2022 16:55:59 - INFO - __main__ - Global step 450 Train loss 0.211311 ACC 0.5625 on epoch=224
03/10/2022 16:56:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.209056 on epoch=229
03/10/2022 16:56:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.082024 on epoch=234
03/10/2022 16:56:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.054286 on epoch=239
03/10/2022 16:56:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.073458 on epoch=244
03/10/2022 16:56:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.121215 on epoch=249
03/10/2022 16:56:24 - INFO - __main__ - Global step 500 Train loss 0.108008 ACC 0.78125 on epoch=249
03/10/2022 16:56:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.038657 on epoch=254
03/10/2022 16:56:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.030121 on epoch=259
03/10/2022 16:56:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.036088 on epoch=264
03/10/2022 16:56:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.030342 on epoch=269
03/10/2022 16:56:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023405 on epoch=274
03/10/2022 16:56:49 - INFO - __main__ - Global step 550 Train loss 0.031723 ACC 0.84375 on epoch=274
03/10/2022 16:56:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009809 on epoch=279
03/10/2022 16:56:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.005478 on epoch=284
03/10/2022 16:57:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012841 on epoch=289
03/10/2022 16:57:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002355 on epoch=294
03/10/2022 16:57:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.014044 on epoch=299
03/10/2022 16:57:13 - INFO - __main__ - Global step 600 Train loss 0.008905 ACC 0.9375 on epoch=299
03/10/2022 16:57:13 - INFO - __main__ - save last model!
03/10/2022 16:57:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:57:14 - INFO - __main__ - Printing 3 examples
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:57:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:57:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:57:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:57:14 - INFO - __main__ - Printing 3 examples
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:57:14 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:57:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:57:14 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:57:21 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 16:57:21 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 16:57:21 - INFO - __main__ - Printing 3 examples
03/10/2022 16:57:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 16:57:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 16:57:21 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 16:57:21 - INFO - __main__ - ['sentence 1']
03/10/2022 16:57:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:57:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:57:22 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 16:57:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.0003_8_predictions.txt
03/10/2022 16:57:24 - INFO - __main__ - ACC on test data: 0.9950
03/10/2022 16:57:24 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.995
03/10/2022 16:57:24 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0002, bsz=8 ...
03/10/2022 16:57:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:57:25 - INFO - __main__ - Starting training!
03/10/2022 16:57:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:57:25 - INFO - __main__ - Printing 3 examples
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:57:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:57:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:57:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:57:25 - INFO - __main__ - Printing 3 examples
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 16:57:25 - INFO - __main__ - ['sentence 2']
03/10/2022 16:57:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:57:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:57:26 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:57:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 16:57:35 - INFO - __main__ - Starting training!
03/10/2022 16:57:39 - INFO - __main__ - Step 10 Global step 10 Train loss 21.467445 on epoch=4
03/10/2022 16:57:43 - INFO - __main__ - Step 20 Global step 20 Train loss 17.791243 on epoch=9
03/10/2022 16:57:48 - INFO - __main__ - Step 30 Global step 30 Train loss 15.112215 on epoch=14
03/10/2022 16:57:53 - INFO - __main__ - Step 40 Global step 40 Train loss 14.735227 on epoch=19
03/10/2022 16:57:57 - INFO - __main__ - Step 50 Global step 50 Train loss 14.252240 on epoch=24
03/10/2022 16:57:58 - INFO - __main__ - Global step 50 Train loss 16.671673 ACC 0.0 on epoch=24
03/10/2022 16:58:04 - INFO - __main__ - Step 60 Global step 60 Train loss 13.478844 on epoch=29
03/10/2022 16:58:08 - INFO - __main__ - Step 70 Global step 70 Train loss 12.391794 on epoch=34
03/10/2022 16:58:13 - INFO - __main__ - Step 80 Global step 80 Train loss 11.880686 on epoch=39
03/10/2022 16:58:18 - INFO - __main__ - Step 90 Global step 90 Train loss 11.458074 on epoch=44
03/10/2022 16:58:23 - INFO - __main__ - Step 100 Global step 100 Train loss 10.612794 on epoch=49
03/10/2022 16:58:23 - INFO - __main__ - Global step 100 Train loss 11.964438 ACC 0.0 on epoch=49
03/10/2022 16:58:28 - INFO - __main__ - Step 110 Global step 110 Train loss 10.315906 on epoch=54
03/10/2022 16:58:33 - INFO - __main__ - Step 120 Global step 120 Train loss 9.872826 on epoch=59
03/10/2022 16:58:38 - INFO - __main__ - Step 130 Global step 130 Train loss 9.331933 on epoch=64
03/10/2022 16:58:43 - INFO - __main__ - Step 140 Global step 140 Train loss 8.700404 on epoch=69
03/10/2022 16:58:48 - INFO - __main__ - Step 150 Global step 150 Train loss 7.245927 on epoch=74
03/10/2022 16:58:48 - INFO - __main__ - Global step 150 Train loss 9.093399 ACC 0.0 on epoch=74
03/10/2022 16:58:53 - INFO - __main__ - Step 160 Global step 160 Train loss 5.681846 on epoch=79
03/10/2022 16:58:58 - INFO - __main__ - Step 170 Global step 170 Train loss 2.978755 on epoch=84
03/10/2022 16:59:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.453173 on epoch=89
03/10/2022 16:59:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.478774 on epoch=94
03/10/2022 16:59:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.482137 on epoch=99
03/10/2022 16:59:12 - INFO - __main__ - Global step 200 Train loss 2.014937 ACC 0.5 on epoch=99
03/10/2022 16:59:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.242138 on epoch=104
03/10/2022 16:59:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.250126 on epoch=109
03/10/2022 16:59:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.261530 on epoch=114
03/10/2022 16:59:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.249430 on epoch=119
03/10/2022 16:59:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.272223 on epoch=124
03/10/2022 16:59:38 - INFO - __main__ - Global step 250 Train loss 0.255090 ACC 0.5 on epoch=124
03/10/2022 16:59:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.223949 on epoch=129
03/10/2022 16:59:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.258995 on epoch=134
03/10/2022 16:59:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.244843 on epoch=139
03/10/2022 16:59:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.238124 on epoch=144
03/10/2022 17:00:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.261604 on epoch=149
03/10/2022 17:00:02 - INFO - __main__ - Global step 300 Train loss 0.245503 ACC 0.5 on epoch=149
03/10/2022 17:00:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.258736 on epoch=154
03/10/2022 17:00:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.231184 on epoch=159
03/10/2022 17:00:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.234404 on epoch=164
03/10/2022 17:00:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.241892 on epoch=169
03/10/2022 17:00:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.262005 on epoch=174
03/10/2022 17:00:27 - INFO - __main__ - Global step 350 Train loss 0.245644 ACC 0.59375 on epoch=174
03/10/2022 17:00:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.227777 on epoch=179
03/10/2022 17:00:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.247232 on epoch=184
03/10/2022 17:00:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.244301 on epoch=189
03/10/2022 17:00:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.230930 on epoch=194
03/10/2022 17:00:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.255088 on epoch=199
03/10/2022 17:00:52 - INFO - __main__ - Global step 400 Train loss 0.241066 ACC 0.5 on epoch=199
03/10/2022 17:00:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.229117 on epoch=204
03/10/2022 17:01:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.240681 on epoch=209
03/10/2022 17:01:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.246972 on epoch=214
03/10/2022 17:01:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.231342 on epoch=219
03/10/2022 17:01:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.234803 on epoch=224
03/10/2022 17:01:16 - INFO - __main__ - Global step 450 Train loss 0.236583 ACC 0.5 on epoch=224
03/10/2022 17:01:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.219281 on epoch=229
03/10/2022 17:01:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.251848 on epoch=234
03/10/2022 17:01:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.246212 on epoch=239
03/10/2022 17:01:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.222936 on epoch=244
03/10/2022 17:01:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.233436 on epoch=249
03/10/2022 17:01:40 - INFO - __main__ - Global step 500 Train loss 0.234743 ACC 0.5625 on epoch=249
03/10/2022 17:01:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.237862 on epoch=254
03/10/2022 17:01:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.242755 on epoch=259
03/10/2022 17:01:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.231441 on epoch=264
03/10/2022 17:01:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.231232 on epoch=269
03/10/2022 17:02:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.245611 on epoch=274
03/10/2022 17:02:04 - INFO - __main__ - Global step 550 Train loss 0.237780 ACC 0.5 on epoch=274
03/10/2022 17:02:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.228662 on epoch=279
03/10/2022 17:02:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.235070 on epoch=284
03/10/2022 17:02:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.249731 on epoch=289
03/10/2022 17:02:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.224079 on epoch=294
03/10/2022 17:02:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.228785 on epoch=299
03/10/2022 17:02:28 - INFO - __main__ - Global step 600 Train loss 0.233265 ACC 0.53125 on epoch=299
03/10/2022 17:02:28 - INFO - __main__ - save last model!
03/10/2022 17:02:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:02:29 - INFO - __main__ - Printing 3 examples
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:02:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:02:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:02:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:02:29 - INFO - __main__ - Printing 3 examples
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 17:02:29 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:29 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:02:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:02:29 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:02:36 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:02:36 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:02:36 - INFO - __main__ - Printing 3 examples
03/10/2022 17:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:02:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:02:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:02:36 - INFO - __main__ - ['sentence 1']
03/10/2022 17:02:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:02:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:02:37 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:02:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.0002_8_predictions.txt
03/10/2022 17:02:39 - INFO - __main__ - ACC on test data: 0.4750
03/10/2022 17:02:39 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.475
03/10/2022 17:02:39 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0001, bsz=8 ...
03/10/2022 17:02:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:02:40 - INFO - __main__ - Printing 3 examples
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:02:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:02:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:02:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:02:40 - INFO - __main__ - Printing 3 examples
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/10/2022 17:02:40 - INFO - __main__ - ['sentence 2']
03/10/2022 17:02:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:02:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:02:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:02:40 - INFO - __main__ - Starting training!
03/10/2022 17:02:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:02:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:02:50 - INFO - __main__ - Starting training!
03/10/2022 17:02:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.334755 on epoch=4
03/10/2022 17:02:59 - INFO - __main__ - Step 20 Global step 20 Train loss 20.879955 on epoch=9
03/10/2022 17:03:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.299686 on epoch=14
03/10/2022 17:03:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.368527 on epoch=19
03/10/2022 17:03:14 - INFO - __main__ - Step 50 Global step 50 Train loss 14.828006 on epoch=24
03/10/2022 17:03:15 - INFO - __main__ - Global step 50 Train loss 18.142185 ACC 0.1875 on epoch=24
03/10/2022 17:03:20 - INFO - __main__ - Step 60 Global step 60 Train loss 13.329455 on epoch=29
03/10/2022 17:03:25 - INFO - __main__ - Step 70 Global step 70 Train loss 14.097499 on epoch=34
03/10/2022 17:03:30 - INFO - __main__ - Step 80 Global step 80 Train loss 13.850279 on epoch=39
03/10/2022 17:03:35 - INFO - __main__ - Step 90 Global step 90 Train loss 13.188383 on epoch=44
03/10/2022 17:03:40 - INFO - __main__ - Step 100 Global step 100 Train loss 12.956599 on epoch=49
03/10/2022 17:03:40 - INFO - __main__ - Global step 100 Train loss 13.484444 ACC 0.21875 on epoch=49
03/10/2022 17:03:46 - INFO - __main__ - Step 110 Global step 110 Train loss 12.544991 on epoch=54
03/10/2022 17:03:51 - INFO - __main__ - Step 120 Global step 120 Train loss 11.999748 on epoch=59
03/10/2022 17:03:55 - INFO - __main__ - Step 130 Global step 130 Train loss 12.086261 on epoch=64
03/10/2022 17:04:00 - INFO - __main__ - Step 140 Global step 140 Train loss 12.022795 on epoch=69
03/10/2022 17:04:05 - INFO - __main__ - Step 150 Global step 150 Train loss 11.303666 on epoch=74
03/10/2022 17:04:06 - INFO - __main__ - Global step 150 Train loss 11.991491 ACC 0.125 on epoch=74
03/10/2022 17:04:10 - INFO - __main__ - Step 160 Global step 160 Train loss 11.558428 on epoch=79
03/10/2022 17:04:15 - INFO - __main__ - Step 170 Global step 170 Train loss 10.826567 on epoch=84
03/10/2022 17:04:20 - INFO - __main__ - Step 180 Global step 180 Train loss 10.998557 on epoch=89
03/10/2022 17:04:25 - INFO - __main__ - Step 190 Global step 190 Train loss 11.158861 on epoch=94
03/10/2022 17:04:30 - INFO - __main__ - Step 200 Global step 200 Train loss 10.272150 on epoch=99
03/10/2022 17:04:30 - INFO - __main__ - Global step 200 Train loss 10.962913 ACC 0.1875 on epoch=99
03/10/2022 17:04:35 - INFO - __main__ - Step 210 Global step 210 Train loss 9.919147 on epoch=104
03/10/2022 17:04:40 - INFO - __main__ - Step 220 Global step 220 Train loss 9.630059 on epoch=109
03/10/2022 17:04:44 - INFO - __main__ - Step 230 Global step 230 Train loss 9.272249 on epoch=114
03/10/2022 17:04:49 - INFO - __main__ - Step 240 Global step 240 Train loss 9.157447 on epoch=119
03/10/2022 17:04:54 - INFO - __main__ - Step 250 Global step 250 Train loss 8.315945 on epoch=124
03/10/2022 17:04:55 - INFO - __main__ - Global step 250 Train loss 9.258969 ACC 0.0625 on epoch=124
03/10/2022 17:04:59 - INFO - __main__ - Step 260 Global step 260 Train loss 7.447571 on epoch=129
03/10/2022 17:05:04 - INFO - __main__ - Step 270 Global step 270 Train loss 6.152064 on epoch=134
03/10/2022 17:05:09 - INFO - __main__ - Step 280 Global step 280 Train loss 3.581845 on epoch=139
03/10/2022 17:05:14 - INFO - __main__ - Step 290 Global step 290 Train loss 2.179760 on epoch=144
03/10/2022 17:05:18 - INFO - __main__ - Step 300 Global step 300 Train loss 1.590561 on epoch=149
03/10/2022 17:05:19 - INFO - __main__ - Global step 300 Train loss 4.190360 ACC 0.5 on epoch=149
03/10/2022 17:05:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.317801 on epoch=154
03/10/2022 17:05:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.072115 on epoch=159
03/10/2022 17:05:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.693685 on epoch=164
03/10/2022 17:05:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.410099 on epoch=169
03/10/2022 17:05:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.161171 on epoch=174
03/10/2022 17:05:44 - INFO - __main__ - Global step 350 Train loss 0.730974 ACC 0.53125 on epoch=174
03/10/2022 17:05:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.316874 on epoch=179
03/10/2022 17:05:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.133070 on epoch=184
03/10/2022 17:05:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.163170 on epoch=189
03/10/2022 17:06:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.145559 on epoch=194
03/10/2022 17:06:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.067111 on epoch=199
03/10/2022 17:06:09 - INFO - __main__ - Global step 400 Train loss 0.165157 ACC 0.5 on epoch=199
03/10/2022 17:06:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.095929 on epoch=204
03/10/2022 17:06:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.082786 on epoch=209
03/10/2022 17:06:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.075428 on epoch=214
03/10/2022 17:06:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.065403 on epoch=219
03/10/2022 17:06:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.077081 on epoch=224
03/10/2022 17:06:33 - INFO - __main__ - Global step 450 Train loss 0.079326 ACC 0.5625 on epoch=224
03/10/2022 17:06:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.108420 on epoch=229
03/10/2022 17:06:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.073944 on epoch=234
03/10/2022 17:06:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.058294 on epoch=239
03/10/2022 17:06:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.033839 on epoch=244
03/10/2022 17:06:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.084008 on epoch=249
03/10/2022 17:06:58 - INFO - __main__ - Global step 500 Train loss 0.071701 ACC 0.4375 on epoch=249
03/10/2022 17:07:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027907 on epoch=254
03/10/2022 17:07:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.046301 on epoch=259
03/10/2022 17:07:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.024565 on epoch=264
03/10/2022 17:07:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.045950 on epoch=269
03/10/2022 17:07:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.060111 on epoch=274
03/10/2022 17:07:23 - INFO - __main__ - Global step 550 Train loss 0.040967 ACC 0.5625 on epoch=274
03/10/2022 17:07:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.013110 on epoch=279
03/10/2022 17:07:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.025089 on epoch=284
03/10/2022 17:07:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.079066 on epoch=289
03/10/2022 17:07:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032407 on epoch=294
03/10/2022 17:07:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.030065 on epoch=299
03/10/2022 17:07:47 - INFO - __main__ - Global step 600 Train loss 0.035947 ACC 0.59375 on epoch=299
03/10/2022 17:07:48 - INFO - __main__ - save last model!
03/10/2022 17:07:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:07:48 - INFO - __main__ - Printing 3 examples
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:07:48 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:07:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:07:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:07:48 - INFO - __main__ - Printing 3 examples
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:07:48 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:48 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:07:48 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:07:48 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:07:55 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:07:56 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:07:56 - INFO - __main__ - Printing 3 examples
03/10/2022 17:07:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:07:56 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:07:56 - INFO - __main__ - ['sentence 2']
03/10/2022 17:07:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:07:56 - INFO - __main__ - ['sentence 1']
03/10/2022 17:07:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:07:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:07:56 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:07:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.0001_8_predictions.txt
03/10/2022 17:07:58 - INFO - __main__ - ACC on test data: 0.6500
03/10/2022 17:07:59 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.65
03/10/2022 17:07:59 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0005, bsz=8 ...
03/10/2022 17:07:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:07:59 - INFO - __main__ - Starting training!
03/10/2022 17:08:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:08:00 - INFO - __main__ - Printing 3 examples
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:08:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:08:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:08:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:08:00 - INFO - __main__ - Printing 3 examples
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:08:00 - INFO - __main__ - ['sentence 2']
03/10/2022 17:08:00 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:08:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:08:00 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:08:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:08:09 - INFO - __main__ - Starting training!
03/10/2022 17:08:13 - INFO - __main__ - Step 10 Global step 10 Train loss 22.890509 on epoch=4
03/10/2022 17:08:17 - INFO - __main__ - Step 20 Global step 20 Train loss 19.456844 on epoch=9
03/10/2022 17:08:22 - INFO - __main__ - Step 30 Global step 30 Train loss 15.516536 on epoch=14
03/10/2022 17:08:27 - INFO - __main__ - Step 40 Global step 40 Train loss 13.162140 on epoch=19
03/10/2022 17:08:32 - INFO - __main__ - Step 50 Global step 50 Train loss 11.301278 on epoch=24
03/10/2022 17:08:39 - INFO - __main__ - Global step 50 Train loss 16.465462 ACC 0.0 on epoch=24
03/10/2022 17:08:45 - INFO - __main__ - Step 60 Global step 60 Train loss 10.051431 on epoch=29
03/10/2022 17:08:50 - INFO - __main__ - Step 70 Global step 70 Train loss 9.092165 on epoch=34
03/10/2022 17:08:54 - INFO - __main__ - Step 80 Global step 80 Train loss 5.666762 on epoch=39
03/10/2022 17:08:59 - INFO - __main__ - Step 90 Global step 90 Train loss 1.091955 on epoch=44
03/10/2022 17:09:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.556778 on epoch=49
03/10/2022 17:09:04 - INFO - __main__ - Global step 100 Train loss 5.291819 ACC 0.46875 on epoch=49
03/10/2022 17:09:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.603636 on epoch=54
03/10/2022 17:09:15 - INFO - __main__ - Step 120 Global step 120 Train loss 1.298463 on epoch=59
03/10/2022 17:09:20 - INFO - __main__ - Step 130 Global step 130 Train loss 1.860600 on epoch=64
03/10/2022 17:09:25 - INFO - __main__ - Step 140 Global step 140 Train loss 1.086195 on epoch=69
03/10/2022 17:09:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.918596 on epoch=74
03/10/2022 17:09:30 - INFO - __main__ - Global step 150 Train loss 1.153498 ACC 0.5 on epoch=74
03/10/2022 17:09:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.700557 on epoch=79
03/10/2022 17:09:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.453014 on epoch=84
03/10/2022 17:09:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.577197 on epoch=89
03/10/2022 17:09:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.469963 on epoch=94
03/10/2022 17:09:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.612374 on epoch=99
03/10/2022 17:10:06 - INFO - __main__ - Global step 200 Train loss 0.762621 ACC 0.0 on epoch=99
03/10/2022 17:10:11 - INFO - __main__ - Step 210 Global step 210 Train loss 3.259035 on epoch=104
03/10/2022 17:10:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.302784 on epoch=109
03/10/2022 17:10:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.344180 on epoch=114
03/10/2022 17:10:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.292058 on epoch=119
03/10/2022 17:10:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.237867 on epoch=124
03/10/2022 17:10:31 - INFO - __main__ - Global step 250 Train loss 0.887185 ACC 0.5 on epoch=124
03/10/2022 17:10:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.251971 on epoch=129
03/10/2022 17:10:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.247686 on epoch=134
03/10/2022 17:10:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.245360 on epoch=139
03/10/2022 17:10:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.244407 on epoch=144
03/10/2022 17:10:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.235931 on epoch=149
03/10/2022 17:10:56 - INFO - __main__ - Global step 300 Train loss 0.245071 ACC 0.5 on epoch=149
03/10/2022 17:11:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.239024 on epoch=154
03/10/2022 17:11:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.225031 on epoch=159
03/10/2022 17:11:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.240879 on epoch=164
03/10/2022 17:11:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.232175 on epoch=169
03/10/2022 17:11:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.273573 on epoch=174
03/10/2022 17:11:20 - INFO - __main__ - Global step 350 Train loss 0.242136 ACC 0.5 on epoch=174
03/10/2022 17:11:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.261088 on epoch=179
03/10/2022 17:11:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.234268 on epoch=184
03/10/2022 17:11:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.234052 on epoch=189
03/10/2022 17:11:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.242475 on epoch=194
03/10/2022 17:11:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333407 on epoch=199
03/10/2022 17:11:45 - INFO - __main__ - Global step 400 Train loss 0.261058 ACC 0.5 on epoch=199
03/10/2022 17:11:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.237900 on epoch=204
03/10/2022 17:11:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.254043 on epoch=209
03/10/2022 17:11:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.254817 on epoch=214
03/10/2022 17:12:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.279877 on epoch=219
03/10/2022 17:12:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.241400 on epoch=224
03/10/2022 17:12:09 - INFO - __main__ - Global step 450 Train loss 0.253607 ACC 0.5 on epoch=224
03/10/2022 17:12:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.244289 on epoch=229
03/10/2022 17:12:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.248732 on epoch=234
03/10/2022 17:12:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.244210 on epoch=239
03/10/2022 17:12:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.238475 on epoch=244
03/10/2022 17:12:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.220489 on epoch=249
03/10/2022 17:12:34 - INFO - __main__ - Global step 500 Train loss 0.239239 ACC 0.5 on epoch=249
03/10/2022 17:12:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.226566 on epoch=254
03/10/2022 17:12:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.232799 on epoch=259
03/10/2022 17:12:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.237800 on epoch=264
03/10/2022 17:12:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.244003 on epoch=269
03/10/2022 17:12:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.238176 on epoch=274
03/10/2022 17:12:58 - INFO - __main__ - Global step 550 Train loss 0.235869 ACC 0.53125 on epoch=274
03/10/2022 17:13:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.252076 on epoch=279
03/10/2022 17:13:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.236635 on epoch=284
03/10/2022 17:13:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.223834 on epoch=289
03/10/2022 17:13:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.230599 on epoch=294
03/10/2022 17:13:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.229175 on epoch=299
03/10/2022 17:13:24 - INFO - __main__ - Global step 600 Train loss 0.234464 ACC 0.5 on epoch=299
03/10/2022 17:13:24 - INFO - __main__ - save last model!
03/10/2022 17:13:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:13:24 - INFO - __main__ - Printing 3 examples
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:13:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:13:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:13:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:13:24 - INFO - __main__ - Printing 3 examples
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:24 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:13:24 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:13:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:13:25 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:13:31 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:13:32 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:13:32 - INFO - __main__ - Printing 3 examples
03/10/2022 17:13:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:13:32 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:13:32 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:32 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:13:32 - INFO - __main__ - ['sentence 1']
03/10/2022 17:13:32 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:13:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:13:32 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:13:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.0005_8_predictions.txt
03/10/2022 17:13:34 - INFO - __main__ - ACC on test data: 0.4750
03/10/2022 17:13:35 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.475
03/10/2022 17:13:35 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0003, bsz=8 ...
03/10/2022 17:13:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:13:35 - INFO - __main__ - Starting training!
03/10/2022 17:13:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:13:36 - INFO - __main__ - Printing 3 examples
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:13:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:13:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:13:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:13:36 - INFO - __main__ - Printing 3 examples
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:13:36 - INFO - __main__ - ['sentence 2']
03/10/2022 17:13:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:13:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:13:36 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:13:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:13:46 - INFO - __main__ - Starting training!
03/10/2022 17:13:50 - INFO - __main__ - Step 10 Global step 10 Train loss 21.070181 on epoch=4
03/10/2022 17:13:55 - INFO - __main__ - Step 20 Global step 20 Train loss 17.125612 on epoch=9
03/10/2022 17:13:59 - INFO - __main__ - Step 30 Global step 30 Train loss 14.152895 on epoch=14
03/10/2022 17:14:04 - INFO - __main__ - Step 40 Global step 40 Train loss 13.730904 on epoch=19
03/10/2022 17:14:09 - INFO - __main__ - Step 50 Global step 50 Train loss 12.790665 on epoch=24
03/10/2022 17:14:09 - INFO - __main__ - Global step 50 Train loss 15.774052 ACC 0.0 on epoch=24
03/10/2022 17:14:14 - INFO - __main__ - Step 60 Global step 60 Train loss 12.331706 on epoch=29
03/10/2022 17:14:19 - INFO - __main__ - Step 70 Global step 70 Train loss 11.459656 on epoch=34
03/10/2022 17:14:24 - INFO - __main__ - Step 80 Global step 80 Train loss 10.914675 on epoch=39
03/10/2022 17:14:29 - INFO - __main__ - Step 90 Global step 90 Train loss 10.045118 on epoch=44
03/10/2022 17:14:34 - INFO - __main__ - Step 100 Global step 100 Train loss 8.519574 on epoch=49
03/10/2022 17:14:34 - INFO - __main__ - Global step 100 Train loss 10.654145 ACC 0.0625 on epoch=49
03/10/2022 17:14:40 - INFO - __main__ - Step 110 Global step 110 Train loss 7.934142 on epoch=54
03/10/2022 17:14:45 - INFO - __main__ - Step 120 Global step 120 Train loss 4.243295 on epoch=59
03/10/2022 17:14:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.401065 on epoch=64
03/10/2022 17:14:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.255743 on epoch=69
03/10/2022 17:15:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.256068 on epoch=74
03/10/2022 17:15:00 - INFO - __main__ - Global step 150 Train loss 2.818063 ACC 0.5 on epoch=74
03/10/2022 17:15:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.269153 on epoch=79
03/10/2022 17:15:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.267890 on epoch=84
03/10/2022 17:15:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.267679 on epoch=89
03/10/2022 17:15:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.253406 on epoch=94
03/10/2022 17:15:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.265018 on epoch=99
03/10/2022 17:15:25 - INFO - __main__ - Global step 200 Train loss 0.264629 ACC 0.65625 on epoch=99
03/10/2022 17:15:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.237811 on epoch=104
03/10/2022 17:15:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.240909 on epoch=109
03/10/2022 17:15:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.242059 on epoch=114
03/10/2022 17:15:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.258059 on epoch=119
03/10/2022 17:15:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.236475 on epoch=124
03/10/2022 17:15:51 - INFO - __main__ - Global step 250 Train loss 0.243062 ACC 0.5 on epoch=124
03/10/2022 17:15:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.236634 on epoch=129
03/10/2022 17:16:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.246171 on epoch=134
03/10/2022 17:16:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.236942 on epoch=139
03/10/2022 17:16:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.221213 on epoch=144
03/10/2022 17:16:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.227202 on epoch=149
03/10/2022 17:16:16 - INFO - __main__ - Global step 300 Train loss 0.233632 ACC 0.5 on epoch=149
03/10/2022 17:16:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.239087 on epoch=154
03/10/2022 17:16:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.238596 on epoch=159
03/10/2022 17:16:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.233586 on epoch=164
03/10/2022 17:16:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.246580 on epoch=169
03/10/2022 17:16:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.223694 on epoch=174
03/10/2022 17:16:40 - INFO - __main__ - Global step 350 Train loss 0.236309 ACC 0.5 on epoch=174
03/10/2022 17:16:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.238758 on epoch=179
03/10/2022 17:16:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.232779 on epoch=184
03/10/2022 17:16:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.240769 on epoch=189
03/10/2022 17:17:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.223254 on epoch=194
03/10/2022 17:17:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.228347 on epoch=199
03/10/2022 17:17:05 - INFO - __main__ - Global step 400 Train loss 0.232781 ACC 0.5 on epoch=199
03/10/2022 17:17:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.233492 on epoch=204
03/10/2022 17:17:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.224733 on epoch=209
03/10/2022 17:17:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.187283 on epoch=214
03/10/2022 17:17:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.189383 on epoch=219
03/10/2022 17:17:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.304759 on epoch=224
03/10/2022 17:17:29 - INFO - __main__ - Global step 450 Train loss 0.227930 ACC 0.78125 on epoch=224
03/10/2022 17:17:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.209962 on epoch=229
03/10/2022 17:17:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.155861 on epoch=234
03/10/2022 17:17:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.140941 on epoch=239
03/10/2022 17:17:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.126419 on epoch=244
03/10/2022 17:17:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112414 on epoch=249
03/10/2022 17:17:54 - INFO - __main__ - Global step 500 Train loss 0.149119 ACC 0.6875 on epoch=249
03/10/2022 17:17:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.102008 on epoch=254
03/10/2022 17:18:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.076942 on epoch=259
03/10/2022 17:18:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.076492 on epoch=264
03/10/2022 17:18:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.116206 on epoch=269
03/10/2022 17:18:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.096232 on epoch=274
03/10/2022 17:18:18 - INFO - __main__ - Global step 550 Train loss 0.093576 ACC 0.6875 on epoch=274
03/10/2022 17:18:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.079350 on epoch=279
03/10/2022 17:18:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.061115 on epoch=284
03/10/2022 17:18:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.056561 on epoch=289
03/10/2022 17:18:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.062179 on epoch=294
03/10/2022 17:18:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.031865 on epoch=299
03/10/2022 17:18:42 - INFO - __main__ - Global step 600 Train loss 0.058214 ACC 0.71875 on epoch=299
03/10/2022 17:18:42 - INFO - __main__ - save last model!
03/10/2022 17:18:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:18:43 - INFO - __main__ - Printing 3 examples
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:18:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:18:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:18:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:18:43 - INFO - __main__ - Printing 3 examples
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:18:43 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:43 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:18:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:18:43 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:18:49 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:18:50 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:18:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:18:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:18:50 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:18:50 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:18:50 - INFO - __main__ - ['sentence 1']
03/10/2022 17:18:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:18:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:18:50 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:18:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:18:52 - INFO - __main__ - Starting training!
03/10/2022 17:18:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.0003_8_predictions.txt
03/10/2022 17:18:52 - INFO - __main__ - ACC on test data: 0.6850
03/10/2022 17:18:53 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0003, bsz=8, dev_performance=0.78125, test_performance=0.685
03/10/2022 17:18:53 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0002, bsz=8 ...
03/10/2022 17:18:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:18:54 - INFO - __main__ - Printing 3 examples
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:18:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:18:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:18:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:18:54 - INFO - __main__ - Printing 3 examples
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:18:54 - INFO - __main__ - ['sentence 2']
03/10/2022 17:18:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:18:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:18:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:19:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:19:03 - INFO - __main__ - Starting training!
03/10/2022 17:19:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.118610 on epoch=4
03/10/2022 17:19:11 - INFO - __main__ - Step 20 Global step 20 Train loss 20.909376 on epoch=9
03/10/2022 17:19:16 - INFO - __main__ - Step 30 Global step 30 Train loss 17.868563 on epoch=14
03/10/2022 17:19:21 - INFO - __main__ - Step 40 Global step 40 Train loss 15.490993 on epoch=19
03/10/2022 17:19:26 - INFO - __main__ - Step 50 Global step 50 Train loss 13.463188 on epoch=24
03/10/2022 17:19:27 - INFO - __main__ - Global step 50 Train loss 17.770145 ACC 0.03125 on epoch=24
03/10/2022 17:19:32 - INFO - __main__ - Step 60 Global step 60 Train loss 13.387195 on epoch=29
03/10/2022 17:19:37 - INFO - __main__ - Step 70 Global step 70 Train loss 12.914126 on epoch=34
03/10/2022 17:19:42 - INFO - __main__ - Step 80 Global step 80 Train loss 11.971614 on epoch=39
03/10/2022 17:19:47 - INFO - __main__ - Step 90 Global step 90 Train loss 11.317392 on epoch=44
03/10/2022 17:19:52 - INFO - __main__ - Step 100 Global step 100 Train loss 10.846357 on epoch=49
03/10/2022 17:19:52 - INFO - __main__ - Global step 100 Train loss 12.087337 ACC 0.03125 on epoch=49
03/10/2022 17:19:57 - INFO - __main__ - Step 110 Global step 110 Train loss 9.649684 on epoch=54
03/10/2022 17:20:02 - INFO - __main__ - Step 120 Global step 120 Train loss 9.701864 on epoch=59
03/10/2022 17:20:07 - INFO - __main__ - Step 130 Global step 130 Train loss 9.094088 on epoch=64
03/10/2022 17:20:12 - INFO - __main__ - Step 140 Global step 140 Train loss 8.208876 on epoch=69
03/10/2022 17:20:17 - INFO - __main__ - Step 150 Global step 150 Train loss 7.102713 on epoch=74
03/10/2022 17:20:19 - INFO - __main__ - Global step 150 Train loss 8.751446 ACC 0.21875 on epoch=74
03/10/2022 17:20:24 - INFO - __main__ - Step 160 Global step 160 Train loss 5.306964 on epoch=79
03/10/2022 17:20:29 - INFO - __main__ - Step 170 Global step 170 Train loss 3.377840 on epoch=84
03/10/2022 17:20:34 - INFO - __main__ - Step 180 Global step 180 Train loss 1.948366 on epoch=89
03/10/2022 17:20:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.641793 on epoch=94
03/10/2022 17:20:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.508511 on epoch=99
03/10/2022 17:20:45 - INFO - __main__ - Global step 200 Train loss 2.356695 ACC 0.5 on epoch=99
03/10/2022 17:20:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.386701 on epoch=104
03/10/2022 17:20:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.235343 on epoch=109
03/10/2022 17:21:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.213181 on epoch=114
03/10/2022 17:21:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.203056 on epoch=119
03/10/2022 17:21:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.264005 on epoch=124
03/10/2022 17:21:11 - INFO - __main__ - Global step 250 Train loss 0.260457 ACC 0.46875 on epoch=124
03/10/2022 17:21:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.174317 on epoch=129
03/10/2022 17:21:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.186430 on epoch=134
03/10/2022 17:21:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.161776 on epoch=139
03/10/2022 17:21:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.180629 on epoch=144
03/10/2022 17:21:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.169586 on epoch=149
03/10/2022 17:21:36 - INFO - __main__ - Global step 300 Train loss 0.174547 ACC 0.6875 on epoch=149
03/10/2022 17:21:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.165055 on epoch=154
03/10/2022 17:21:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.166441 on epoch=159
03/10/2022 17:21:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.140769 on epoch=164
03/10/2022 17:21:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.138033 on epoch=169
03/10/2022 17:22:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.122300 on epoch=174
03/10/2022 17:22:01 - INFO - __main__ - Global step 350 Train loss 0.146520 ACC 0.59375 on epoch=174
03/10/2022 17:22:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.122007 on epoch=179
03/10/2022 17:22:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.093563 on epoch=184
03/10/2022 17:22:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.103471 on epoch=189
03/10/2022 17:22:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.106339 on epoch=194
03/10/2022 17:22:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.086056 on epoch=199
03/10/2022 17:22:26 - INFO - __main__ - Global step 400 Train loss 0.102287 ACC 0.65625 on epoch=199
03/10/2022 17:22:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.094156 on epoch=204
03/10/2022 17:22:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.085853 on epoch=209
03/10/2022 17:22:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.073451 on epoch=214
03/10/2022 17:22:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.126563 on epoch=219
03/10/2022 17:22:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.095873 on epoch=224
03/10/2022 17:22:50 - INFO - __main__ - Global step 450 Train loss 0.095179 ACC 0.5625 on epoch=224
03/10/2022 17:22:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.074491 on epoch=229
03/10/2022 17:23:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.095562 on epoch=234
03/10/2022 17:23:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.096975 on epoch=239
03/10/2022 17:23:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.069833 on epoch=244
03/10/2022 17:23:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.075486 on epoch=249
03/10/2022 17:23:15 - INFO - __main__ - Global step 500 Train loss 0.082470 ACC 0.65625 on epoch=249
03/10/2022 17:23:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.078508 on epoch=254
03/10/2022 17:23:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.081297 on epoch=259
03/10/2022 17:23:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.082887 on epoch=264
03/10/2022 17:23:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.067005 on epoch=269
03/10/2022 17:23:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.086651 on epoch=274
03/10/2022 17:23:39 - INFO - __main__ - Global step 550 Train loss 0.079270 ACC 0.6875 on epoch=274
03/10/2022 17:23:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.087401 on epoch=279
03/10/2022 17:23:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.060077 on epoch=284
03/10/2022 17:23:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.051079 on epoch=289
03/10/2022 17:23:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.063922 on epoch=294
03/10/2022 17:24:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.043199 on epoch=299
03/10/2022 17:24:04 - INFO - __main__ - Global step 600 Train loss 0.061136 ACC 0.65625 on epoch=299
03/10/2022 17:24:04 - INFO - __main__ - save last model!
03/10/2022 17:24:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:24:04 - INFO - __main__ - Printing 3 examples
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:24:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:24:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:24:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:24:04 - INFO - __main__ - Printing 3 examples
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:24:04 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:24:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:24:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:24:11 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:24:11 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:24:11 - INFO - __main__ - Printing 3 examples
03/10/2022 17:24:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:24:11 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:24:12 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:12 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:24:12 - INFO - __main__ - ['sentence 1']
03/10/2022 17:24:12 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:24:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:24:12 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:24:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:24:14 - INFO - __main__ - Starting training!
03/10/2022 17:24:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.0002_8_predictions.txt
03/10/2022 17:24:14 - INFO - __main__ - ACC on test data: 0.6400
03/10/2022 17:24:14 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.64
03/10/2022 17:24:15 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0001, bsz=8 ...
03/10/2022 17:24:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:24:15 - INFO - __main__ - Printing 3 examples
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:24:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:24:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:24:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:24:15 - INFO - __main__ - Printing 3 examples
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/10/2022 17:24:15 - INFO - __main__ - ['sentence 2']
03/10/2022 17:24:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:24:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:24:16 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:24:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:24:25 - INFO - __main__ - Starting training!
03/10/2022 17:24:29 - INFO - __main__ - Step 10 Global step 10 Train loss 21.316151 on epoch=4
03/10/2022 17:24:34 - INFO - __main__ - Step 20 Global step 20 Train loss 20.056194 on epoch=9
03/10/2022 17:24:38 - INFO - __main__ - Step 30 Global step 30 Train loss 16.682985 on epoch=14
03/10/2022 17:24:43 - INFO - __main__ - Step 40 Global step 40 Train loss 15.235392 on epoch=19
03/10/2022 17:24:48 - INFO - __main__ - Step 50 Global step 50 Train loss 15.083486 on epoch=24
03/10/2022 17:24:51 - INFO - __main__ - Global step 50 Train loss 17.674841 ACC 0.0 on epoch=24
03/10/2022 17:24:56 - INFO - __main__ - Step 60 Global step 60 Train loss 14.397017 on epoch=29
03/10/2022 17:25:01 - INFO - __main__ - Step 70 Global step 70 Train loss 14.215421 on epoch=34
03/10/2022 17:25:06 - INFO - __main__ - Step 80 Global step 80 Train loss 13.469533 on epoch=39
03/10/2022 17:25:11 - INFO - __main__ - Step 90 Global step 90 Train loss 13.052478 on epoch=44
03/10/2022 17:25:16 - INFO - __main__ - Step 100 Global step 100 Train loss 12.814781 on epoch=49
03/10/2022 17:25:16 - INFO - __main__ - Global step 100 Train loss 13.589847 ACC 0.0 on epoch=49
03/10/2022 17:25:21 - INFO - __main__ - Step 110 Global step 110 Train loss 12.657752 on epoch=54
03/10/2022 17:25:26 - INFO - __main__ - Step 120 Global step 120 Train loss 12.400465 on epoch=59
03/10/2022 17:25:31 - INFO - __main__ - Step 130 Global step 130 Train loss 11.540689 on epoch=64
03/10/2022 17:25:35 - INFO - __main__ - Step 140 Global step 140 Train loss 11.345575 on epoch=69
03/10/2022 17:25:40 - INFO - __main__ - Step 150 Global step 150 Train loss 10.896604 on epoch=74
03/10/2022 17:25:41 - INFO - __main__ - Global step 150 Train loss 11.768216 ACC 0.0 on epoch=74
03/10/2022 17:25:45 - INFO - __main__ - Step 160 Global step 160 Train loss 11.437593 on epoch=79
03/10/2022 17:25:50 - INFO - __main__ - Step 170 Global step 170 Train loss 11.052158 on epoch=84
03/10/2022 17:25:55 - INFO - __main__ - Step 180 Global step 180 Train loss 10.557711 on epoch=89
03/10/2022 17:26:00 - INFO - __main__ - Step 190 Global step 190 Train loss 10.289122 on epoch=94
03/10/2022 17:26:05 - INFO - __main__ - Step 200 Global step 200 Train loss 9.947742 on epoch=99
03/10/2022 17:26:05 - INFO - __main__ - Global step 200 Train loss 10.656866 ACC 0.0 on epoch=99
03/10/2022 17:26:10 - INFO - __main__ - Step 210 Global step 210 Train loss 9.709600 on epoch=104
03/10/2022 17:26:15 - INFO - __main__ - Step 220 Global step 220 Train loss 9.499525 on epoch=109
03/10/2022 17:26:20 - INFO - __main__ - Step 230 Global step 230 Train loss 8.982168 on epoch=114
03/10/2022 17:26:24 - INFO - __main__ - Step 240 Global step 240 Train loss 9.085387 on epoch=119
03/10/2022 17:26:29 - INFO - __main__ - Step 250 Global step 250 Train loss 8.470705 on epoch=124
03/10/2022 17:26:30 - INFO - __main__ - Global step 250 Train loss 9.149478 ACC 0.0 on epoch=124
03/10/2022 17:26:34 - INFO - __main__ - Step 260 Global step 260 Train loss 8.450804 on epoch=129
03/10/2022 17:26:39 - INFO - __main__ - Step 270 Global step 270 Train loss 7.345259 on epoch=134
03/10/2022 17:26:44 - INFO - __main__ - Step 280 Global step 280 Train loss 6.505517 on epoch=139
03/10/2022 17:26:49 - INFO - __main__ - Step 290 Global step 290 Train loss 2.452157 on epoch=144
03/10/2022 17:26:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.826707 on epoch=149
03/10/2022 17:26:54 - INFO - __main__ - Global step 300 Train loss 5.116089 ACC 0.625 on epoch=149
03/10/2022 17:27:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.220312 on epoch=154
03/10/2022 17:27:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.184918 on epoch=159
03/10/2022 17:27:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.194759 on epoch=164
03/10/2022 17:27:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.124262 on epoch=169
03/10/2022 17:27:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.236735 on epoch=174
03/10/2022 17:27:19 - INFO - __main__ - Global step 350 Train loss 0.192197 ACC 0.625 on epoch=174
03/10/2022 17:27:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.117118 on epoch=179
03/10/2022 17:27:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.118571 on epoch=184
03/10/2022 17:27:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.202851 on epoch=189
03/10/2022 17:27:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.096218 on epoch=194
03/10/2022 17:27:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.107827 on epoch=199
03/10/2022 17:27:43 - INFO - __main__ - Global step 400 Train loss 0.128517 ACC 0.65625 on epoch=199
03/10/2022 17:27:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.101051 on epoch=204
03/10/2022 17:27:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.080157 on epoch=209
03/10/2022 17:27:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.117852 on epoch=214
03/10/2022 17:28:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.104361 on epoch=219
03/10/2022 17:28:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.105627 on epoch=224
03/10/2022 17:28:09 - INFO - __main__ - Global step 450 Train loss 0.101809 ACC 0.65625 on epoch=224
03/10/2022 17:28:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.076443 on epoch=229
03/10/2022 17:28:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.059735 on epoch=234
03/10/2022 17:28:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.064291 on epoch=239
03/10/2022 17:28:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.059675 on epoch=244
03/10/2022 17:28:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.054560 on epoch=249
03/10/2022 17:28:33 - INFO - __main__ - Global step 500 Train loss 0.062941 ACC 0.71875 on epoch=249
03/10/2022 17:28:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.060295 on epoch=254
03/10/2022 17:28:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.046916 on epoch=259
03/10/2022 17:28:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.061463 on epoch=264
03/10/2022 17:28:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.029749 on epoch=269
03/10/2022 17:28:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.031803 on epoch=274
03/10/2022 17:28:58 - INFO - __main__ - Global step 550 Train loss 0.046045 ACC 0.71875 on epoch=274
03/10/2022 17:29:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.094688 on epoch=279
03/10/2022 17:29:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.041044 on epoch=284
03/10/2022 17:29:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.025322 on epoch=289
03/10/2022 17:29:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032095 on epoch=294
03/10/2022 17:29:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.021671 on epoch=299
03/10/2022 17:29:23 - INFO - __main__ - Global step 600 Train loss 0.042964 ACC 0.71875 on epoch=299
03/10/2022 17:29:23 - INFO - __main__ - save last model!
03/10/2022 17:29:30 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:29:30 - INFO - __main__ - Start tokenizing ... 200 instances
03/10/2022 17:29:30 - INFO - __main__ - Printing 3 examples
03/10/2022 17:29:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/10/2022 17:29:30 - INFO - __main__ - ['sentence 2']
03/10/2022 17:29:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/10/2022 17:29:30 - INFO - __main__ - ['sentence 2']
03/10/2022 17:29:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/10/2022 17:29:30 - INFO - __main__ - ['sentence 1']
03/10/2022 17:29:30 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:29:31 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:29:31 - INFO - __main__ - Loaded 200 examples from test data
03/10/2022 17:29:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.0001_8_predictions.txt
03/10/2022 17:29:33 - INFO - __main__ - ACC on test data: 0.6800
03/10/2022 17:29:33 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.0001, bsz=8, dev_performance=0.71875, test_performance=0.68
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0005099773406982422 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15516", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6501, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15517", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6501, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6501, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15540): No such process
Task: crawl_domain, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_voqftnst/none_14e1zf3r
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_voqftnst/none_14e1zf3r/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_voqftnst/none_14e1zf3r/attempt_0/1/error.json
03/10/2022 17:29:42 - INFO - __main__ - Namespace(task_dir='data/crawl_domain/', task_name='crawl_domain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-crawl_domain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 17:29:42 - INFO - __main__ - models/T5-large-ft-random/singletask-crawl_domain
Output directory () already exists and is not empty.
03/10/2022 17:29:42 - INFO - __main__ - Namespace(task_dir='data/crawl_domain/', task_name='crawl_domain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-crawl_domain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 17:29:42 - INFO - __main__ - models/T5-large-ft-random/singletask-crawl_domain
03/10/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 17:29:42 - INFO - __main__ - args.device: cuda:0
03/10/2022 17:29:42 - INFO - __main__ - Using 2 gpus
03/10/2022 17:29:42 - INFO - __main__ - Fine-tuning the following samples: ['crawl_domain_32_100', 'crawl_domain_32_13', 'crawl_domain_32_21', 'crawl_domain_32_42', 'crawl_domain_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 17:29:42 - INFO - __main__ - args.device: cuda:1
03/10/2022 17:29:42 - INFO - __main__ - Using 2 gpus
03/10/2022 17:29:42 - INFO - __main__ - Fine-tuning the following samples: ['crawl_domain_32_100', 'crawl_domain_32_13', 'crawl_domain_32_21', 'crawl_domain_32_42', 'crawl_domain_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 17:29:49 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0005, bsz=8 ...
03/10/2022 17:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:29:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:29:50 - INFO - __main__ - ['List Cast']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:29:50 - INFO - __main__ - ['Leidinger']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:29:50 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:29:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:29:50 - INFO - __main__ - ['List Cast']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:29:50 - INFO - __main__ - ['Leidinger']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:29:50 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:29:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:29:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:29:50 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:29:50 - INFO - __main__ - ['Friend']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:29:50 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:29:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:29:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:29:50 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:29:50 - INFO - __main__ - ['Friend']
03/10/2022 17:29:50 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:29:50 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:29:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:29:50 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:29:50 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:30:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:30:02 - INFO - __main__ - Starting training!
03/10/2022 17:30:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:30:02 - INFO - __main__ - Starting training!
03/10/2022 17:30:06 - INFO - __main__ - Step 10 Global step 10 Train loss 19.770868 on epoch=4
03/10/2022 17:30:11 - INFO - __main__ - Step 20 Global step 20 Train loss 17.858011 on epoch=9
03/10/2022 17:30:15 - INFO - __main__ - Step 30 Global step 30 Train loss 11.591599 on epoch=14
03/10/2022 17:30:20 - INFO - __main__ - Step 40 Global step 40 Train loss 10.363333 on epoch=19
03/10/2022 17:30:25 - INFO - __main__ - Step 50 Global step 50 Train loss 9.019181 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 17:30:25 - INFO - __main__ - Global step 50 Train loss 13.720598 EM 0.0 on epoch=24
03/10/2022 17:30:31 - INFO - __main__ - Step 60 Global step 60 Train loss 7.347328 on epoch=29
03/10/2022 17:30:36 - INFO - __main__ - Step 70 Global step 70 Train loss 5.937170 on epoch=34
03/10/2022 17:30:41 - INFO - __main__ - Step 80 Global step 80 Train loss 4.976209 on epoch=39
03/10/2022 17:30:45 - INFO - __main__ - Step 90 Global step 90 Train loss 4.079841 on epoch=44
03/10/2022 17:30:50 - INFO - __main__ - Step 100 Global step 100 Train loss 3.090210 on epoch=49
03/10/2022 17:30:51 - INFO - __main__ - Global step 100 Train loss 5.086152 EM 0.0 on epoch=49
03/10/2022 17:30:55 - INFO - __main__ - Step 110 Global step 110 Train loss 2.594819 on epoch=54
03/10/2022 17:31:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.209362 on epoch=59
03/10/2022 17:31:05 - INFO - __main__ - Step 130 Global step 130 Train loss 2.099404 on epoch=64
03/10/2022 17:31:09 - INFO - __main__ - Step 140 Global step 140 Train loss 2.077057 on epoch=69
03/10/2022 17:31:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.863445 on epoch=74
03/10/2022 17:31:15 - INFO - __main__ - Global step 150 Train loss 2.168818 EM 0.0 on epoch=74
03/10/2022 17:31:19 - INFO - __main__ - Step 160 Global step 160 Train loss 1.926301 on epoch=79
03/10/2022 17:31:24 - INFO - __main__ - Step 170 Global step 170 Train loss 2.012275 on epoch=84
03/10/2022 17:31:29 - INFO - __main__ - Step 180 Global step 180 Train loss 1.606894 on epoch=89
03/10/2022 17:31:34 - INFO - __main__ - Step 190 Global step 190 Train loss 1.542193 on epoch=94
03/10/2022 17:31:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.354919 on epoch=99
03/10/2022 17:31:39 - INFO - __main__ - Global step 200 Train loss 1.688516 EM 0.0 on epoch=99
03/10/2022 17:31:43 - INFO - __main__ - Step 210 Global step 210 Train loss 1.469077 on epoch=104
03/10/2022 17:31:48 - INFO - __main__ - Step 220 Global step 220 Train loss 1.434195 on epoch=109
03/10/2022 17:31:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.369336 on epoch=114
03/10/2022 17:31:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.205539 on epoch=119
03/10/2022 17:32:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.233317 on epoch=124
03/10/2022 17:32:02 - INFO - __main__ - Global step 250 Train loss 1.342293 EM 0.0 on epoch=124
03/10/2022 17:32:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.366744 on epoch=129
03/10/2022 17:32:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.153974 on epoch=134
03/10/2022 17:32:16 - INFO - __main__ - Step 280 Global step 280 Train loss 1.239811 on epoch=139
03/10/2022 17:32:21 - INFO - __main__ - Step 290 Global step 290 Train loss 1.066404 on epoch=144
03/10/2022 17:32:26 - INFO - __main__ - Step 300 Global step 300 Train loss 1.012691 on epoch=149
03/10/2022 17:32:26 - INFO - __main__ - Global step 300 Train loss 1.167925 EM 0.0 on epoch=149
03/10/2022 17:32:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.968545 on epoch=154
03/10/2022 17:32:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.970211 on epoch=159
03/10/2022 17:32:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.006533 on epoch=164
03/10/2022 17:32:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.984134 on epoch=169
03/10/2022 17:32:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.947628 on epoch=174
03/10/2022 17:32:50 - INFO - __main__ - Global step 350 Train loss 0.975410 EM 0.0 on epoch=174
03/10/2022 17:32:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.019074 on epoch=179
03/10/2022 17:32:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.967517 on epoch=184
03/10/2022 17:33:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.860765 on epoch=189
03/10/2022 17:33:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.842457 on epoch=194
03/10/2022 17:33:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.852712 on epoch=199
03/10/2022 17:33:13 - INFO - __main__ - Global step 400 Train loss 0.908505 EM 0.0 on epoch=199
03/10/2022 17:33:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.873724 on epoch=204
03/10/2022 17:33:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.817979 on epoch=209
03/10/2022 17:33:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.759369 on epoch=214
03/10/2022 17:33:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.754455 on epoch=219
03/10/2022 17:33:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.790121 on epoch=224
03/10/2022 17:33:37 - INFO - __main__ - Global step 450 Train loss 0.799130 EM 0.0 on epoch=224
03/10/2022 17:33:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.827518 on epoch=229
03/10/2022 17:33:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.858680 on epoch=234
03/10/2022 17:33:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.761981 on epoch=239
03/10/2022 17:33:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.751088 on epoch=244
03/10/2022 17:34:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.708959 on epoch=249
03/10/2022 17:34:00 - INFO - __main__ - Global step 500 Train loss 0.781645 EM 0.0 on epoch=249
03/10/2022 17:34:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.737379 on epoch=254
03/10/2022 17:34:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.707467 on epoch=259
03/10/2022 17:34:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.749701 on epoch=264
03/10/2022 17:34:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.720468 on epoch=269
03/10/2022 17:34:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.736473 on epoch=274
03/10/2022 17:34:24 - INFO - __main__ - Global step 550 Train loss 0.730298 EM 0.0 on epoch=274
03/10/2022 17:34:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.734786 on epoch=279
03/10/2022 17:34:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.723345 on epoch=284
03/10/2022 17:34:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.735425 on epoch=289
03/10/2022 17:34:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.729330 on epoch=294
03/10/2022 17:34:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.687948 on epoch=299
03/10/2022 17:34:48 - INFO - __main__ - Global step 600 Train loss 0.722167 EM 0.0 on epoch=299
03/10/2022 17:34:48 - INFO - __main__ - save last model!
03/10/2022 17:34:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:34:49 - INFO - __main__ - Printing 3 examples
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:34:49 - INFO - __main__ - ['List Cast']
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:34:49 - INFO - __main__ - ['Leidinger']
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:34:49 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:34:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:34:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:34:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:34:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:34:49 - INFO - __main__ - Printing 3 examples
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:34:49 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:34:49 - INFO - __main__ - ['Friend']
03/10/2022 17:34:49 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:34:49 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:34:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:34:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:34:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:34:57 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:34:57 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 17:34:57 - INFO - __main__ - Printing 3 examples
03/10/2022 17:34:57 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 17:34:57 - INFO - __main__ - ['Whole S M']
03/10/2022 17:34:57 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 17:34:57 - INFO - __main__ - ['pushin Daisies']
03/10/2022 17:34:57 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 17:34:57 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 17:34:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:34:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:34:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:34:58 - INFO - __main__ - Starting training!
03/10/2022 17:35:00 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 17:37:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0005_8_predictions.txt
03/10/2022 17:37:48 - INFO - __main__ - EM on test data: 0.0041
03/10/2022 17:37:49 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.00409626216077829
03/10/2022 17:37:49 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0003, bsz=8 ...
03/10/2022 17:37:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:37:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:37:50 - INFO - __main__ - ['List Cast']
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:37:50 - INFO - __main__ - ['Leidinger']
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:37:50 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:37:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:37:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:37:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:37:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:37:50 - INFO - __main__ - Printing 3 examples
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:37:50 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:37:50 - INFO - __main__ - ['Friend']
03/10/2022 17:37:50 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:37:50 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:37:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:37:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:37:50 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:38:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:38:00 - INFO - __main__ - Starting training!
03/10/2022 17:38:04 - INFO - __main__ - Step 10 Global step 10 Train loss 20.058319 on epoch=4
03/10/2022 17:38:09 - INFO - __main__ - Step 20 Global step 20 Train loss 17.285984 on epoch=9
03/10/2022 17:38:13 - INFO - __main__ - Step 30 Global step 30 Train loss 12.834514 on epoch=14
03/10/2022 17:38:18 - INFO - __main__ - Step 40 Global step 40 Train loss 10.951889 on epoch=19
03/10/2022 17:38:23 - INFO - __main__ - Step 50 Global step 50 Train loss 10.133731 on epoch=24
03/10/2022 17:38:32 - INFO - __main__ - Global step 50 Train loss 14.252887 EM 0.03125 on epoch=24
03/10/2022 17:38:37 - INFO - __main__ - Step 60 Global step 60 Train loss 9.018760 on epoch=29
03/10/2022 17:38:42 - INFO - __main__ - Step 70 Global step 70 Train loss 8.067491 on epoch=34
03/10/2022 17:38:46 - INFO - __main__ - Step 80 Global step 80 Train loss 7.192767 on epoch=39
03/10/2022 17:38:51 - INFO - __main__ - Step 90 Global step 90 Train loss 5.996198 on epoch=44
03/10/2022 17:38:56 - INFO - __main__ - Step 100 Global step 100 Train loss 5.789332 on epoch=49
03/10/2022 17:38:57 - INFO - __main__ - Global step 100 Train loss 7.212910 EM 0.0 on epoch=49
03/10/2022 17:39:01 - INFO - __main__ - Step 110 Global step 110 Train loss 5.290674 on epoch=54
03/10/2022 17:39:06 - INFO - __main__ - Step 120 Global step 120 Train loss 4.636157 on epoch=59
03/10/2022 17:39:11 - INFO - __main__ - Step 130 Global step 130 Train loss 4.244074 on epoch=64
03/10/2022 17:39:16 - INFO - __main__ - Step 140 Global step 140 Train loss 4.428479 on epoch=69
03/10/2022 17:39:20 - INFO - __main__ - Step 150 Global step 150 Train loss 3.956909 on epoch=74
03/10/2022 17:39:21 - INFO - __main__ - Global step 150 Train loss 4.511259 EM 0.03125 on epoch=74
03/10/2022 17:39:26 - INFO - __main__ - Step 160 Global step 160 Train loss 3.421632 on epoch=79
03/10/2022 17:39:31 - INFO - __main__ - Step 170 Global step 170 Train loss 3.098789 on epoch=84
03/10/2022 17:39:35 - INFO - __main__ - Step 180 Global step 180 Train loss 2.777063 on epoch=89
03/10/2022 17:39:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.598073 on epoch=94
03/10/2022 17:39:45 - INFO - __main__ - Step 200 Global step 200 Train loss 2.312686 on epoch=99
03/10/2022 17:39:46 - INFO - __main__ - Global step 200 Train loss 2.841649 EM 0.0 on epoch=99
03/10/2022 17:39:50 - INFO - __main__ - Step 210 Global step 210 Train loss 2.723462 on epoch=104
03/10/2022 17:39:55 - INFO - __main__ - Step 220 Global step 220 Train loss 2.109910 on epoch=109
03/10/2022 17:40:00 - INFO - __main__ - Step 230 Global step 230 Train loss 1.845263 on epoch=114
03/10/2022 17:40:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.840023 on epoch=119
03/10/2022 17:40:10 - INFO - __main__ - Step 250 Global step 250 Train loss 1.845993 on epoch=124
03/10/2022 17:40:10 - INFO - __main__ - Global step 250 Train loss 2.072930 EM 0.0 on epoch=124
03/10/2022 17:40:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.664130 on epoch=129
03/10/2022 17:40:20 - INFO - __main__ - Step 270 Global step 270 Train loss 1.814436 on epoch=134
03/10/2022 17:40:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.787267 on epoch=139
03/10/2022 17:40:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.486624 on epoch=144
03/10/2022 17:40:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.567119 on epoch=149
03/10/2022 17:40:35 - INFO - __main__ - Global step 300 Train loss 1.663915 EM 0.0 on epoch=149
03/10/2022 17:40:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.553427 on epoch=154
03/10/2022 17:40:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.681809 on epoch=159
03/10/2022 17:40:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.619184 on epoch=164
03/10/2022 17:40:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.467288 on epoch=169
03/10/2022 17:40:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.403820 on epoch=174
03/10/2022 17:40:59 - INFO - __main__ - Global step 350 Train loss 1.545106 EM 0.0 on epoch=174
03/10/2022 17:41:04 - INFO - __main__ - Step 360 Global step 360 Train loss 1.301231 on epoch=179
03/10/2022 17:41:09 - INFO - __main__ - Step 370 Global step 370 Train loss 1.393660 on epoch=184
03/10/2022 17:41:14 - INFO - __main__ - Step 380 Global step 380 Train loss 1.140334 on epoch=189
03/10/2022 17:41:18 - INFO - __main__ - Step 390 Global step 390 Train loss 1.256243 on epoch=194
03/10/2022 17:41:23 - INFO - __main__ - Step 400 Global step 400 Train loss 1.291202 on epoch=199
03/10/2022 17:41:24 - INFO - __main__ - Global step 400 Train loss 1.276534 EM 0.0 on epoch=199
03/10/2022 17:41:28 - INFO - __main__ - Step 410 Global step 410 Train loss 1.248139 on epoch=204
03/10/2022 17:41:33 - INFO - __main__ - Step 420 Global step 420 Train loss 1.390286 on epoch=209
03/10/2022 17:41:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.125880 on epoch=214
03/10/2022 17:41:43 - INFO - __main__ - Step 440 Global step 440 Train loss 1.204214 on epoch=219
03/10/2022 17:41:48 - INFO - __main__ - Step 450 Global step 450 Train loss 1.172266 on epoch=224
03/10/2022 17:41:48 - INFO - __main__ - Global step 450 Train loss 1.228157 EM 0.0 on epoch=224
03/10/2022 17:41:53 - INFO - __main__ - Step 460 Global step 460 Train loss 1.149371 on epoch=229
03/10/2022 17:41:58 - INFO - __main__ - Step 470 Global step 470 Train loss 1.099946 on epoch=234
03/10/2022 17:42:03 - INFO - __main__ - Step 480 Global step 480 Train loss 1.016441 on epoch=239
03/10/2022 17:42:08 - INFO - __main__ - Step 490 Global step 490 Train loss 1.018435 on epoch=244
03/10/2022 17:42:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.084572 on epoch=249
03/10/2022 17:42:13 - INFO - __main__ - Global step 500 Train loss 1.073753 EM 0.0 on epoch=249
03/10/2022 17:42:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.090947 on epoch=254
03/10/2022 17:42:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.936741 on epoch=259
03/10/2022 17:42:27 - INFO - __main__ - Step 530 Global step 530 Train loss 1.028027 on epoch=264
03/10/2022 17:42:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.911998 on epoch=269
03/10/2022 17:42:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.910192 on epoch=274
03/10/2022 17:42:38 - INFO - __main__ - Global step 550 Train loss 0.975581 EM 0.0 on epoch=274
03/10/2022 17:42:42 - INFO - __main__ - Step 560 Global step 560 Train loss 1.053294 on epoch=279
03/10/2022 17:42:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.996756 on epoch=284
03/10/2022 17:42:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.982844 on epoch=289
03/10/2022 17:42:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.845429 on epoch=294
03/10/2022 17:43:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.937841 on epoch=299
03/10/2022 17:43:02 - INFO - __main__ - Global step 600 Train loss 0.963233 EM 0.0 on epoch=299
03/10/2022 17:43:02 - INFO - __main__ - save last model!
03/10/2022 17:43:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:43:03 - INFO - __main__ - Printing 3 examples
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:43:03 - INFO - __main__ - ['List Cast']
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:43:03 - INFO - __main__ - ['Leidinger']
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:43:03 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:43:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:43:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:43:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:43:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:43:03 - INFO - __main__ - Printing 3 examples
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:43:03 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:43:03 - INFO - __main__ - ['Friend']
03/10/2022 17:43:03 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:43:03 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:43:03 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:43:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:43:03 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:43:09 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:43:10 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 17:43:10 - INFO - __main__ - Printing 3 examples
03/10/2022 17:43:10 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 17:43:10 - INFO - __main__ - ['Whole S M']
03/10/2022 17:43:10 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 17:43:10 - INFO - __main__ - ['pushin Daisies']
03/10/2022 17:43:10 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 17:43:10 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 17:43:10 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:43:11 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:43:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:43:12 - INFO - __main__ - Starting training!
03/10/2022 17:43:13 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 17:51:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0003_8_predictions.txt
03/10/2022 17:51:42 - INFO - __main__ - EM on test data: 0.0189
03/10/2022 17:51:42 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.01894521249359959
03/10/2022 17:51:42 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0002, bsz=8 ...
03/10/2022 17:51:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:51:43 - INFO - __main__ - Printing 3 examples
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:51:43 - INFO - __main__ - ['List Cast']
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:51:43 - INFO - __main__ - ['Leidinger']
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:51:43 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:51:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:51:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:51:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:51:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:51:43 - INFO - __main__ - Printing 3 examples
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:51:43 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:51:43 - INFO - __main__ - ['Friend']
03/10/2022 17:51:43 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:51:43 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:51:43 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:51:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:51:43 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:51:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:51:53 - INFO - __main__ - Starting training!
03/10/2022 17:51:57 - INFO - __main__ - Step 10 Global step 10 Train loss 20.302624 on epoch=4
03/10/2022 17:52:01 - INFO - __main__ - Step 20 Global step 20 Train loss 17.525249 on epoch=9
03/10/2022 17:52:06 - INFO - __main__ - Step 30 Global step 30 Train loss 15.431314 on epoch=14
03/10/2022 17:52:11 - INFO - __main__ - Step 40 Global step 40 Train loss 13.426900 on epoch=19
03/10/2022 17:52:16 - INFO - __main__ - Step 50 Global step 50 Train loss 11.655492 on epoch=24
03/10/2022 17:52:25 - INFO - __main__ - Global step 50 Train loss 15.668314 EM 0.0 on epoch=24
03/10/2022 17:52:30 - INFO - __main__ - Step 60 Global step 60 Train loss 11.400643 on epoch=29
03/10/2022 17:52:35 - INFO - __main__ - Step 70 Global step 70 Train loss 10.374076 on epoch=34
03/10/2022 17:52:39 - INFO - __main__ - Step 80 Global step 80 Train loss 9.824293 on epoch=39
03/10/2022 17:52:44 - INFO - __main__ - Step 90 Global step 90 Train loss 8.790123 on epoch=44
03/10/2022 17:52:49 - INFO - __main__ - Step 100 Global step 100 Train loss 8.288466 on epoch=49
03/10/2022 17:52:50 - INFO - __main__ - Global step 100 Train loss 9.735520 EM 0.0 on epoch=49
03/10/2022 17:52:55 - INFO - __main__ - Step 110 Global step 110 Train loss 7.993811 on epoch=54
03/10/2022 17:53:00 - INFO - __main__ - Step 120 Global step 120 Train loss 6.720711 on epoch=59
03/10/2022 17:53:05 - INFO - __main__ - Step 130 Global step 130 Train loss 7.594251 on epoch=64
03/10/2022 17:53:09 - INFO - __main__ - Step 140 Global step 140 Train loss 6.837809 on epoch=69
03/10/2022 17:53:14 - INFO - __main__ - Step 150 Global step 150 Train loss 6.374009 on epoch=74
03/10/2022 17:53:15 - INFO - __main__ - Global step 150 Train loss 7.104118 EM 0.0 on epoch=74
03/10/2022 17:53:20 - INFO - __main__ - Step 160 Global step 160 Train loss 5.804210 on epoch=79
03/10/2022 17:53:25 - INFO - __main__ - Step 170 Global step 170 Train loss 5.467048 on epoch=84
03/10/2022 17:53:30 - INFO - __main__ - Step 180 Global step 180 Train loss 5.053056 on epoch=89
03/10/2022 17:53:34 - INFO - __main__ - Step 190 Global step 190 Train loss 4.673263 on epoch=94
03/10/2022 17:53:39 - INFO - __main__ - Step 200 Global step 200 Train loss 4.323789 on epoch=99
03/10/2022 17:53:40 - INFO - __main__ - Global step 200 Train loss 5.064273 EM 0.0 on epoch=99
03/10/2022 17:53:45 - INFO - __main__ - Step 210 Global step 210 Train loss 4.073206 on epoch=104
03/10/2022 17:53:49 - INFO - __main__ - Step 220 Global step 220 Train loss 3.823708 on epoch=109
03/10/2022 17:53:54 - INFO - __main__ - Step 230 Global step 230 Train loss 3.769143 on epoch=114
03/10/2022 17:53:59 - INFO - __main__ - Step 240 Global step 240 Train loss 3.427969 on epoch=119
03/10/2022 17:54:04 - INFO - __main__ - Step 250 Global step 250 Train loss 3.460960 on epoch=124
03/10/2022 17:54:05 - INFO - __main__ - Global step 250 Train loss 3.710997 EM 0.0 on epoch=124
03/10/2022 17:54:09 - INFO - __main__ - Step 260 Global step 260 Train loss 3.094649 on epoch=129
03/10/2022 17:54:14 - INFO - __main__ - Step 270 Global step 270 Train loss 2.637827 on epoch=134
03/10/2022 17:54:19 - INFO - __main__ - Step 280 Global step 280 Train loss 2.386196 on epoch=139
03/10/2022 17:54:24 - INFO - __main__ - Step 290 Global step 290 Train loss 2.814116 on epoch=144
03/10/2022 17:54:29 - INFO - __main__ - Step 300 Global step 300 Train loss 2.234683 on epoch=149
03/10/2022 17:54:29 - INFO - __main__ - Global step 300 Train loss 2.633494 EM 0.0 on epoch=149
03/10/2022 17:54:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.205093 on epoch=154
03/10/2022 17:54:39 - INFO - __main__ - Step 320 Global step 320 Train loss 1.900340 on epoch=159
03/10/2022 17:54:43 - INFO - __main__ - Step 330 Global step 330 Train loss 1.918033 on epoch=164
03/10/2022 17:54:48 - INFO - __main__ - Step 340 Global step 340 Train loss 2.145755 on epoch=169
03/10/2022 17:54:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.994842 on epoch=174
03/10/2022 17:54:53 - INFO - __main__ - Global step 350 Train loss 2.032812 EM 0.0 on epoch=174
03/10/2022 17:54:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.883506 on epoch=179
03/10/2022 17:55:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.581956 on epoch=184
03/10/2022 17:55:08 - INFO - __main__ - Step 380 Global step 380 Train loss 1.663433 on epoch=189
03/10/2022 17:55:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.657005 on epoch=194
03/10/2022 17:55:18 - INFO - __main__ - Step 400 Global step 400 Train loss 1.704031 on epoch=199
03/10/2022 17:55:18 - INFO - __main__ - Global step 400 Train loss 1.697986 EM 0.0 on epoch=199
03/10/2022 17:55:23 - INFO - __main__ - Step 410 Global step 410 Train loss 1.581368 on epoch=204
03/10/2022 17:55:28 - INFO - __main__ - Step 420 Global step 420 Train loss 1.722572 on epoch=209
03/10/2022 17:55:33 - INFO - __main__ - Step 430 Global step 430 Train loss 1.583027 on epoch=214
03/10/2022 17:55:38 - INFO - __main__ - Step 440 Global step 440 Train loss 1.617220 on epoch=219
03/10/2022 17:55:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.540220 on epoch=224
03/10/2022 17:55:43 - INFO - __main__ - Global step 450 Train loss 1.608881 EM 0.0 on epoch=224
03/10/2022 17:55:48 - INFO - __main__ - Step 460 Global step 460 Train loss 1.593560 on epoch=229
03/10/2022 17:55:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.433217 on epoch=234
03/10/2022 17:55:58 - INFO - __main__ - Step 480 Global step 480 Train loss 1.461754 on epoch=239
03/10/2022 17:56:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.406774 on epoch=244
03/10/2022 17:56:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.486869 on epoch=249
03/10/2022 17:56:08 - INFO - __main__ - Global step 500 Train loss 1.476435 EM 0.0 on epoch=249
03/10/2022 17:56:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.395399 on epoch=254
03/10/2022 17:56:17 - INFO - __main__ - Step 520 Global step 520 Train loss 1.526172 on epoch=259
03/10/2022 17:56:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.281569 on epoch=264
03/10/2022 17:56:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.433830 on epoch=269
03/10/2022 17:56:32 - INFO - __main__ - Step 550 Global step 550 Train loss 1.227060 on epoch=274
03/10/2022 17:56:33 - INFO - __main__ - Global step 550 Train loss 1.372806 EM 0.0 on epoch=274
03/10/2022 17:56:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.215128 on epoch=279
03/10/2022 17:56:42 - INFO - __main__ - Step 570 Global step 570 Train loss 1.311141 on epoch=284
03/10/2022 17:56:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.235101 on epoch=289
03/10/2022 17:56:52 - INFO - __main__ - Step 590 Global step 590 Train loss 1.238240 on epoch=294
03/10/2022 17:56:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.282243 on epoch=299
03/10/2022 17:56:57 - INFO - __main__ - Global step 600 Train loss 1.256371 EM 0.0 on epoch=299
03/10/2022 17:56:57 - INFO - __main__ - save last model!
03/10/2022 17:56:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:56:58 - INFO - __main__ - Printing 3 examples
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 17:56:58 - INFO - __main__ - ['List Cast']
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 17:56:58 - INFO - __main__ - ['Leidinger']
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 17:56:58 - INFO - __main__ - ['Successful Meetings']
03/10/2022 17:56:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:56:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:56:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:56:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:56:58 - INFO - __main__ - Printing 3 examples
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 17:56:58 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 17:56:58 - INFO - __main__ - ['Friend']
03/10/2022 17:56:58 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 17:56:58 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 17:56:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:56:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:56:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:57:04 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 17:57:05 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 17:57:05 - INFO - __main__ - Printing 3 examples
03/10/2022 17:57:05 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 17:57:05 - INFO - __main__ - ['Whole S M']
03/10/2022 17:57:05 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 17:57:05 - INFO - __main__ - ['pushin Daisies']
03/10/2022 17:57:05 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 17:57:05 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 17:57:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:57:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:57:08 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 17:57:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 17:57:09 - INFO - __main__ - Starting training!
03/10/2022 18:04:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0002_8_predictions.txt
03/10/2022 18:04:38 - INFO - __main__ - EM on test data: 0.0010
03/10/2022 18:04:39 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0010240655401945725
03/10/2022 18:04:39 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0001, bsz=8 ...
03/10/2022 18:04:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:04:40 - INFO - __main__ - Printing 3 examples
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] listcast
03/10/2022 18:04:40 - INFO - __main__ - ['List Cast']
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] leidinger
03/10/2022 18:04:40 - INFO - __main__ - ['Leidinger']
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/10/2022 18:04:40 - INFO - __main__ - ['Successful Meetings']
03/10/2022 18:04:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:04:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:04:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:04:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:04:40 - INFO - __main__ - Printing 3 examples
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/10/2022 18:04:40 - INFO - __main__ - ['Touchscreen hardware']
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] friend
03/10/2022 18:04:40 - INFO - __main__ - ['Friend']
03/10/2022 18:04:40 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/10/2022 18:04:40 - INFO - __main__ - ['Peter Butler Properties']
03/10/2022 18:04:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:04:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:04:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:04:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:04:50 - INFO - __main__ - Starting training!
03/10/2022 18:04:55 - INFO - __main__ - Step 10 Global step 10 Train loss 19.892750 on epoch=4
03/10/2022 18:04:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.873722 on epoch=9
03/10/2022 18:05:04 - INFO - __main__ - Step 30 Global step 30 Train loss 18.152744 on epoch=14
03/10/2022 18:05:09 - INFO - __main__ - Step 40 Global step 40 Train loss 17.566187 on epoch=19
03/10/2022 18:05:14 - INFO - __main__ - Step 50 Global step 50 Train loss 15.593717 on epoch=24
03/10/2022 18:05:26 - INFO - __main__ - Global step 50 Train loss 18.215822 EM 0.0 on epoch=24
03/10/2022 18:05:31 - INFO - __main__ - Step 60 Global step 60 Train loss 15.093424 on epoch=29
03/10/2022 18:05:36 - INFO - __main__ - Step 70 Global step 70 Train loss 14.112646 on epoch=34
03/10/2022 18:05:41 - INFO - __main__ - Step 80 Global step 80 Train loss 13.822725 on epoch=39
03/10/2022 18:05:45 - INFO - __main__ - Step 90 Global step 90 Train loss 12.845247 on epoch=44
03/10/2022 18:05:50 - INFO - __main__ - Step 100 Global step 100 Train loss 12.912407 on epoch=49
03/10/2022 18:05:58 - INFO - __main__ - Global step 100 Train loss 13.757290 EM 0.0 on epoch=49
03/10/2022 18:06:03 - INFO - __main__ - Step 110 Global step 110 Train loss 11.940862 on epoch=54
03/10/2022 18:06:08 - INFO - __main__ - Step 120 Global step 120 Train loss 11.783564 on epoch=59
03/10/2022 18:06:13 - INFO - __main__ - Step 130 Global step 130 Train loss 11.784189 on epoch=64
03/10/2022 18:06:17 - INFO - __main__ - Step 140 Global step 140 Train loss 10.742000 on epoch=69
03/10/2022 18:06:22 - INFO - __main__ - Step 150 Global step 150 Train loss 11.082103 on epoch=74
03/10/2022 18:06:23 - INFO - __main__ - Global step 150 Train loss 11.466544 EM 0.0 on epoch=74
03/10/2022 18:06:28 - INFO - __main__ - Step 160 Global step 160 Train loss 10.430242 on epoch=79
03/10/2022 18:06:33 - INFO - __main__ - Step 170 Global step 170 Train loss 10.046401 on epoch=84
03/10/2022 18:06:37 - INFO - __main__ - Step 180 Global step 180 Train loss 9.588122 on epoch=89
03/10/2022 18:06:42 - INFO - __main__ - Step 190 Global step 190 Train loss 9.593474 on epoch=94
03/10/2022 18:06:47 - INFO - __main__ - Step 200 Global step 200 Train loss 9.218088 on epoch=99
03/10/2022 18:06:48 - INFO - __main__ - Global step 200 Train loss 9.775266 EM 0.0 on epoch=99
03/10/2022 18:06:53 - INFO - __main__ - Step 210 Global step 210 Train loss 9.105588 on epoch=104
03/10/2022 18:06:57 - INFO - __main__ - Step 220 Global step 220 Train loss 8.880101 on epoch=109
03/10/2022 18:07:02 - INFO - __main__ - Step 230 Global step 230 Train loss 8.684797 on epoch=114
03/10/2022 18:07:07 - INFO - __main__ - Step 240 Global step 240 Train loss 8.299627 on epoch=119
03/10/2022 18:07:12 - INFO - __main__ - Step 250 Global step 250 Train loss 8.267752 on epoch=124
03/10/2022 18:07:12 - INFO - __main__ - Global step 250 Train loss 8.647573 EM 0.0 on epoch=124
03/10/2022 18:07:17 - INFO - __main__ - Step 260 Global step 260 Train loss 7.667945 on epoch=129
03/10/2022 18:07:22 - INFO - __main__ - Step 270 Global step 270 Train loss 7.801826 on epoch=134
03/10/2022 18:07:27 - INFO - __main__ - Step 280 Global step 280 Train loss 7.277096 on epoch=139
03/10/2022 18:07:32 - INFO - __main__ - Step 290 Global step 290 Train loss 6.948691 on epoch=144
03/10/2022 18:07:36 - INFO - __main__ - Step 300 Global step 300 Train loss 7.253336 on epoch=149
03/10/2022 18:07:37 - INFO - __main__ - Global step 300 Train loss 7.389779 EM 0.0 on epoch=149
03/10/2022 18:07:42 - INFO - __main__ - Step 310 Global step 310 Train loss 7.075575 on epoch=154
03/10/2022 18:07:47 - INFO - __main__ - Step 320 Global step 320 Train loss 6.702263 on epoch=159
03/10/2022 18:07:51 - INFO - __main__ - Step 330 Global step 330 Train loss 6.518688 on epoch=164
03/10/2022 18:07:56 - INFO - __main__ - Step 340 Global step 340 Train loss 6.456491 on epoch=169
03/10/2022 18:08:01 - INFO - __main__ - Step 350 Global step 350 Train loss 6.543900 on epoch=174
03/10/2022 18:08:01 - INFO - __main__ - Global step 350 Train loss 6.659384 EM 0.0 on epoch=174
03/10/2022 18:08:06 - INFO - __main__ - Step 360 Global step 360 Train loss 6.289783 on epoch=179
03/10/2022 18:08:11 - INFO - __main__ - Step 370 Global step 370 Train loss 5.911571 on epoch=184
03/10/2022 18:08:16 - INFO - __main__ - Step 380 Global step 380 Train loss 5.887015 on epoch=189
03/10/2022 18:08:20 - INFO - __main__ - Step 390 Global step 390 Train loss 5.432060 on epoch=194
03/10/2022 18:08:25 - INFO - __main__ - Step 400 Global step 400 Train loss 5.341794 on epoch=199
03/10/2022 18:08:26 - INFO - __main__ - Global step 400 Train loss 5.772444 EM 0.0 on epoch=199
03/10/2022 18:08:31 - INFO - __main__ - Step 410 Global step 410 Train loss 5.309854 on epoch=204
03/10/2022 18:08:35 - INFO - __main__ - Step 420 Global step 420 Train loss 4.925790 on epoch=209
03/10/2022 18:08:40 - INFO - __main__ - Step 430 Global step 430 Train loss 4.971354 on epoch=214
03/10/2022 18:08:45 - INFO - __main__ - Step 440 Global step 440 Train loss 4.645549 on epoch=219
03/10/2022 18:08:50 - INFO - __main__ - Step 450 Global step 450 Train loss 4.207684 on epoch=224
03/10/2022 18:08:50 - INFO - __main__ - Global step 450 Train loss 4.812046 EM 0.0 on epoch=224
03/10/2022 18:08:55 - INFO - __main__ - Step 460 Global step 460 Train loss 4.185947 on epoch=229
03/10/2022 18:09:00 - INFO - __main__ - Step 470 Global step 470 Train loss 4.090515 on epoch=234
03/10/2022 18:09:05 - INFO - __main__ - Step 480 Global step 480 Train loss 3.888356 on epoch=239
03/10/2022 18:09:09 - INFO - __main__ - Step 490 Global step 490 Train loss 3.729445 on epoch=244
03/10/2022 18:09:14 - INFO - __main__ - Step 500 Global step 500 Train loss 3.697469 on epoch=249
03/10/2022 18:09:15 - INFO - __main__ - Global step 500 Train loss 3.918346 EM 0.0 on epoch=249
03/10/2022 18:09:20 - INFO - __main__ - Step 510 Global step 510 Train loss 3.292602 on epoch=254
03/10/2022 18:09:24 - INFO - __main__ - Step 520 Global step 520 Train loss 3.118775 on epoch=259
03/10/2022 18:09:29 - INFO - __main__ - Step 530 Global step 530 Train loss 3.084145 on epoch=264
03/10/2022 18:09:34 - INFO - __main__ - Step 540 Global step 540 Train loss 2.871497 on epoch=269
03/10/2022 18:09:39 - INFO - __main__ - Step 550 Global step 550 Train loss 2.825990 on epoch=274
03/10/2022 18:09:39 - INFO - __main__ - Global step 550 Train loss 3.038602 EM 0.0 on epoch=274
03/10/2022 18:09:44 - INFO - __main__ - Step 560 Global step 560 Train loss 2.658493 on epoch=279
03/10/2022 18:09:49 - INFO - __main__ - Step 570 Global step 570 Train loss 2.639215 on epoch=284
03/10/2022 18:09:53 - INFO - __main__ - Step 580 Global step 580 Train loss 2.546841 on epoch=289
03/10/2022 18:09:58 - INFO - __main__ - Step 590 Global step 590 Train loss 2.159204 on epoch=294
03/10/2022 18:10:03 - INFO - __main__ - Step 600 Global step 600 Train loss 2.319166 on epoch=299
03/10/2022 18:10:04 - INFO - __main__ - Global step 600 Train loss 2.464584 EM 0.0 on epoch=299
03/10/2022 18:10:04 - INFO - __main__ - save last model!
03/10/2022 18:10:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:10:04 - INFO - __main__ - Printing 3 examples
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:10:04 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:10:04 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:10:04 - INFO - __main__ - ['Olio']
03/10/2022 18:10:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:10:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:10:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:10:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:10:04 - INFO - __main__ - Printing 3 examples
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:10:04 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:10:04 - INFO - __main__ - ['Acuren']
03/10/2022 18:10:04 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:10:04 - INFO - __main__ - ['J for Justice']
03/10/2022 18:10:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:10:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:10:04 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:10:10 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:10:11 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:10:11 - INFO - __main__ - Printing 3 examples
03/10/2022 18:10:11 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:10:11 - INFO - __main__ - ['Whole S M']
03/10/2022 18:10:11 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:10:11 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:10:11 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:10:11 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:10:11 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:10:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:10:13 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:10:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:10:14 - INFO - __main__ - Starting training!
03/10/2022 18:20:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0001_8_predictions.txt
03/10/2022 18:20:18 - INFO - __main__ - EM on test data: 0.0046
03/10/2022 18:20:21 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.004608294930875576
03/10/2022 18:20:21 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0005, bsz=8 ...
03/10/2022 18:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:20:22 - INFO - __main__ - Printing 3 examples
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:20:22 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:20:22 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:20:22 - INFO - __main__ - ['Olio']
03/10/2022 18:20:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:20:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:20:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:20:22 - INFO - __main__ - Printing 3 examples
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:20:22 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:20:22 - INFO - __main__ - ['Acuren']
03/10/2022 18:20:22 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:20:22 - INFO - __main__ - ['J for Justice']
03/10/2022 18:20:22 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:20:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:20:22 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:20:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:20:32 - INFO - __main__ - Starting training!
03/10/2022 18:20:36 - INFO - __main__ - Step 10 Global step 10 Train loss 22.169977 on epoch=4
03/10/2022 18:20:40 - INFO - __main__ - Step 20 Global step 20 Train loss 20.221508 on epoch=9
03/10/2022 18:20:45 - INFO - __main__ - Step 30 Global step 30 Train loss 14.238935 on epoch=14
03/10/2022 18:20:50 - INFO - __main__ - Step 40 Global step 40 Train loss 12.513116 on epoch=19
03/10/2022 18:20:54 - INFO - __main__ - Step 50 Global step 50 Train loss 10.752254 on epoch=24
03/10/2022 18:20:57 - INFO - __main__ - Global step 50 Train loss 15.979157 EM 0.03125 on epoch=24
03/10/2022 18:21:02 - INFO - __main__ - Step 60 Global step 60 Train loss 9.530540 on epoch=29
03/10/2022 18:21:07 - INFO - __main__ - Step 70 Global step 70 Train loss 8.314863 on epoch=34
03/10/2022 18:21:11 - INFO - __main__ - Step 80 Global step 80 Train loss 7.085550 on epoch=39
03/10/2022 18:21:16 - INFO - __main__ - Step 90 Global step 90 Train loss 6.243340 on epoch=44
03/10/2022 18:21:21 - INFO - __main__ - Step 100 Global step 100 Train loss 5.203651 on epoch=49
03/10/2022 18:21:22 - INFO - __main__ - Global step 100 Train loss 7.275589 EM 0.0 on epoch=49
03/10/2022 18:21:26 - INFO - __main__ - Step 110 Global step 110 Train loss 4.820553 on epoch=54
03/10/2022 18:21:31 - INFO - __main__ - Step 120 Global step 120 Train loss 3.838403 on epoch=59
03/10/2022 18:21:36 - INFO - __main__ - Step 130 Global step 130 Train loss 3.060731 on epoch=64
03/10/2022 18:21:41 - INFO - __main__ - Step 140 Global step 140 Train loss 2.863333 on epoch=69
03/10/2022 18:21:45 - INFO - __main__ - Step 150 Global step 150 Train loss 3.000518 on epoch=74
03/10/2022 18:21:46 - INFO - __main__ - Global step 150 Train loss 3.516708 EM 0.0 on epoch=74
03/10/2022 18:21:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.357654 on epoch=79
03/10/2022 18:21:56 - INFO - __main__ - Step 170 Global step 170 Train loss 2.844096 on epoch=84
03/10/2022 18:22:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.449019 on epoch=89
03/10/2022 18:22:05 - INFO - __main__ - Step 190 Global step 190 Train loss 2.348243 on epoch=94
03/10/2022 18:22:10 - INFO - __main__ - Step 200 Global step 200 Train loss 2.144969 on epoch=99
03/10/2022 18:22:10 - INFO - __main__ - Global step 200 Train loss 2.428796 EM 0.0 on epoch=99
03/10/2022 18:22:15 - INFO - __main__ - Step 210 Global step 210 Train loss 2.064688 on epoch=104
03/10/2022 18:22:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.799168 on epoch=109
03/10/2022 18:22:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.841351 on epoch=114
03/10/2022 18:22:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.955490 on epoch=119
03/10/2022 18:22:34 - INFO - __main__ - Step 250 Global step 250 Train loss 1.487027 on epoch=124
03/10/2022 18:22:34 - INFO - __main__ - Global step 250 Train loss 1.829545 EM 0.0 on epoch=124
03/10/2022 18:22:39 - INFO - __main__ - Step 260 Global step 260 Train loss 1.884562 on epoch=129
03/10/2022 18:22:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.604274 on epoch=134
03/10/2022 18:22:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.743397 on epoch=139
03/10/2022 18:22:53 - INFO - __main__ - Step 290 Global step 290 Train loss 1.692279 on epoch=144
03/10/2022 18:22:58 - INFO - __main__ - Step 300 Global step 300 Train loss 1.797992 on epoch=149
03/10/2022 18:22:59 - INFO - __main__ - Global step 300 Train loss 1.744501 EM 0.0 on epoch=149
03/10/2022 18:23:03 - INFO - __main__ - Step 310 Global step 310 Train loss 1.600315 on epoch=154
03/10/2022 18:23:08 - INFO - __main__ - Step 320 Global step 320 Train loss 1.424097 on epoch=159
03/10/2022 18:23:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.351904 on epoch=164
03/10/2022 18:23:18 - INFO - __main__ - Step 340 Global step 340 Train loss 1.367917 on epoch=169
03/10/2022 18:23:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.255611 on epoch=174
03/10/2022 18:23:23 - INFO - __main__ - Global step 350 Train loss 1.399969 EM 0.0 on epoch=174
03/10/2022 18:23:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.290851 on epoch=179
03/10/2022 18:23:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.101979 on epoch=184
03/10/2022 18:23:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.288876 on epoch=189
03/10/2022 18:23:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.223155 on epoch=194
03/10/2022 18:23:47 - INFO - __main__ - Step 400 Global step 400 Train loss 1.294296 on epoch=199
03/10/2022 18:23:47 - INFO - __main__ - Global step 400 Train loss 1.239831 EM 0.0 on epoch=199
03/10/2022 18:23:52 - INFO - __main__ - Step 410 Global step 410 Train loss 1.084398 on epoch=204
03/10/2022 18:23:57 - INFO - __main__ - Step 420 Global step 420 Train loss 1.067977 on epoch=209
03/10/2022 18:24:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.128812 on epoch=214
03/10/2022 18:24:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.102931 on epoch=219
03/10/2022 18:24:11 - INFO - __main__ - Step 450 Global step 450 Train loss 1.030616 on epoch=224
03/10/2022 18:24:11 - INFO - __main__ - Global step 450 Train loss 1.082947 EM 0.0 on epoch=224
03/10/2022 18:24:16 - INFO - __main__ - Step 460 Global step 460 Train loss 1.072186 on epoch=229
03/10/2022 18:24:21 - INFO - __main__ - Step 470 Global step 470 Train loss 1.074908 on epoch=234
03/10/2022 18:24:26 - INFO - __main__ - Step 480 Global step 480 Train loss 1.018913 on epoch=239
03/10/2022 18:24:30 - INFO - __main__ - Step 490 Global step 490 Train loss 1.054736 on epoch=244
03/10/2022 18:24:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.986788 on epoch=249
03/10/2022 18:24:36 - INFO - __main__ - Global step 500 Train loss 1.041506 EM 0.0 on epoch=249
03/10/2022 18:24:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.929797 on epoch=254
03/10/2022 18:24:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.933387 on epoch=259
03/10/2022 18:24:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.964878 on epoch=264
03/10/2022 18:24:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.977489 on epoch=269
03/10/2022 18:24:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.914367 on epoch=274
03/10/2022 18:25:00 - INFO - __main__ - Global step 550 Train loss 0.943984 EM 0.0 on epoch=274
03/10/2022 18:25:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.970450 on epoch=279
03/10/2022 18:25:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.965549 on epoch=284
03/10/2022 18:25:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.897421 on epoch=289
03/10/2022 18:25:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.915446 on epoch=294
03/10/2022 18:25:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.924659 on epoch=299
03/10/2022 18:25:24 - INFO - __main__ - Global step 600 Train loss 0.934705 EM 0.0 on epoch=299
03/10/2022 18:25:24 - INFO - __main__ - save last model!
03/10/2022 18:25:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:25:24 - INFO - __main__ - Printing 3 examples
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:25:24 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:25:24 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:25:24 - INFO - __main__ - ['Olio']
03/10/2022 18:25:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:25:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:25:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:25:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:25:24 - INFO - __main__ - Printing 3 examples
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:25:24 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:25:24 - INFO - __main__ - ['Acuren']
03/10/2022 18:25:24 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:25:24 - INFO - __main__ - ['J for Justice']
03/10/2022 18:25:24 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:25:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:25:25 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:25:31 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:25:31 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:25:31 - INFO - __main__ - Printing 3 examples
03/10/2022 18:25:31 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:25:31 - INFO - __main__ - ['Whole S M']
03/10/2022 18:25:31 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:25:31 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:25:31 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:25:31 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:25:31 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:25:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:25:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:25:34 - INFO - __main__ - Starting training!
03/10/2022 18:25:34 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:27:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0005_8_predictions.txt
03/10/2022 18:27:50 - INFO - __main__ - EM on test data: 0.0097
03/10/2022 18:27:52 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.00972862263184844
03/10/2022 18:27:52 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0003, bsz=8 ...
03/10/2022 18:27:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:27:53 - INFO - __main__ - Printing 3 examples
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:27:53 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:27:53 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:27:53 - INFO - __main__ - ['Olio']
03/10/2022 18:27:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:27:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:27:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:27:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:27:53 - INFO - __main__ - Printing 3 examples
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:27:53 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:27:53 - INFO - __main__ - ['Acuren']
03/10/2022 18:27:53 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:27:53 - INFO - __main__ - ['J for Justice']
03/10/2022 18:27:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:27:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:27:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:28:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:28:03 - INFO - __main__ - Starting training!
03/10/2022 18:28:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.703747 on epoch=4
03/10/2022 18:28:11 - INFO - __main__ - Step 20 Global step 20 Train loss 20.342865 on epoch=9
03/10/2022 18:28:16 - INFO - __main__ - Step 30 Global step 30 Train loss 15.632631 on epoch=14
03/10/2022 18:28:21 - INFO - __main__ - Step 40 Global step 40 Train loss 13.674799 on epoch=19
03/10/2022 18:28:25 - INFO - __main__ - Step 50 Global step 50 Train loss 12.034645 on epoch=24
03/10/2022 18:28:32 - INFO - __main__ - Global step 50 Train loss 16.677738 EM 0.0 on epoch=24
03/10/2022 18:28:38 - INFO - __main__ - Step 60 Global step 60 Train loss 10.185628 on epoch=29
03/10/2022 18:28:42 - INFO - __main__ - Step 70 Global step 70 Train loss 9.018618 on epoch=34
03/10/2022 18:28:47 - INFO - __main__ - Step 80 Global step 80 Train loss 8.424570 on epoch=39
03/10/2022 18:28:52 - INFO - __main__ - Step 90 Global step 90 Train loss 7.483356 on epoch=44
03/10/2022 18:28:57 - INFO - __main__ - Step 100 Global step 100 Train loss 7.374215 on epoch=49
03/10/2022 18:28:57 - INFO - __main__ - Global step 100 Train loss 8.497278 EM 0.03125 on epoch=49
03/10/2022 18:29:03 - INFO - __main__ - Step 110 Global step 110 Train loss 6.834193 on epoch=54
03/10/2022 18:29:07 - INFO - __main__ - Step 120 Global step 120 Train loss 6.132937 on epoch=59
03/10/2022 18:29:12 - INFO - __main__ - Step 130 Global step 130 Train loss 6.532866 on epoch=64
03/10/2022 18:29:17 - INFO - __main__ - Step 140 Global step 140 Train loss 5.625341 on epoch=69
03/10/2022 18:29:22 - INFO - __main__ - Step 150 Global step 150 Train loss 5.442763 on epoch=74
03/10/2022 18:29:22 - INFO - __main__ - Global step 150 Train loss 6.113620 EM 0.0 on epoch=74
03/10/2022 18:29:27 - INFO - __main__ - Step 160 Global step 160 Train loss 4.902516 on epoch=79
03/10/2022 18:29:32 - INFO - __main__ - Step 170 Global step 170 Train loss 4.453235 on epoch=84
03/10/2022 18:29:37 - INFO - __main__ - Step 180 Global step 180 Train loss 3.758096 on epoch=89
03/10/2022 18:29:41 - INFO - __main__ - Step 190 Global step 190 Train loss 3.604235 on epoch=94
03/10/2022 18:29:46 - INFO - __main__ - Step 200 Global step 200 Train loss 3.052634 on epoch=99
03/10/2022 18:29:47 - INFO - __main__ - Global step 200 Train loss 3.954143 EM 0.0 on epoch=99
03/10/2022 18:29:51 - INFO - __main__ - Step 210 Global step 210 Train loss 3.054484 on epoch=104
03/10/2022 18:29:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.924891 on epoch=109
03/10/2022 18:30:01 - INFO - __main__ - Step 230 Global step 230 Train loss 2.460902 on epoch=114
03/10/2022 18:30:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.600865 on epoch=119
03/10/2022 18:30:10 - INFO - __main__ - Step 250 Global step 250 Train loss 2.601402 on epoch=124
03/10/2022 18:30:11 - INFO - __main__ - Global step 250 Train loss 2.728509 EM 0.0 on epoch=124
03/10/2022 18:30:16 - INFO - __main__ - Step 260 Global step 260 Train loss 2.389353 on epoch=129
03/10/2022 18:30:21 - INFO - __main__ - Step 270 Global step 270 Train loss 2.154083 on epoch=134
03/10/2022 18:30:25 - INFO - __main__ - Step 280 Global step 280 Train loss 2.019657 on epoch=139
03/10/2022 18:30:30 - INFO - __main__ - Step 290 Global step 290 Train loss 2.340535 on epoch=144
03/10/2022 18:30:35 - INFO - __main__ - Step 300 Global step 300 Train loss 2.067037 on epoch=149
03/10/2022 18:30:35 - INFO - __main__ - Global step 300 Train loss 2.194133 EM 0.0 on epoch=149
03/10/2022 18:30:40 - INFO - __main__ - Step 310 Global step 310 Train loss 2.083169 on epoch=154
03/10/2022 18:30:45 - INFO - __main__ - Step 320 Global step 320 Train loss 2.170076 on epoch=159
03/10/2022 18:30:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.887007 on epoch=164
03/10/2022 18:30:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.918579 on epoch=169
03/10/2022 18:30:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.520724 on epoch=174
03/10/2022 18:31:00 - INFO - __main__ - Global step 350 Train loss 1.915911 EM 0.0 on epoch=174
03/10/2022 18:31:04 - INFO - __main__ - Step 360 Global step 360 Train loss 2.038957 on epoch=179
03/10/2022 18:31:09 - INFO - __main__ - Step 370 Global step 370 Train loss 1.722996 on epoch=184
03/10/2022 18:31:14 - INFO - __main__ - Step 380 Global step 380 Train loss 1.502611 on epoch=189
03/10/2022 18:31:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.663976 on epoch=194
03/10/2022 18:31:23 - INFO - __main__ - Step 400 Global step 400 Train loss 1.565831 on epoch=199
03/10/2022 18:31:24 - INFO - __main__ - Global step 400 Train loss 1.698874 EM 0.0 on epoch=199
03/10/2022 18:31:29 - INFO - __main__ - Step 410 Global step 410 Train loss 1.662265 on epoch=204
03/10/2022 18:31:33 - INFO - __main__ - Step 420 Global step 420 Train loss 1.475981 on epoch=209
03/10/2022 18:31:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.637490 on epoch=214
03/10/2022 18:31:43 - INFO - __main__ - Step 440 Global step 440 Train loss 1.417440 on epoch=219
03/10/2022 18:31:47 - INFO - __main__ - Step 450 Global step 450 Train loss 1.281596 on epoch=224
03/10/2022 18:31:48 - INFO - __main__ - Global step 450 Train loss 1.494954 EM 0.0 on epoch=224
03/10/2022 18:31:53 - INFO - __main__ - Step 460 Global step 460 Train loss 1.391114 on epoch=229
03/10/2022 18:31:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.449098 on epoch=234
03/10/2022 18:32:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.482256 on epoch=239
03/10/2022 18:32:07 - INFO - __main__ - Step 490 Global step 490 Train loss 1.401990 on epoch=244
03/10/2022 18:32:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.421142 on epoch=249
03/10/2022 18:32:12 - INFO - __main__ - Global step 500 Train loss 1.429120 EM 0.0 on epoch=249
03/10/2022 18:32:17 - INFO - __main__ - Step 510 Global step 510 Train loss 1.308004 on epoch=254
03/10/2022 18:32:22 - INFO - __main__ - Step 520 Global step 520 Train loss 1.384291 on epoch=259
03/10/2022 18:32:26 - INFO - __main__ - Step 530 Global step 530 Train loss 1.217751 on epoch=264
03/10/2022 18:32:31 - INFO - __main__ - Step 540 Global step 540 Train loss 1.228646 on epoch=269
03/10/2022 18:32:36 - INFO - __main__ - Step 550 Global step 550 Train loss 1.264971 on epoch=274
03/10/2022 18:32:37 - INFO - __main__ - Global step 550 Train loss 1.280732 EM 0.0 on epoch=274
03/10/2022 18:32:41 - INFO - __main__ - Step 560 Global step 560 Train loss 1.168584 on epoch=279
03/10/2022 18:32:46 - INFO - __main__ - Step 570 Global step 570 Train loss 1.098431 on epoch=284
03/10/2022 18:32:51 - INFO - __main__ - Step 580 Global step 580 Train loss 1.058475 on epoch=289
03/10/2022 18:32:55 - INFO - __main__ - Step 590 Global step 590 Train loss 1.049651 on epoch=294
03/10/2022 18:33:00 - INFO - __main__ - Step 600 Global step 600 Train loss 1.122995 on epoch=299
03/10/2022 18:33:01 - INFO - __main__ - Global step 600 Train loss 1.099627 EM 0.0 on epoch=299
03/10/2022 18:33:01 - INFO - __main__ - save last model!
03/10/2022 18:33:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:33:01 - INFO - __main__ - Printing 3 examples
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:33:01 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:33:01 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:33:01 - INFO - __main__ - ['Olio']
03/10/2022 18:33:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:33:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:33:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:33:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:33:01 - INFO - __main__ - Printing 3 examples
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:33:01 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:33:01 - INFO - __main__ - ['Acuren']
03/10/2022 18:33:01 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:33:01 - INFO - __main__ - ['J for Justice']
03/10/2022 18:33:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:33:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:33:02 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:33:08 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:33:09 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:33:09 - INFO - __main__ - Printing 3 examples
03/10/2022 18:33:09 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:33:09 - INFO - __main__ - ['Whole S M']
03/10/2022 18:33:09 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:33:09 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:33:09 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:33:09 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:33:09 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:33:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:33:11 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:33:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:33:12 - INFO - __main__ - Starting training!
03/10/2022 18:33:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0003_8_predictions.txt
03/10/2022 18:33:58 - INFO - __main__ - EM on test data: 0.0205
03/10/2022 18:33:58 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.02048131080389145
03/10/2022 18:33:58 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0002, bsz=8 ...
03/10/2022 18:33:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:33:59 - INFO - __main__ - Printing 3 examples
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:33:59 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:33:59 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:33:59 - INFO - __main__ - ['Olio']
03/10/2022 18:33:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:33:59 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:33:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:33:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:33:59 - INFO - __main__ - Printing 3 examples
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:33:59 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:33:59 - INFO - __main__ - ['Acuren']
03/10/2022 18:33:59 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:33:59 - INFO - __main__ - ['J for Justice']
03/10/2022 18:33:59 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:33:59 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:33:59 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:34:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:34:09 - INFO - __main__ - Starting training!
03/10/2022 18:34:13 - INFO - __main__ - Step 10 Global step 10 Train loss 22.339066 on epoch=4
03/10/2022 18:34:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.970472 on epoch=9
03/10/2022 18:34:22 - INFO - __main__ - Step 30 Global step 30 Train loss 16.356356 on epoch=14
03/10/2022 18:34:27 - INFO - __main__ - Step 40 Global step 40 Train loss 14.057981 on epoch=19
03/10/2022 18:34:32 - INFO - __main__ - Step 50 Global step 50 Train loss 12.080463 on epoch=24
03/10/2022 18:34:34 - INFO - __main__ - Global step 50 Train loss 16.760868 EM 0.0 on epoch=24
03/10/2022 18:34:39 - INFO - __main__ - Step 60 Global step 60 Train loss 11.749080 on epoch=29
03/10/2022 18:34:44 - INFO - __main__ - Step 70 Global step 70 Train loss 10.995880 on epoch=34
03/10/2022 18:34:49 - INFO - __main__ - Step 80 Global step 80 Train loss 9.478316 on epoch=39
03/10/2022 18:34:54 - INFO - __main__ - Step 90 Global step 90 Train loss 8.558725 on epoch=44
03/10/2022 18:34:58 - INFO - __main__ - Step 100 Global step 100 Train loss 8.480043 on epoch=49
03/10/2022 18:34:59 - INFO - __main__ - Global step 100 Train loss 9.852409 EM 0.03125 on epoch=49
03/10/2022 18:35:05 - INFO - __main__ - Step 110 Global step 110 Train loss 7.974475 on epoch=54
03/10/2022 18:35:09 - INFO - __main__ - Step 120 Global step 120 Train loss 7.924251 on epoch=59
03/10/2022 18:35:14 - INFO - __main__ - Step 130 Global step 130 Train loss 7.617412 on epoch=64
03/10/2022 18:35:19 - INFO - __main__ - Step 140 Global step 140 Train loss 7.459931 on epoch=69
03/10/2022 18:35:24 - INFO - __main__ - Step 150 Global step 150 Train loss 6.741387 on epoch=74
03/10/2022 18:35:24 - INFO - __main__ - Global step 150 Train loss 7.543491 EM 0.03125 on epoch=74
03/10/2022 18:35:29 - INFO - __main__ - Step 160 Global step 160 Train loss 6.587377 on epoch=79
03/10/2022 18:35:34 - INFO - __main__ - Step 170 Global step 170 Train loss 6.303933 on epoch=84
03/10/2022 18:35:38 - INFO - __main__ - Step 180 Global step 180 Train loss 5.998719 on epoch=89
03/10/2022 18:35:43 - INFO - __main__ - Step 190 Global step 190 Train loss 5.927079 on epoch=94
03/10/2022 18:35:48 - INFO - __main__ - Step 200 Global step 200 Train loss 6.034231 on epoch=99
03/10/2022 18:35:48 - INFO - __main__ - Global step 200 Train loss 6.170267 EM 0.03125 on epoch=99
03/10/2022 18:35:53 - INFO - __main__ - Step 210 Global step 210 Train loss 5.526314 on epoch=104
03/10/2022 18:35:58 - INFO - __main__ - Step 220 Global step 220 Train loss 4.861691 on epoch=109
03/10/2022 18:36:03 - INFO - __main__ - Step 230 Global step 230 Train loss 4.819733 on epoch=114
03/10/2022 18:36:08 - INFO - __main__ - Step 240 Global step 240 Train loss 4.682811 on epoch=119
03/10/2022 18:36:12 - INFO - __main__ - Step 250 Global step 250 Train loss 3.706438 on epoch=124
03/10/2022 18:36:13 - INFO - __main__ - Global step 250 Train loss 4.719398 EM 0.0 on epoch=124
03/10/2022 18:36:18 - INFO - __main__ - Step 260 Global step 260 Train loss 3.796453 on epoch=129
03/10/2022 18:36:23 - INFO - __main__ - Step 270 Global step 270 Train loss 3.707843 on epoch=134
03/10/2022 18:36:27 - INFO - __main__ - Step 280 Global step 280 Train loss 3.334915 on epoch=139
03/10/2022 18:36:32 - INFO - __main__ - Step 290 Global step 290 Train loss 3.414559 on epoch=144
03/10/2022 18:36:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.785232 on epoch=149
03/10/2022 18:36:38 - INFO - __main__ - Global step 300 Train loss 3.407801 EM 0.0 on epoch=149
03/10/2022 18:36:42 - INFO - __main__ - Step 310 Global step 310 Train loss 2.717520 on epoch=154
03/10/2022 18:36:47 - INFO - __main__ - Step 320 Global step 320 Train loss 3.053342 on epoch=159
03/10/2022 18:36:52 - INFO - __main__ - Step 330 Global step 330 Train loss 2.681825 on epoch=164
03/10/2022 18:36:57 - INFO - __main__ - Step 340 Global step 340 Train loss 2.378758 on epoch=169
03/10/2022 18:37:01 - INFO - __main__ - Step 350 Global step 350 Train loss 2.522587 on epoch=174
03/10/2022 18:37:02 - INFO - __main__ - Global step 350 Train loss 2.670806 EM 0.0 on epoch=174
03/10/2022 18:37:07 - INFO - __main__ - Step 360 Global step 360 Train loss 2.444372 on epoch=179
03/10/2022 18:37:12 - INFO - __main__ - Step 370 Global step 370 Train loss 2.261226 on epoch=184
03/10/2022 18:37:16 - INFO - __main__ - Step 380 Global step 380 Train loss 2.207701 on epoch=189
03/10/2022 18:37:21 - INFO - __main__ - Step 390 Global step 390 Train loss 2.361298 on epoch=194
03/10/2022 18:37:26 - INFO - __main__ - Step 400 Global step 400 Train loss 2.405033 on epoch=199
03/10/2022 18:37:26 - INFO - __main__ - Global step 400 Train loss 2.335926 EM 0.0 on epoch=199
03/10/2022 18:37:31 - INFO - __main__ - Step 410 Global step 410 Train loss 2.371053 on epoch=204
03/10/2022 18:37:36 - INFO - __main__ - Step 420 Global step 420 Train loss 2.193228 on epoch=209
03/10/2022 18:37:41 - INFO - __main__ - Step 430 Global step 430 Train loss 2.345191 on epoch=214
03/10/2022 18:37:45 - INFO - __main__ - Step 440 Global step 440 Train loss 2.121703 on epoch=219
03/10/2022 18:37:50 - INFO - __main__ - Step 450 Global step 450 Train loss 2.218090 on epoch=224
03/10/2022 18:37:51 - INFO - __main__ - Global step 450 Train loss 2.249853 EM 0.0 on epoch=224
03/10/2022 18:37:55 - INFO - __main__ - Step 460 Global step 460 Train loss 1.968992 on epoch=229
03/10/2022 18:38:00 - INFO - __main__ - Step 470 Global step 470 Train loss 2.098025 on epoch=234
03/10/2022 18:38:05 - INFO - __main__ - Step 480 Global step 480 Train loss 1.971522 on epoch=239
03/10/2022 18:38:10 - INFO - __main__ - Step 490 Global step 490 Train loss 1.806972 on epoch=244
03/10/2022 18:38:14 - INFO - __main__ - Step 500 Global step 500 Train loss 1.725082 on epoch=249
03/10/2022 18:38:15 - INFO - __main__ - Global step 500 Train loss 1.914119 EM 0.0 on epoch=249
03/10/2022 18:38:20 - INFO - __main__ - Step 510 Global step 510 Train loss 2.061547 on epoch=254
03/10/2022 18:38:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.983790 on epoch=259
03/10/2022 18:38:29 - INFO - __main__ - Step 530 Global step 530 Train loss 1.715874 on epoch=264
03/10/2022 18:38:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.672368 on epoch=269
03/10/2022 18:38:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.842315 on epoch=274
03/10/2022 18:38:39 - INFO - __main__ - Global step 550 Train loss 1.855179 EM 0.0 on epoch=274
03/10/2022 18:38:44 - INFO - __main__ - Step 560 Global step 560 Train loss 1.933135 on epoch=279
03/10/2022 18:38:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.663825 on epoch=284
03/10/2022 18:38:54 - INFO - __main__ - Step 580 Global step 580 Train loss 1.718590 on epoch=289
03/10/2022 18:38:58 - INFO - __main__ - Step 590 Global step 590 Train loss 1.486512 on epoch=294
03/10/2022 18:39:03 - INFO - __main__ - Step 600 Global step 600 Train loss 1.716027 on epoch=299
03/10/2022 18:39:04 - INFO - __main__ - Global step 600 Train loss 1.703618 EM 0.0 on epoch=299
03/10/2022 18:39:04 - INFO - __main__ - save last model!
03/10/2022 18:39:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:39:04 - INFO - __main__ - Printing 3 examples
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:39:04 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:39:04 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:39:04 - INFO - __main__ - ['Olio']
03/10/2022 18:39:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:39:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:39:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:39:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:39:04 - INFO - __main__ - Printing 3 examples
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:39:04 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:39:04 - INFO - __main__ - ['Acuren']
03/10/2022 18:39:04 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:39:04 - INFO - __main__ - ['J for Justice']
03/10/2022 18:39:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:39:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:39:04 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:39:11 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:39:11 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:39:11 - INFO - __main__ - Printing 3 examples
03/10/2022 18:39:11 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:39:11 - INFO - __main__ - ['Whole S M']
03/10/2022 18:39:11 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:39:11 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:39:11 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:39:11 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:39:11 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:39:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:39:14 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:39:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:39:15 - INFO - __main__ - Starting training!
03/10/2022 18:40:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0002_8_predictions.txt
03/10/2022 18:40:01 - INFO - __main__ - EM on test data: 0.0261
03/10/2022 18:40:01 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.026113671274961597
03/10/2022 18:40:01 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0001, bsz=8 ...
03/10/2022 18:40:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:40:02 - INFO - __main__ - Printing 3 examples
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/10/2022 18:40:02 - INFO - __main__ - ['Grall Realty Group']
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/10/2022 18:40:02 - INFO - __main__ - ['e Local Plumbers']
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] olio
03/10/2022 18:40:02 - INFO - __main__ - ['Olio']
03/10/2022 18:40:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:40:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:40:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:40:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:40:02 - INFO - __main__ - Printing 3 examples
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] muscatinearc
03/10/2022 18:40:02 - INFO - __main__ - ['Muscatine A R C']
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] acuren
03/10/2022 18:40:02 - INFO - __main__ - ['Acuren']
03/10/2022 18:40:02 - INFO - __main__ -  [crawl_domain] jforjustice
03/10/2022 18:40:02 - INFO - __main__ - ['J for Justice']
03/10/2022 18:40:02 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:40:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:40:02 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:40:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:40:12 - INFO - __main__ - Starting training!
03/10/2022 18:40:16 - INFO - __main__ - Step 10 Global step 10 Train loss 22.333233 on epoch=4
03/10/2022 18:40:20 - INFO - __main__ - Step 20 Global step 20 Train loss 20.904930 on epoch=9
03/10/2022 18:40:25 - INFO - __main__ - Step 30 Global step 30 Train loss 18.848286 on epoch=14
03/10/2022 18:40:30 - INFO - __main__ - Step 40 Global step 40 Train loss 16.661087 on epoch=19
03/10/2022 18:40:34 - INFO - __main__ - Step 50 Global step 50 Train loss 17.004063 on epoch=24
03/10/2022 18:40:44 - INFO - __main__ - Global step 50 Train loss 19.150322 EM 0.0 on epoch=24
03/10/2022 18:40:49 - INFO - __main__ - Step 60 Global step 60 Train loss 15.261194 on epoch=29
03/10/2022 18:40:54 - INFO - __main__ - Step 70 Global step 70 Train loss 13.981600 on epoch=34
03/10/2022 18:40:59 - INFO - __main__ - Step 80 Global step 80 Train loss 13.090628 on epoch=39
03/10/2022 18:41:04 - INFO - __main__ - Step 90 Global step 90 Train loss 12.458365 on epoch=44
03/10/2022 18:41:09 - INFO - __main__ - Step 100 Global step 100 Train loss 11.763903 on epoch=49
03/10/2022 18:41:11 - INFO - __main__ - Global step 100 Train loss 13.311139 EM 0.03125 on epoch=49
03/10/2022 18:41:16 - INFO - __main__ - Step 110 Global step 110 Train loss 11.525214 on epoch=54
03/10/2022 18:41:21 - INFO - __main__ - Step 120 Global step 120 Train loss 11.082205 on epoch=59
03/10/2022 18:41:26 - INFO - __main__ - Step 130 Global step 130 Train loss 10.214021 on epoch=64
03/10/2022 18:41:31 - INFO - __main__ - Step 140 Global step 140 Train loss 9.554288 on epoch=69
03/10/2022 18:41:35 - INFO - __main__ - Step 150 Global step 150 Train loss 9.426364 on epoch=74
03/10/2022 18:41:36 - INFO - __main__ - Global step 150 Train loss 10.360418 EM 0.0625 on epoch=74
03/10/2022 18:41:41 - INFO - __main__ - Step 160 Global step 160 Train loss 8.962543 on epoch=79
03/10/2022 18:41:46 - INFO - __main__ - Step 170 Global step 170 Train loss 8.710011 on epoch=84
03/10/2022 18:41:51 - INFO - __main__ - Step 180 Global step 180 Train loss 8.504992 on epoch=89
03/10/2022 18:41:56 - INFO - __main__ - Step 190 Global step 190 Train loss 8.596883 on epoch=94
03/10/2022 18:42:00 - INFO - __main__ - Step 200 Global step 200 Train loss 8.488923 on epoch=99
03/10/2022 18:42:01 - INFO - __main__ - Global step 200 Train loss 8.652671 EM 0.0625 on epoch=99
03/10/2022 18:42:06 - INFO - __main__ - Step 210 Global step 210 Train loss 7.659068 on epoch=104
03/10/2022 18:42:11 - INFO - __main__ - Step 220 Global step 220 Train loss 7.805391 on epoch=109
03/10/2022 18:42:15 - INFO - __main__ - Step 230 Global step 230 Train loss 7.506803 on epoch=114
03/10/2022 18:42:20 - INFO - __main__ - Step 240 Global step 240 Train loss 7.491287 on epoch=119
03/10/2022 18:42:25 - INFO - __main__ - Step 250 Global step 250 Train loss 7.480849 on epoch=124
03/10/2022 18:42:26 - INFO - __main__ - Global step 250 Train loss 7.588680 EM 0.0 on epoch=124
03/10/2022 18:42:31 - INFO - __main__ - Step 260 Global step 260 Train loss 7.241086 on epoch=129
03/10/2022 18:42:35 - INFO - __main__ - Step 270 Global step 270 Train loss 6.874936 on epoch=134
03/10/2022 18:42:40 - INFO - __main__ - Step 280 Global step 280 Train loss 6.890382 on epoch=139
03/10/2022 18:42:45 - INFO - __main__ - Step 290 Global step 290 Train loss 6.985963 on epoch=144
03/10/2022 18:42:49 - INFO - __main__ - Step 300 Global step 300 Train loss 6.809590 on epoch=149
03/10/2022 18:42:50 - INFO - __main__ - Global step 300 Train loss 6.960391 EM 0.0 on epoch=149
03/10/2022 18:42:55 - INFO - __main__ - Step 310 Global step 310 Train loss 6.614006 on epoch=154
03/10/2022 18:43:00 - INFO - __main__ - Step 320 Global step 320 Train loss 6.581793 on epoch=159
03/10/2022 18:43:04 - INFO - __main__ - Step 330 Global step 330 Train loss 6.206095 on epoch=164
03/10/2022 18:43:09 - INFO - __main__ - Step 340 Global step 340 Train loss 6.565237 on epoch=169
03/10/2022 18:43:14 - INFO - __main__ - Step 350 Global step 350 Train loss 6.119951 on epoch=174
03/10/2022 18:43:14 - INFO - __main__ - Global step 350 Train loss 6.417417 EM 0.0 on epoch=174
03/10/2022 18:43:19 - INFO - __main__ - Step 360 Global step 360 Train loss 6.000666 on epoch=179
03/10/2022 18:43:24 - INFO - __main__ - Step 370 Global step 370 Train loss 5.894360 on epoch=184
03/10/2022 18:43:29 - INFO - __main__ - Step 380 Global step 380 Train loss 5.569457 on epoch=189
03/10/2022 18:43:33 - INFO - __main__ - Step 390 Global step 390 Train loss 5.834033 on epoch=194
03/10/2022 18:43:38 - INFO - __main__ - Step 400 Global step 400 Train loss 5.363996 on epoch=199
03/10/2022 18:43:39 - INFO - __main__ - Global step 400 Train loss 5.732503 EM 0.0 on epoch=199
03/10/2022 18:43:43 - INFO - __main__ - Step 410 Global step 410 Train loss 5.241941 on epoch=204
03/10/2022 18:43:48 - INFO - __main__ - Step 420 Global step 420 Train loss 4.984397 on epoch=209
03/10/2022 18:43:53 - INFO - __main__ - Step 430 Global step 430 Train loss 4.899800 on epoch=214
03/10/2022 18:43:58 - INFO - __main__ - Step 440 Global step 440 Train loss 5.382716 on epoch=219
03/10/2022 18:44:02 - INFO - __main__ - Step 450 Global step 450 Train loss 4.673515 on epoch=224
03/10/2022 18:44:03 - INFO - __main__ - Global step 450 Train loss 5.036474 EM 0.0 on epoch=224
03/10/2022 18:44:08 - INFO - __main__ - Step 460 Global step 460 Train loss 4.436614 on epoch=229
03/10/2022 18:44:13 - INFO - __main__ - Step 470 Global step 470 Train loss 4.438901 on epoch=234
03/10/2022 18:44:17 - INFO - __main__ - Step 480 Global step 480 Train loss 3.983407 on epoch=239
03/10/2022 18:44:22 - INFO - __main__ - Step 490 Global step 490 Train loss 4.192550 on epoch=244
03/10/2022 18:44:27 - INFO - __main__ - Step 500 Global step 500 Train loss 3.728811 on epoch=249
03/10/2022 18:44:27 - INFO - __main__ - Global step 500 Train loss 4.156057 EM 0.0 on epoch=249
03/10/2022 18:44:32 - INFO - __main__ - Step 510 Global step 510 Train loss 3.730157 on epoch=254
03/10/2022 18:44:37 - INFO - __main__ - Step 520 Global step 520 Train loss 3.460644 on epoch=259
03/10/2022 18:44:42 - INFO - __main__ - Step 530 Global step 530 Train loss 3.549564 on epoch=264
03/10/2022 18:44:46 - INFO - __main__ - Step 540 Global step 540 Train loss 3.132710 on epoch=269
03/10/2022 18:44:51 - INFO - __main__ - Step 550 Global step 550 Train loss 3.200030 on epoch=274
03/10/2022 18:44:52 - INFO - __main__ - Global step 550 Train loss 3.414621 EM 0.0 on epoch=274
03/10/2022 18:44:56 - INFO - __main__ - Step 560 Global step 560 Train loss 3.268746 on epoch=279
03/10/2022 18:45:01 - INFO - __main__ - Step 570 Global step 570 Train loss 2.940121 on epoch=284
03/10/2022 18:45:06 - INFO - __main__ - Step 580 Global step 580 Train loss 2.708812 on epoch=289
03/10/2022 18:45:11 - INFO - __main__ - Step 590 Global step 590 Train loss 2.686605 on epoch=294
03/10/2022 18:45:15 - INFO - __main__ - Step 600 Global step 600 Train loss 3.074431 on epoch=299
03/10/2022 18:45:16 - INFO - __main__ - Global step 600 Train loss 2.935743 EM 0.0 on epoch=299
03/10/2022 18:45:16 - INFO - __main__ - save last model!
03/10/2022 18:45:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:45:16 - INFO - __main__ - Printing 3 examples
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 18:45:16 - INFO - __main__ - ['Rapamune']
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 18:45:16 - INFO - __main__ - ['G B O G H']
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 18:45:16 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 18:45:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:45:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:45:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:45:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:45:16 - INFO - __main__ - Printing 3 examples
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 18:45:16 - INFO - __main__ - ['Seating Renovators']
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 18:45:16 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 18:45:16 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 18:45:16 - INFO - __main__ - ['Dick For President']
03/10/2022 18:45:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:45:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:45:16 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:45:23 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:45:23 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:45:23 - INFO - __main__ - Printing 3 examples
03/10/2022 18:45:23 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:45:23 - INFO - __main__ - ['Whole S M']
03/10/2022 18:45:23 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:45:23 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:45:23 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:45:23 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:45:23 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:45:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:45:26 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:45:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:45:26 - INFO - __main__ - Starting training!
03/10/2022 18:46:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0001_8_predictions.txt
03/10/2022 18:46:09 - INFO - __main__ - EM on test data: 0.0317
03/10/2022 18:46:10 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0001, bsz=8, dev_performance=0.0625, test_performance=0.031746031746031744
03/10/2022 18:46:15 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0005, bsz=8 ...
03/10/2022 18:46:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:46:16 - INFO - __main__ - Printing 3 examples
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 18:46:16 - INFO - __main__ - ['Rapamune']
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 18:46:16 - INFO - __main__ - ['G B O G H']
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 18:46:16 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 18:46:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:46:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:46:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:46:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:46:16 - INFO - __main__ - Printing 3 examples
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 18:46:16 - INFO - __main__ - ['Seating Renovators']
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 18:46:16 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 18:46:16 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 18:46:16 - INFO - __main__ - ['Dick For President']
03/10/2022 18:46:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:46:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:46:16 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:46:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:46:26 - INFO - __main__ - Starting training!
03/10/2022 18:46:30 - INFO - __main__ - Step 10 Global step 10 Train loss 20.473562 on epoch=4
03/10/2022 18:46:34 - INFO - __main__ - Step 20 Global step 20 Train loss 17.102077 on epoch=9
03/10/2022 18:46:39 - INFO - __main__ - Step 30 Global step 30 Train loss 12.600191 on epoch=14
03/10/2022 18:46:43 - INFO - __main__ - Step 40 Global step 40 Train loss 9.944158 on epoch=19
03/10/2022 18:46:48 - INFO - __main__ - Step 50 Global step 50 Train loss 8.714159 on epoch=24
03/10/2022 18:46:49 - INFO - __main__ - Global step 50 Train loss 13.766831 EM 0.03125 on epoch=24
03/10/2022 18:46:54 - INFO - __main__ - Step 60 Global step 60 Train loss 7.580853 on epoch=29
03/10/2022 18:46:59 - INFO - __main__ - Step 70 Global step 70 Train loss 6.704120 on epoch=34
03/10/2022 18:47:04 - INFO - __main__ - Step 80 Global step 80 Train loss 5.839250 on epoch=39
03/10/2022 18:47:09 - INFO - __main__ - Step 90 Global step 90 Train loss 4.572320 on epoch=44
03/10/2022 18:47:13 - INFO - __main__ - Step 100 Global step 100 Train loss 3.786701 on epoch=49
03/10/2022 18:47:14 - INFO - __main__ - Global step 100 Train loss 5.696649 EM 0.0 on epoch=49
03/10/2022 18:47:19 - INFO - __main__ - Step 110 Global step 110 Train loss 2.921597 on epoch=54
03/10/2022 18:47:24 - INFO - __main__ - Step 120 Global step 120 Train loss 2.581199 on epoch=59
03/10/2022 18:47:28 - INFO - __main__ - Step 130 Global step 130 Train loss 2.095411 on epoch=64
03/10/2022 18:47:33 - INFO - __main__ - Step 140 Global step 140 Train loss 2.078347 on epoch=69
03/10/2022 18:47:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.930353 on epoch=74
03/10/2022 18:47:38 - INFO - __main__ - Global step 150 Train loss 2.321381 EM 0.0 on epoch=74
03/10/2022 18:47:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.836854 on epoch=79
03/10/2022 18:47:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.808818 on epoch=84
03/10/2022 18:47:53 - INFO - __main__ - Step 180 Global step 180 Train loss 1.559372 on epoch=89
03/10/2022 18:47:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.534971 on epoch=94
03/10/2022 18:48:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.379954 on epoch=99
03/10/2022 18:48:03 - INFO - __main__ - Global step 200 Train loss 1.623994 EM 0.0 on epoch=99
03/10/2022 18:48:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.654553 on epoch=104
03/10/2022 18:48:13 - INFO - __main__ - Step 220 Global step 220 Train loss 1.416261 on epoch=109
03/10/2022 18:48:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.224980 on epoch=114
03/10/2022 18:48:22 - INFO - __main__ - Step 240 Global step 240 Train loss 1.332610 on epoch=119
03/10/2022 18:48:27 - INFO - __main__ - Step 250 Global step 250 Train loss 1.295149 on epoch=124
03/10/2022 18:48:28 - INFO - __main__ - Global step 250 Train loss 1.384711 EM 0.0 on epoch=124
03/10/2022 18:48:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.090281 on epoch=129
03/10/2022 18:48:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.076682 on epoch=134
03/10/2022 18:48:42 - INFO - __main__ - Step 280 Global step 280 Train loss 1.070308 on epoch=139
03/10/2022 18:48:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.058923 on epoch=144
03/10/2022 18:48:52 - INFO - __main__ - Step 300 Global step 300 Train loss 1.167037 on epoch=149
03/10/2022 18:48:52 - INFO - __main__ - Global step 300 Train loss 1.092646 EM 0.0 on epoch=149
03/10/2022 18:48:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.950481 on epoch=154
03/10/2022 18:49:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.949804 on epoch=159
03/10/2022 18:49:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.858291 on epoch=164
03/10/2022 18:49:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.863899 on epoch=169
03/10/2022 18:49:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.974849 on epoch=174
03/10/2022 18:49:17 - INFO - __main__ - Global step 350 Train loss 0.919465 EM 0.0 on epoch=174
03/10/2022 18:49:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.848649 on epoch=179
03/10/2022 18:49:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.876758 on epoch=184
03/10/2022 18:49:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.913612 on epoch=189
03/10/2022 18:49:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.973451 on epoch=194
03/10/2022 18:49:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.776810 on epoch=199
03/10/2022 18:49:41 - INFO - __main__ - Global step 400 Train loss 0.877856 EM 0.0 on epoch=199
03/10/2022 18:49:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.011209 on epoch=204
03/10/2022 18:49:51 - INFO - __main__ - Step 420 Global step 420 Train loss 1.171334 on epoch=209
03/10/2022 18:49:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.889129 on epoch=214
03/10/2022 18:50:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.752456 on epoch=219
03/10/2022 18:50:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.789873 on epoch=224
03/10/2022 18:50:06 - INFO - __main__ - Global step 450 Train loss 0.922800 EM 0.0 on epoch=224
03/10/2022 18:50:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.763665 on epoch=229
03/10/2022 18:50:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.837603 on epoch=234
03/10/2022 18:50:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.804668 on epoch=239
03/10/2022 18:50:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.734627 on epoch=244
03/10/2022 18:50:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.776882 on epoch=249
03/10/2022 18:50:30 - INFO - __main__ - Global step 500 Train loss 0.783489 EM 0.0 on epoch=249
03/10/2022 18:50:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.754056 on epoch=254
03/10/2022 18:50:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.727317 on epoch=259
03/10/2022 18:50:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.677725 on epoch=264
03/10/2022 18:50:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.720338 on epoch=269
03/10/2022 18:50:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.689257 on epoch=274
03/10/2022 18:50:54 - INFO - __main__ - Global step 550 Train loss 0.713739 EM 0.0 on epoch=274
03/10/2022 18:50:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.682198 on epoch=279
03/10/2022 18:51:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.702137 on epoch=284
03/10/2022 18:51:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.698568 on epoch=289
03/10/2022 18:51:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.690437 on epoch=294
03/10/2022 18:51:18 - INFO - __main__ - Step 600 Global step 600 Train loss 1.317943 on epoch=299
03/10/2022 18:51:18 - INFO - __main__ - Global step 600 Train loss 0.818256 EM 0.0 on epoch=299
03/10/2022 18:51:18 - INFO - __main__ - save last model!
03/10/2022 18:51:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:51:19 - INFO - __main__ - Printing 3 examples
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 18:51:19 - INFO - __main__ - ['Rapamune']
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 18:51:19 - INFO - __main__ - ['G B O G H']
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 18:51:19 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 18:51:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:51:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:51:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:51:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:51:19 - INFO - __main__ - Printing 3 examples
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 18:51:19 - INFO - __main__ - ['Seating Renovators']
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 18:51:19 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 18:51:19 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 18:51:19 - INFO - __main__ - ['Dick For President']
03/10/2022 18:51:19 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:51:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:51:19 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:51:25 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:51:26 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:51:26 - INFO - __main__ - Printing 3 examples
03/10/2022 18:51:26 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:51:26 - INFO - __main__ - ['Whole S M']
03/10/2022 18:51:26 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:51:26 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:51:26 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:51:26 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:51:26 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:51:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:51:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:51:28 - INFO - __main__ - Starting training!
03/10/2022 18:51:28 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 18:52:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0005_8_predictions.txt
03/10/2022 18:52:02 - INFO - __main__ - EM on test data: 0.0108
03/10/2022 18:52:02 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.010752688172043012
03/10/2022 18:52:02 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0003, bsz=8 ...
03/10/2022 18:52:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:52:03 - INFO - __main__ - Printing 3 examples
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 18:52:03 - INFO - __main__ - ['Rapamune']
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 18:52:03 - INFO - __main__ - ['G B O G H']
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 18:52:03 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 18:52:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:52:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:52:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:52:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:52:03 - INFO - __main__ - Printing 3 examples
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 18:52:03 - INFO - __main__ - ['Seating Renovators']
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 18:52:03 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 18:52:03 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 18:52:03 - INFO - __main__ - ['Dick For President']
03/10/2022 18:52:03 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:52:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:52:03 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:52:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:52:13 - INFO - __main__ - Starting training!
03/10/2022 18:52:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.998993 on epoch=4
03/10/2022 18:52:22 - INFO - __main__ - Step 20 Global step 20 Train loss 19.478764 on epoch=9
03/10/2022 18:52:26 - INFO - __main__ - Step 30 Global step 30 Train loss 14.617435 on epoch=14
03/10/2022 18:52:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.790887 on epoch=19
03/10/2022 18:52:36 - INFO - __main__ - Step 50 Global step 50 Train loss 11.374367 on epoch=24
03/10/2022 18:52:42 - INFO - __main__ - Global step 50 Train loss 15.852090 EM 0.0 on epoch=24
03/10/2022 18:52:48 - INFO - __main__ - Step 60 Global step 60 Train loss 10.205836 on epoch=29
03/10/2022 18:52:52 - INFO - __main__ - Step 70 Global step 70 Train loss 8.568349 on epoch=34
03/10/2022 18:52:57 - INFO - __main__ - Step 80 Global step 80 Train loss 7.681559 on epoch=39
03/10/2022 18:53:02 - INFO - __main__ - Step 90 Global step 90 Train loss 6.640673 on epoch=44
03/10/2022 18:53:06 - INFO - __main__ - Step 100 Global step 100 Train loss 6.040024 on epoch=49
03/10/2022 18:53:07 - INFO - __main__ - Global step 100 Train loss 7.827288 EM 0.0 on epoch=49
03/10/2022 18:53:12 - INFO - __main__ - Step 110 Global step 110 Train loss 5.362897 on epoch=54
03/10/2022 18:53:16 - INFO - __main__ - Step 120 Global step 120 Train loss 5.205953 on epoch=59
03/10/2022 18:53:21 - INFO - __main__ - Step 130 Global step 130 Train loss 4.447089 on epoch=64
03/10/2022 18:53:26 - INFO - __main__ - Step 140 Global step 140 Train loss 4.109331 on epoch=69
03/10/2022 18:53:30 - INFO - __main__ - Step 150 Global step 150 Train loss 3.556246 on epoch=74
03/10/2022 18:53:31 - INFO - __main__ - Global step 150 Train loss 4.536303 EM 0.0 on epoch=74
03/10/2022 18:53:35 - INFO - __main__ - Step 160 Global step 160 Train loss 3.427744 on epoch=79
03/10/2022 18:53:40 - INFO - __main__ - Step 170 Global step 170 Train loss 3.107492 on epoch=84
03/10/2022 18:53:45 - INFO - __main__ - Step 180 Global step 180 Train loss 2.775034 on epoch=89
03/10/2022 18:53:49 - INFO - __main__ - Step 190 Global step 190 Train loss 2.785329 on epoch=94
03/10/2022 18:53:54 - INFO - __main__ - Step 200 Global step 200 Train loss 2.367526 on epoch=99
03/10/2022 18:53:55 - INFO - __main__ - Global step 200 Train loss 2.892625 EM 0.0 on epoch=99
03/10/2022 18:53:59 - INFO - __main__ - Step 210 Global step 210 Train loss 2.068147 on epoch=104
03/10/2022 18:54:04 - INFO - __main__ - Step 220 Global step 220 Train loss 2.003557 on epoch=109
03/10/2022 18:54:09 - INFO - __main__ - Step 230 Global step 230 Train loss 2.001660 on epoch=114
03/10/2022 18:54:13 - INFO - __main__ - Step 240 Global step 240 Train loss 2.180093 on epoch=119
03/10/2022 18:54:18 - INFO - __main__ - Step 250 Global step 250 Train loss 2.078785 on epoch=124
03/10/2022 18:54:19 - INFO - __main__ - Global step 250 Train loss 2.066448 EM 0.0 on epoch=124
03/10/2022 18:54:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.671244 on epoch=129
03/10/2022 18:54:28 - INFO - __main__ - Step 270 Global step 270 Train loss 1.738765 on epoch=134
03/10/2022 18:54:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.882882 on epoch=139
03/10/2022 18:54:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.781144 on epoch=144
03/10/2022 18:54:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.684622 on epoch=149
03/10/2022 18:54:43 - INFO - __main__ - Global step 300 Train loss 1.751732 EM 0.0 on epoch=149
03/10/2022 18:54:47 - INFO - __main__ - Step 310 Global step 310 Train loss 1.735126 on epoch=154
03/10/2022 18:54:52 - INFO - __main__ - Step 320 Global step 320 Train loss 1.758482 on epoch=159
03/10/2022 18:54:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.683275 on epoch=164
03/10/2022 18:55:01 - INFO - __main__ - Step 340 Global step 340 Train loss 1.540267 on epoch=169
03/10/2022 18:55:06 - INFO - __main__ - Step 350 Global step 350 Train loss 1.080065 on epoch=174
03/10/2022 18:55:06 - INFO - __main__ - Global step 350 Train loss 1.559443 EM 0.0 on epoch=174
03/10/2022 18:55:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.457415 on epoch=179
03/10/2022 18:55:16 - INFO - __main__ - Step 370 Global step 370 Train loss 1.378126 on epoch=184
03/10/2022 18:55:20 - INFO - __main__ - Step 380 Global step 380 Train loss 1.369893 on epoch=189
03/10/2022 18:55:25 - INFO - __main__ - Step 390 Global step 390 Train loss 1.314355 on epoch=194
03/10/2022 18:55:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.206287 on epoch=199
03/10/2022 18:55:30 - INFO - __main__ - Global step 400 Train loss 1.345215 EM 0.0 on epoch=199
03/10/2022 18:55:35 - INFO - __main__ - Step 410 Global step 410 Train loss 1.222011 on epoch=204
03/10/2022 18:55:40 - INFO - __main__ - Step 420 Global step 420 Train loss 1.213051 on epoch=209
03/10/2022 18:55:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.282299 on epoch=214
03/10/2022 18:55:49 - INFO - __main__ - Step 440 Global step 440 Train loss 1.206775 on epoch=219
03/10/2022 18:55:54 - INFO - __main__ - Step 450 Global step 450 Train loss 1.175566 on epoch=224
03/10/2022 18:55:54 - INFO - __main__ - Global step 450 Train loss 1.219941 EM 0.0 on epoch=224
03/10/2022 18:55:59 - INFO - __main__ - Step 460 Global step 460 Train loss 1.243715 on epoch=229
03/10/2022 18:56:04 - INFO - __main__ - Step 470 Global step 470 Train loss 1.174916 on epoch=234
03/10/2022 18:56:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.166954 on epoch=239
03/10/2022 18:56:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.096593 on epoch=244
03/10/2022 18:56:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.956705 on epoch=249
03/10/2022 18:56:18 - INFO - __main__ - Global step 500 Train loss 1.127777 EM 0.0 on epoch=249
03/10/2022 18:56:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.140281 on epoch=254
03/10/2022 18:56:28 - INFO - __main__ - Step 520 Global step 520 Train loss 1.043778 on epoch=259
03/10/2022 18:56:33 - INFO - __main__ - Step 530 Global step 530 Train loss 1.038207 on epoch=264
03/10/2022 18:56:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.954810 on epoch=269
03/10/2022 18:56:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.002023 on epoch=274
03/10/2022 18:56:43 - INFO - __main__ - Global step 550 Train loss 1.035820 EM 0.0 on epoch=274
03/10/2022 18:56:47 - INFO - __main__ - Step 560 Global step 560 Train loss 1.007635 on epoch=279
03/10/2022 18:56:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.916458 on epoch=284
03/10/2022 18:56:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.883709 on epoch=289
03/10/2022 18:57:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.929211 on epoch=294
03/10/2022 18:57:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.840987 on epoch=299
03/10/2022 18:57:06 - INFO - __main__ - Global step 600 Train loss 0.915600 EM 0.0 on epoch=299
03/10/2022 18:57:06 - INFO - __main__ - save last model!
03/10/2022 18:57:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:57:07 - INFO - __main__ - Printing 3 examples
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 18:57:07 - INFO - __main__ - ['Rapamune']
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 18:57:07 - INFO - __main__ - ['G B O G H']
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 18:57:07 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 18:57:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:57:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:57:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:57:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:57:07 - INFO - __main__ - Printing 3 examples
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 18:57:07 - INFO - __main__ - ['Seating Renovators']
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 18:57:07 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 18:57:07 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 18:57:07 - INFO - __main__ - ['Dick For President']
03/10/2022 18:57:07 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:57:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:57:07 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:57:14 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 18:57:14 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 18:57:14 - INFO - __main__ - Printing 3 examples
03/10/2022 18:57:14 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 18:57:14 - INFO - __main__ - ['Whole S M']
03/10/2022 18:57:14 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 18:57:14 - INFO - __main__ - ['pushin Daisies']
03/10/2022 18:57:14 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 18:57:14 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 18:57:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:57:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:57:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 18:57:16 - INFO - __main__ - Starting training!
03/10/2022 18:57:17 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:03:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0003_8_predictions.txt
03/10/2022 19:03:17 - INFO - __main__ - EM on test data: 0.0061
03/10/2022 19:03:17 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.006144393241167435
03/10/2022 19:03:17 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0002, bsz=8 ...
03/10/2022 19:03:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:03:18 - INFO - __main__ - Printing 3 examples
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 19:03:18 - INFO - __main__ - ['Rapamune']
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 19:03:18 - INFO - __main__ - ['G B O G H']
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 19:03:18 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 19:03:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:03:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:03:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:03:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:03:18 - INFO - __main__ - Printing 3 examples
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 19:03:18 - INFO - __main__ - ['Seating Renovators']
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 19:03:18 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 19:03:18 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 19:03:18 - INFO - __main__ - ['Dick For President']
03/10/2022 19:03:18 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:03:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:03:18 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:03:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:03:28 - INFO - __main__ - Starting training!
03/10/2022 19:03:32 - INFO - __main__ - Step 10 Global step 10 Train loss 21.057262 on epoch=4
03/10/2022 19:03:37 - INFO - __main__ - Step 20 Global step 20 Train loss 18.237810 on epoch=9
03/10/2022 19:03:41 - INFO - __main__ - Step 30 Global step 30 Train loss 15.540550 on epoch=14
03/10/2022 19:03:46 - INFO - __main__ - Step 40 Global step 40 Train loss 13.283406 on epoch=19
03/10/2022 19:03:51 - INFO - __main__ - Step 50 Global step 50 Train loss 12.134954 on epoch=24
03/10/2022 19:03:59 - INFO - __main__ - Global step 50 Train loss 16.050797 EM 0.0 on epoch=24
03/10/2022 19:04:04 - INFO - __main__ - Step 60 Global step 60 Train loss 10.863177 on epoch=29
03/10/2022 19:04:09 - INFO - __main__ - Step 70 Global step 70 Train loss 9.969539 on epoch=34
03/10/2022 19:04:14 - INFO - __main__ - Step 80 Global step 80 Train loss 9.352541 on epoch=39
03/10/2022 19:04:19 - INFO - __main__ - Step 90 Global step 90 Train loss 8.724901 on epoch=44
03/10/2022 19:04:23 - INFO - __main__ - Step 100 Global step 100 Train loss 7.826723 on epoch=49
03/10/2022 19:04:24 - INFO - __main__ - Global step 100 Train loss 9.347376 EM 0.0 on epoch=49
03/10/2022 19:04:29 - INFO - __main__ - Step 110 Global step 110 Train loss 6.990656 on epoch=54
03/10/2022 19:04:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.749289 on epoch=59
03/10/2022 19:04:38 - INFO - __main__ - Step 130 Global step 130 Train loss 6.109398 on epoch=64
03/10/2022 19:04:43 - INFO - __main__ - Step 140 Global step 140 Train loss 5.594159 on epoch=69
03/10/2022 19:04:48 - INFO - __main__ - Step 150 Global step 150 Train loss 5.053899 on epoch=74
03/10/2022 19:04:48 - INFO - __main__ - Global step 150 Train loss 6.099480 EM 0.0 on epoch=74
03/10/2022 19:04:53 - INFO - __main__ - Step 160 Global step 160 Train loss 5.311047 on epoch=79
03/10/2022 19:04:58 - INFO - __main__ - Step 170 Global step 170 Train loss 4.852649 on epoch=84
03/10/2022 19:05:03 - INFO - __main__ - Step 180 Global step 180 Train loss 4.445070 on epoch=89
03/10/2022 19:05:08 - INFO - __main__ - Step 190 Global step 190 Train loss 4.443383 on epoch=94
03/10/2022 19:05:12 - INFO - __main__ - Step 200 Global step 200 Train loss 3.770454 on epoch=99
03/10/2022 19:05:13 - INFO - __main__ - Global step 200 Train loss 4.564520 EM 0.0 on epoch=99
03/10/2022 19:05:18 - INFO - __main__ - Step 210 Global step 210 Train loss 3.621825 on epoch=104
03/10/2022 19:05:23 - INFO - __main__ - Step 220 Global step 220 Train loss 3.455277 on epoch=109
03/10/2022 19:05:27 - INFO - __main__ - Step 230 Global step 230 Train loss 3.414426 on epoch=114
03/10/2022 19:05:32 - INFO - __main__ - Step 240 Global step 240 Train loss 3.057835 on epoch=119
03/10/2022 19:05:37 - INFO - __main__ - Step 250 Global step 250 Train loss 3.038542 on epoch=124
03/10/2022 19:05:38 - INFO - __main__ - Global step 250 Train loss 3.317581 EM 0.0 on epoch=124
03/10/2022 19:05:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.753289 on epoch=129
03/10/2022 19:05:47 - INFO - __main__ - Step 270 Global step 270 Train loss 2.775823 on epoch=134
03/10/2022 19:05:52 - INFO - __main__ - Step 280 Global step 280 Train loss 2.352251 on epoch=139
03/10/2022 19:05:57 - INFO - __main__ - Step 290 Global step 290 Train loss 2.442210 on epoch=144
03/10/2022 19:06:02 - INFO - __main__ - Step 300 Global step 300 Train loss 2.124836 on epoch=149
03/10/2022 19:06:02 - INFO - __main__ - Global step 300 Train loss 2.489682 EM 0.0 on epoch=149
03/10/2022 19:06:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.833335 on epoch=154
03/10/2022 19:06:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.969305 on epoch=159
03/10/2022 19:06:17 - INFO - __main__ - Step 330 Global step 330 Train loss 2.102699 on epoch=164
03/10/2022 19:06:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.733040 on epoch=169
03/10/2022 19:06:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.051501 on epoch=174
03/10/2022 19:06:27 - INFO - __main__ - Global step 350 Train loss 1.937976 EM 0.0 on epoch=174
03/10/2022 19:06:31 - INFO - __main__ - Step 360 Global step 360 Train loss 1.905054 on epoch=179
03/10/2022 19:06:36 - INFO - __main__ - Step 370 Global step 370 Train loss 1.916871 on epoch=184
03/10/2022 19:06:41 - INFO - __main__ - Step 380 Global step 380 Train loss 1.794221 on epoch=189
03/10/2022 19:06:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.605260 on epoch=194
03/10/2022 19:06:50 - INFO - __main__ - Step 400 Global step 400 Train loss 1.729269 on epoch=199
03/10/2022 19:06:51 - INFO - __main__ - Global step 400 Train loss 1.790135 EM 0.0 on epoch=199
03/10/2022 19:06:56 - INFO - __main__ - Step 410 Global step 410 Train loss 1.580611 on epoch=204
03/10/2022 19:07:01 - INFO - __main__ - Step 420 Global step 420 Train loss 1.529434 on epoch=209
03/10/2022 19:07:05 - INFO - __main__ - Step 430 Global step 430 Train loss 1.590841 on epoch=214
03/10/2022 19:07:10 - INFO - __main__ - Step 440 Global step 440 Train loss 1.652608 on epoch=219
03/10/2022 19:07:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.369171 on epoch=224
03/10/2022 19:07:15 - INFO - __main__ - Global step 450 Train loss 1.544533 EM 0.0 on epoch=224
03/10/2022 19:07:20 - INFO - __main__ - Step 460 Global step 460 Train loss 1.429484 on epoch=229
03/10/2022 19:07:25 - INFO - __main__ - Step 470 Global step 470 Train loss 1.329444 on epoch=234
03/10/2022 19:07:30 - INFO - __main__ - Step 480 Global step 480 Train loss 1.368961 on epoch=239
03/10/2022 19:07:35 - INFO - __main__ - Step 490 Global step 490 Train loss 1.457696 on epoch=244
03/10/2022 19:07:39 - INFO - __main__ - Step 500 Global step 500 Train loss 1.422102 on epoch=249
03/10/2022 19:07:40 - INFO - __main__ - Global step 500 Train loss 1.401537 EM 0.0 on epoch=249
03/10/2022 19:07:45 - INFO - __main__ - Step 510 Global step 510 Train loss 1.551998 on epoch=254
03/10/2022 19:07:49 - INFO - __main__ - Step 520 Global step 520 Train loss 1.274534 on epoch=259
03/10/2022 19:07:54 - INFO - __main__ - Step 530 Global step 530 Train loss 1.197560 on epoch=264
03/10/2022 19:07:59 - INFO - __main__ - Step 540 Global step 540 Train loss 1.237410 on epoch=269
03/10/2022 19:08:04 - INFO - __main__ - Step 550 Global step 550 Train loss 1.380481 on epoch=274
03/10/2022 19:08:04 - INFO - __main__ - Global step 550 Train loss 1.328397 EM 0.0 on epoch=274
03/10/2022 19:08:09 - INFO - __main__ - Step 560 Global step 560 Train loss 1.473024 on epoch=279
03/10/2022 19:08:14 - INFO - __main__ - Step 570 Global step 570 Train loss 1.329017 on epoch=284
03/10/2022 19:08:19 - INFO - __main__ - Step 580 Global step 580 Train loss 1.335759 on epoch=289
03/10/2022 19:08:23 - INFO - __main__ - Step 590 Global step 590 Train loss 1.279342 on epoch=294
03/10/2022 19:08:28 - INFO - __main__ - Step 600 Global step 600 Train loss 1.291241 on epoch=299
03/10/2022 19:08:29 - INFO - __main__ - Global step 600 Train loss 1.341676 EM 0.0 on epoch=299
03/10/2022 19:08:29 - INFO - __main__ - save last model!
03/10/2022 19:08:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:08:29 - INFO - __main__ - Printing 3 examples
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 19:08:29 - INFO - __main__ - ['Rapamune']
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 19:08:29 - INFO - __main__ - ['G B O G H']
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 19:08:29 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 19:08:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:08:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:08:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:08:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:08:29 - INFO - __main__ - Printing 3 examples
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 19:08:29 - INFO - __main__ - ['Seating Renovators']
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 19:08:29 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 19:08:29 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 19:08:29 - INFO - __main__ - ['Dick For President']
03/10/2022 19:08:29 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:08:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:08:29 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:08:36 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:08:36 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:08:36 - INFO - __main__ - Printing 3 examples
03/10/2022 19:08:36 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:08:36 - INFO - __main__ - ['Whole S M']
03/10/2022 19:08:36 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:08:36 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:08:36 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:08:36 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:08:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:08:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:08:39 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:08:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:08:40 - INFO - __main__ - Starting training!
03/10/2022 19:16:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0002_8_predictions.txt
03/10/2022 19:16:08 - INFO - __main__ - EM on test data: 0.0041
03/10/2022 19:16:08 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.00409626216077829
03/10/2022 19:16:08 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0001, bsz=8 ...
03/10/2022 19:16:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:16:09 - INFO - __main__ - Printing 3 examples
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] rapamune
03/10/2022 19:16:09 - INFO - __main__ - ['Rapamune']
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] gbogh
03/10/2022 19:16:09 - INFO - __main__ - ['G B O G H']
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/10/2022 19:16:09 - INFO - __main__ - ['Car Shield Careers']
03/10/2022 19:16:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:16:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:16:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:16:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:16:09 - INFO - __main__ - Printing 3 examples
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/10/2022 19:16:09 - INFO - __main__ - ['Seating Renovators']
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] pinballheaven
03/10/2022 19:16:09 - INFO - __main__ - ['Pinball Heaven']
03/10/2022 19:16:09 - INFO - __main__ -  [crawl_domain] dickforpresident
03/10/2022 19:16:09 - INFO - __main__ - ['Dick For President']
03/10/2022 19:16:09 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:16:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:16:09 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:16:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:16:19 - INFO - __main__ - Starting training!
03/10/2022 19:16:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.320641 on epoch=4
03/10/2022 19:16:29 - INFO - __main__ - Step 20 Global step 20 Train loss 19.066389 on epoch=9
03/10/2022 19:16:34 - INFO - __main__ - Step 30 Global step 30 Train loss 15.111023 on epoch=14
03/10/2022 19:16:39 - INFO - __main__ - Step 40 Global step 40 Train loss 14.936064 on epoch=19
03/10/2022 19:16:44 - INFO - __main__ - Step 50 Global step 50 Train loss 13.413083 on epoch=24
03/10/2022 19:16:54 - INFO - __main__ - Global step 50 Train loss 16.569441 EM 0.0 on epoch=24
03/10/2022 19:17:00 - INFO - __main__ - Step 60 Global step 60 Train loss 12.184072 on epoch=29
03/10/2022 19:17:04 - INFO - __main__ - Step 70 Global step 70 Train loss 11.289807 on epoch=34
03/10/2022 19:17:09 - INFO - __main__ - Step 80 Global step 80 Train loss 10.339511 on epoch=39
03/10/2022 19:17:14 - INFO - __main__ - Step 90 Global step 90 Train loss 9.615134 on epoch=44
03/10/2022 19:17:19 - INFO - __main__ - Step 100 Global step 100 Train loss 9.155445 on epoch=49
03/10/2022 19:17:20 - INFO - __main__ - Global step 100 Train loss 10.516794 EM 0.03125 on epoch=49
03/10/2022 19:17:25 - INFO - __main__ - Step 110 Global step 110 Train loss 8.371181 on epoch=54
03/10/2022 19:17:30 - INFO - __main__ - Step 120 Global step 120 Train loss 8.291022 on epoch=59
03/10/2022 19:17:35 - INFO - __main__ - Step 130 Global step 130 Train loss 8.082277 on epoch=64
03/10/2022 19:17:39 - INFO - __main__ - Step 140 Global step 140 Train loss 7.389915 on epoch=69
03/10/2022 19:17:44 - INFO - __main__ - Step 150 Global step 150 Train loss 7.037613 on epoch=74
03/10/2022 19:17:45 - INFO - __main__ - Global step 150 Train loss 7.834401 EM 0.03125 on epoch=74
03/10/2022 19:17:49 - INFO - __main__ - Step 160 Global step 160 Train loss 6.774835 on epoch=79
03/10/2022 19:17:54 - INFO - __main__ - Step 170 Global step 170 Train loss 6.680477 on epoch=84
03/10/2022 19:17:59 - INFO - __main__ - Step 180 Global step 180 Train loss 6.450836 on epoch=89
03/10/2022 19:18:04 - INFO - __main__ - Step 190 Global step 190 Train loss 5.921579 on epoch=94
03/10/2022 19:18:09 - INFO - __main__ - Step 200 Global step 200 Train loss 5.807946 on epoch=99
03/10/2022 19:18:09 - INFO - __main__ - Global step 200 Train loss 6.327135 EM 0.0625 on epoch=99
03/10/2022 19:18:15 - INFO - __main__ - Step 210 Global step 210 Train loss 5.913124 on epoch=104
03/10/2022 19:18:20 - INFO - __main__ - Step 220 Global step 220 Train loss 5.435072 on epoch=109
03/10/2022 19:18:25 - INFO - __main__ - Step 230 Global step 230 Train loss 5.787513 on epoch=114
03/10/2022 19:18:29 - INFO - __main__ - Step 240 Global step 240 Train loss 5.189694 on epoch=119
03/10/2022 19:18:34 - INFO - __main__ - Step 250 Global step 250 Train loss 5.420630 on epoch=124
03/10/2022 19:18:35 - INFO - __main__ - Global step 250 Train loss 5.549207 EM 0.0625 on epoch=124
03/10/2022 19:18:40 - INFO - __main__ - Step 260 Global step 260 Train loss 5.253611 on epoch=129
03/10/2022 19:18:44 - INFO - __main__ - Step 270 Global step 270 Train loss 5.359959 on epoch=134
03/10/2022 19:18:49 - INFO - __main__ - Step 280 Global step 280 Train loss 4.960279 on epoch=139
03/10/2022 19:18:54 - INFO - __main__ - Step 290 Global step 290 Train loss 5.099278 on epoch=144
03/10/2022 19:18:59 - INFO - __main__ - Step 300 Global step 300 Train loss 4.839833 on epoch=149
03/10/2022 19:18:59 - INFO - __main__ - Global step 300 Train loss 5.102592 EM 0.0 on epoch=149
03/10/2022 19:19:04 - INFO - __main__ - Step 310 Global step 310 Train loss 4.986241 on epoch=154
03/10/2022 19:19:09 - INFO - __main__ - Step 320 Global step 320 Train loss 4.696243 on epoch=159
03/10/2022 19:19:14 - INFO - __main__ - Step 330 Global step 330 Train loss 4.264941 on epoch=164
03/10/2022 19:19:19 - INFO - __main__ - Step 340 Global step 340 Train loss 4.474205 on epoch=169
03/10/2022 19:19:24 - INFO - __main__ - Step 350 Global step 350 Train loss 4.266383 on epoch=174
03/10/2022 19:19:24 - INFO - __main__ - Global step 350 Train loss 4.537603 EM 0.03125 on epoch=174
03/10/2022 19:19:29 - INFO - __main__ - Step 360 Global step 360 Train loss 4.223494 on epoch=179
03/10/2022 19:19:34 - INFO - __main__ - Step 370 Global step 370 Train loss 4.076279 on epoch=184
03/10/2022 19:19:39 - INFO - __main__ - Step 380 Global step 380 Train loss 4.136218 on epoch=189
03/10/2022 19:19:44 - INFO - __main__ - Step 390 Global step 390 Train loss 4.053675 on epoch=194
03/10/2022 19:19:48 - INFO - __main__ - Step 400 Global step 400 Train loss 3.818007 on epoch=199
03/10/2022 19:19:49 - INFO - __main__ - Global step 400 Train loss 4.061534 EM 0.0 on epoch=199
03/10/2022 19:19:54 - INFO - __main__ - Step 410 Global step 410 Train loss 3.518750 on epoch=204
03/10/2022 19:19:59 - INFO - __main__ - Step 420 Global step 420 Train loss 3.704357 on epoch=209
03/10/2022 19:20:04 - INFO - __main__ - Step 430 Global step 430 Train loss 3.606122 on epoch=214
03/10/2022 19:20:08 - INFO - __main__ - Step 440 Global step 440 Train loss 3.290643 on epoch=219
03/10/2022 19:20:13 - INFO - __main__ - Step 450 Global step 450 Train loss 2.996598 on epoch=224
03/10/2022 19:20:14 - INFO - __main__ - Global step 450 Train loss 3.423294 EM 0.0 on epoch=224
03/10/2022 19:20:19 - INFO - __main__ - Step 460 Global step 460 Train loss 2.975445 on epoch=229
03/10/2022 19:20:23 - INFO - __main__ - Step 470 Global step 470 Train loss 2.736679 on epoch=234
03/10/2022 19:20:28 - INFO - __main__ - Step 480 Global step 480 Train loss 2.568830 on epoch=239
03/10/2022 19:20:33 - INFO - __main__ - Step 490 Global step 490 Train loss 2.490206 on epoch=244
03/10/2022 19:20:38 - INFO - __main__ - Step 500 Global step 500 Train loss 2.655273 on epoch=249
03/10/2022 19:20:38 - INFO - __main__ - Global step 500 Train loss 2.685287 EM 0.0 on epoch=249
03/10/2022 19:20:43 - INFO - __main__ - Step 510 Global step 510 Train loss 2.689381 on epoch=254
03/10/2022 19:20:48 - INFO - __main__ - Step 520 Global step 520 Train loss 2.343803 on epoch=259
03/10/2022 19:20:53 - INFO - __main__ - Step 530 Global step 530 Train loss 2.348356 on epoch=264
03/10/2022 19:20:58 - INFO - __main__ - Step 540 Global step 540 Train loss 2.169055 on epoch=269
03/10/2022 19:21:02 - INFO - __main__ - Step 550 Global step 550 Train loss 2.068954 on epoch=274
03/10/2022 19:21:03 - INFO - __main__ - Global step 550 Train loss 2.323910 EM 0.0 on epoch=274
03/10/2022 19:21:08 - INFO - __main__ - Step 560 Global step 560 Train loss 2.442138 on epoch=279
03/10/2022 19:21:13 - INFO - __main__ - Step 570 Global step 570 Train loss 2.056323 on epoch=284
03/10/2022 19:21:18 - INFO - __main__ - Step 580 Global step 580 Train loss 2.197450 on epoch=289
03/10/2022 19:21:22 - INFO - __main__ - Step 590 Global step 590 Train loss 1.927421 on epoch=294
03/10/2022 19:21:27 - INFO - __main__ - Step 600 Global step 600 Train loss 2.120018 on epoch=299
03/10/2022 19:21:28 - INFO - __main__ - Global step 600 Train loss 2.148669 EM 0.0 on epoch=299
03/10/2022 19:21:28 - INFO - __main__ - save last model!
03/10/2022 19:21:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:21:28 - INFO - __main__ - Printing 3 examples
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:21:28 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:21:28 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:21:28 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:21:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:21:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:21:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:21:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:21:28 - INFO - __main__ - Printing 3 examples
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:21:28 - INFO - __main__ - ['N F M S']
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:21:28 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:21:28 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:21:28 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:21:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:21:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:21:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:21:35 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:21:35 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:21:35 - INFO - __main__ - Printing 3 examples
03/10/2022 19:21:35 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:21:35 - INFO - __main__ - ['Whole S M']
03/10/2022 19:21:35 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:21:35 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:21:35 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:21:35 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:21:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:21:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:21:38 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:21:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:21:38 - INFO - __main__ - Starting training!
03/10/2022 19:22:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0001_8_predictions.txt
03/10/2022 19:22:15 - INFO - __main__ - EM on test data: 0.0123
03/10/2022 19:22:15 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0001, bsz=8, dev_performance=0.0625, test_performance=0.01228878648233487
03/10/2022 19:22:15 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0005, bsz=8 ...
03/10/2022 19:22:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:22:16 - INFO - __main__ - Printing 3 examples
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:22:16 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:22:16 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:22:16 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:22:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:22:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:22:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:22:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:22:16 - INFO - __main__ - Printing 3 examples
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:22:16 - INFO - __main__ - ['N F M S']
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:22:16 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:22:16 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:22:16 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:22:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:22:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:22:16 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:22:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:22:26 - INFO - __main__ - Starting training!
03/10/2022 19:22:30 - INFO - __main__ - Step 10 Global step 10 Train loss 20.641169 on epoch=4
03/10/2022 19:22:34 - INFO - __main__ - Step 20 Global step 20 Train loss 16.783310 on epoch=9
03/10/2022 19:22:39 - INFO - __main__ - Step 30 Global step 30 Train loss 12.391582 on epoch=14
03/10/2022 19:22:44 - INFO - __main__ - Step 40 Global step 40 Train loss 10.307453 on epoch=19
03/10/2022 19:22:48 - INFO - __main__ - Step 50 Global step 50 Train loss 9.002833 on epoch=24
03/10/2022 19:22:49 - INFO - __main__ - Global step 50 Train loss 13.825269 EM 0.0 on epoch=24
03/10/2022 19:22:54 - INFO - __main__ - Step 60 Global step 60 Train loss 8.050835 on epoch=29
03/10/2022 19:22:59 - INFO - __main__ - Step 70 Global step 70 Train loss 7.041064 on epoch=34
03/10/2022 19:23:04 - INFO - __main__ - Step 80 Global step 80 Train loss 5.928793 on epoch=39
03/10/2022 19:23:09 - INFO - __main__ - Step 90 Global step 90 Train loss 4.777258 on epoch=44
03/10/2022 19:23:13 - INFO - __main__ - Step 100 Global step 100 Train loss 3.437063 on epoch=49
03/10/2022 19:23:14 - INFO - __main__ - Global step 100 Train loss 5.847003 EM 0.0 on epoch=49
03/10/2022 19:23:19 - INFO - __main__ - Step 110 Global step 110 Train loss 2.633318 on epoch=54
03/10/2022 19:23:23 - INFO - __main__ - Step 120 Global step 120 Train loss 2.439102 on epoch=59
03/10/2022 19:23:28 - INFO - __main__ - Step 130 Global step 130 Train loss 2.134680 on epoch=64
03/10/2022 19:23:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.906890 on epoch=69
03/10/2022 19:23:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.791299 on epoch=74
03/10/2022 19:23:38 - INFO - __main__ - Global step 150 Train loss 2.181058 EM 0.0 on epoch=74
03/10/2022 19:23:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.909026 on epoch=79
03/10/2022 19:23:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.768615 on epoch=84
03/10/2022 19:23:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.865551 on epoch=89
03/10/2022 19:23:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.569129 on epoch=94
03/10/2022 19:24:01 - INFO - __main__ - Step 200 Global step 200 Train loss 1.332790 on epoch=99
03/10/2022 19:24:02 - INFO - __main__ - Global step 200 Train loss 1.689022 EM 0.0 on epoch=99
03/10/2022 19:24:06 - INFO - __main__ - Step 210 Global step 210 Train loss 1.601101 on epoch=104
03/10/2022 19:24:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.414912 on epoch=109
03/10/2022 19:24:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.837583 on epoch=114
03/10/2022 19:24:20 - INFO - __main__ - Step 240 Global step 240 Train loss 1.467316 on epoch=119
03/10/2022 19:24:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.318700 on epoch=124
03/10/2022 19:24:26 - INFO - __main__ - Global step 250 Train loss 1.527923 EM 0.0 on epoch=124
03/10/2022 19:24:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.514723 on epoch=129
03/10/2022 19:24:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.120717 on epoch=134
03/10/2022 19:24:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.337132 on epoch=139
03/10/2022 19:24:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.069124 on epoch=144
03/10/2022 19:24:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.098876 on epoch=149
03/10/2022 19:24:50 - INFO - __main__ - Global step 300 Train loss 1.228114 EM 0.0 on epoch=149
03/10/2022 19:24:54 - INFO - __main__ - Step 310 Global step 310 Train loss 1.144178 on epoch=154
03/10/2022 19:24:59 - INFO - __main__ - Step 320 Global step 320 Train loss 1.127544 on epoch=159
03/10/2022 19:25:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.106681 on epoch=164
03/10/2022 19:25:08 - INFO - __main__ - Step 340 Global step 340 Train loss 1.111224 on epoch=169
03/10/2022 19:25:13 - INFO - __main__ - Step 350 Global step 350 Train loss 1.038609 on epoch=174
03/10/2022 19:25:14 - INFO - __main__ - Global step 350 Train loss 1.105647 EM 0.0 on epoch=174
03/10/2022 19:25:18 - INFO - __main__ - Step 360 Global step 360 Train loss 1.007262 on epoch=179
03/10/2022 19:25:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.008098 on epoch=184
03/10/2022 19:25:28 - INFO - __main__ - Step 380 Global step 380 Train loss 1.026545 on epoch=189
03/10/2022 19:25:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.971649 on epoch=194
03/10/2022 19:25:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.993139 on epoch=199
03/10/2022 19:25:37 - INFO - __main__ - Global step 400 Train loss 1.001339 EM 0.0 on epoch=199
03/10/2022 19:25:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.877580 on epoch=204
03/10/2022 19:25:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.932770 on epoch=209
03/10/2022 19:25:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.823703 on epoch=214
03/10/2022 19:25:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.844447 on epoch=219
03/10/2022 19:26:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.835919 on epoch=224
03/10/2022 19:26:02 - INFO - __main__ - Global step 450 Train loss 0.862884 EM 0.0 on epoch=224
03/10/2022 19:26:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.826338 on epoch=229
03/10/2022 19:26:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.741522 on epoch=234
03/10/2022 19:26:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.820622 on epoch=239
03/10/2022 19:26:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.858702 on epoch=244
03/10/2022 19:26:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.839448 on epoch=249
03/10/2022 19:26:26 - INFO - __main__ - Global step 500 Train loss 0.817327 EM 0.0 on epoch=249
03/10/2022 19:26:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.866404 on epoch=254
03/10/2022 19:26:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.842654 on epoch=259
03/10/2022 19:26:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.836167 on epoch=264
03/10/2022 19:26:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.810071 on epoch=269
03/10/2022 19:26:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.715684 on epoch=274
03/10/2022 19:26:50 - INFO - __main__ - Global step 550 Train loss 0.814196 EM 0.0 on epoch=274
03/10/2022 19:26:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.811716 on epoch=279
03/10/2022 19:26:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.856362 on epoch=284
03/10/2022 19:27:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.883111 on epoch=289
03/10/2022 19:27:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.748538 on epoch=294
03/10/2022 19:27:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.806697 on epoch=299
03/10/2022 19:27:14 - INFO - __main__ - Global step 600 Train loss 0.821285 EM 0.0 on epoch=299
03/10/2022 19:27:14 - INFO - __main__ - save last model!
03/10/2022 19:27:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:27:14 - INFO - __main__ - Printing 3 examples
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:27:14 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:27:14 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:27:14 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:27:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:27:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:27:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:27:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:27:14 - INFO - __main__ - Printing 3 examples
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:27:14 - INFO - __main__ - ['N F M S']
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:27:14 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:27:14 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:27:14 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:27:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:27:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:27:14 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:27:21 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:27:21 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:27:21 - INFO - __main__ - Printing 3 examples
03/10/2022 19:27:21 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:27:21 - INFO - __main__ - ['Whole S M']
03/10/2022 19:27:21 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:27:21 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:27:21 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:27:21 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:27:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:27:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:27:24 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:27:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:27:25 - INFO - __main__ - Starting training!
03/10/2022 19:30:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0005_8_predictions.txt
03/10/2022 19:30:43 - INFO - __main__ - EM on test data: 0.0015
03/10/2022 19:30:44 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0015360983102918587
03/10/2022 19:30:44 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0003, bsz=8 ...
03/10/2022 19:30:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:30:45 - INFO - __main__ - Printing 3 examples
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:30:45 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:30:45 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:30:45 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:30:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:30:45 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:30:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:30:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:30:45 - INFO - __main__ - Printing 3 examples
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:30:45 - INFO - __main__ - ['N F M S']
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:30:45 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:30:45 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:30:45 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:30:45 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:30:45 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:30:45 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:30:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:30:55 - INFO - __main__ - Starting training!
03/10/2022 19:30:59 - INFO - __main__ - Step 10 Global step 10 Train loss 20.168402 on epoch=4
03/10/2022 19:31:03 - INFO - __main__ - Step 20 Global step 20 Train loss 17.639172 on epoch=9
03/10/2022 19:31:08 - INFO - __main__ - Step 30 Global step 30 Train loss 13.303424 on epoch=14
03/10/2022 19:31:12 - INFO - __main__ - Step 40 Global step 40 Train loss 11.372456 on epoch=19
03/10/2022 19:31:17 - INFO - __main__ - Step 50 Global step 50 Train loss 10.179957 on epoch=24
03/10/2022 19:31:22 - INFO - __main__ - Global step 50 Train loss 14.532681 EM 0.0 on epoch=24
03/10/2022 19:31:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.073816 on epoch=29
03/10/2022 19:31:32 - INFO - __main__ - Step 70 Global step 70 Train loss 7.949934 on epoch=34
03/10/2022 19:31:37 - INFO - __main__ - Step 80 Global step 80 Train loss 6.856830 on epoch=39
03/10/2022 19:31:42 - INFO - __main__ - Step 90 Global step 90 Train loss 6.110747 on epoch=44
03/10/2022 19:31:46 - INFO - __main__ - Step 100 Global step 100 Train loss 5.445693 on epoch=49
03/10/2022 19:31:47 - INFO - __main__ - Global step 100 Train loss 7.087404 EM 0.03125 on epoch=49
03/10/2022 19:31:53 - INFO - __main__ - Step 110 Global step 110 Train loss 4.862219 on epoch=54
03/10/2022 19:31:57 - INFO - __main__ - Step 120 Global step 120 Train loss 4.351862 on epoch=59
03/10/2022 19:32:02 - INFO - __main__ - Step 130 Global step 130 Train loss 4.400249 on epoch=64
03/10/2022 19:32:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.070715 on epoch=69
03/10/2022 19:32:12 - INFO - __main__ - Step 150 Global step 150 Train loss 3.343708 on epoch=74
03/10/2022 19:32:12 - INFO - __main__ - Global step 150 Train loss 4.205751 EM 0.0 on epoch=74
03/10/2022 19:32:17 - INFO - __main__ - Step 160 Global step 160 Train loss 3.276582 on epoch=79
03/10/2022 19:32:22 - INFO - __main__ - Step 170 Global step 170 Train loss 2.763104 on epoch=84
03/10/2022 19:32:26 - INFO - __main__ - Step 180 Global step 180 Train loss 2.287498 on epoch=89
03/10/2022 19:32:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.158873 on epoch=94
03/10/2022 19:32:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.989051 on epoch=99
03/10/2022 19:32:36 - INFO - __main__ - Global step 200 Train loss 2.495022 EM 0.0 on epoch=99
03/10/2022 19:32:41 - INFO - __main__ - Step 210 Global step 210 Train loss 1.930729 on epoch=104
03/10/2022 19:32:46 - INFO - __main__ - Step 220 Global step 220 Train loss 1.829607 on epoch=109
03/10/2022 19:32:51 - INFO - __main__ - Step 230 Global step 230 Train loss 1.840060 on epoch=114
03/10/2022 19:32:55 - INFO - __main__ - Step 240 Global step 240 Train loss 2.100725 on epoch=119
03/10/2022 19:33:00 - INFO - __main__ - Step 250 Global step 250 Train loss 1.544070 on epoch=124
03/10/2022 19:33:01 - INFO - __main__ - Global step 250 Train loss 1.849038 EM 0.0 on epoch=124
03/10/2022 19:33:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.785022 on epoch=129
03/10/2022 19:33:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.835312 on epoch=134
03/10/2022 19:33:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.821734 on epoch=139
03/10/2022 19:33:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.528181 on epoch=144
03/10/2022 19:33:24 - INFO - __main__ - Step 300 Global step 300 Train loss 1.793028 on epoch=149
03/10/2022 19:33:25 - INFO - __main__ - Global step 300 Train loss 1.752655 EM 0.0 on epoch=149
03/10/2022 19:33:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.541242 on epoch=154
03/10/2022 19:33:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.503420 on epoch=159
03/10/2022 19:33:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.523582 on epoch=164
03/10/2022 19:33:44 - INFO - __main__ - Step 340 Global step 340 Train loss 1.506121 on epoch=169
03/10/2022 19:33:48 - INFO - __main__ - Step 350 Global step 350 Train loss 1.361735 on epoch=174
03/10/2022 19:33:49 - INFO - __main__ - Global step 350 Train loss 1.487220 EM 0.0 on epoch=174
03/10/2022 19:33:53 - INFO - __main__ - Step 360 Global step 360 Train loss 1.340684 on epoch=179
03/10/2022 19:33:58 - INFO - __main__ - Step 370 Global step 370 Train loss 1.297813 on epoch=184
03/10/2022 19:34:03 - INFO - __main__ - Step 380 Global step 380 Train loss 1.310400 on epoch=189
03/10/2022 19:34:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.249434 on epoch=194
03/10/2022 19:34:12 - INFO - __main__ - Step 400 Global step 400 Train loss 1.408894 on epoch=199
03/10/2022 19:34:13 - INFO - __main__ - Global step 400 Train loss 1.321445 EM 0.0 on epoch=199
03/10/2022 19:34:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.349543 on epoch=204
03/10/2022 19:34:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.403485 on epoch=209
03/10/2022 19:34:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.210877 on epoch=214
03/10/2022 19:34:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.247156 on epoch=219
03/10/2022 19:34:36 - INFO - __main__ - Step 450 Global step 450 Train loss 1.272632 on epoch=224
03/10/2022 19:34:37 - INFO - __main__ - Global step 450 Train loss 1.296739 EM 0.0 on epoch=224
03/10/2022 19:34:41 - INFO - __main__ - Step 460 Global step 460 Train loss 1.149818 on epoch=229
03/10/2022 19:34:46 - INFO - __main__ - Step 470 Global step 470 Train loss 1.169174 on epoch=234
03/10/2022 19:34:50 - INFO - __main__ - Step 480 Global step 480 Train loss 1.075751 on epoch=239
03/10/2022 19:34:55 - INFO - __main__ - Step 490 Global step 490 Train loss 1.057661 on epoch=244
03/10/2022 19:35:00 - INFO - __main__ - Step 500 Global step 500 Train loss 1.031940 on epoch=249
03/10/2022 19:35:00 - INFO - __main__ - Global step 500 Train loss 1.096869 EM 0.0 on epoch=249
03/10/2022 19:35:05 - INFO - __main__ - Step 510 Global step 510 Train loss 1.022718 on epoch=254
03/10/2022 19:35:10 - INFO - __main__ - Step 520 Global step 520 Train loss 1.048243 on epoch=259
03/10/2022 19:35:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.920996 on epoch=264
03/10/2022 19:35:19 - INFO - __main__ - Step 540 Global step 540 Train loss 1.043845 on epoch=269
03/10/2022 19:35:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.901838 on epoch=274
03/10/2022 19:35:24 - INFO - __main__ - Global step 550 Train loss 0.987528 EM 0.0 on epoch=274
03/10/2022 19:35:29 - INFO - __main__ - Step 560 Global step 560 Train loss 1.042765 on epoch=279
03/10/2022 19:35:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956232 on epoch=284
03/10/2022 19:35:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.954260 on epoch=289
03/10/2022 19:35:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.877376 on epoch=294
03/10/2022 19:35:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.958155 on epoch=299
03/10/2022 19:35:49 - INFO - __main__ - Global step 600 Train loss 0.957758 EM 0.0 on epoch=299
03/10/2022 19:35:49 - INFO - __main__ - save last model!
03/10/2022 19:35:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:35:49 - INFO - __main__ - Printing 3 examples
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:35:49 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:35:49 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:35:49 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:35:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:35:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:35:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:35:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:35:49 - INFO - __main__ - Printing 3 examples
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:35:49 - INFO - __main__ - ['N F M S']
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:35:49 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:35:49 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:35:49 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:35:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:35:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:35:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:35:55 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:35:56 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:35:56 - INFO - __main__ - Printing 3 examples
03/10/2022 19:35:56 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:35:56 - INFO - __main__ - ['Whole S M']
03/10/2022 19:35:56 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:35:56 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:35:56 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:35:56 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:35:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:35:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:35:59 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:35:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:35:59 - INFO - __main__ - Starting training!
03/10/2022 19:36:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0003_8_predictions.txt
03/10/2022 19:36:45 - INFO - __main__ - EM on test data: 0.0169
03/10/2022 19:36:45 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.016897081413210446
03/10/2022 19:36:45 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0002, bsz=8 ...
03/10/2022 19:36:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:36:46 - INFO - __main__ - Printing 3 examples
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:36:46 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:36:46 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:36:46 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:36:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:36:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:36:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:36:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:36:46 - INFO - __main__ - Printing 3 examples
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:36:46 - INFO - __main__ - ['N F M S']
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:36:46 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:36:46 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:36:46 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:36:46 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:36:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:36:46 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:36:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:36:56 - INFO - __main__ - Starting training!
03/10/2022 19:37:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.066547 on epoch=4
03/10/2022 19:37:04 - INFO - __main__ - Step 20 Global step 20 Train loss 18.077887 on epoch=9
03/10/2022 19:37:09 - INFO - __main__ - Step 30 Global step 30 Train loss 16.788851 on epoch=14
03/10/2022 19:37:13 - INFO - __main__ - Step 40 Global step 40 Train loss 14.024854 on epoch=19
03/10/2022 19:37:18 - INFO - __main__ - Step 50 Global step 50 Train loss 13.551190 on epoch=24
03/10/2022 19:37:27 - INFO - __main__ - Global step 50 Train loss 16.701866 EM 0.0 on epoch=24
03/10/2022 19:37:32 - INFO - __main__ - Step 60 Global step 60 Train loss 12.869360 on epoch=29
03/10/2022 19:37:37 - INFO - __main__ - Step 70 Global step 70 Train loss 12.629152 on epoch=34
03/10/2022 19:37:42 - INFO - __main__ - Step 80 Global step 80 Train loss 11.377266 on epoch=39
03/10/2022 19:37:46 - INFO - __main__ - Step 90 Global step 90 Train loss 10.488613 on epoch=44
03/10/2022 19:37:51 - INFO - __main__ - Step 100 Global step 100 Train loss 9.891205 on epoch=49
03/10/2022 19:37:52 - INFO - __main__ - Global step 100 Train loss 11.451119 EM 0.0 on epoch=49
03/10/2022 19:37:57 - INFO - __main__ - Step 110 Global step 110 Train loss 9.038453 on epoch=54
03/10/2022 19:38:01 - INFO - __main__ - Step 120 Global step 120 Train loss 8.493065 on epoch=59
03/10/2022 19:38:06 - INFO - __main__ - Step 130 Global step 130 Train loss 8.021463 on epoch=64
03/10/2022 19:38:11 - INFO - __main__ - Step 140 Global step 140 Train loss 7.621083 on epoch=69
03/10/2022 19:38:16 - INFO - __main__ - Step 150 Global step 150 Train loss 7.253566 on epoch=74
03/10/2022 19:38:16 - INFO - __main__ - Global step 150 Train loss 8.085526 EM 0.0 on epoch=74
03/10/2022 19:38:21 - INFO - __main__ - Step 160 Global step 160 Train loss 6.442595 on epoch=79
03/10/2022 19:38:26 - INFO - __main__ - Step 170 Global step 170 Train loss 6.386209 on epoch=84
03/10/2022 19:38:31 - INFO - __main__ - Step 180 Global step 180 Train loss 5.939996 on epoch=89
03/10/2022 19:38:35 - INFO - __main__ - Step 190 Global step 190 Train loss 5.515197 on epoch=94
03/10/2022 19:38:40 - INFO - __main__ - Step 200 Global step 200 Train loss 5.170688 on epoch=99
03/10/2022 19:38:41 - INFO - __main__ - Global step 200 Train loss 5.890937 EM 0.0 on epoch=99
03/10/2022 19:38:45 - INFO - __main__ - Step 210 Global step 210 Train loss 4.539303 on epoch=104
03/10/2022 19:38:50 - INFO - __main__ - Step 220 Global step 220 Train loss 4.334866 on epoch=109
03/10/2022 19:38:55 - INFO - __main__ - Step 230 Global step 230 Train loss 3.652869 on epoch=114
03/10/2022 19:39:00 - INFO - __main__ - Step 240 Global step 240 Train loss 3.405354 on epoch=119
03/10/2022 19:39:04 - INFO - __main__ - Step 250 Global step 250 Train loss 3.222134 on epoch=124
03/10/2022 19:39:05 - INFO - __main__ - Global step 250 Train loss 3.830905 EM 0.0 on epoch=124
03/10/2022 19:39:10 - INFO - __main__ - Step 260 Global step 260 Train loss 3.076876 on epoch=129
03/10/2022 19:39:14 - INFO - __main__ - Step 270 Global step 270 Train loss 2.705405 on epoch=134
03/10/2022 19:39:19 - INFO - __main__ - Step 280 Global step 280 Train loss 2.462991 on epoch=139
03/10/2022 19:39:24 - INFO - __main__ - Step 290 Global step 290 Train loss 2.391930 on epoch=144
03/10/2022 19:39:29 - INFO - __main__ - Step 300 Global step 300 Train loss 2.074502 on epoch=149
03/10/2022 19:39:29 - INFO - __main__ - Global step 300 Train loss 2.542341 EM 0.0 on epoch=149
03/10/2022 19:39:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.169536 on epoch=154
03/10/2022 19:39:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.063700 on epoch=159
03/10/2022 19:39:44 - INFO - __main__ - Step 330 Global step 330 Train loss 2.306985 on epoch=164
03/10/2022 19:39:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.817906 on epoch=169
03/10/2022 19:39:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.791309 on epoch=174
03/10/2022 19:39:54 - INFO - __main__ - Global step 350 Train loss 2.029887 EM 0.0 on epoch=174
03/10/2022 19:39:59 - INFO - __main__ - Step 360 Global step 360 Train loss 1.665859 on epoch=179
03/10/2022 19:40:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.899434 on epoch=184
03/10/2022 19:40:08 - INFO - __main__ - Step 380 Global step 380 Train loss 1.905968 on epoch=189
03/10/2022 19:40:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.716524 on epoch=194
03/10/2022 19:40:17 - INFO - __main__ - Step 400 Global step 400 Train loss 1.982593 on epoch=199
03/10/2022 19:40:18 - INFO - __main__ - Global step 400 Train loss 1.834076 EM 0.0 on epoch=199
03/10/2022 19:40:23 - INFO - __main__ - Step 410 Global step 410 Train loss 1.670531 on epoch=204
03/10/2022 19:40:27 - INFO - __main__ - Step 420 Global step 420 Train loss 1.726192 on epoch=209
03/10/2022 19:40:32 - INFO - __main__ - Step 430 Global step 430 Train loss 1.619317 on epoch=214
03/10/2022 19:40:37 - INFO - __main__ - Step 440 Global step 440 Train loss 1.657575 on epoch=219
03/10/2022 19:40:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.834209 on epoch=224
03/10/2022 19:40:42 - INFO - __main__ - Global step 450 Train loss 1.701565 EM 0.0 on epoch=224
03/10/2022 19:40:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.759073 on epoch=229
03/10/2022 19:40:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.552463 on epoch=234
03/10/2022 19:40:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.558401 on epoch=239
03/10/2022 19:41:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.459815 on epoch=244
03/10/2022 19:41:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.562851 on epoch=249
03/10/2022 19:41:07 - INFO - __main__ - Global step 500 Train loss 1.578521 EM 0.0 on epoch=249
03/10/2022 19:41:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.578943 on epoch=254
03/10/2022 19:41:17 - INFO - __main__ - Step 520 Global step 520 Train loss 1.434775 on epoch=259
03/10/2022 19:41:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.292102 on epoch=264
03/10/2022 19:41:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.462906 on epoch=269
03/10/2022 19:41:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.425776 on epoch=274
03/10/2022 19:41:32 - INFO - __main__ - Global step 550 Train loss 1.438900 EM 0.0 on epoch=274
03/10/2022 19:41:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.323335 on epoch=279
03/10/2022 19:41:42 - INFO - __main__ - Step 570 Global step 570 Train loss 1.569090 on epoch=284
03/10/2022 19:41:46 - INFO - __main__ - Step 580 Global step 580 Train loss 1.529825 on epoch=289
03/10/2022 19:41:51 - INFO - __main__ - Step 590 Global step 590 Train loss 1.537436 on epoch=294
03/10/2022 19:41:56 - INFO - __main__ - Step 600 Global step 600 Train loss 1.212131 on epoch=299
03/10/2022 19:41:57 - INFO - __main__ - Global step 600 Train loss 1.434363 EM 0.0 on epoch=299
03/10/2022 19:41:57 - INFO - __main__ - save last model!
03/10/2022 19:41:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:41:57 - INFO - __main__ - Printing 3 examples
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:41:57 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:41:57 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:41:57 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:41:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:41:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:41:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:41:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:41:57 - INFO - __main__ - Printing 3 examples
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:41:57 - INFO - __main__ - ['N F M S']
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:41:57 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:41:57 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:41:57 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:41:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:41:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:41:57 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:42:04 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:42:04 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:42:04 - INFO - __main__ - Printing 3 examples
03/10/2022 19:42:04 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:42:04 - INFO - __main__ - ['Whole S M']
03/10/2022 19:42:04 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:42:04 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:42:04 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:42:04 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:42:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:42:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:42:07 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:42:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:42:07 - INFO - __main__ - Starting training!
03/10/2022 19:51:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0002_8_predictions.txt
03/10/2022 19:51:02 - INFO - __main__ - EM on test data: 0.0010
03/10/2022 19:51:02 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0010240655401945725
03/10/2022 19:51:02 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0001, bsz=8 ...
03/10/2022 19:51:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:51:03 - INFO - __main__ - Printing 3 examples
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/10/2022 19:51:03 - INFO - __main__ - ['D W glass Markerboards']
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] aromacomposer
03/10/2022 19:51:03 - INFO - __main__ - ['aroma Composer']
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/10/2022 19:51:03 - INFO - __main__ - ['J M Brodrick']
03/10/2022 19:51:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:51:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:51:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:51:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:51:03 - INFO - __main__ - Printing 3 examples
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] nfms
03/10/2022 19:51:03 - INFO - __main__ - ['N F M S']
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/10/2022 19:51:03 - INFO - __main__ - ['Love world television ministry']
03/10/2022 19:51:03 - INFO - __main__ -  [crawl_domain] shirkmusic
03/10/2022 19:51:03 - INFO - __main__ - ['Shirk Music']
03/10/2022 19:51:03 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:51:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:51:03 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:51:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:51:13 - INFO - __main__ - Starting training!
03/10/2022 19:51:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.202028 on epoch=4
03/10/2022 19:51:22 - INFO - __main__ - Step 20 Global step 20 Train loss 17.969721 on epoch=9
03/10/2022 19:51:26 - INFO - __main__ - Step 30 Global step 30 Train loss 17.160624 on epoch=14
03/10/2022 19:51:31 - INFO - __main__ - Step 40 Global step 40 Train loss 15.182925 on epoch=19
03/10/2022 19:51:36 - INFO - __main__ - Step 50 Global step 50 Train loss 14.754292 on epoch=24
03/10/2022 19:51:45 - INFO - __main__ - Global step 50 Train loss 17.053917 EM 0.0 on epoch=24
03/10/2022 19:51:50 - INFO - __main__ - Step 60 Global step 60 Train loss 13.774915 on epoch=29
03/10/2022 19:51:55 - INFO - __main__ - Step 70 Global step 70 Train loss 12.489703 on epoch=34
03/10/2022 19:52:00 - INFO - __main__ - Step 80 Global step 80 Train loss 12.845505 on epoch=39
03/10/2022 19:52:05 - INFO - __main__ - Step 90 Global step 90 Train loss 11.520163 on epoch=44
03/10/2022 19:52:10 - INFO - __main__ - Step 100 Global step 100 Train loss 11.157951 on epoch=49
03/10/2022 19:52:17 - INFO - __main__ - Global step 100 Train loss 12.357648 EM 0.0 on epoch=49
03/10/2022 19:52:22 - INFO - __main__ - Step 110 Global step 110 Train loss 11.627232 on epoch=54
03/10/2022 19:52:27 - INFO - __main__ - Step 120 Global step 120 Train loss 11.143096 on epoch=59
03/10/2022 19:52:31 - INFO - __main__ - Step 130 Global step 130 Train loss 10.753961 on epoch=64
03/10/2022 19:52:36 - INFO - __main__ - Step 140 Global step 140 Train loss 10.560368 on epoch=69
03/10/2022 19:52:41 - INFO - __main__ - Step 150 Global step 150 Train loss 10.026731 on epoch=74
03/10/2022 19:52:43 - INFO - __main__ - Global step 150 Train loss 10.822278 EM 0.0 on epoch=74
03/10/2022 19:52:48 - INFO - __main__ - Step 160 Global step 160 Train loss 9.666493 on epoch=79
03/10/2022 19:52:52 - INFO - __main__ - Step 170 Global step 170 Train loss 9.385073 on epoch=84
03/10/2022 19:52:57 - INFO - __main__ - Step 180 Global step 180 Train loss 8.980004 on epoch=89
03/10/2022 19:53:02 - INFO - __main__ - Step 190 Global step 190 Train loss 8.846939 on epoch=94
03/10/2022 19:53:07 - INFO - __main__ - Step 200 Global step 200 Train loss 8.307763 on epoch=99
03/10/2022 19:53:07 - INFO - __main__ - Global step 200 Train loss 9.037254 EM 0.0 on epoch=99
03/10/2022 19:53:12 - INFO - __main__ - Step 210 Global step 210 Train loss 8.607520 on epoch=104
03/10/2022 19:53:17 - INFO - __main__ - Step 220 Global step 220 Train loss 7.680250 on epoch=109
03/10/2022 19:53:22 - INFO - __main__ - Step 230 Global step 230 Train loss 7.735811 on epoch=114
03/10/2022 19:53:27 - INFO - __main__ - Step 240 Global step 240 Train loss 7.435762 on epoch=119
03/10/2022 19:53:31 - INFO - __main__ - Step 250 Global step 250 Train loss 6.982578 on epoch=124
03/10/2022 19:53:32 - INFO - __main__ - Global step 250 Train loss 7.688385 EM 0.0 on epoch=124
03/10/2022 19:53:37 - INFO - __main__ - Step 260 Global step 260 Train loss 6.997376 on epoch=129
03/10/2022 19:53:42 - INFO - __main__ - Step 270 Global step 270 Train loss 6.727931 on epoch=134
03/10/2022 19:53:47 - INFO - __main__ - Step 280 Global step 280 Train loss 6.713638 on epoch=139
03/10/2022 19:53:51 - INFO - __main__ - Step 290 Global step 290 Train loss 6.452781 on epoch=144
03/10/2022 19:53:56 - INFO - __main__ - Step 300 Global step 300 Train loss 6.463544 on epoch=149
03/10/2022 19:53:57 - INFO - __main__ - Global step 300 Train loss 6.671054 EM 0.0 on epoch=149
03/10/2022 19:54:02 - INFO - __main__ - Step 310 Global step 310 Train loss 6.157945 on epoch=154
03/10/2022 19:54:06 - INFO - __main__ - Step 320 Global step 320 Train loss 6.032699 on epoch=159
03/10/2022 19:54:11 - INFO - __main__ - Step 330 Global step 330 Train loss 5.535845 on epoch=164
03/10/2022 19:54:16 - INFO - __main__ - Step 340 Global step 340 Train loss 5.379111 on epoch=169
03/10/2022 19:54:21 - INFO - __main__ - Step 350 Global step 350 Train loss 4.844237 on epoch=174
03/10/2022 19:54:21 - INFO - __main__ - Global step 350 Train loss 5.589967 EM 0.0 on epoch=174
03/10/2022 19:54:26 - INFO - __main__ - Step 360 Global step 360 Train loss 4.899179 on epoch=179
03/10/2022 19:54:31 - INFO - __main__ - Step 370 Global step 370 Train loss 4.791266 on epoch=184
03/10/2022 19:54:36 - INFO - __main__ - Step 380 Global step 380 Train loss 4.394280 on epoch=189
03/10/2022 19:54:40 - INFO - __main__ - Step 390 Global step 390 Train loss 4.233732 on epoch=194
03/10/2022 19:54:45 - INFO - __main__ - Step 400 Global step 400 Train loss 4.316030 on epoch=199
03/10/2022 19:54:46 - INFO - __main__ - Global step 400 Train loss 4.526897 EM 0.0 on epoch=199
03/10/2022 19:54:50 - INFO - __main__ - Step 410 Global step 410 Train loss 3.868865 on epoch=204
03/10/2022 19:54:55 - INFO - __main__ - Step 420 Global step 420 Train loss 3.753970 on epoch=209
03/10/2022 19:55:00 - INFO - __main__ - Step 430 Global step 430 Train loss 3.643304 on epoch=214
03/10/2022 19:55:04 - INFO - __main__ - Step 440 Global step 440 Train loss 3.358409 on epoch=219
03/10/2022 19:55:09 - INFO - __main__ - Step 450 Global step 450 Train loss 3.138919 on epoch=224
03/10/2022 19:55:10 - INFO - __main__ - Global step 450 Train loss 3.552693 EM 0.0 on epoch=224
03/10/2022 19:55:15 - INFO - __main__ - Step 460 Global step 460 Train loss 2.999937 on epoch=229
03/10/2022 19:55:19 - INFO - __main__ - Step 470 Global step 470 Train loss 3.166857 on epoch=234
03/10/2022 19:55:24 - INFO - __main__ - Step 480 Global step 480 Train loss 2.546609 on epoch=239
03/10/2022 19:55:29 - INFO - __main__ - Step 490 Global step 490 Train loss 2.647700 on epoch=244
03/10/2022 19:55:34 - INFO - __main__ - Step 500 Global step 500 Train loss 2.667511 on epoch=249
03/10/2022 19:55:34 - INFO - __main__ - Global step 500 Train loss 2.805723 EM 0.0 on epoch=249
03/10/2022 19:55:39 - INFO - __main__ - Step 510 Global step 510 Train loss 2.717798 on epoch=254
03/10/2022 19:55:44 - INFO - __main__ - Step 520 Global step 520 Train loss 2.415378 on epoch=259
03/10/2022 19:55:49 - INFO - __main__ - Step 530 Global step 530 Train loss 2.323331 on epoch=264
03/10/2022 19:55:53 - INFO - __main__ - Step 540 Global step 540 Train loss 2.080748 on epoch=269
03/10/2022 19:55:58 - INFO - __main__ - Step 550 Global step 550 Train loss 2.333452 on epoch=274
03/10/2022 19:55:59 - INFO - __main__ - Global step 550 Train loss 2.374141 EM 0.0 on epoch=274
03/10/2022 19:56:03 - INFO - __main__ - Step 560 Global step 560 Train loss 2.279894 on epoch=279
03/10/2022 19:56:08 - INFO - __main__ - Step 570 Global step 570 Train loss 2.481958 on epoch=284
03/10/2022 19:56:13 - INFO - __main__ - Step 580 Global step 580 Train loss 2.332525 on epoch=289
03/10/2022 19:56:18 - INFO - __main__ - Step 590 Global step 590 Train loss 2.182814 on epoch=294
03/10/2022 19:56:22 - INFO - __main__ - Step 600 Global step 600 Train loss 2.358635 on epoch=299
03/10/2022 19:56:23 - INFO - __main__ - Global step 600 Train loss 2.327165 EM 0.0 on epoch=299
03/10/2022 19:56:23 - INFO - __main__ - save last model!
03/10/2022 19:56:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:56:23 - INFO - __main__ - Printing 3 examples
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 19:56:23 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 19:56:23 - INFO - __main__ - ['Jim fargiano']
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 19:56:23 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 19:56:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:56:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:56:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:56:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:56:23 - INFO - __main__ - Printing 3 examples
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 19:56:23 - INFO - __main__ - ['Find Local Electric']
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 19:56:23 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 19:56:23 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 19:56:23 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 19:56:23 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:56:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:56:23 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:56:30 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 19:56:30 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 19:56:30 - INFO - __main__ - Printing 3 examples
03/10/2022 19:56:30 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 19:56:30 - INFO - __main__ - ['Whole S M']
03/10/2022 19:56:30 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 19:56:30 - INFO - __main__ - ['pushin Daisies']
03/10/2022 19:56:30 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 19:56:30 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 19:56:30 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:56:31 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:56:33 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 19:56:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 19:56:33 - INFO - __main__ - Starting training!
03/10/2022 20:06:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0001_8_predictions.txt
03/10/2022 20:06:34 - INFO - __main__ - EM on test data: 0.0000
03/10/2022 20:06:34 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/10/2022 20:06:34 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0005, bsz=8 ...
03/10/2022 20:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:06:35 - INFO - __main__ - Printing 3 examples
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:06:35 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:06:35 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:06:35 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:06:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:06:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:06:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:06:35 - INFO - __main__ - Printing 3 examples
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:06:35 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:06:35 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:06:35 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:06:35 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:06:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:06:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:06:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:06:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:06:45 - INFO - __main__ - Starting training!
03/10/2022 20:06:49 - INFO - __main__ - Step 10 Global step 10 Train loss 21.392817 on epoch=4
03/10/2022 20:06:54 - INFO - __main__ - Step 20 Global step 20 Train loss 15.385191 on epoch=9
03/10/2022 20:06:59 - INFO - __main__ - Step 30 Global step 30 Train loss 12.623703 on epoch=14
03/10/2022 20:07:04 - INFO - __main__ - Step 40 Global step 40 Train loss 10.054950 on epoch=19
03/10/2022 20:07:08 - INFO - __main__ - Step 50 Global step 50 Train loss 8.689352 on epoch=24
03/10/2022 20:07:12 - INFO - __main__ - Global step 50 Train loss 13.629202 EM 0.0 on epoch=24
03/10/2022 20:07:17 - INFO - __main__ - Step 60 Global step 60 Train loss 7.242790 on epoch=29
03/10/2022 20:07:22 - INFO - __main__ - Step 70 Global step 70 Train loss 6.372913 on epoch=34
03/10/2022 20:07:27 - INFO - __main__ - Step 80 Global step 80 Train loss 5.375432 on epoch=39
03/10/2022 20:07:32 - INFO - __main__ - Step 90 Global step 90 Train loss 4.711534 on epoch=44
03/10/2022 20:07:37 - INFO - __main__ - Step 100 Global step 100 Train loss 3.930841 on epoch=49
03/10/2022 20:07:38 - INFO - __main__ - Global step 100 Train loss 5.526703 EM 0.0 on epoch=49
03/10/2022 20:07:42 - INFO - __main__ - Step 110 Global step 110 Train loss 3.264167 on epoch=54
03/10/2022 20:07:47 - INFO - __main__ - Step 120 Global step 120 Train loss 2.736776 on epoch=59
03/10/2022 20:07:52 - INFO - __main__ - Step 130 Global step 130 Train loss 2.644321 on epoch=64
03/10/2022 20:07:57 - INFO - __main__ - Step 140 Global step 140 Train loss 2.552358 on epoch=69
03/10/2022 20:08:02 - INFO - __main__ - Step 150 Global step 150 Train loss 2.536237 on epoch=74
03/10/2022 20:08:03 - INFO - __main__ - Global step 150 Train loss 2.746772 EM 0.0 on epoch=74
03/10/2022 20:08:07 - INFO - __main__ - Step 160 Global step 160 Train loss 2.396605 on epoch=79
03/10/2022 20:08:12 - INFO - __main__ - Step 170 Global step 170 Train loss 2.070202 on epoch=84
03/10/2022 20:08:17 - INFO - __main__ - Step 180 Global step 180 Train loss 2.136462 on epoch=89
03/10/2022 20:08:22 - INFO - __main__ - Step 190 Global step 190 Train loss 2.067081 on epoch=94
03/10/2022 20:08:27 - INFO - __main__ - Step 200 Global step 200 Train loss 2.175739 on epoch=99
03/10/2022 20:08:27 - INFO - __main__ - Global step 200 Train loss 2.169218 EM 0.0 on epoch=99
03/10/2022 20:08:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.945720 on epoch=104
03/10/2022 20:08:37 - INFO - __main__ - Step 220 Global step 220 Train loss 1.664150 on epoch=109
03/10/2022 20:08:42 - INFO - __main__ - Step 230 Global step 230 Train loss 1.724065 on epoch=114
03/10/2022 20:08:47 - INFO - __main__ - Step 240 Global step 240 Train loss 1.592459 on epoch=119
03/10/2022 20:08:52 - INFO - __main__ - Step 250 Global step 250 Train loss 1.523502 on epoch=124
03/10/2022 20:08:52 - INFO - __main__ - Global step 250 Train loss 1.689979 EM 0.0 on epoch=124
03/10/2022 20:08:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.344600 on epoch=129
03/10/2022 20:09:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.617836 on epoch=134
03/10/2022 20:09:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.499631 on epoch=139
03/10/2022 20:09:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.329859 on epoch=144
03/10/2022 20:09:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.399770 on epoch=149
03/10/2022 20:09:17 - INFO - __main__ - Global step 300 Train loss 1.438339 EM 0.0 on epoch=149
03/10/2022 20:09:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.359706 on epoch=154
03/10/2022 20:09:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.316322 on epoch=159
03/10/2022 20:09:32 - INFO - __main__ - Step 330 Global step 330 Train loss 1.241067 on epoch=164
03/10/2022 20:09:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.168339 on epoch=169
03/10/2022 20:09:41 - INFO - __main__ - Step 350 Global step 350 Train loss 1.294265 on epoch=174
03/10/2022 20:09:42 - INFO - __main__ - Global step 350 Train loss 1.275940 EM 0.0 on epoch=174
03/10/2022 20:09:47 - INFO - __main__ - Step 360 Global step 360 Train loss 1.190042 on epoch=179
03/10/2022 20:09:51 - INFO - __main__ - Step 370 Global step 370 Train loss 1.148447 on epoch=184
03/10/2022 20:09:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.067672 on epoch=189
03/10/2022 20:10:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.975841 on epoch=194
03/10/2022 20:10:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.127317 on epoch=199
03/10/2022 20:10:06 - INFO - __main__ - Global step 400 Train loss 1.101864 EM 0.0 on epoch=199
03/10/2022 20:10:11 - INFO - __main__ - Step 410 Global step 410 Train loss 1.025315 on epoch=204
03/10/2022 20:10:15 - INFO - __main__ - Step 420 Global step 420 Train loss 1.017051 on epoch=209
03/10/2022 20:10:20 - INFO - __main__ - Step 430 Global step 430 Train loss 1.060579 on epoch=214
03/10/2022 20:10:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.961013 on epoch=219
03/10/2022 20:10:29 - INFO - __main__ - Step 450 Global step 450 Train loss 1.006285 on epoch=224
03/10/2022 20:10:30 - INFO - __main__ - Global step 450 Train loss 1.014049 EM 0.0 on epoch=224
03/10/2022 20:10:35 - INFO - __main__ - Step 460 Global step 460 Train loss 1.022498 on epoch=229
03/10/2022 20:10:39 - INFO - __main__ - Step 470 Global step 470 Train loss 1.008595 on epoch=234
03/10/2022 20:10:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.990247 on epoch=239
03/10/2022 20:10:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.917587 on epoch=244
03/10/2022 20:10:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.823242 on epoch=249
03/10/2022 20:10:54 - INFO - __main__ - Global step 500 Train loss 0.952434 EM 0.0 on epoch=249
03/10/2022 20:10:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.945854 on epoch=254
03/10/2022 20:11:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.835644 on epoch=259
03/10/2022 20:11:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.877343 on epoch=264
03/10/2022 20:11:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.835283 on epoch=269
03/10/2022 20:11:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.788883 on epoch=274
03/10/2022 20:11:18 - INFO - __main__ - Global step 550 Train loss 0.856602 EM 0.0 on epoch=274
03/10/2022 20:11:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.793078 on epoch=279
03/10/2022 20:11:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.776926 on epoch=284
03/10/2022 20:11:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.843690 on epoch=289
03/10/2022 20:11:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.649935 on epoch=294
03/10/2022 20:11:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.772904 on epoch=299
03/10/2022 20:11:42 - INFO - __main__ - Global step 600 Train loss 0.767307 EM 0.0 on epoch=299
03/10/2022 20:11:42 - INFO - __main__ - save last model!
03/10/2022 20:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:11:42 - INFO - __main__ - Printing 3 examples
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:11:42 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:11:42 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:11:42 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:11:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:11:42 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:11:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:11:42 - INFO - __main__ - Printing 3 examples
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:11:42 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:11:42 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:11:42 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:11:42 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:11:42 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:11:42 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:11:42 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:11:48 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:11:49 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 20:11:49 - INFO - __main__ - Printing 3 examples
03/10/2022 20:11:49 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 20:11:49 - INFO - __main__ - ['Whole S M']
03/10/2022 20:11:49 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 20:11:49 - INFO - __main__ - ['pushin Daisies']
03/10/2022 20:11:49 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 20:11:49 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 20:11:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:11:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:11:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:11:52 - INFO - __main__ - Starting training!
03/10/2022 20:11:52 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 20:13:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0005_8_predictions.txt
03/10/2022 20:13:47 - INFO - __main__ - EM on test data: 0.0143
03/10/2022 20:13:49 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.014336917562724014
03/10/2022 20:13:49 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0003, bsz=8 ...
03/10/2022 20:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:13:50 - INFO - __main__ - Printing 3 examples
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:13:50 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:13:50 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:13:50 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:13:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:13:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:13:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:13:50 - INFO - __main__ - Printing 3 examples
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:13:50 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:13:50 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:13:50 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:13:50 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:13:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:13:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:13:50 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:14:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:14:00 - INFO - __main__ - Starting training!
03/10/2022 20:14:04 - INFO - __main__ - Step 10 Global step 10 Train loss 22.239128 on epoch=4
03/10/2022 20:14:08 - INFO - __main__ - Step 20 Global step 20 Train loss 18.652187 on epoch=9
03/10/2022 20:14:13 - INFO - __main__ - Step 30 Global step 30 Train loss 16.428905 on epoch=14
03/10/2022 20:14:17 - INFO - __main__ - Step 40 Global step 40 Train loss 14.145081 on epoch=19
03/10/2022 20:14:22 - INFO - __main__ - Step 50 Global step 50 Train loss 12.248583 on epoch=24
03/10/2022 20:14:30 - INFO - __main__ - Global step 50 Train loss 16.742777 EM 0.0 on epoch=24
03/10/2022 20:14:35 - INFO - __main__ - Step 60 Global step 60 Train loss 11.215422 on epoch=29
03/10/2022 20:14:40 - INFO - __main__ - Step 70 Global step 70 Train loss 9.937567 on epoch=34
03/10/2022 20:14:45 - INFO - __main__ - Step 80 Global step 80 Train loss 9.538462 on epoch=39
03/10/2022 20:14:50 - INFO - __main__ - Step 90 Global step 90 Train loss 8.449139 on epoch=44
03/10/2022 20:14:54 - INFO - __main__ - Step 100 Global step 100 Train loss 8.095617 on epoch=49
03/10/2022 20:14:55 - INFO - __main__ - Global step 100 Train loss 9.447242 EM 0.03125 on epoch=49
03/10/2022 20:15:00 - INFO - __main__ - Step 110 Global step 110 Train loss 6.836401 on epoch=54
03/10/2022 20:15:05 - INFO - __main__ - Step 120 Global step 120 Train loss 6.284898 on epoch=59
03/10/2022 20:15:10 - INFO - __main__ - Step 130 Global step 130 Train loss 5.489193 on epoch=64
03/10/2022 20:15:15 - INFO - __main__ - Step 140 Global step 140 Train loss 5.362124 on epoch=69
03/10/2022 20:15:19 - INFO - __main__ - Step 150 Global step 150 Train loss 5.037354 on epoch=74
03/10/2022 20:15:20 - INFO - __main__ - Global step 150 Train loss 5.801994 EM 0.0 on epoch=74
03/10/2022 20:15:25 - INFO - __main__ - Step 160 Global step 160 Train loss 4.193202 on epoch=79
03/10/2022 20:15:30 - INFO - __main__ - Step 170 Global step 170 Train loss 3.943006 on epoch=84
03/10/2022 20:15:34 - INFO - __main__ - Step 180 Global step 180 Train loss 3.913870 on epoch=89
03/10/2022 20:15:39 - INFO - __main__ - Step 190 Global step 190 Train loss 3.088883 on epoch=94
03/10/2022 20:15:44 - INFO - __main__ - Step 200 Global step 200 Train loss 3.114126 on epoch=99
03/10/2022 20:15:44 - INFO - __main__ - Global step 200 Train loss 3.650617 EM 0.0 on epoch=99
03/10/2022 20:15:49 - INFO - __main__ - Step 210 Global step 210 Train loss 2.648342 on epoch=104
03/10/2022 20:15:54 - INFO - __main__ - Step 220 Global step 220 Train loss 2.276562 on epoch=109
03/10/2022 20:15:58 - INFO - __main__ - Step 230 Global step 230 Train loss 2.377223 on epoch=114
03/10/2022 20:16:03 - INFO - __main__ - Step 240 Global step 240 Train loss 2.577532 on epoch=119
03/10/2022 20:16:08 - INFO - __main__ - Step 250 Global step 250 Train loss 2.365644 on epoch=124
03/10/2022 20:16:08 - INFO - __main__ - Global step 250 Train loss 2.449061 EM 0.0 on epoch=124
03/10/2022 20:16:13 - INFO - __main__ - Step 260 Global step 260 Train loss 2.104185 on epoch=129
03/10/2022 20:16:18 - INFO - __main__ - Step 270 Global step 270 Train loss 2.079000 on epoch=134
03/10/2022 20:16:23 - INFO - __main__ - Step 280 Global step 280 Train loss 2.116897 on epoch=139
03/10/2022 20:16:27 - INFO - __main__ - Step 290 Global step 290 Train loss 2.007790 on epoch=144
03/10/2022 20:16:32 - INFO - __main__ - Step 300 Global step 300 Train loss 1.812988 on epoch=149
03/10/2022 20:16:32 - INFO - __main__ - Global step 300 Train loss 2.024172 EM 0.0 on epoch=149
03/10/2022 20:16:37 - INFO - __main__ - Step 310 Global step 310 Train loss 1.798351 on epoch=154
03/10/2022 20:16:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.979113 on epoch=159
03/10/2022 20:16:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.954975 on epoch=164
03/10/2022 20:16:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.669406 on epoch=169
03/10/2022 20:16:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.779647 on epoch=174
03/10/2022 20:16:56 - INFO - __main__ - Global step 350 Train loss 1.836298 EM 0.0 on epoch=174
03/10/2022 20:17:01 - INFO - __main__ - Step 360 Global step 360 Train loss 1.699513 on epoch=179
03/10/2022 20:17:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.733286 on epoch=184
03/10/2022 20:17:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.625175 on epoch=189
03/10/2022 20:17:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.666866 on epoch=194
03/10/2022 20:17:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.530119 on epoch=199
03/10/2022 20:17:20 - INFO - __main__ - Global step 400 Train loss 1.650992 EM 0.0 on epoch=199
03/10/2022 20:17:24 - INFO - __main__ - Step 410 Global step 410 Train loss 1.461432 on epoch=204
03/10/2022 20:17:29 - INFO - __main__ - Step 420 Global step 420 Train loss 1.580910 on epoch=209
03/10/2022 20:17:34 - INFO - __main__ - Step 430 Global step 430 Train loss 1.468972 on epoch=214
03/10/2022 20:17:38 - INFO - __main__ - Step 440 Global step 440 Train loss 1.403731 on epoch=219
03/10/2022 20:17:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.531794 on epoch=224
03/10/2022 20:17:43 - INFO - __main__ - Global step 450 Train loss 1.489368 EM 0.0 on epoch=224
03/10/2022 20:17:48 - INFO - __main__ - Step 460 Global step 460 Train loss 1.347290 on epoch=229
03/10/2022 20:17:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.357488 on epoch=234
03/10/2022 20:17:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.358803 on epoch=239
03/10/2022 20:18:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.260397 on epoch=244
03/10/2022 20:18:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.377137 on epoch=249
03/10/2022 20:18:07 - INFO - __main__ - Global step 500 Train loss 1.340223 EM 0.0 on epoch=249
03/10/2022 20:18:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.392755 on epoch=254
03/10/2022 20:18:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.380357 on epoch=259
03/10/2022 20:18:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.395746 on epoch=264
03/10/2022 20:18:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.140144 on epoch=269
03/10/2022 20:18:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.226855 on epoch=274
03/10/2022 20:18:32 - INFO - __main__ - Global step 550 Train loss 1.307171 EM 0.0 on epoch=274
03/10/2022 20:18:36 - INFO - __main__ - Step 560 Global step 560 Train loss 1.203625 on epoch=279
03/10/2022 20:18:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.167034 on epoch=284
03/10/2022 20:18:46 - INFO - __main__ - Step 580 Global step 580 Train loss 1.062467 on epoch=289
03/10/2022 20:18:51 - INFO - __main__ - Step 590 Global step 590 Train loss 1.101015 on epoch=294
03/10/2022 20:18:56 - INFO - __main__ - Step 600 Global step 600 Train loss 1.171195 on epoch=299
03/10/2022 20:18:56 - INFO - __main__ - Global step 600 Train loss 1.141067 EM 0.0 on epoch=299
03/10/2022 20:18:56 - INFO - __main__ - save last model!
03/10/2022 20:18:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:18:57 - INFO - __main__ - Printing 3 examples
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:18:57 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:18:57 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:18:57 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:18:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:18:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:18:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:18:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:18:57 - INFO - __main__ - Printing 3 examples
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:18:57 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:18:57 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:18:57 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:18:57 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:18:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:18:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:18:57 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:19:03 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:19:04 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 20:19:04 - INFO - __main__ - Printing 3 examples
03/10/2022 20:19:04 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 20:19:04 - INFO - __main__ - ['Whole S M']
03/10/2022 20:19:04 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 20:19:04 - INFO - __main__ - ['pushin Daisies']
03/10/2022 20:19:04 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 20:19:04 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 20:19:04 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:19:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:19:06 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 20:19:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:19:06 - INFO - __main__ - Starting training!
03/10/2022 20:19:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0003_8_predictions.txt
03/10/2022 20:19:40 - INFO - __main__ - EM on test data: 0.0041
03/10/2022 20:19:41 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.00409626216077829
03/10/2022 20:19:41 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0002, bsz=8 ...
03/10/2022 20:19:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:19:41 - INFO - __main__ - Printing 3 examples
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:19:41 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:19:41 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:19:41 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:19:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:19:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:19:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:19:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:19:41 - INFO - __main__ - Printing 3 examples
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:19:41 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:19:41 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:19:41 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:19:41 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:19:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:19:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:19:42 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:19:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:19:51 - INFO - __main__ - Starting training!
03/10/2022 20:19:55 - INFO - __main__ - Step 10 Global step 10 Train loss 21.617632 on epoch=4
03/10/2022 20:20:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.362301 on epoch=9
03/10/2022 20:20:05 - INFO - __main__ - Step 30 Global step 30 Train loss 14.898984 on epoch=14
03/10/2022 20:20:09 - INFO - __main__ - Step 40 Global step 40 Train loss 13.371157 on epoch=19
03/10/2022 20:20:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.465695 on epoch=24
03/10/2022 20:20:24 - INFO - __main__ - Global step 50 Train loss 15.943153 EM 0.0 on epoch=24
03/10/2022 20:20:30 - INFO - __main__ - Step 60 Global step 60 Train loss 10.655437 on epoch=29
03/10/2022 20:20:34 - INFO - __main__ - Step 70 Global step 70 Train loss 9.883430 on epoch=34
03/10/2022 20:20:39 - INFO - __main__ - Step 80 Global step 80 Train loss 9.279242 on epoch=39
03/10/2022 20:20:44 - INFO - __main__ - Step 90 Global step 90 Train loss 8.270137 on epoch=44
03/10/2022 20:20:49 - INFO - __main__ - Step 100 Global step 100 Train loss 7.313438 on epoch=49
03/10/2022 20:20:49 - INFO - __main__ - Global step 100 Train loss 9.080337 EM 0.03125 on epoch=49
03/10/2022 20:20:55 - INFO - __main__ - Step 110 Global step 110 Train loss 7.006866 on epoch=54
03/10/2022 20:21:00 - INFO - __main__ - Step 120 Global step 120 Train loss 6.698251 on epoch=59
03/10/2022 20:21:05 - INFO - __main__ - Step 130 Global step 130 Train loss 6.651324 on epoch=64
03/10/2022 20:21:09 - INFO - __main__ - Step 140 Global step 140 Train loss 6.132501 on epoch=69
03/10/2022 20:21:14 - INFO - __main__ - Step 150 Global step 150 Train loss 5.976411 on epoch=74
03/10/2022 20:21:15 - INFO - __main__ - Global step 150 Train loss 6.493071 EM 0.03125 on epoch=74
03/10/2022 20:21:20 - INFO - __main__ - Step 160 Global step 160 Train loss 5.732610 on epoch=79
03/10/2022 20:21:24 - INFO - __main__ - Step 170 Global step 170 Train loss 5.585776 on epoch=84
03/10/2022 20:21:29 - INFO - __main__ - Step 180 Global step 180 Train loss 4.990995 on epoch=89
03/10/2022 20:21:34 - INFO - __main__ - Step 190 Global step 190 Train loss 4.749539 on epoch=94
03/10/2022 20:21:39 - INFO - __main__ - Step 200 Global step 200 Train loss 4.853705 on epoch=99
03/10/2022 20:21:40 - INFO - __main__ - Global step 200 Train loss 5.182525 EM 0.03125 on epoch=99
03/10/2022 20:21:44 - INFO - __main__ - Step 210 Global step 210 Train loss 4.077638 on epoch=104
03/10/2022 20:21:49 - INFO - __main__ - Step 220 Global step 220 Train loss 3.726320 on epoch=109
03/10/2022 20:21:54 - INFO - __main__ - Step 230 Global step 230 Train loss 3.760906 on epoch=114
03/10/2022 20:21:59 - INFO - __main__ - Step 240 Global step 240 Train loss 3.227713 on epoch=119
03/10/2022 20:22:03 - INFO - __main__ - Step 250 Global step 250 Train loss 3.092629 on epoch=124
03/10/2022 20:22:04 - INFO - __main__ - Global step 250 Train loss 3.577041 EM 0.03125 on epoch=124
03/10/2022 20:22:09 - INFO - __main__ - Step 260 Global step 260 Train loss 2.724927 on epoch=129
03/10/2022 20:22:14 - INFO - __main__ - Step 270 Global step 270 Train loss 2.722033 on epoch=134
03/10/2022 20:22:18 - INFO - __main__ - Step 280 Global step 280 Train loss 2.613707 on epoch=139
03/10/2022 20:22:23 - INFO - __main__ - Step 290 Global step 290 Train loss 2.666839 on epoch=144
03/10/2022 20:22:28 - INFO - __main__ - Step 300 Global step 300 Train loss 2.723284 on epoch=149
03/10/2022 20:22:28 - INFO - __main__ - Global step 300 Train loss 2.690157 EM 0.0 on epoch=149
03/10/2022 20:22:33 - INFO - __main__ - Step 310 Global step 310 Train loss 2.631853 on epoch=154
03/10/2022 20:22:38 - INFO - __main__ - Step 320 Global step 320 Train loss 2.293093 on epoch=159
03/10/2022 20:22:43 - INFO - __main__ - Step 330 Global step 330 Train loss 2.091580 on epoch=164
03/10/2022 20:22:48 - INFO - __main__ - Step 340 Global step 340 Train loss 2.446470 on epoch=169
03/10/2022 20:22:52 - INFO - __main__ - Step 350 Global step 350 Train loss 2.029450 on epoch=174
03/10/2022 20:22:53 - INFO - __main__ - Global step 350 Train loss 2.298489 EM 0.0 on epoch=174
03/10/2022 20:22:58 - INFO - __main__ - Step 360 Global step 360 Train loss 2.346496 on epoch=179
03/10/2022 20:23:03 - INFO - __main__ - Step 370 Global step 370 Train loss 2.120278 on epoch=184
03/10/2022 20:23:07 - INFO - __main__ - Step 380 Global step 380 Train loss 1.981319 on epoch=189
03/10/2022 20:23:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.904577 on epoch=194
03/10/2022 20:23:17 - INFO - __main__ - Step 400 Global step 400 Train loss 1.915955 on epoch=199
03/10/2022 20:23:17 - INFO - __main__ - Global step 400 Train loss 2.053725 EM 0.03125 on epoch=199
03/10/2022 20:23:22 - INFO - __main__ - Step 410 Global step 410 Train loss 1.877183 on epoch=204
03/10/2022 20:23:27 - INFO - __main__ - Step 420 Global step 420 Train loss 1.772429 on epoch=209
03/10/2022 20:23:32 - INFO - __main__ - Step 430 Global step 430 Train loss 1.657562 on epoch=214
03/10/2022 20:23:37 - INFO - __main__ - Step 440 Global step 440 Train loss 2.038433 on epoch=219
03/10/2022 20:23:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.949430 on epoch=224
03/10/2022 20:23:42 - INFO - __main__ - Global step 450 Train loss 1.859007 EM 0.03125 on epoch=224
03/10/2022 20:23:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.718562 on epoch=229
03/10/2022 20:23:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.887143 on epoch=234
03/10/2022 20:23:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.896161 on epoch=239
03/10/2022 20:24:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.590579 on epoch=244
03/10/2022 20:24:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.668683 on epoch=249
03/10/2022 20:24:07 - INFO - __main__ - Global step 500 Train loss 1.752226 EM 0.03125 on epoch=249
03/10/2022 20:24:11 - INFO - __main__ - Step 510 Global step 510 Train loss 1.621604 on epoch=254
03/10/2022 20:24:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.651054 on epoch=259
03/10/2022 20:24:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.569062 on epoch=264
03/10/2022 20:24:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.539647 on epoch=269
03/10/2022 20:24:30 - INFO - __main__ - Step 550 Global step 550 Train loss 1.548244 on epoch=274
03/10/2022 20:24:31 - INFO - __main__ - Global step 550 Train loss 1.585922 EM 0.03125 on epoch=274
03/10/2022 20:24:36 - INFO - __main__ - Step 560 Global step 560 Train loss 1.697539 on epoch=279
03/10/2022 20:24:40 - INFO - __main__ - Step 570 Global step 570 Train loss 1.488209 on epoch=284
03/10/2022 20:24:45 - INFO - __main__ - Step 580 Global step 580 Train loss 1.307869 on epoch=289
03/10/2022 20:24:50 - INFO - __main__ - Step 590 Global step 590 Train loss 1.493213 on epoch=294
03/10/2022 20:24:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.470385 on epoch=299
03/10/2022 20:24:55 - INFO - __main__ - Global step 600 Train loss 1.491443 EM 0.0 on epoch=299
03/10/2022 20:24:55 - INFO - __main__ - save last model!
03/10/2022 20:24:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:24:56 - INFO - __main__ - Printing 3 examples
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:24:56 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:24:56 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:24:56 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:24:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:24:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:24:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:24:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:24:56 - INFO - __main__ - Printing 3 examples
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:24:56 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:24:56 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:24:56 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:24:56 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:24:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:24:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:24:56 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:25:02 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:25:03 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 20:25:03 - INFO - __main__ - Printing 3 examples
03/10/2022 20:25:03 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 20:25:03 - INFO - __main__ - ['Whole S M']
03/10/2022 20:25:03 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 20:25:03 - INFO - __main__ - ['pushin Daisies']
03/10/2022 20:25:03 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 20:25:03 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 20:25:03 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:25:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:25:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:25:05 - INFO - __main__ - Starting training!
03/10/2022 20:25:05 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 20:25:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0002_8_predictions.txt
03/10/2022 20:25:52 - INFO - __main__ - EM on test data: 0.0466
03/10/2022 20:25:53 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.04659498207885305
03/10/2022 20:25:53 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0001, bsz=8 ...
03/10/2022 20:25:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:25:54 - INFO - __main__ - Printing 3 examples
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/10/2022 20:25:54 - INFO - __main__ - ['The Fun house Seattle']
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] jimfargiano
03/10/2022 20:25:54 - INFO - __main__ - ['Jim fargiano']
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/10/2022 20:25:54 - INFO - __main__ - ['Inspired Word N Y C']
03/10/2022 20:25:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:25:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:25:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:25:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:25:54 - INFO - __main__ - Printing 3 examples
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/10/2022 20:25:54 - INFO - __main__ - ['Find Local Electric']
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/10/2022 20:25:54 - INFO - __main__ - ['Dilena Brothers']
03/10/2022 20:25:54 - INFO - __main__ -  [crawl_domain] redboypizza
03/10/2022 20:25:54 - INFO - __main__ - ['Red Boy Pizza']
03/10/2022 20:25:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:25:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:25:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:26:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:26:03 - INFO - __main__ - Starting training!
03/10/2022 20:26:07 - INFO - __main__ - Step 10 Global step 10 Train loss 22.520102 on epoch=4
03/10/2022 20:26:12 - INFO - __main__ - Step 20 Global step 20 Train loss 19.896761 on epoch=9
03/10/2022 20:26:16 - INFO - __main__ - Step 30 Global step 30 Train loss 18.185671 on epoch=14
03/10/2022 20:26:21 - INFO - __main__ - Step 40 Global step 40 Train loss 16.612915 on epoch=19
03/10/2022 20:26:26 - INFO - __main__ - Step 50 Global step 50 Train loss 16.769011 on epoch=24
03/10/2022 20:26:35 - INFO - __main__ - Global step 50 Train loss 18.796892 EM 0.0 on epoch=24
03/10/2022 20:26:40 - INFO - __main__ - Step 60 Global step 60 Train loss 15.120436 on epoch=29
03/10/2022 20:26:45 - INFO - __main__ - Step 70 Global step 70 Train loss 13.858754 on epoch=34
03/10/2022 20:26:50 - INFO - __main__ - Step 80 Global step 80 Train loss 13.124771 on epoch=39
03/10/2022 20:26:55 - INFO - __main__ - Step 90 Global step 90 Train loss 12.307360 on epoch=44
03/10/2022 20:26:59 - INFO - __main__ - Step 100 Global step 100 Train loss 11.270405 on epoch=49
03/10/2022 20:27:07 - INFO - __main__ - Global step 100 Train loss 13.136344 EM 0.0 on epoch=49
03/10/2022 20:27:12 - INFO - __main__ - Step 110 Global step 110 Train loss 11.090390 on epoch=54
03/10/2022 20:27:17 - INFO - __main__ - Step 120 Global step 120 Train loss 10.631172 on epoch=59
03/10/2022 20:27:21 - INFO - __main__ - Step 130 Global step 130 Train loss 10.486858 on epoch=64
03/10/2022 20:27:26 - INFO - __main__ - Step 140 Global step 140 Train loss 9.430625 on epoch=69
03/10/2022 20:27:31 - INFO - __main__ - Step 150 Global step 150 Train loss 9.186206 on epoch=74
03/10/2022 20:27:35 - INFO - __main__ - Global step 150 Train loss 10.165051 EM 0.03125 on epoch=74
03/10/2022 20:27:40 - INFO - __main__ - Step 160 Global step 160 Train loss 8.963963 on epoch=79
03/10/2022 20:27:45 - INFO - __main__ - Step 170 Global step 170 Train loss 8.772748 on epoch=84
03/10/2022 20:27:50 - INFO - __main__ - Step 180 Global step 180 Train loss 8.103705 on epoch=89
03/10/2022 20:27:54 - INFO - __main__ - Step 190 Global step 190 Train loss 7.855959 on epoch=94
03/10/2022 20:27:59 - INFO - __main__ - Step 200 Global step 200 Train loss 7.394065 on epoch=99
03/10/2022 20:28:01 - INFO - __main__ - Global step 200 Train loss 8.218088 EM 0.03125 on epoch=99
03/10/2022 20:28:05 - INFO - __main__ - Step 210 Global step 210 Train loss 7.367898 on epoch=104
03/10/2022 20:28:10 - INFO - __main__ - Step 220 Global step 220 Train loss 7.086570 on epoch=109
03/10/2022 20:28:15 - INFO - __main__ - Step 230 Global step 230 Train loss 6.779267 on epoch=114
03/10/2022 20:28:20 - INFO - __main__ - Step 240 Global step 240 Train loss 6.530344 on epoch=119
03/10/2022 20:28:24 - INFO - __main__ - Step 250 Global step 250 Train loss 6.336271 on epoch=124
03/10/2022 20:28:25 - INFO - __main__ - Global step 250 Train loss 6.820070 EM 0.03125 on epoch=124
03/10/2022 20:28:30 - INFO - __main__ - Step 260 Global step 260 Train loss 6.309571 on epoch=129
03/10/2022 20:28:35 - INFO - __main__ - Step 270 Global step 270 Train loss 6.344987 on epoch=134
03/10/2022 20:28:40 - INFO - __main__ - Step 280 Global step 280 Train loss 6.071633 on epoch=139
03/10/2022 20:28:44 - INFO - __main__ - Step 290 Global step 290 Train loss 5.658174 on epoch=144
03/10/2022 20:28:49 - INFO - __main__ - Step 300 Global step 300 Train loss 5.988915 on epoch=149
03/10/2022 20:28:50 - INFO - __main__ - Global step 300 Train loss 6.074656 EM 0.03125 on epoch=149
03/10/2022 20:28:55 - INFO - __main__ - Step 310 Global step 310 Train loss 5.443411 on epoch=154
03/10/2022 20:28:59 - INFO - __main__ - Step 320 Global step 320 Train loss 5.522728 on epoch=159
03/10/2022 20:29:04 - INFO - __main__ - Step 330 Global step 330 Train loss 5.614560 on epoch=164
03/10/2022 20:29:09 - INFO - __main__ - Step 340 Global step 340 Train loss 5.133795 on epoch=169
03/10/2022 20:29:14 - INFO - __main__ - Step 350 Global step 350 Train loss 5.708201 on epoch=174
03/10/2022 20:29:14 - INFO - __main__ - Global step 350 Train loss 5.484539 EM 0.03125 on epoch=174
03/10/2022 20:29:19 - INFO - __main__ - Step 360 Global step 360 Train loss 5.263837 on epoch=179
03/10/2022 20:29:24 - INFO - __main__ - Step 370 Global step 370 Train loss 4.960139 on epoch=184
03/10/2022 20:29:29 - INFO - __main__ - Step 380 Global step 380 Train loss 5.276629 on epoch=189
03/10/2022 20:29:34 - INFO - __main__ - Step 390 Global step 390 Train loss 5.091897 on epoch=194
03/10/2022 20:29:39 - INFO - __main__ - Step 400 Global step 400 Train loss 4.658868 on epoch=199
03/10/2022 20:29:39 - INFO - __main__ - Global step 400 Train loss 5.050274 EM 0.03125 on epoch=199
03/10/2022 20:29:44 - INFO - __main__ - Step 410 Global step 410 Train loss 4.531845 on epoch=204
03/10/2022 20:29:49 - INFO - __main__ - Step 420 Global step 420 Train loss 4.361402 on epoch=209
03/10/2022 20:29:54 - INFO - __main__ - Step 430 Global step 430 Train loss 4.154398 on epoch=214
03/10/2022 20:29:59 - INFO - __main__ - Step 440 Global step 440 Train loss 3.490924 on epoch=219
03/10/2022 20:30:04 - INFO - __main__ - Step 450 Global step 450 Train loss 3.706483 on epoch=224
03/10/2022 20:30:04 - INFO - __main__ - Global step 450 Train loss 4.049010 EM 0.03125 on epoch=224
03/10/2022 20:30:09 - INFO - __main__ - Step 460 Global step 460 Train loss 3.368670 on epoch=229
03/10/2022 20:30:14 - INFO - __main__ - Step 470 Global step 470 Train loss 3.633094 on epoch=234
03/10/2022 20:30:19 - INFO - __main__ - Step 480 Global step 480 Train loss 3.327130 on epoch=239
03/10/2022 20:30:24 - INFO - __main__ - Step 490 Global step 490 Train loss 3.350820 on epoch=244
03/10/2022 20:30:29 - INFO - __main__ - Step 500 Global step 500 Train loss 3.384992 on epoch=249
03/10/2022 20:30:29 - INFO - __main__ - Global step 500 Train loss 3.412941 EM 0.03125 on epoch=249
03/10/2022 20:30:34 - INFO - __main__ - Step 510 Global step 510 Train loss 2.747145 on epoch=254
03/10/2022 20:30:39 - INFO - __main__ - Step 520 Global step 520 Train loss 3.093906 on epoch=259
03/10/2022 20:30:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.899033 on epoch=264
03/10/2022 20:30:49 - INFO - __main__ - Step 540 Global step 540 Train loss 2.692406 on epoch=269
03/10/2022 20:30:54 - INFO - __main__ - Step 550 Global step 550 Train loss 2.447490 on epoch=274
03/10/2022 20:30:54 - INFO - __main__ - Global step 550 Train loss 2.775996 EM 0.03125 on epoch=274
03/10/2022 20:30:59 - INFO - __main__ - Step 560 Global step 560 Train loss 2.362051 on epoch=279
03/10/2022 20:31:04 - INFO - __main__ - Step 570 Global step 570 Train loss 2.366635 on epoch=284
03/10/2022 20:31:09 - INFO - __main__ - Step 580 Global step 580 Train loss 2.743466 on epoch=289
03/10/2022 20:31:13 - INFO - __main__ - Step 590 Global step 590 Train loss 2.585457 on epoch=294
03/10/2022 20:31:18 - INFO - __main__ - Step 600 Global step 600 Train loss 2.743617 on epoch=299
03/10/2022 20:31:19 - INFO - __main__ - Global step 600 Train loss 2.560245 EM 0.0 on epoch=299
03/10/2022 20:31:19 - INFO - __main__ - save last model!
03/10/2022 20:31:26 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:31:26 - INFO - __main__ - Start tokenizing ... 1953 instances
03/10/2022 20:31:26 - INFO - __main__ - Printing 3 examples
03/10/2022 20:31:26 - INFO - __main__ -  [crawl_domain] wholesm
03/10/2022 20:31:26 - INFO - __main__ - ['Whole S M']
03/10/2022 20:31:26 - INFO - __main__ -  [crawl_domain] pushindaisies
03/10/2022 20:31:26 - INFO - __main__ - ['pushin Daisies']
03/10/2022 20:31:26 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/10/2022 20:31:26 - INFO - __main__ - ['Bradford Loomis']
03/10/2022 20:31:26 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:31:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:31:29 - INFO - __main__ - Loaded 1953 examples from test data
03/10/2022 20:33:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0001_8_predictions.txt
03/10/2022 20:33:13 - INFO - __main__ - EM on test data: 0.0502
03/10/2022 20:33:14 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0001, bsz=8, dev_performance=0.03125, test_performance=0.05017921146953405
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003612041473388672 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15547", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 11016, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15548", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 11016, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 11016, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15571): No such process
Task: amazon_polarity, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_07n4j0xu/none_hbe4bste
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_07n4j0xu/none_hbe4bste/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_07n4j0xu/none_hbe4bste/attempt_0/1/error.json
03/10/2022 20:33:19 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 20:33:19 - INFO - __main__ - models/T5-large-ft-random/singletask-amazon_polarity
Output directory () already exists and is not empty.
03/10/2022 20:33:19 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 20:33:19 - INFO - __main__ - models/T5-large-ft-random/singletask-amazon_polarity
03/10/2022 20:33:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 20:33:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 20:33:22 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 20:33:22 - INFO - __main__ - args.device: cuda:0
03/10/2022 20:33:22 - INFO - __main__ - Using 2 gpus
03/10/2022 20:33:22 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 20:33:22 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 20:33:22 - INFO - __main__ - args.device: cuda:1
03/10/2022 20:33:22 - INFO - __main__ - Using 2 gpus
03/10/2022 20:33:22 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 20:33:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0005, bsz=8 ...
03/10/2022 20:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:33:28 - INFO - __main__ - Printing 3 examples
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:33:28 - INFO - __main__ - Printing 3 examples
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:33:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:33:28 - INFO - __main__ - Printing 3 examples
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:33:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:33:28 - INFO - __main__ - Printing 3 examples
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:33:28 - INFO - __main__ - ['positive']
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:33:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:33:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:33:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:33:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:33:39 - INFO - __main__ - Starting training!
03/10/2022 20:33:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:33:40 - INFO - __main__ - Starting training!
03/10/2022 20:33:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.088821 on epoch=4
03/10/2022 20:33:49 - INFO - __main__ - Step 20 Global step 20 Train loss 16.699091 on epoch=9
03/10/2022 20:33:53 - INFO - __main__ - Step 30 Global step 30 Train loss 14.897072 on epoch=14
03/10/2022 20:33:57 - INFO - __main__ - Step 40 Global step 40 Train loss 13.574679 on epoch=19
03/10/2022 20:34:02 - INFO - __main__ - Step 50 Global step 50 Train loss 12.176883 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 20:34:05 - INFO - __main__ - Global step 50 Train loss 16.087309 Classification-F1 0.0 on epoch=24
03/10/2022 20:34:11 - INFO - __main__ - Step 60 Global step 60 Train loss 9.118742 on epoch=29
03/10/2022 20:34:15 - INFO - __main__ - Step 70 Global step 70 Train loss 2.583767 on epoch=34
03/10/2022 20:34:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.432294 on epoch=39
03/10/2022 20:34:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.496052 on epoch=44
03/10/2022 20:34:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.331646 on epoch=49
03/10/2022 20:34:30 - INFO - __main__ - Global step 100 Train loss 2.792500 Classification-F1 0.6536796536796536 on epoch=49
03/10/2022 20:34:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.229247 on epoch=54
03/10/2022 20:34:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.212397 on epoch=59
03/10/2022 20:34:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.567583 on epoch=64
03/10/2022 20:34:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.028654 on epoch=69
03/10/2022 20:34:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.039273 on epoch=74
03/10/2022 20:34:56 - INFO - __main__ - Global step 150 Train loss 0.215431 Classification-F1 0.9375 on epoch=74
03/10/2022 20:35:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005189 on epoch=79
03/10/2022 20:35:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.006823 on epoch=84
03/10/2022 20:35:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.001738 on epoch=89
03/10/2022 20:35:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.043085 on epoch=94
03/10/2022 20:35:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001441 on epoch=99
03/10/2022 20:35:21 - INFO - __main__ - Global step 200 Train loss 0.011655 Classification-F1 0.9687194525904204 on epoch=99
03/10/2022 20:35:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001764 on epoch=104
03/10/2022 20:35:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004115 on epoch=109
03/10/2022 20:35:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003064 on epoch=114
03/10/2022 20:35:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001775 on epoch=119
03/10/2022 20:35:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001234 on epoch=124
03/10/2022 20:35:46 - INFO - __main__ - Global step 250 Train loss 0.002391 Classification-F1 0.906158357771261 on epoch=124
03/10/2022 20:35:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000722 on epoch=129
03/10/2022 20:35:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000513 on epoch=134
03/10/2022 20:36:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.028706 on epoch=139
03/10/2022 20:36:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002052 on epoch=144
03/10/2022 20:36:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000281 on epoch=149
03/10/2022 20:36:09 - INFO - __main__ - Global step 300 Train loss 0.006455 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 20:36:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000246 on epoch=154
03/10/2022 20:36:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000174 on epoch=159
03/10/2022 20:36:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001361 on epoch=164
03/10/2022 20:36:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000143 on epoch=169
03/10/2022 20:36:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000208 on epoch=174
03/10/2022 20:36:34 - INFO - __main__ - Global step 350 Train loss 0.000426 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 20:36:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000092 on epoch=179
03/10/2022 20:36:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000067 on epoch=184
03/10/2022 20:36:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002285 on epoch=189
03/10/2022 20:36:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000140 on epoch=194
03/10/2022 20:36:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000085 on epoch=199
03/10/2022 20:36:58 - INFO - __main__ - Global step 400 Train loss 0.000534 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 20:37:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000089 on epoch=204
03/10/2022 20:37:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000236 on epoch=209
03/10/2022 20:37:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000027 on epoch=214
03/10/2022 20:37:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000026 on epoch=219
03/10/2022 20:37:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000069 on epoch=224
03/10/2022 20:37:22 - INFO - __main__ - Global step 450 Train loss 0.000090 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 20:37:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001198 on epoch=229
03/10/2022 20:37:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000148 on epoch=234
03/10/2022 20:37:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000019 on epoch=239
03/10/2022 20:37:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000208 on epoch=244
03/10/2022 20:37:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000023 on epoch=249
03/10/2022 20:37:47 - INFO - __main__ - Global step 500 Train loss 0.000319 Classification-F1 0.906158357771261 on epoch=249
03/10/2022 20:37:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000008 on epoch=254
03/10/2022 20:37:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000013 on epoch=259
03/10/2022 20:38:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000017 on epoch=264
03/10/2022 20:38:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000012 on epoch=269
03/10/2022 20:38:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000014 on epoch=274
03/10/2022 20:38:11 - INFO - __main__ - Global step 550 Train loss 0.000013 Classification-F1 0.906158357771261 on epoch=274
03/10/2022 20:38:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.043785 on epoch=279
03/10/2022 20:38:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000197 on epoch=284
03/10/2022 20:38:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000166 on epoch=289
03/10/2022 20:38:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000080 on epoch=294
03/10/2022 20:38:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000094 on epoch=299
03/10/2022 20:38:36 - INFO - __main__ - Global step 600 Train loss 0.008864 Classification-F1 0.9372549019607843 on epoch=299
03/10/2022 20:38:36 - INFO - __main__ - save last model!
03/10/2022 20:38:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:38:37 - INFO - __main__ - Printing 3 examples
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:38:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:38:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:38:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:38:37 - INFO - __main__ - Printing 3 examples
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:38:37 - INFO - __main__ - ['positive']
03/10/2022 20:38:37 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:38:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:38:37 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:38:43 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:38:43 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 20:38:43 - INFO - __main__ - Printing 3 examples
03/10/2022 20:38:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 20:38:43 - INFO - __main__ - ['negative']
03/10/2022 20:38:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 20:38:43 - INFO - __main__ - ['negative']
03/10/2022 20:38:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 20:38:43 - INFO - __main__ - ['negative']
03/10/2022 20:38:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:38:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:38:45 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 20:38:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:38:47 - INFO - __main__ - Starting training!
03/10/2022 20:38:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0005_8_predictions.txt
03/10/2022 20:38:59 - INFO - __main__ - Classification-F1 on test data: 0.9089
03/10/2022 20:39:00 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9089234045832545
03/10/2022 20:39:00 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0003, bsz=8 ...
03/10/2022 20:39:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:39:01 - INFO - __main__ - Printing 3 examples
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:39:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:39:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:39:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:39:01 - INFO - __main__ - Printing 3 examples
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:39:01 - INFO - __main__ - ['positive']
03/10/2022 20:39:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:39:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:39:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:39:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:39:10 - INFO - __main__ - Starting training!
03/10/2022 20:39:14 - INFO - __main__ - Step 10 Global step 10 Train loss 22.450546 on epoch=4
03/10/2022 20:39:19 - INFO - __main__ - Step 20 Global step 20 Train loss 17.266712 on epoch=9
03/10/2022 20:39:24 - INFO - __main__ - Step 30 Global step 30 Train loss 16.416533 on epoch=14
03/10/2022 20:39:29 - INFO - __main__ - Step 40 Global step 40 Train loss 14.255020 on epoch=19
03/10/2022 20:39:34 - INFO - __main__ - Step 50 Global step 50 Train loss 13.355293 on epoch=24
03/10/2022 20:39:44 - INFO - __main__ - Global step 50 Train loss 16.748821 Classification-F1 0.0 on epoch=24
03/10/2022 20:39:49 - INFO - __main__ - Step 60 Global step 60 Train loss 12.151666 on epoch=29
03/10/2022 20:39:54 - INFO - __main__ - Step 70 Global step 70 Train loss 11.541567 on epoch=34
03/10/2022 20:39:59 - INFO - __main__ - Step 80 Global step 80 Train loss 9.853167 on epoch=39
03/10/2022 20:40:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.807027 on epoch=44
03/10/2022 20:40:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.503125 on epoch=49
03/10/2022 20:40:09 - INFO - __main__ - Global step 100 Train loss 7.771310 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 20:40:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.056018 on epoch=54
03/10/2022 20:40:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.696585 on epoch=59
03/10/2022 20:40:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.878008 on epoch=64
03/10/2022 20:40:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.298288 on epoch=69
03/10/2022 20:40:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.275002 on epoch=74
03/10/2022 20:40:36 - INFO - __main__ - Global step 150 Train loss 0.640780 Classification-F1 0.36374269005847953 on epoch=74
03/10/2022 20:40:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.339806 on epoch=79
03/10/2022 20:40:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.311447 on epoch=84
03/10/2022 20:40:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.297865 on epoch=89
03/10/2022 20:40:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.322350 on epoch=94
03/10/2022 20:41:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.259252 on epoch=99
03/10/2022 20:41:02 - INFO - __main__ - Global step 200 Train loss 0.306144 Classification-F1 0.4920634920634921 on epoch=99
03/10/2022 20:41:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.320352 on epoch=104
03/10/2022 20:41:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.493819 on epoch=109
03/10/2022 20:41:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.655245 on epoch=114
03/10/2022 20:41:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.335450 on epoch=119
03/10/2022 20:41:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.347648 on epoch=124
03/10/2022 20:41:29 - INFO - __main__ - Global step 250 Train loss 0.430503 Classification-F1 0.5134502923976608 on epoch=124
03/10/2022 20:41:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.331173 on epoch=129
03/10/2022 20:41:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.301956 on epoch=134
03/10/2022 20:41:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.399953 on epoch=139
03/10/2022 20:41:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.327192 on epoch=144
03/10/2022 20:41:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.349462 on epoch=149
03/10/2022 20:41:55 - INFO - __main__ - Global step 300 Train loss 0.341947 Classification-F1 0.4181818181818182 on epoch=149
03/10/2022 20:42:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.277174 on epoch=154
03/10/2022 20:42:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.341923 on epoch=159
03/10/2022 20:42:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.342109 on epoch=164
03/10/2022 20:42:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.340683 on epoch=169
03/10/2022 20:42:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.348636 on epoch=174
03/10/2022 20:42:20 - INFO - __main__ - Global step 350 Train loss 0.330105 Classification-F1 0.3992490613266583 on epoch=174
03/10/2022 20:42:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.313679 on epoch=179
03/10/2022 20:42:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.316938 on epoch=184
03/10/2022 20:42:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.333955 on epoch=189
03/10/2022 20:42:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.325551 on epoch=194
03/10/2022 20:42:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.319121 on epoch=199
03/10/2022 20:42:45 - INFO - __main__ - Global step 400 Train loss 0.321849 Classification-F1 0.5333333333333333 on epoch=199
03/10/2022 20:42:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.310184 on epoch=204
03/10/2022 20:42:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.286815 on epoch=209
03/10/2022 20:43:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.255326 on epoch=214
03/10/2022 20:43:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.293251 on epoch=219
03/10/2022 20:43:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.310589 on epoch=224
03/10/2022 20:43:11 - INFO - __main__ - Global step 450 Train loss 0.291233 Classification-F1 0.37254901960784315 on epoch=224
03/10/2022 20:43:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.290296 on epoch=229
03/10/2022 20:43:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.273337 on epoch=234
03/10/2022 20:43:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.249093 on epoch=239
03/10/2022 20:43:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.269030 on epoch=244
03/10/2022 20:43:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.284494 on epoch=249
03/10/2022 20:43:36 - INFO - __main__ - Global step 500 Train loss 0.273250 Classification-F1 0.5835835835835835 on epoch=249
03/10/2022 20:43:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.268593 on epoch=254
03/10/2022 20:43:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.317596 on epoch=259
03/10/2022 20:43:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.280769 on epoch=264
03/10/2022 20:43:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.276138 on epoch=269
03/10/2022 20:44:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.252422 on epoch=274
03/10/2022 20:44:02 - INFO - __main__ - Global step 550 Train loss 0.279103 Classification-F1 0.6532019704433498 on epoch=274
03/10/2022 20:44:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.177254 on epoch=279
03/10/2022 20:44:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.196359 on epoch=284
03/10/2022 20:44:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.264927 on epoch=289
03/10/2022 20:44:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.223306 on epoch=294
03/10/2022 20:44:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.205151 on epoch=299
03/10/2022 20:44:29 - INFO - __main__ - Global step 600 Train loss 0.213400 Classification-F1 0.6825396825396826 on epoch=299
03/10/2022 20:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:44:29 - INFO - __main__ - Printing 3 examples
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:44:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:44:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:44:29 - INFO - __main__ - Printing 3 examples
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:44:29 - INFO - __main__ - ['positive']
03/10/2022 20:44:29 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:44:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:44:29 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:44:30 - INFO - __main__ - save last model!
03/10/2022 20:44:36 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:44:37 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 20:44:37 - INFO - __main__ - Printing 3 examples
03/10/2022 20:44:37 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 20:44:37 - INFO - __main__ - ['negative']
03/10/2022 20:44:37 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 20:44:37 - INFO - __main__ - ['negative']
03/10/2022 20:44:37 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 20:44:37 - INFO - __main__ - ['negative']
03/10/2022 20:44:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:44:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:44:38 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 20:44:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:44:39 - INFO - __main__ - Starting training!
03/10/2022 20:44:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0003_8_predictions.txt
03/10/2022 20:44:53 - INFO - __main__ - Classification-F1 on test data: 0.6449
03/10/2022 20:44:53 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.6825396825396826, test_performance=0.6448945799646102
03/10/2022 20:44:53 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0002, bsz=8 ...
03/10/2022 20:44:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:44:54 - INFO - __main__ - Printing 3 examples
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:44:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:44:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:44:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:44:54 - INFO - __main__ - Printing 3 examples
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:44:54 - INFO - __main__ - ['positive']
03/10/2022 20:44:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:44:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:44:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:45:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:45:03 - INFO - __main__ - Starting training!
03/10/2022 20:45:07 - INFO - __main__ - Step 10 Global step 10 Train loss 22.182211 on epoch=4
03/10/2022 20:45:12 - INFO - __main__ - Step 20 Global step 20 Train loss 19.298891 on epoch=9
03/10/2022 20:45:17 - INFO - __main__ - Step 30 Global step 30 Train loss 16.924904 on epoch=14
03/10/2022 20:45:21 - INFO - __main__ - Step 40 Global step 40 Train loss 16.268661 on epoch=19
03/10/2022 20:45:26 - INFO - __main__ - Step 50 Global step 50 Train loss 14.967832 on epoch=24
03/10/2022 20:45:27 - INFO - __main__ - Global step 50 Train loss 17.928499 Classification-F1 0.0 on epoch=24
03/10/2022 20:45:32 - INFO - __main__ - Step 60 Global step 60 Train loss 14.611003 on epoch=29
03/10/2022 20:45:37 - INFO - __main__ - Step 70 Global step 70 Train loss 12.842792 on epoch=34
03/10/2022 20:45:42 - INFO - __main__ - Step 80 Global step 80 Train loss 12.892672 on epoch=39
03/10/2022 20:45:47 - INFO - __main__ - Step 90 Global step 90 Train loss 11.882254 on epoch=44
03/10/2022 20:45:51 - INFO - __main__ - Step 100 Global step 100 Train loss 11.061594 on epoch=49
03/10/2022 20:45:52 - INFO - __main__ - Global step 100 Train loss 12.658064 Classification-F1 0.0 on epoch=49
03/10/2022 20:45:57 - INFO - __main__ - Step 110 Global step 110 Train loss 9.695046 on epoch=54
03/10/2022 20:46:01 - INFO - __main__ - Step 120 Global step 120 Train loss 7.658845 on epoch=59
03/10/2022 20:46:06 - INFO - __main__ - Step 130 Global step 130 Train loss 4.888590 on epoch=64
03/10/2022 20:46:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.477153 on epoch=69
03/10/2022 20:46:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.232917 on epoch=74
03/10/2022 20:46:16 - INFO - __main__ - Global step 150 Train loss 4.590510 Classification-F1 0.9375 on epoch=74
03/10/2022 20:46:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.436172 on epoch=79
03/10/2022 20:46:25 - INFO - __main__ - Step 170 Global step 170 Train loss 2.542928 on epoch=84
03/10/2022 20:46:30 - INFO - __main__ - Step 180 Global step 180 Train loss 1.861578 on epoch=89
03/10/2022 20:46:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.112195 on epoch=94
03/10/2022 20:46:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.611054 on epoch=99
03/10/2022 20:46:40 - INFO - __main__ - Global step 200 Train loss 1.512785 Classification-F1 0.3992490613266583 on epoch=99
03/10/2022 20:46:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.474309 on epoch=104
03/10/2022 20:46:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.524734 on epoch=109
03/10/2022 20:46:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.423920 on epoch=114
03/10/2022 20:46:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.343409 on epoch=119
03/10/2022 20:47:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.449154 on epoch=124
03/10/2022 20:47:04 - INFO - __main__ - Global step 250 Train loss 0.443105 Classification-F1 0.4682306940371457 on epoch=124
03/10/2022 20:47:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.400022 on epoch=129
03/10/2022 20:47:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.432462 on epoch=134
03/10/2022 20:47:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.367010 on epoch=139
03/10/2022 20:47:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.405357 on epoch=144
03/10/2022 20:47:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.345616 on epoch=149
03/10/2022 20:47:28 - INFO - __main__ - Global step 300 Train loss 0.390093 Classification-F1 0.5151515151515151 on epoch=149
03/10/2022 20:47:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.338864 on epoch=154
03/10/2022 20:47:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.357364 on epoch=159
03/10/2022 20:47:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.334703 on epoch=164
03/10/2022 20:47:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360634 on epoch=169
03/10/2022 20:47:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.379042 on epoch=174
03/10/2022 20:47:52 - INFO - __main__ - Global step 350 Train loss 0.354121 Classification-F1 0.4385964912280702 on epoch=174
03/10/2022 20:47:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.323665 on epoch=179
03/10/2022 20:48:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331689 on epoch=184
03/10/2022 20:48:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.313764 on epoch=189
03/10/2022 20:48:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.296254 on epoch=194
03/10/2022 20:48:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.361726 on epoch=199
03/10/2022 20:48:17 - INFO - __main__ - Global step 400 Train loss 0.325420 Classification-F1 0.6862745098039216 on epoch=199
03/10/2022 20:48:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.357778 on epoch=204
03/10/2022 20:48:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.415190 on epoch=209
03/10/2022 20:48:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.349806 on epoch=214
03/10/2022 20:48:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.318413 on epoch=219
03/10/2022 20:48:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.346564 on epoch=224
03/10/2022 20:48:42 - INFO - __main__ - Global step 450 Train loss 0.357550 Classification-F1 0.4817813765182186 on epoch=224
03/10/2022 20:48:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.350521 on epoch=229
03/10/2022 20:48:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.340484 on epoch=234
03/10/2022 20:48:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.345979 on epoch=239
03/10/2022 20:49:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.332773 on epoch=244
03/10/2022 20:49:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.347063 on epoch=249
03/10/2022 20:49:08 - INFO - __main__ - Global step 500 Train loss 0.343364 Classification-F1 0.4589371980676329 on epoch=249
03/10/2022 20:49:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.317397 on epoch=254
03/10/2022 20:49:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.356903 on epoch=259
03/10/2022 20:49:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.329633 on epoch=264
03/10/2022 20:49:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.309777 on epoch=269
03/10/2022 20:49:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.298435 on epoch=274
03/10/2022 20:49:33 - INFO - __main__ - Global step 550 Train loss 0.322429 Classification-F1 0.5835835835835835 on epoch=274
03/10/2022 20:49:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.357353 on epoch=279
03/10/2022 20:49:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.329751 on epoch=284
03/10/2022 20:49:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.318213 on epoch=289
03/10/2022 20:49:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.300330 on epoch=294
03/10/2022 20:49:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.323384 on epoch=299
03/10/2022 20:49:58 - INFO - __main__ - Global step 600 Train loss 0.325806 Classification-F1 0.39756367663344405 on epoch=299
03/10/2022 20:49:58 - INFO - __main__ - save last model!
03/10/2022 20:49:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:49:58 - INFO - __main__ - Printing 3 examples
03/10/2022 20:49:58 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:49:58 - INFO - __main__ - ['positive']
03/10/2022 20:49:58 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:49:58 - INFO - __main__ - ['positive']
03/10/2022 20:49:58 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:49:58 - INFO - __main__ - ['positive']
03/10/2022 20:49:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:49:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:49:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:49:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:49:59 - INFO - __main__ - Printing 3 examples
03/10/2022 20:49:59 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:49:59 - INFO - __main__ - ['positive']
03/10/2022 20:49:59 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:49:59 - INFO - __main__ - ['positive']
03/10/2022 20:49:59 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:49:59 - INFO - __main__ - ['positive']
03/10/2022 20:49:59 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:49:59 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:49:59 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:50:05 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:50:05 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 20:50:05 - INFO - __main__ - Printing 3 examples
03/10/2022 20:50:05 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 20:50:05 - INFO - __main__ - ['negative']
03/10/2022 20:50:05 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 20:50:05 - INFO - __main__ - ['negative']
03/10/2022 20:50:05 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 20:50:05 - INFO - __main__ - ['negative']
03/10/2022 20:50:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:50:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:50:07 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 20:50:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:50:09 - INFO - __main__ - Starting training!
03/10/2022 20:50:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0002_8_predictions.txt
03/10/2022 20:50:20 - INFO - __main__ - Classification-F1 on test data: 0.9460
03/10/2022 20:50:22 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.9459991359861757
03/10/2022 20:50:22 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0001, bsz=8 ...
03/10/2022 20:50:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:50:23 - INFO - __main__ - Printing 3 examples
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:50:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:50:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:50:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:50:23 - INFO - __main__ - Printing 3 examples
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 20:50:23 - INFO - __main__ - ['positive']
03/10/2022 20:50:23 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:50:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:50:23 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:50:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:50:32 - INFO - __main__ - Starting training!
03/10/2022 20:50:36 - INFO - __main__ - Step 10 Global step 10 Train loss 23.350376 on epoch=4
03/10/2022 20:50:41 - INFO - __main__ - Step 20 Global step 20 Train loss 21.112301 on epoch=9
03/10/2022 20:50:46 - INFO - __main__ - Step 30 Global step 30 Train loss 18.029680 on epoch=14
03/10/2022 20:50:51 - INFO - __main__ - Step 40 Global step 40 Train loss 16.769276 on epoch=19
03/10/2022 20:50:56 - INFO - __main__ - Step 50 Global step 50 Train loss 17.164574 on epoch=24
03/10/2022 20:51:06 - INFO - __main__ - Global step 50 Train loss 19.285242 Classification-F1 0.0 on epoch=24
03/10/2022 20:51:12 - INFO - __main__ - Step 60 Global step 60 Train loss 15.913518 on epoch=29
03/10/2022 20:51:17 - INFO - __main__ - Step 70 Global step 70 Train loss 15.516785 on epoch=34
03/10/2022 20:51:22 - INFO - __main__ - Step 80 Global step 80 Train loss 15.510806 on epoch=39
03/10/2022 20:51:27 - INFO - __main__ - Step 90 Global step 90 Train loss 15.276628 on epoch=44
03/10/2022 20:51:32 - INFO - __main__ - Step 100 Global step 100 Train loss 14.036064 on epoch=49
03/10/2022 20:51:40 - INFO - __main__ - Global step 100 Train loss 15.250762 Classification-F1 0.0 on epoch=49
03/10/2022 20:51:45 - INFO - __main__ - Step 110 Global step 110 Train loss 13.422679 on epoch=54
03/10/2022 20:51:50 - INFO - __main__ - Step 120 Global step 120 Train loss 13.748500 on epoch=59
03/10/2022 20:51:55 - INFO - __main__ - Step 130 Global step 130 Train loss 13.225159 on epoch=64
03/10/2022 20:52:00 - INFO - __main__ - Step 140 Global step 140 Train loss 13.049443 on epoch=69
03/10/2022 20:52:06 - INFO - __main__ - Step 150 Global step 150 Train loss 12.540441 on epoch=74
03/10/2022 20:52:10 - INFO - __main__ - Global step 150 Train loss 13.197244 Classification-F1 0.0 on epoch=74
03/10/2022 20:52:15 - INFO - __main__ - Step 160 Global step 160 Train loss 12.538486 on epoch=79
03/10/2022 20:52:20 - INFO - __main__ - Step 170 Global step 170 Train loss 11.412712 on epoch=84
03/10/2022 20:52:25 - INFO - __main__ - Step 180 Global step 180 Train loss 10.855384 on epoch=89
03/10/2022 20:52:31 - INFO - __main__ - Step 190 Global step 190 Train loss 6.536744 on epoch=94
03/10/2022 20:52:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.761459 on epoch=99
03/10/2022 20:52:36 - INFO - __main__ - Global step 200 Train loss 8.420957 Classification-F1 0.7408906882591093 on epoch=99
03/10/2022 20:52:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.387333 on epoch=104
03/10/2022 20:52:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.762463 on epoch=109
03/10/2022 20:52:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.262734 on epoch=114
03/10/2022 20:52:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.494995 on epoch=119
03/10/2022 20:53:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.214649 on epoch=124
03/10/2022 20:53:03 - INFO - __main__ - Global step 250 Train loss 0.624435 Classification-F1 0.9375 on epoch=124
03/10/2022 20:53:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.292797 on epoch=129
03/10/2022 20:53:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.160868 on epoch=134
03/10/2022 20:53:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.123253 on epoch=139
03/10/2022 20:53:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.117365 on epoch=144
03/10/2022 20:53:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.065113 on epoch=149
03/10/2022 20:53:29 - INFO - __main__ - Global step 300 Train loss 0.151879 Classification-F1 0.9372549019607843 on epoch=149
03/10/2022 20:53:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.068167 on epoch=154
03/10/2022 20:53:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.048344 on epoch=159
03/10/2022 20:53:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.026037 on epoch=164
03/10/2022 20:53:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017085 on epoch=169
03/10/2022 20:53:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.015146 on epoch=174
03/10/2022 20:53:55 - INFO - __main__ - Global step 350 Train loss 0.034956 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 20:54:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.012050 on epoch=179
03/10/2022 20:54:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.013543 on epoch=184
03/10/2022 20:54:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025599 on epoch=189
03/10/2022 20:54:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.007921 on epoch=194
03/10/2022 20:54:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.027348 on epoch=199
03/10/2022 20:54:22 - INFO - __main__ - Global step 400 Train loss 0.017292 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 20:54:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001352 on epoch=204
03/10/2022 20:54:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001815 on epoch=209
03/10/2022 20:54:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001272 on epoch=214
03/10/2022 20:54:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002182 on epoch=219
03/10/2022 20:54:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000469 on epoch=224
03/10/2022 20:54:48 - INFO - __main__ - Global step 450 Train loss 0.001418 Classification-F1 1.0 on epoch=224
03/10/2022 20:54:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020404 on epoch=229
03/10/2022 20:54:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001926 on epoch=234
03/10/2022 20:55:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001378 on epoch=239
03/10/2022 20:55:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002929 on epoch=244
03/10/2022 20:55:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000334 on epoch=249
03/10/2022 20:55:14 - INFO - __main__ - Global step 500 Train loss 0.005394 Classification-F1 1.0 on epoch=249
03/10/2022 20:55:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000824 on epoch=254
03/10/2022 20:55:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003451 on epoch=259
03/10/2022 20:55:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001658 on epoch=264
03/10/2022 20:55:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000369 on epoch=269
03/10/2022 20:55:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000462 on epoch=274
03/10/2022 20:55:40 - INFO - __main__ - Global step 550 Train loss 0.001353 Classification-F1 1.0 on epoch=274
03/10/2022 20:55:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000860 on epoch=279
03/10/2022 20:55:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001606 on epoch=284
03/10/2022 20:55:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000912 on epoch=289
03/10/2022 20:56:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000324 on epoch=294
03/10/2022 20:56:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000197 on epoch=299
03/10/2022 20:56:06 - INFO - __main__ - Global step 600 Train loss 0.000780 Classification-F1 1.0 on epoch=299
03/10/2022 20:56:06 - INFO - __main__ - save last model!
03/10/2022 20:56:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:56:06 - INFO - __main__ - Printing 3 examples
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:56:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:56:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:56:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:56:06 - INFO - __main__ - Printing 3 examples
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 20:56:06 - INFO - __main__ - ['negative']
03/10/2022 20:56:06 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:56:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:56:06 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:56:13 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 20:56:13 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 20:56:13 - INFO - __main__ - Printing 3 examples
03/10/2022 20:56:13 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 20:56:13 - INFO - __main__ - ['negative']
03/10/2022 20:56:13 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 20:56:13 - INFO - __main__ - ['negative']
03/10/2022 20:56:13 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 20:56:13 - INFO - __main__ - ['negative']
03/10/2022 20:56:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:56:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:56:15 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 20:56:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:56:16 - INFO - __main__ - Starting training!
03/10/2022 20:56:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0001_8_predictions.txt
03/10/2022 20:56:39 - INFO - __main__ - Classification-F1 on test data: 0.4569
03/10/2022 20:56:40 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.4569136537097226
03/10/2022 20:56:40 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0005, bsz=8 ...
03/10/2022 20:56:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:56:41 - INFO - __main__ - Printing 3 examples
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:56:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:56:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:56:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:56:41 - INFO - __main__ - Printing 3 examples
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 20:56:41 - INFO - __main__ - ['negative']
03/10/2022 20:56:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:56:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:56:41 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:56:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 20:56:51 - INFO - __main__ - Starting training!
03/10/2022 20:56:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.678394 on epoch=4
03/10/2022 20:57:00 - INFO - __main__ - Step 20 Global step 20 Train loss 17.665001 on epoch=9
03/10/2022 20:57:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.538038 on epoch=14
03/10/2022 20:57:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.117441 on epoch=19
03/10/2022 20:57:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.824023 on epoch=24
03/10/2022 20:57:22 - INFO - __main__ - Global step 50 Train loss 16.764580 Classification-F1 0.0 on epoch=24
03/10/2022 20:57:27 - INFO - __main__ - Step 60 Global step 60 Train loss 7.677657 on epoch=29
03/10/2022 20:57:32 - INFO - __main__ - Step 70 Global step 70 Train loss 1.796773 on epoch=34
03/10/2022 20:57:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.398216 on epoch=39
03/10/2022 20:57:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.449718 on epoch=44
03/10/2022 20:57:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.423573 on epoch=49
03/10/2022 20:57:49 - INFO - __main__ - Global step 100 Train loss 2.349187 Classification-F1 0.0 on epoch=49
03/10/2022 20:57:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.380036 on epoch=54
03/10/2022 20:57:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.489807 on epoch=59
03/10/2022 20:58:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.894923 on epoch=64
03/10/2022 20:58:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.748891 on epoch=69
03/10/2022 20:58:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.463929 on epoch=74
03/10/2022 20:58:14 - INFO - __main__ - Global step 150 Train loss 0.795517 Classification-F1 0.6945917285259808 on epoch=74
03/10/2022 20:58:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.137712 on epoch=79
03/10/2022 20:58:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.146729 on epoch=84
03/10/2022 20:58:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.157339 on epoch=89
03/10/2022 20:58:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.240239 on epoch=94
03/10/2022 20:58:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.404249 on epoch=99
03/10/2022 20:58:39 - INFO - __main__ - Global step 200 Train loss 0.217254 Classification-F1 0.6267232237539766 on epoch=99
03/10/2022 20:58:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.361187 on epoch=104
03/10/2022 20:58:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.351794 on epoch=109
03/10/2022 20:58:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.358448 on epoch=114
03/10/2022 20:58:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.333869 on epoch=119
03/10/2022 20:59:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.376546 on epoch=124
03/10/2022 20:59:04 - INFO - __main__ - Global step 250 Train loss 0.356369 Classification-F1 0.539313399778516 on epoch=124
03/10/2022 20:59:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.355392 on epoch=129
03/10/2022 20:59:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.336506 on epoch=134
03/10/2022 20:59:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.378076 on epoch=139
03/10/2022 20:59:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.391015 on epoch=144
03/10/2022 20:59:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.375728 on epoch=149
03/10/2022 20:59:29 - INFO - __main__ - Global step 300 Train loss 0.367343 Classification-F1 0.3191489361702127 on epoch=149
03/10/2022 20:59:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.350861 on epoch=154
03/10/2022 20:59:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.345288 on epoch=159
03/10/2022 20:59:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.326039 on epoch=164
03/10/2022 20:59:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.345468 on epoch=169
03/10/2022 20:59:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.355583 on epoch=174
03/10/2022 20:59:54 - INFO - __main__ - Global step 350 Train loss 0.344648 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 20:59:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.344189 on epoch=179
03/10/2022 21:00:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.363120 on epoch=184
03/10/2022 21:00:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342415 on epoch=189
03/10/2022 21:00:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.353160 on epoch=194
03/10/2022 21:00:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.341650 on epoch=199
03/10/2022 21:00:20 - INFO - __main__ - Global step 400 Train loss 0.348907 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 21:00:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.352127 on epoch=204
03/10/2022 21:00:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.354942 on epoch=209
03/10/2022 21:00:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.379469 on epoch=214
03/10/2022 21:00:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.352136 on epoch=219
03/10/2022 21:00:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.332392 on epoch=224
03/10/2022 21:00:44 - INFO - __main__ - Global step 450 Train loss 0.354213 Classification-F1 0.5636363636363637 on epoch=224
03/10/2022 21:00:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.330716 on epoch=229
03/10/2022 21:00:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.351570 on epoch=234
03/10/2022 21:00:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.356971 on epoch=239
03/10/2022 21:01:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.340944 on epoch=244
03/10/2022 21:01:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.357734 on epoch=249
03/10/2022 21:01:09 - INFO - __main__ - Global step 500 Train loss 0.347587 Classification-F1 0.7810361681329424 on epoch=249
03/10/2022 21:01:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.367456 on epoch=254
03/10/2022 21:01:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.359235 on epoch=259
03/10/2022 21:01:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.336806 on epoch=264
03/10/2022 21:01:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.360546 on epoch=269
03/10/2022 21:01:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.353456 on epoch=274
03/10/2022 21:01:35 - INFO - __main__ - Global step 550 Train loss 0.355500 Classification-F1 0.3992490613266583 on epoch=274
03/10/2022 21:01:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.384417 on epoch=279
03/10/2022 21:01:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.329745 on epoch=284
03/10/2022 21:01:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.365269 on epoch=289
03/10/2022 21:01:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.346752 on epoch=294
03/10/2022 21:01:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.343598 on epoch=299
03/10/2022 21:01:59 - INFO - __main__ - Global step 600 Train loss 0.353956 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 21:01:59 - INFO - __main__ - save last model!
03/10/2022 21:02:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:02:00 - INFO - __main__ - Printing 3 examples
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:02:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:02:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:02:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:02:00 - INFO - __main__ - Printing 3 examples
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:02:00 - INFO - __main__ - ['negative']
03/10/2022 21:02:00 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:02:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:02:00 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:02:06 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:02:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:02:06 - INFO - __main__ - Printing 3 examples
03/10/2022 21:02:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:02:06 - INFO - __main__ - ['negative']
03/10/2022 21:02:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:02:06 - INFO - __main__ - ['negative']
03/10/2022 21:02:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:02:06 - INFO - __main__ - ['negative']
03/10/2022 21:02:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:02:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:02:08 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:02:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:02:10 - INFO - __main__ - Starting training!
03/10/2022 21:02:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0005_8_predictions.txt
03/10/2022 21:02:41 - INFO - __main__ - Classification-F1 on test data: 0.2739
03/10/2022 21:02:42 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.7810361681329424, test_performance=0.2739054105503617
03/10/2022 21:02:42 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0003, bsz=8 ...
03/10/2022 21:02:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:02:43 - INFO - __main__ - Printing 3 examples
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:02:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:02:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:02:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:02:43 - INFO - __main__ - Printing 3 examples
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:02:43 - INFO - __main__ - ['negative']
03/10/2022 21:02:43 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:02:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:02:43 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:02:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:02:52 - INFO - __main__ - Starting training!
03/10/2022 21:02:56 - INFO - __main__ - Step 10 Global step 10 Train loss 22.014746 on epoch=4
03/10/2022 21:03:01 - INFO - __main__ - Step 20 Global step 20 Train loss 18.429630 on epoch=9
03/10/2022 21:03:06 - INFO - __main__ - Step 30 Global step 30 Train loss 16.211752 on epoch=14
03/10/2022 21:03:11 - INFO - __main__ - Step 40 Global step 40 Train loss 14.851277 on epoch=19
03/10/2022 21:03:15 - INFO - __main__ - Step 50 Global step 50 Train loss 14.018236 on epoch=24
03/10/2022 21:03:25 - INFO - __main__ - Global step 50 Train loss 17.105129 Classification-F1 0.0 on epoch=24
03/10/2022 21:03:30 - INFO - __main__ - Step 60 Global step 60 Train loss 13.002149 on epoch=29
03/10/2022 21:03:35 - INFO - __main__ - Step 70 Global step 70 Train loss 12.342925 on epoch=34
03/10/2022 21:03:40 - INFO - __main__ - Step 80 Global step 80 Train loss 9.267306 on epoch=39
03/10/2022 21:03:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.649744 on epoch=44
03/10/2022 21:03:49 - INFO - __main__ - Step 100 Global step 100 Train loss 7.306134 on epoch=49
03/10/2022 21:03:50 - INFO - __main__ - Global step 100 Train loss 9.713651 Classification-F1 0.0 on epoch=49
03/10/2022 21:03:55 - INFO - __main__ - Step 110 Global step 110 Train loss 3.337065 on epoch=54
03/10/2022 21:04:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.460373 on epoch=59
03/10/2022 21:04:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.177398 on epoch=64
03/10/2022 21:04:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.132874 on epoch=69
03/10/2022 21:04:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.050293 on epoch=74
03/10/2022 21:04:15 - INFO - __main__ - Global step 150 Train loss 0.831601 Classification-F1 0.875 on epoch=74
03/10/2022 21:04:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.028848 on epoch=79
03/10/2022 21:04:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.010855 on epoch=84
03/10/2022 21:04:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.010520 on epoch=89
03/10/2022 21:04:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.012375 on epoch=94
03/10/2022 21:04:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.024895 on epoch=99
03/10/2022 21:04:41 - INFO - __main__ - Global step 200 Train loss 0.017499 Classification-F1 0.9054187192118226 on epoch=99
03/10/2022 21:04:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005826 on epoch=104
03/10/2022 21:04:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002635 on epoch=109
03/10/2022 21:04:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001569 on epoch=114
03/10/2022 21:05:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.004648 on epoch=119
03/10/2022 21:05:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.003260 on epoch=124
03/10/2022 21:05:06 - INFO - __main__ - Global step 250 Train loss 0.003587 Classification-F1 0.9054187192118226 on epoch=124
03/10/2022 21:05:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.022348 on epoch=129
03/10/2022 21:05:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002365 on epoch=134
03/10/2022 21:05:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003973 on epoch=139
03/10/2022 21:05:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.014832 on epoch=144
03/10/2022 21:05:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.007792 on epoch=149
03/10/2022 21:05:31 - INFO - __main__ - Global step 300 Train loss 0.010262 Classification-F1 0.8745098039215686 on epoch=149
03/10/2022 21:05:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.042268 on epoch=154
03/10/2022 21:05:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.047188 on epoch=159
03/10/2022 21:05:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000785 on epoch=164
03/10/2022 21:05:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001237 on epoch=169
03/10/2022 21:05:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002109 on epoch=174
03/10/2022 21:05:55 - INFO - __main__ - Global step 350 Train loss 0.018717 Classification-F1 0.9054187192118226 on epoch=174
03/10/2022 21:06:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000878 on epoch=179
03/10/2022 21:06:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001043 on epoch=184
03/10/2022 21:06:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000724 on epoch=189
03/10/2022 21:06:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000674 on epoch=194
03/10/2022 21:06:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001088 on epoch=199
03/10/2022 21:06:20 - INFO - __main__ - Global step 400 Train loss 0.000882 Classification-F1 0.9054187192118226 on epoch=199
03/10/2022 21:06:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000660 on epoch=204
03/10/2022 21:06:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000545 on epoch=209
03/10/2022 21:06:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.031545 on epoch=214
03/10/2022 21:06:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001008 on epoch=219
03/10/2022 21:06:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000806 on epoch=224
03/10/2022 21:06:46 - INFO - __main__ - Global step 450 Train loss 0.006913 Classification-F1 0.9372549019607843 on epoch=224
03/10/2022 21:06:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.009456 on epoch=229
03/10/2022 21:06:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001348 on epoch=234
03/10/2022 21:07:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012841 on epoch=239
03/10/2022 21:07:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000880 on epoch=244
03/10/2022 21:07:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003164 on epoch=249
03/10/2022 21:07:11 - INFO - __main__ - Global step 500 Train loss 0.005538 Classification-F1 0.9372549019607843 on epoch=249
03/10/2022 21:07:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000131 on epoch=254
03/10/2022 21:07:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002994 on epoch=259
03/10/2022 21:07:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000193 on epoch=264
03/10/2022 21:07:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001018 on epoch=269
03/10/2022 21:07:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004282 on epoch=274
03/10/2022 21:07:36 - INFO - __main__ - Global step 550 Train loss 0.001724 Classification-F1 0.9054187192118226 on epoch=274
03/10/2022 21:07:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000229 on epoch=279
03/10/2022 21:07:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000235 on epoch=284
03/10/2022 21:07:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000521 on epoch=289
03/10/2022 21:07:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000028 on epoch=294
03/10/2022 21:08:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000697 on epoch=299
03/10/2022 21:08:02 - INFO - __main__ - Global step 600 Train loss 0.000342 Classification-F1 0.9054187192118226 on epoch=299
03/10/2022 21:08:02 - INFO - __main__ - save last model!
03/10/2022 21:08:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:08:02 - INFO - __main__ - Printing 3 examples
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:08:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:08:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:08:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:08:02 - INFO - __main__ - Printing 3 examples
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:08:02 - INFO - __main__ - ['negative']
03/10/2022 21:08:02 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:08:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:08:02 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:08:08 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:08:09 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:08:09 - INFO - __main__ - Printing 3 examples
03/10/2022 21:08:09 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:08:09 - INFO - __main__ - ['negative']
03/10/2022 21:08:09 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:08:09 - INFO - __main__ - ['negative']
03/10/2022 21:08:09 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:08:09 - INFO - __main__ - ['negative']
03/10/2022 21:08:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:08:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:08:10 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:08:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:08:13 - INFO - __main__ - Starting training!
03/10/2022 21:08:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0003_8_predictions.txt
03/10/2022 21:08:26 - INFO - __main__ - Classification-F1 on test data: 0.6040
03/10/2022 21:08:26 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.603993789494431
03/10/2022 21:08:26 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0002, bsz=8 ...
03/10/2022 21:08:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:08:27 - INFO - __main__ - Printing 3 examples
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:08:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:08:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:08:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:08:27 - INFO - __main__ - Printing 3 examples
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:08:27 - INFO - __main__ - ['negative']
03/10/2022 21:08:27 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:08:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:08:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:08:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:08:38 - INFO - __main__ - Starting training!
03/10/2022 21:08:42 - INFO - __main__ - Step 10 Global step 10 Train loss 22.657948 on epoch=4
03/10/2022 21:08:47 - INFO - __main__ - Step 20 Global step 20 Train loss 17.918150 on epoch=9
03/10/2022 21:08:52 - INFO - __main__ - Step 30 Global step 30 Train loss 16.446228 on epoch=14
03/10/2022 21:08:56 - INFO - __main__ - Step 40 Global step 40 Train loss 15.591855 on epoch=19
03/10/2022 21:09:01 - INFO - __main__ - Step 50 Global step 50 Train loss 13.948126 on epoch=24
03/10/2022 21:09:03 - INFO - __main__ - Global step 50 Train loss 17.312460 Classification-F1 0.0 on epoch=24
03/10/2022 21:09:09 - INFO - __main__ - Step 60 Global step 60 Train loss 13.871733 on epoch=29
03/10/2022 21:09:14 - INFO - __main__ - Step 70 Global step 70 Train loss 12.820196 on epoch=34
03/10/2022 21:09:18 - INFO - __main__ - Step 80 Global step 80 Train loss 12.306083 on epoch=39
03/10/2022 21:09:23 - INFO - __main__ - Step 90 Global step 90 Train loss 11.776954 on epoch=44
03/10/2022 21:09:28 - INFO - __main__ - Step 100 Global step 100 Train loss 8.765771 on epoch=49
03/10/2022 21:09:28 - INFO - __main__ - Global step 100 Train loss 11.908147 Classification-F1 0.0 on epoch=49
03/10/2022 21:09:33 - INFO - __main__ - Step 110 Global step 110 Train loss 4.962437 on epoch=54
03/10/2022 21:09:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.908227 on epoch=59
03/10/2022 21:09:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.135192 on epoch=64
03/10/2022 21:09:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.065525 on epoch=69
03/10/2022 21:09:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.046343 on epoch=74
03/10/2022 21:09:53 - INFO - __main__ - Global step 150 Train loss 1.223545 Classification-F1 0.9375 on epoch=74
03/10/2022 21:09:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.198479 on epoch=79
03/10/2022 21:10:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.089561 on epoch=84
03/10/2022 21:10:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.006425 on epoch=89
03/10/2022 21:10:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.006005 on epoch=94
03/10/2022 21:10:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.023701 on epoch=99
03/10/2022 21:10:19 - INFO - __main__ - Global step 200 Train loss 0.064834 Classification-F1 0.9687194525904204 on epoch=99
03/10/2022 21:10:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.002625 on epoch=104
03/10/2022 21:10:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.001582 on epoch=109
03/10/2022 21:10:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000706 on epoch=114
03/10/2022 21:10:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.317918 on epoch=119
03/10/2022 21:10:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.074913 on epoch=124
03/10/2022 21:10:44 - INFO - __main__ - Global step 250 Train loss 0.079549 Classification-F1 0.9687194525904204 on epoch=124
03/10/2022 21:10:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.028309 on epoch=129
03/10/2022 21:10:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001288 on epoch=134
03/10/2022 21:10:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000392 on epoch=139
03/10/2022 21:11:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000218 on epoch=144
03/10/2022 21:11:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000174 on epoch=149
03/10/2022 21:11:09 - INFO - __main__ - Global step 300 Train loss 0.006076 Classification-F1 1.0 on epoch=149
03/10/2022 21:11:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.195121 on epoch=154
03/10/2022 21:11:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000184 on epoch=159
03/10/2022 21:11:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.077040 on epoch=164
03/10/2022 21:11:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002218 on epoch=169
03/10/2022 21:11:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.030343 on epoch=174
03/10/2022 21:11:35 - INFO - __main__ - Global step 350 Train loss 0.060981 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 21:11:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001556 on epoch=179
03/10/2022 21:11:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.064016 on epoch=184
03/10/2022 21:11:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.010448 on epoch=189
03/10/2022 21:11:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000172 on epoch=194
03/10/2022 21:11:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000056 on epoch=199
03/10/2022 21:12:00 - INFO - __main__ - Global step 400 Train loss 0.015250 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 21:12:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000192 on epoch=204
03/10/2022 21:12:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000238 on epoch=209
03/10/2022 21:12:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000030 on epoch=214
03/10/2022 21:12:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000327 on epoch=219
03/10/2022 21:12:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000022 on epoch=224
03/10/2022 21:12:25 - INFO - __main__ - Global step 450 Train loss 0.000162 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 21:12:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.134353 on epoch=229
03/10/2022 21:12:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000545 on epoch=234
03/10/2022 21:12:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.345149 on epoch=239
03/10/2022 21:12:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.067909 on epoch=244
03/10/2022 21:12:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000246 on epoch=249
03/10/2022 21:12:50 - INFO - __main__ - Global step 500 Train loss 0.109640 Classification-F1 0.906158357771261 on epoch=249
03/10/2022 21:12:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.023861 on epoch=254
03/10/2022 21:13:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000854 on epoch=259
03/10/2022 21:13:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000109 on epoch=264
03/10/2022 21:13:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001172 on epoch=269
03/10/2022 21:13:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000976 on epoch=274
03/10/2022 21:13:15 - INFO - __main__ - Global step 550 Train loss 0.005394 Classification-F1 0.9375 on epoch=274
03/10/2022 21:13:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000323 on epoch=279
03/10/2022 21:13:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.056509 on epoch=284
03/10/2022 21:13:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000110 on epoch=289
03/10/2022 21:13:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000076 on epoch=294
03/10/2022 21:13:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000153 on epoch=299
03/10/2022 21:13:40 - INFO - __main__ - Global step 600 Train loss 0.011434 Classification-F1 0.9375 on epoch=299
03/10/2022 21:13:40 - INFO - __main__ - save last model!
03/10/2022 21:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:41 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:13:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:41 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:13:41 - INFO - __main__ - ['negative']
03/10/2022 21:13:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:13:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:41 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:13:47 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:13:48 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:13:48 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:13:48 - INFO - __main__ - ['negative']
03/10/2022 21:13:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:13:48 - INFO - __main__ - ['negative']
03/10/2022 21:13:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:13:48 - INFO - __main__ - ['negative']
03/10/2022 21:13:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:13:48 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:49 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:13:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:13:51 - INFO - __main__ - Starting training!
03/10/2022 21:14:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0002_8_predictions.txt
03/10/2022 21:14:04 - INFO - __main__ - Classification-F1 on test data: 0.9389
03/10/2022 21:14:05 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9389070776651285
03/10/2022 21:14:05 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0001, bsz=8 ...
03/10/2022 21:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:14:06 - INFO - __main__ - Printing 3 examples
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:14:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:14:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:14:06 - INFO - __main__ - Printing 3 examples
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:14:06 - INFO - __main__ - ['negative']
03/10/2022 21:14:06 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:14:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:14:06 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:14:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:14:16 - INFO - __main__ - Starting training!
03/10/2022 21:14:23 - INFO - __main__ - Step 10 Global step 10 Train loss 23.670954 on epoch=4
03/10/2022 21:14:28 - INFO - __main__ - Step 20 Global step 20 Train loss 18.377687 on epoch=9
03/10/2022 21:14:33 - INFO - __main__ - Step 30 Global step 30 Train loss 17.213085 on epoch=14
03/10/2022 21:14:38 - INFO - __main__ - Step 40 Global step 40 Train loss 17.102783 on epoch=19
03/10/2022 21:14:43 - INFO - __main__ - Step 50 Global step 50 Train loss 15.146101 on epoch=24
03/10/2022 21:14:53 - INFO - __main__ - Global step 50 Train loss 18.302122 Classification-F1 0.0 on epoch=24
03/10/2022 21:14:58 - INFO - __main__ - Step 60 Global step 60 Train loss 15.745010 on epoch=29
03/10/2022 21:15:03 - INFO - __main__ - Step 70 Global step 70 Train loss 14.847064 on epoch=34
03/10/2022 21:15:08 - INFO - __main__ - Step 80 Global step 80 Train loss 15.135315 on epoch=39
03/10/2022 21:15:13 - INFO - __main__ - Step 90 Global step 90 Train loss 15.249496 on epoch=44
03/10/2022 21:15:18 - INFO - __main__ - Step 100 Global step 100 Train loss 14.161306 on epoch=49
03/10/2022 21:15:24 - INFO - __main__ - Global step 100 Train loss 15.027639 Classification-F1 0.0 on epoch=49
03/10/2022 21:15:29 - INFO - __main__ - Step 110 Global step 110 Train loss 13.537671 on epoch=54
03/10/2022 21:15:34 - INFO - __main__ - Step 120 Global step 120 Train loss 13.830145 on epoch=59
03/10/2022 21:15:39 - INFO - __main__ - Step 130 Global step 130 Train loss 13.507220 on epoch=64
03/10/2022 21:15:44 - INFO - __main__ - Step 140 Global step 140 Train loss 12.799348 on epoch=69
03/10/2022 21:15:48 - INFO - __main__ - Step 150 Global step 150 Train loss 12.550844 on epoch=74
03/10/2022 21:15:53 - INFO - __main__ - Global step 150 Train loss 13.245045 Classification-F1 0.0 on epoch=74
03/10/2022 21:15:58 - INFO - __main__ - Step 160 Global step 160 Train loss 11.868431 on epoch=79
03/10/2022 21:16:03 - INFO - __main__ - Step 170 Global step 170 Train loss 11.599307 on epoch=84
03/10/2022 21:16:08 - INFO - __main__ - Step 180 Global step 180 Train loss 11.135634 on epoch=89
03/10/2022 21:16:13 - INFO - __main__ - Step 190 Global step 190 Train loss 10.037599 on epoch=94
03/10/2022 21:16:17 - INFO - __main__ - Step 200 Global step 200 Train loss 8.290104 on epoch=99
03/10/2022 21:16:19 - INFO - __main__ - Global step 200 Train loss 10.586216 Classification-F1 0.041666666666666664 on epoch=99
03/10/2022 21:16:26 - INFO - __main__ - Step 210 Global step 210 Train loss 6.357329 on epoch=104
03/10/2022 21:16:31 - INFO - __main__ - Step 220 Global step 220 Train loss 3.497171 on epoch=109
03/10/2022 21:16:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.432600 on epoch=114
03/10/2022 21:16:40 - INFO - __main__ - Step 240 Global step 240 Train loss 1.006384 on epoch=119
03/10/2022 21:16:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.967502 on epoch=124
03/10/2022 21:16:46 - INFO - __main__ - Global step 250 Train loss 2.652197 Classification-F1 0.15360983102918588 on epoch=124
03/10/2022 21:16:52 - INFO - __main__ - Step 260 Global step 260 Train loss 1.305800 on epoch=129
03/10/2022 21:16:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.089291 on epoch=134
03/10/2022 21:17:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.071935 on epoch=139
03/10/2022 21:17:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.713537 on epoch=144
03/10/2022 21:17:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.629506 on epoch=149
03/10/2022 21:17:15 - INFO - __main__ - Global step 300 Train loss 0.962014 Classification-F1 0.13043478260869562 on epoch=149
03/10/2022 21:17:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.777235 on epoch=154
03/10/2022 21:17:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.759668 on epoch=159
03/10/2022 21:17:30 - INFO - __main__ - Step 330 Global step 330 Train loss 1.030035 on epoch=164
03/10/2022 21:17:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.587639 on epoch=169
03/10/2022 21:17:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.385206 on epoch=174
03/10/2022 21:17:41 - INFO - __main__ - Global step 350 Train loss 0.707957 Classification-F1 0.49090909090909085 on epoch=174
03/10/2022 21:17:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.368787 on epoch=179
03/10/2022 21:17:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.378612 on epoch=184
03/10/2022 21:17:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.344907 on epoch=189
03/10/2022 21:18:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.345426 on epoch=194
03/10/2022 21:18:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.311621 on epoch=199
03/10/2022 21:18:07 - INFO - __main__ - Global step 400 Train loss 0.349871 Classification-F1 0.6825396825396826 on epoch=199
03/10/2022 21:18:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.502050 on epoch=204
03/10/2022 21:18:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.458955 on epoch=209
03/10/2022 21:18:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.305438 on epoch=214
03/10/2022 21:18:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.278876 on epoch=219
03/10/2022 21:18:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.354281 on epoch=224
03/10/2022 21:18:32 - INFO - __main__ - Global step 450 Train loss 0.379920 Classification-F1 0.6113360323886641 on epoch=224
03/10/2022 21:18:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.292672 on epoch=229
03/10/2022 21:18:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.253416 on epoch=234
03/10/2022 21:18:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.272510 on epoch=239
03/10/2022 21:18:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.304299 on epoch=244
03/10/2022 21:18:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276426 on epoch=249
03/10/2022 21:18:57 - INFO - __main__ - Global step 500 Train loss 0.279865 Classification-F1 0.875 on epoch=249
03/10/2022 21:19:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.278950 on epoch=254
03/10/2022 21:19:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.289842 on epoch=259
03/10/2022 21:19:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.265846 on epoch=264
03/10/2022 21:19:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.220781 on epoch=269
03/10/2022 21:19:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.227364 on epoch=274
03/10/2022 21:19:23 - INFO - __main__ - Global step 550 Train loss 0.256557 Classification-F1 0.7793103448275862 on epoch=274
03/10/2022 21:19:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.223501 on epoch=279
03/10/2022 21:19:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.208241 on epoch=284
03/10/2022 21:19:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167019 on epoch=289
03/10/2022 21:19:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.195511 on epoch=294
03/10/2022 21:19:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.211189 on epoch=299
03/10/2022 21:19:48 - INFO - __main__ - Global step 600 Train loss 0.201092 Classification-F1 0.8435972629521017 on epoch=299
03/10/2022 21:19:48 - INFO - __main__ - save last model!
03/10/2022 21:19:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:19:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:19:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:19:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:19:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:19:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:19:49 - INFO - __main__ - ['positive']
03/10/2022 21:19:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:19:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:19:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:19:55 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:19:56 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:19:56 - INFO - __main__ - Printing 3 examples
03/10/2022 21:19:56 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:19:56 - INFO - __main__ - ['negative']
03/10/2022 21:19:56 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:19:56 - INFO - __main__ - ['negative']
03/10/2022 21:19:56 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:19:56 - INFO - __main__ - ['negative']
03/10/2022 21:19:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:19:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:19:58 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:19:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:19:59 - INFO - __main__ - Starting training!
03/10/2022 21:20:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0001_8_predictions.txt
03/10/2022 21:20:12 - INFO - __main__ - Classification-F1 on test data: 0.7874
03/10/2022 21:20:12 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.875, test_performance=0.787350725805549
03/10/2022 21:20:12 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0005, bsz=8 ...
03/10/2022 21:20:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:20:13 - INFO - __main__ - Printing 3 examples
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:20:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:20:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:20:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:20:13 - INFO - __main__ - Printing 3 examples
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:20:13 - INFO - __main__ - ['positive']
03/10/2022 21:20:13 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:20:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:20:13 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:20:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:20:24 - INFO - __main__ - Starting training!
03/10/2022 21:20:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.465708 on epoch=4
03/10/2022 21:20:32 - INFO - __main__ - Step 20 Global step 20 Train loss 19.225107 on epoch=9
03/10/2022 21:20:37 - INFO - __main__ - Step 30 Global step 30 Train loss 16.374500 on epoch=14
03/10/2022 21:20:42 - INFO - __main__ - Step 40 Global step 40 Train loss 14.440104 on epoch=19
03/10/2022 21:20:47 - INFO - __main__ - Step 50 Global step 50 Train loss 12.645704 on epoch=24
03/10/2022 21:20:57 - INFO - __main__ - Global step 50 Train loss 17.230225 Classification-F1 0.0 on epoch=24
03/10/2022 21:21:02 - INFO - __main__ - Step 60 Global step 60 Train loss 9.783524 on epoch=29
03/10/2022 21:21:07 - INFO - __main__ - Step 70 Global step 70 Train loss 3.709040 on epoch=34
03/10/2022 21:21:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.599068 on epoch=39
03/10/2022 21:21:17 - INFO - __main__ - Step 90 Global step 90 Train loss 2.379314 on epoch=44
03/10/2022 21:21:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.494376 on epoch=49
03/10/2022 21:21:22 - INFO - __main__ - Global step 100 Train loss 3.993065 Classification-F1 0.6113360323886641 on epoch=49
03/10/2022 21:21:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.267591 on epoch=54
03/10/2022 21:21:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.732680 on epoch=59
03/10/2022 21:21:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.559440 on epoch=64
03/10/2022 21:21:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.275660 on epoch=69
03/10/2022 21:21:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.183720 on epoch=74
03/10/2022 21:21:48 - INFO - __main__ - Global step 150 Train loss 0.603818 Classification-F1 0.8435972629521017 on epoch=74
03/10/2022 21:21:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.125530 on epoch=79
03/10/2022 21:21:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.117123 on epoch=84
03/10/2022 21:22:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.168275 on epoch=89
03/10/2022 21:22:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.121782 on epoch=94
03/10/2022 21:22:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.026667 on epoch=99
03/10/2022 21:22:14 - INFO - __main__ - Global step 200 Train loss 0.111875 Classification-F1 0.9372549019607843 on epoch=99
03/10/2022 21:22:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.038052 on epoch=104
03/10/2022 21:22:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.026163 on epoch=109
03/10/2022 21:22:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.350606 on epoch=114
03/10/2022 21:22:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.241416 on epoch=119
03/10/2022 21:22:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.099874 on epoch=124
03/10/2022 21:22:40 - INFO - __main__ - Global step 250 Train loss 0.151222 Classification-F1 0.7757757757757757 on epoch=124
03/10/2022 21:22:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.057261 on epoch=129
03/10/2022 21:22:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.188531 on epoch=134
03/10/2022 21:22:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027228 on epoch=139
03/10/2022 21:23:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.072980 on epoch=144
03/10/2022 21:23:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.045355 on epoch=149
03/10/2022 21:23:05 - INFO - __main__ - Global step 300 Train loss 0.078271 Classification-F1 0.873015873015873 on epoch=149
03/10/2022 21:23:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.041676 on epoch=154
03/10/2022 21:23:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.052644 on epoch=159
03/10/2022 21:23:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.025254 on epoch=164
03/10/2022 21:23:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.014648 on epoch=169
03/10/2022 21:23:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.072520 on epoch=174
03/10/2022 21:23:30 - INFO - __main__ - Global step 350 Train loss 0.041348 Classification-F1 0.3992490613266583 on epoch=174
03/10/2022 21:23:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.107682 on epoch=179
03/10/2022 21:23:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.042138 on epoch=184
03/10/2022 21:23:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047339 on epoch=189
03/10/2022 21:23:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.078599 on epoch=194
03/10/2022 21:23:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.035375 on epoch=199
03/10/2022 21:23:55 - INFO - __main__ - Global step 400 Train loss 0.062227 Classification-F1 0.7810361681329424 on epoch=199
03/10/2022 21:24:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.060592 on epoch=204
03/10/2022 21:24:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.013520 on epoch=209
03/10/2022 21:24:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004584 on epoch=214
03/10/2022 21:24:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001038 on epoch=219
03/10/2022 21:24:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007612 on epoch=224
03/10/2022 21:24:20 - INFO - __main__ - Global step 450 Train loss 0.017469 Classification-F1 0.7757757757757757 on epoch=224
03/10/2022 21:24:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008954 on epoch=229
03/10/2022 21:24:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002035 on epoch=234
03/10/2022 21:24:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003194 on epoch=239
03/10/2022 21:24:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002617 on epoch=244
03/10/2022 21:24:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000474 on epoch=249
03/10/2022 21:24:45 - INFO - __main__ - Global step 500 Train loss 0.003455 Classification-F1 0.8435972629521017 on epoch=249
03/10/2022 21:24:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.031074 on epoch=254
03/10/2022 21:24:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000237 on epoch=259
03/10/2022 21:25:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000267 on epoch=264
03/10/2022 21:25:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004413 on epoch=269
03/10/2022 21:25:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000198 on epoch=274
03/10/2022 21:25:10 - INFO - __main__ - Global step 550 Train loss 0.007238 Classification-F1 0.8117647058823529 on epoch=274
03/10/2022 21:25:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000063 on epoch=279
03/10/2022 21:25:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000321 on epoch=284
03/10/2022 21:25:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000761 on epoch=289
03/10/2022 21:25:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000615 on epoch=294
03/10/2022 21:25:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002438 on epoch=299
03/10/2022 21:25:35 - INFO - __main__ - Global step 600 Train loss 0.000840 Classification-F1 0.8435972629521017 on epoch=299
03/10/2022 21:25:35 - INFO - __main__ - save last model!
03/10/2022 21:25:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:25:36 - INFO - __main__ - Printing 3 examples
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:25:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:25:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:25:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:25:36 - INFO - __main__ - Printing 3 examples
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:25:36 - INFO - __main__ - ['positive']
03/10/2022 21:25:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:25:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:25:36 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:25:42 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:25:43 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:25:43 - INFO - __main__ - Printing 3 examples
03/10/2022 21:25:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:25:43 - INFO - __main__ - ['negative']
03/10/2022 21:25:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:25:43 - INFO - __main__ - ['negative']
03/10/2022 21:25:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:25:43 - INFO - __main__ - ['negative']
03/10/2022 21:25:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:25:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:25:45 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:25:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:25:45 - INFO - __main__ - Starting training!
03/10/2022 21:25:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0005_8_predictions.txt
03/10/2022 21:25:59 - INFO - __main__ - Classification-F1 on test data: 0.9169
03/10/2022 21:26:00 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9169394488582177
03/10/2022 21:26:00 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0003, bsz=8 ...
03/10/2022 21:26:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:26:00 - INFO - __main__ - Printing 3 examples
03/10/2022 21:26:00 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:26:00 - INFO - __main__ - ['positive']
03/10/2022 21:26:00 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:26:00 - INFO - __main__ - ['positive']
03/10/2022 21:26:00 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:26:00 - INFO - __main__ - ['positive']
03/10/2022 21:26:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:26:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:26:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:26:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:26:01 - INFO - __main__ - Printing 3 examples
03/10/2022 21:26:01 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:26:01 - INFO - __main__ - ['positive']
03/10/2022 21:26:01 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:26:01 - INFO - __main__ - ['positive']
03/10/2022 21:26:01 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:26:01 - INFO - __main__ - ['positive']
03/10/2022 21:26:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:26:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:26:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:26:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:26:11 - INFO - __main__ - Starting training!
03/10/2022 21:26:16 - INFO - __main__ - Step 10 Global step 10 Train loss 22.212280 on epoch=4
03/10/2022 21:26:20 - INFO - __main__ - Step 20 Global step 20 Train loss 18.346857 on epoch=9
03/10/2022 21:26:25 - INFO - __main__ - Step 30 Global step 30 Train loss 17.040716 on epoch=14
03/10/2022 21:26:30 - INFO - __main__ - Step 40 Global step 40 Train loss 15.513784 on epoch=19
03/10/2022 21:26:35 - INFO - __main__ - Step 50 Global step 50 Train loss 13.769196 on epoch=24
03/10/2022 21:26:43 - INFO - __main__ - Global step 50 Train loss 17.376568 Classification-F1 0.0 on epoch=24
03/10/2022 21:26:48 - INFO - __main__ - Step 60 Global step 60 Train loss 12.946634 on epoch=29
03/10/2022 21:26:53 - INFO - __main__ - Step 70 Global step 70 Train loss 11.622050 on epoch=34
03/10/2022 21:26:58 - INFO - __main__ - Step 80 Global step 80 Train loss 10.160978 on epoch=39
03/10/2022 21:27:03 - INFO - __main__ - Step 90 Global step 90 Train loss 3.479689 on epoch=44
03/10/2022 21:27:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.432134 on epoch=49
03/10/2022 21:27:09 - INFO - __main__ - Global step 100 Train loss 7.728297 Classification-F1 0.9687194525904204 on epoch=49
03/10/2022 21:27:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.157234 on epoch=54
03/10/2022 21:27:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.068805 on epoch=59
03/10/2022 21:27:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.075144 on epoch=64
03/10/2022 21:27:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.024177 on epoch=69
03/10/2022 21:27:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.010447 on epoch=74
03/10/2022 21:27:35 - INFO - __main__ - Global step 150 Train loss 0.067161 Classification-F1 0.9372549019607843 on epoch=74
03/10/2022 21:27:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.017471 on epoch=79
03/10/2022 21:27:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.016210 on epoch=84
03/10/2022 21:27:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.008841 on epoch=89
03/10/2022 21:27:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.034360 on epoch=94
03/10/2022 21:28:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003390 on epoch=99
03/10/2022 21:28:01 - INFO - __main__ - Global step 200 Train loss 0.016054 Classification-F1 0.9687194525904204 on epoch=99
03/10/2022 21:28:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.003355 on epoch=104
03/10/2022 21:28:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002604 on epoch=109
03/10/2022 21:28:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001294 on epoch=114
03/10/2022 21:28:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001788 on epoch=119
03/10/2022 21:28:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001120 on epoch=124
03/10/2022 21:28:26 - INFO - __main__ - Global step 250 Train loss 0.002032 Classification-F1 0.9687194525904204 on epoch=124
03/10/2022 21:28:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.006167 on epoch=129
03/10/2022 21:28:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001024 on epoch=134
03/10/2022 21:28:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000610 on epoch=139
03/10/2022 21:28:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.016164 on epoch=144
03/10/2022 21:28:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.109547 on epoch=149
03/10/2022 21:28:51 - INFO - __main__ - Global step 300 Train loss 0.026703 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 21:28:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.032580 on epoch=154
03/10/2022 21:29:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.080153 on epoch=159
03/10/2022 21:29:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.433920 on epoch=164
03/10/2022 21:29:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.374850 on epoch=169
03/10/2022 21:29:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.271681 on epoch=174
03/10/2022 21:29:17 - INFO - __main__ - Global step 350 Train loss 0.238637 Classification-F1 0.7793103448275862 on epoch=174
03/10/2022 21:29:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.315837 on epoch=179
03/10/2022 21:29:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.387604 on epoch=184
03/10/2022 21:29:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.303830 on epoch=189
03/10/2022 21:29:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.323788 on epoch=194
03/10/2022 21:29:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.232843 on epoch=199
03/10/2022 21:29:42 - INFO - __main__ - Global step 400 Train loss 0.312781 Classification-F1 0.906158357771261 on epoch=199
03/10/2022 21:29:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.214381 on epoch=204
03/10/2022 21:29:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.190507 on epoch=209
03/10/2022 21:29:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.170226 on epoch=214
03/10/2022 21:30:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.155957 on epoch=219
03/10/2022 21:30:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.109316 on epoch=224
03/10/2022 21:30:07 - INFO - __main__ - Global step 450 Train loss 0.168077 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 21:30:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.311170 on epoch=229
03/10/2022 21:30:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.145068 on epoch=234
03/10/2022 21:30:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.164105 on epoch=239
03/10/2022 21:30:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.065611 on epoch=244
03/10/2022 21:30:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053580 on epoch=249
03/10/2022 21:30:33 - INFO - __main__ - Global step 500 Train loss 0.147907 Classification-F1 0.7702564102564102 on epoch=249
03/10/2022 21:30:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.055702 on epoch=254
03/10/2022 21:30:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.072138 on epoch=259
03/10/2022 21:30:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.073889 on epoch=264
03/10/2022 21:30:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.144095 on epoch=269
03/10/2022 21:30:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.189512 on epoch=274
03/10/2022 21:30:59 - INFO - __main__ - Global step 550 Train loss 0.107067 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 21:31:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.045805 on epoch=279
03/10/2022 21:31:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.155834 on epoch=284
03/10/2022 21:31:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.122931 on epoch=289
03/10/2022 21:31:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.121118 on epoch=294
03/10/2022 21:31:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.230752 on epoch=299
03/10/2022 21:31:24 - INFO - __main__ - Global step 600 Train loss 0.135288 Classification-F1 0.6945917285259808 on epoch=299
03/10/2022 21:31:24 - INFO - __main__ - save last model!
03/10/2022 21:31:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:31:24 - INFO - __main__ - Printing 3 examples
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:31:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:31:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:31:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:31:24 - INFO - __main__ - Printing 3 examples
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:31:24 - INFO - __main__ - ['positive']
03/10/2022 21:31:24 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:31:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:31:25 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:31:31 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:31:31 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:31:31 - INFO - __main__ - Printing 3 examples
03/10/2022 21:31:31 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:31:31 - INFO - __main__ - ['negative']
03/10/2022 21:31:31 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:31:31 - INFO - __main__ - ['negative']
03/10/2022 21:31:31 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:31:31 - INFO - __main__ - ['negative']
03/10/2022 21:31:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:31:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:31:33 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:31:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:31:35 - INFO - __main__ - Starting training!
03/10/2022 21:31:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0003_8_predictions.txt
03/10/2022 21:31:47 - INFO - __main__ - Classification-F1 on test data: 0.9290
03/10/2022 21:31:48 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9289982249556239
03/10/2022 21:31:48 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0002, bsz=8 ...
03/10/2022 21:31:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:31:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:31:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:31:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:31:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:31:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:31:49 - INFO - __main__ - ['positive']
03/10/2022 21:31:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:31:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:31:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:31:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:31:59 - INFO - __main__ - Starting training!
03/10/2022 21:32:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.906343 on epoch=4
03/10/2022 21:32:08 - INFO - __main__ - Step 20 Global step 20 Train loss 19.968283 on epoch=9
03/10/2022 21:32:13 - INFO - __main__ - Step 30 Global step 30 Train loss 17.506687 on epoch=14
03/10/2022 21:32:18 - INFO - __main__ - Step 40 Global step 40 Train loss 16.257349 on epoch=19
03/10/2022 21:32:23 - INFO - __main__ - Step 50 Global step 50 Train loss 15.577960 on epoch=24
03/10/2022 21:32:30 - INFO - __main__ - Global step 50 Train loss 18.643324 Classification-F1 0.0 on epoch=24
03/10/2022 21:32:35 - INFO - __main__ - Step 60 Global step 60 Train loss 15.149335 on epoch=29
03/10/2022 21:32:40 - INFO - __main__ - Step 70 Global step 70 Train loss 13.568758 on epoch=34
03/10/2022 21:32:45 - INFO - __main__ - Step 80 Global step 80 Train loss 13.312548 on epoch=39
03/10/2022 21:32:50 - INFO - __main__ - Step 90 Global step 90 Train loss 12.791578 on epoch=44
03/10/2022 21:32:55 - INFO - __main__ - Step 100 Global step 100 Train loss 11.714728 on epoch=49
03/10/2022 21:32:59 - INFO - __main__ - Global step 100 Train loss 13.307390 Classification-F1 0.008403361344537815 on epoch=49
03/10/2022 21:33:05 - INFO - __main__ - Step 110 Global step 110 Train loss 10.746004 on epoch=54
03/10/2022 21:33:10 - INFO - __main__ - Step 120 Global step 120 Train loss 9.381700 on epoch=59
03/10/2022 21:33:15 - INFO - __main__ - Step 130 Global step 130 Train loss 5.223197 on epoch=64
03/10/2022 21:33:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.617856 on epoch=69
03/10/2022 21:33:24 - INFO - __main__ - Step 150 Global step 150 Train loss 2.418817 on epoch=74
03/10/2022 21:33:25 - INFO - __main__ - Global step 150 Train loss 5.877514 Classification-F1 0.3191489361702127 on epoch=74
03/10/2022 21:33:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.086251 on epoch=79
03/10/2022 21:33:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.799559 on epoch=84
03/10/2022 21:33:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.682189 on epoch=89
03/10/2022 21:33:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.462374 on epoch=94
03/10/2022 21:33:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.408528 on epoch=99
03/10/2022 21:33:51 - INFO - __main__ - Global step 200 Train loss 0.687780 Classification-F1 0.6389743589743591 on epoch=99
03/10/2022 21:33:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.461762 on epoch=104
03/10/2022 21:34:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.547941 on epoch=109
03/10/2022 21:34:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.550324 on epoch=114
03/10/2022 21:34:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.415617 on epoch=119
03/10/2022 21:34:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.488420 on epoch=124
03/10/2022 21:34:17 - INFO - __main__ - Global step 250 Train loss 0.492813 Classification-F1 0.4589371980676329 on epoch=124
03/10/2022 21:34:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.623583 on epoch=129
03/10/2022 21:34:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.415308 on epoch=134
03/10/2022 21:34:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.299497 on epoch=139
03/10/2022 21:34:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.302867 on epoch=144
03/10/2022 21:34:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.335746 on epoch=149
03/10/2022 21:34:43 - INFO - __main__ - Global step 300 Train loss 0.395400 Classification-F1 0.5607843137254902 on epoch=149
03/10/2022 21:34:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.335959 on epoch=154
03/10/2022 21:34:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.352104 on epoch=159
03/10/2022 21:34:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.257370 on epoch=164
03/10/2022 21:35:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.440723 on epoch=169
03/10/2022 21:35:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.320892 on epoch=174
03/10/2022 21:35:08 - INFO - __main__ - Global step 350 Train loss 0.341410 Classification-F1 0.5555555555555556 on epoch=174
03/10/2022 21:35:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.299931 on epoch=179
03/10/2022 21:35:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.305630 on epoch=184
03/10/2022 21:35:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.238483 on epoch=189
03/10/2022 21:35:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.220185 on epoch=194
03/10/2022 21:35:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.222169 on epoch=199
03/10/2022 21:35:34 - INFO - __main__ - Global step 400 Train loss 0.257280 Classification-F1 0.6532019704433498 on epoch=199
03/10/2022 21:35:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.180762 on epoch=204
03/10/2022 21:35:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.230028 on epoch=209
03/10/2022 21:35:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.195424 on epoch=214
03/10/2022 21:35:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.189841 on epoch=219
03/10/2022 21:35:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.137528 on epoch=224
03/10/2022 21:36:00 - INFO - __main__ - Global step 450 Train loss 0.186716 Classification-F1 0.6532019704433498 on epoch=224
03/10/2022 21:36:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.150664 on epoch=229
03/10/2022 21:36:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.140017 on epoch=234
03/10/2022 21:36:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.136042 on epoch=239
03/10/2022 21:36:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.097711 on epoch=244
03/10/2022 21:36:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.172387 on epoch=249
03/10/2022 21:36:25 - INFO - __main__ - Global step 500 Train loss 0.139364 Classification-F1 0.6235294117647059 on epoch=249
03/10/2022 21:36:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.085302 on epoch=254
03/10/2022 21:36:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.118532 on epoch=259
03/10/2022 21:36:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.089938 on epoch=264
03/10/2022 21:36:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.052510 on epoch=269
03/10/2022 21:36:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.113905 on epoch=274
03/10/2022 21:36:51 - INFO - __main__ - Global step 550 Train loss 0.092038 Classification-F1 0.5901477832512315 on epoch=274
03/10/2022 21:36:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026368 on epoch=279
03/10/2022 21:37:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.057810 on epoch=284
03/10/2022 21:37:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036346 on epoch=289
03/10/2022 21:37:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021039 on epoch=294
03/10/2022 21:37:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037295 on epoch=299
03/10/2022 21:37:16 - INFO - __main__ - Global step 600 Train loss 0.035772 Classification-F1 0.6113360323886641 on epoch=299
03/10/2022 21:37:16 - INFO - __main__ - save last model!
03/10/2022 21:37:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:37:17 - INFO - __main__ - Printing 3 examples
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:37:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:37:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:37:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:37:17 - INFO - __main__ - Printing 3 examples
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:37:17 - INFO - __main__ - ['positive']
03/10/2022 21:37:17 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:37:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:37:17 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:37:23 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:37:24 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:37:24 - INFO - __main__ - Printing 3 examples
03/10/2022 21:37:24 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:37:24 - INFO - __main__ - ['negative']
03/10/2022 21:37:24 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:37:24 - INFO - __main__ - ['negative']
03/10/2022 21:37:24 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:37:24 - INFO - __main__ - ['negative']
03/10/2022 21:37:24 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:37:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:37:25 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:37:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:37:27 - INFO - __main__ - Starting training!
03/10/2022 21:37:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0002_8_predictions.txt
03/10/2022 21:37:40 - INFO - __main__ - Classification-F1 on test data: 0.6647
03/10/2022 21:37:40 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.6532019704433498, test_performance=0.6646764025671561
03/10/2022 21:37:40 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0001, bsz=8 ...
03/10/2022 21:37:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:37:41 - INFO - __main__ - Printing 3 examples
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:37:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:37:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:37:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:37:41 - INFO - __main__ - Printing 3 examples
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 21:37:41 - INFO - __main__ - ['positive']
03/10/2022 21:37:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:37:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:37:41 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:37:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:37:50 - INFO - __main__ - Starting training!
03/10/2022 21:37:56 - INFO - __main__ - Step 10 Global step 10 Train loss 23.357590 on epoch=4
03/10/2022 21:38:01 - INFO - __main__ - Step 20 Global step 20 Train loss 19.941040 on epoch=9
03/10/2022 21:38:06 - INFO - __main__ - Step 30 Global step 30 Train loss 17.896994 on epoch=14
03/10/2022 21:38:11 - INFO - __main__ - Step 40 Global step 40 Train loss 17.781626 on epoch=19
03/10/2022 21:38:16 - INFO - __main__ - Step 50 Global step 50 Train loss 16.401131 on epoch=24
03/10/2022 21:38:24 - INFO - __main__ - Global step 50 Train loss 19.075676 Classification-F1 0.0 on epoch=24
03/10/2022 21:38:29 - INFO - __main__ - Step 60 Global step 60 Train loss 15.949445 on epoch=29
03/10/2022 21:38:34 - INFO - __main__ - Step 70 Global step 70 Train loss 15.811399 on epoch=34
03/10/2022 21:38:39 - INFO - __main__ - Step 80 Global step 80 Train loss 16.285486 on epoch=39
03/10/2022 21:38:44 - INFO - __main__ - Step 90 Global step 90 Train loss 14.861842 on epoch=44
03/10/2022 21:38:49 - INFO - __main__ - Step 100 Global step 100 Train loss 14.386148 on epoch=49
03/10/2022 21:38:56 - INFO - __main__ - Global step 100 Train loss 15.458865 Classification-F1 0.0 on epoch=49
03/10/2022 21:39:01 - INFO - __main__ - Step 110 Global step 110 Train loss 14.836618 on epoch=54
03/10/2022 21:39:06 - INFO - __main__ - Step 120 Global step 120 Train loss 14.468143 on epoch=59
03/10/2022 21:39:11 - INFO - __main__ - Step 130 Global step 130 Train loss 13.739863 on epoch=64
03/10/2022 21:39:16 - INFO - __main__ - Step 140 Global step 140 Train loss 13.181025 on epoch=69
03/10/2022 21:39:21 - INFO - __main__ - Step 150 Global step 150 Train loss 12.522114 on epoch=74
03/10/2022 21:39:27 - INFO - __main__ - Global step 150 Train loss 13.749552 Classification-F1 0.0 on epoch=74
03/10/2022 21:39:32 - INFO - __main__ - Step 160 Global step 160 Train loss 12.301744 on epoch=79
03/10/2022 21:39:37 - INFO - __main__ - Step 170 Global step 170 Train loss 10.912993 on epoch=84
03/10/2022 21:39:42 - INFO - __main__ - Step 180 Global step 180 Train loss 11.158826 on epoch=89
03/10/2022 21:39:47 - INFO - __main__ - Step 190 Global step 190 Train loss 10.687723 on epoch=94
03/10/2022 21:39:52 - INFO - __main__ - Step 200 Global step 200 Train loss 9.587111 on epoch=99
03/10/2022 21:39:53 - INFO - __main__ - Global step 200 Train loss 10.929679 Classification-F1 0.0 on epoch=99
03/10/2022 21:39:58 - INFO - __main__ - Step 210 Global step 210 Train loss 8.420752 on epoch=104
03/10/2022 21:40:03 - INFO - __main__ - Step 220 Global step 220 Train loss 2.494663 on epoch=109
03/10/2022 21:40:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.845907 on epoch=114
03/10/2022 21:40:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.640872 on epoch=119
03/10/2022 21:40:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.329459 on epoch=124
03/10/2022 21:40:18 - INFO - __main__ - Global step 250 Train loss 2.546330 Classification-F1 0.9687194525904204 on epoch=124
03/10/2022 21:40:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.204961 on epoch=129
03/10/2022 21:40:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.163022 on epoch=134
03/10/2022 21:40:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100515 on epoch=139
03/10/2022 21:40:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.047267 on epoch=144
03/10/2022 21:40:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.050518 on epoch=149
03/10/2022 21:40:44 - INFO - __main__ - Global step 300 Train loss 0.113257 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 21:40:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.135147 on epoch=154
03/10/2022 21:40:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.109953 on epoch=159
03/10/2022 21:40:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.106206 on epoch=164
03/10/2022 21:41:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022588 on epoch=169
03/10/2022 21:41:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011059 on epoch=174
03/10/2022 21:41:09 - INFO - __main__ - Global step 350 Train loss 0.076991 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 21:41:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.013238 on epoch=179
03/10/2022 21:41:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009443 on epoch=184
03/10/2022 21:41:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.012456 on epoch=189
03/10/2022 21:41:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001454 on epoch=194
03/10/2022 21:41:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.004133 on epoch=199
03/10/2022 21:41:35 - INFO - __main__ - Global step 400 Train loss 0.008145 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 21:41:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002381 on epoch=204
03/10/2022 21:41:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004236 on epoch=209
03/10/2022 21:41:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.002367 on epoch=214
03/10/2022 21:41:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001175 on epoch=219
03/10/2022 21:41:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.221249 on epoch=224
03/10/2022 21:42:00 - INFO - __main__ - Global step 450 Train loss 0.046282 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 21:42:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001899 on epoch=229
03/10/2022 21:42:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010780 on epoch=234
03/10/2022 21:42:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001764 on epoch=239
03/10/2022 21:42:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009782 on epoch=244
03/10/2022 21:42:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.015932 on epoch=249
03/10/2022 21:42:25 - INFO - __main__ - Global step 500 Train loss 0.008031 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 21:42:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001268 on epoch=254
03/10/2022 21:42:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003227 on epoch=259
03/10/2022 21:42:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001228 on epoch=264
03/10/2022 21:42:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000527 on epoch=269
03/10/2022 21:42:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000422 on epoch=274
03/10/2022 21:42:50 - INFO - __main__ - Global step 550 Train loss 0.001334 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 21:42:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000954 on epoch=279
03/10/2022 21:43:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000638 on epoch=284
03/10/2022 21:43:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.008466 on epoch=289
03/10/2022 21:43:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001087 on epoch=294
03/10/2022 21:43:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001791 on epoch=299
03/10/2022 21:43:16 - INFO - __main__ - Global step 600 Train loss 0.002587 Classification-F1 0.9687194525904204 on epoch=299
03/10/2022 21:43:16 - INFO - __main__ - save last model!
03/10/2022 21:43:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:43:16 - INFO - __main__ - Printing 3 examples
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:43:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:43:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:43:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:43:16 - INFO - __main__ - Printing 3 examples
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:43:16 - INFO - __main__ - ['negative']
03/10/2022 21:43:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:43:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:43:16 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:43:23 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:43:23 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:43:23 - INFO - __main__ - Printing 3 examples
03/10/2022 21:43:23 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:43:23 - INFO - __main__ - ['negative']
03/10/2022 21:43:23 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:43:23 - INFO - __main__ - ['negative']
03/10/2022 21:43:23 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:43:23 - INFO - __main__ - ['negative']
03/10/2022 21:43:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:43:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:43:25 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:43:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:43:26 - INFO - __main__ - Starting training!
03/10/2022 21:43:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0001_8_predictions.txt
03/10/2022 21:43:39 - INFO - __main__ - Classification-F1 on test data: 0.9399
03/10/2022 21:43:39 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9399459513562205
03/10/2022 21:43:39 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0005, bsz=8 ...
03/10/2022 21:43:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:43:40 - INFO - __main__ - Printing 3 examples
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:43:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:43:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:43:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:43:40 - INFO - __main__ - Printing 3 examples
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:43:40 - INFO - __main__ - ['negative']
03/10/2022 21:43:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:43:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:43:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:43:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:43:50 - INFO - __main__ - Starting training!
03/10/2022 21:43:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.863840 on epoch=4
03/10/2022 21:44:00 - INFO - __main__ - Step 20 Global step 20 Train loss 16.435137 on epoch=9
03/10/2022 21:44:05 - INFO - __main__ - Step 30 Global step 30 Train loss 14.580835 on epoch=14
03/10/2022 21:44:10 - INFO - __main__ - Step 40 Global step 40 Train loss 11.276888 on epoch=19
03/10/2022 21:44:15 - INFO - __main__ - Step 50 Global step 50 Train loss 9.350591 on epoch=24
03/10/2022 21:44:16 - INFO - __main__ - Global step 50 Train loss 15.101458 Classification-F1 0.19393939393939394 on epoch=24
03/10/2022 21:44:21 - INFO - __main__ - Step 60 Global step 60 Train loss 4.644236 on epoch=29
03/10/2022 21:44:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.883618 on epoch=34
03/10/2022 21:44:30 - INFO - __main__ - Step 80 Global step 80 Train loss 1.817116 on epoch=39
03/10/2022 21:44:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.388940 on epoch=44
03/10/2022 21:44:40 - INFO - __main__ - Step 100 Global step 100 Train loss 1.531934 on epoch=49
03/10/2022 21:44:41 - INFO - __main__ - Global step 100 Train loss 2.453169 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:44:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.278857 on epoch=54
03/10/2022 21:44:51 - INFO - __main__ - Step 120 Global step 120 Train loss 1.393622 on epoch=59
03/10/2022 21:44:56 - INFO - __main__ - Step 130 Global step 130 Train loss 1.400301 on epoch=64
03/10/2022 21:45:01 - INFO - __main__ - Step 140 Global step 140 Train loss 1.383423 on epoch=69
03/10/2022 21:45:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.754945 on epoch=74
03/10/2022 21:45:07 - INFO - __main__ - Global step 150 Train loss 1.242230 Classification-F1 0.5134502923976608 on epoch=74
03/10/2022 21:45:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.198871 on epoch=79
03/10/2022 21:45:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.032141 on epoch=84
03/10/2022 21:45:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.013867 on epoch=89
03/10/2022 21:45:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.005097 on epoch=94
03/10/2022 21:45:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.002021 on epoch=99
03/10/2022 21:45:32 - INFO - __main__ - Global step 200 Train loss 0.050399 Classification-F1 0.9687194525904204 on epoch=99
03/10/2022 21:45:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.054340 on epoch=104
03/10/2022 21:45:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.003751 on epoch=109
03/10/2022 21:45:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003910 on epoch=114
03/10/2022 21:45:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.007776 on epoch=119
03/10/2022 21:45:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000652 on epoch=124
03/10/2022 21:45:58 - INFO - __main__ - Global step 250 Train loss 0.014086 Classification-F1 1.0 on epoch=124
03/10/2022 21:46:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000507 on epoch=129
03/10/2022 21:46:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000181 on epoch=134
03/10/2022 21:46:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.015663 on epoch=139
03/10/2022 21:46:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001107 on epoch=144
03/10/2022 21:46:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000411 on epoch=149
03/10/2022 21:46:23 - INFO - __main__ - Global step 300 Train loss 0.003574 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 21:46:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000237 on epoch=154
03/10/2022 21:46:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000126 on epoch=159
03/10/2022 21:46:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000148 on epoch=164
03/10/2022 21:46:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000059 on epoch=169
03/10/2022 21:46:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000086 on epoch=174
03/10/2022 21:46:48 - INFO - __main__ - Global step 350 Train loss 0.000131 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 21:46:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000108 on epoch=179
03/10/2022 21:46:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000354 on epoch=184
03/10/2022 21:47:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000044 on epoch=189
03/10/2022 21:47:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000163 on epoch=194
03/10/2022 21:47:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000070 on epoch=199
03/10/2022 21:47:13 - INFO - __main__ - Global step 400 Train loss 0.000148 Classification-F1 0.9372549019607843 on epoch=199
03/10/2022 21:47:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000078 on epoch=204
03/10/2022 21:47:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000104 on epoch=209
03/10/2022 21:47:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001411 on epoch=214
03/10/2022 21:47:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000144 on epoch=219
03/10/2022 21:47:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008945 on epoch=224
03/10/2022 21:47:38 - INFO - __main__ - Global step 450 Train loss 0.002136 Classification-F1 1.0 on epoch=224
03/10/2022 21:47:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.095214 on epoch=229
03/10/2022 21:47:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.117520 on epoch=234
03/10/2022 21:47:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.125420 on epoch=239
03/10/2022 21:47:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002036 on epoch=244
03/10/2022 21:48:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000646 on epoch=249
03/10/2022 21:48:03 - INFO - __main__ - Global step 500 Train loss 0.068167 Classification-F1 0.9372549019607843 on epoch=249
03/10/2022 21:48:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011183 on epoch=254
03/10/2022 21:48:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000514 on epoch=259
03/10/2022 21:48:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004135 on epoch=264
03/10/2022 21:48:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.050202 on epoch=269
03/10/2022 21:48:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001317 on epoch=274
03/10/2022 21:48:28 - INFO - __main__ - Global step 550 Train loss 0.013470 Classification-F1 1.0 on epoch=274
03/10/2022 21:48:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000332 on epoch=279
03/10/2022 21:48:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000218 on epoch=284
03/10/2022 21:48:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000123 on epoch=289
03/10/2022 21:48:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000290 on epoch=294
03/10/2022 21:48:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000118 on epoch=299
03/10/2022 21:48:54 - INFO - __main__ - Global step 600 Train loss 0.000216 Classification-F1 1.0 on epoch=299
03/10/2022 21:48:54 - INFO - __main__ - save last model!
03/10/2022 21:48:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:48:54 - INFO - __main__ - Printing 3 examples
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:48:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:48:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:48:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:48:54 - INFO - __main__ - Printing 3 examples
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:48:54 - INFO - __main__ - ['negative']
03/10/2022 21:48:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:48:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:48:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:49:00 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:49:01 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:49:01 - INFO - __main__ - Printing 3 examples
03/10/2022 21:49:01 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:49:01 - INFO - __main__ - ['negative']
03/10/2022 21:49:01 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:49:01 - INFO - __main__ - ['negative']
03/10/2022 21:49:01 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:49:01 - INFO - __main__ - ['negative']
03/10/2022 21:49:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:49:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:49:02 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:49:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:49:05 - INFO - __main__ - Starting training!
03/10/2022 21:49:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0005_8_predictions.txt
03/10/2022 21:49:17 - INFO - __main__ - Classification-F1 on test data: 0.9510
03/10/2022 21:49:18 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.950999558996031
03/10/2022 21:49:18 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0003, bsz=8 ...
03/10/2022 21:49:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:49:19 - INFO - __main__ - Printing 3 examples
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:49:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:49:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:49:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:49:19 - INFO - __main__ - Printing 3 examples
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:49:19 - INFO - __main__ - ['negative']
03/10/2022 21:49:19 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:49:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:49:19 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:49:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:49:29 - INFO - __main__ - Starting training!
03/10/2022 21:49:34 - INFO - __main__ - Step 10 Global step 10 Train loss 22.384933 on epoch=4
03/10/2022 21:49:39 - INFO - __main__ - Step 20 Global step 20 Train loss 16.365101 on epoch=9
03/10/2022 21:49:44 - INFO - __main__ - Step 30 Global step 30 Train loss 15.862419 on epoch=14
03/10/2022 21:49:49 - INFO - __main__ - Step 40 Global step 40 Train loss 14.650485 on epoch=19
03/10/2022 21:49:54 - INFO - __main__ - Step 50 Global step 50 Train loss 13.043261 on epoch=24
03/10/2022 21:50:03 - INFO - __main__ - Global step 50 Train loss 16.461241 Classification-F1 0.0 on epoch=24
03/10/2022 21:50:09 - INFO - __main__ - Step 60 Global step 60 Train loss 12.311401 on epoch=29
03/10/2022 21:50:14 - INFO - __main__ - Step 70 Global step 70 Train loss 12.148779 on epoch=34
03/10/2022 21:50:19 - INFO - __main__ - Step 80 Global step 80 Train loss 9.549019 on epoch=39
03/10/2022 21:50:24 - INFO - __main__ - Step 90 Global step 90 Train loss 4.959825 on epoch=44
03/10/2022 21:50:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.506199 on epoch=49
03/10/2022 21:50:29 - INFO - __main__ - Global step 100 Train loss 8.095045 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:50:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.318810 on epoch=54
03/10/2022 21:50:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.149518 on epoch=59
03/10/2022 21:50:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.078661 on epoch=64
03/10/2022 21:50:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.027722 on epoch=69
03/10/2022 21:50:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.014712 on epoch=74
03/10/2022 21:50:56 - INFO - __main__ - Global step 150 Train loss 0.117884 Classification-F1 0.873015873015873 on epoch=74
03/10/2022 21:51:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.031640 on epoch=79
03/10/2022 21:51:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.005121 on epoch=84
03/10/2022 21:51:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.011141 on epoch=89
03/10/2022 21:51:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.009484 on epoch=94
03/10/2022 21:51:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.054101 on epoch=99
03/10/2022 21:51:22 - INFO - __main__ - Global step 200 Train loss 0.022298 Classification-F1 0.9372549019607843 on epoch=99
03/10/2022 21:51:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001818 on epoch=104
03/10/2022 21:51:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000835 on epoch=109
03/10/2022 21:51:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001030 on epoch=114
03/10/2022 21:51:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000991 on epoch=119
03/10/2022 21:51:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001488 on epoch=124
03/10/2022 21:51:48 - INFO - __main__ - Global step 250 Train loss 0.001233 Classification-F1 0.9372549019607843 on epoch=124
03/10/2022 21:51:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000928 on epoch=129
03/10/2022 21:51:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.003781 on epoch=134
03/10/2022 21:52:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002514 on epoch=139
03/10/2022 21:52:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000590 on epoch=144
03/10/2022 21:52:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000347 on epoch=149
03/10/2022 21:52:14 - INFO - __main__ - Global step 300 Train loss 0.001632 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 21:52:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000056 on epoch=154
03/10/2022 21:52:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000185 on epoch=159
03/10/2022 21:52:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000124 on epoch=164
03/10/2022 21:52:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000100 on epoch=169
03/10/2022 21:52:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000079 on epoch=174
03/10/2022 21:52:40 - INFO - __main__ - Global step 350 Train loss 0.000109 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 21:52:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000184 on epoch=179
03/10/2022 21:52:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000111 on epoch=184
03/10/2022 21:52:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000069 on epoch=189
03/10/2022 21:53:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000037 on epoch=194
03/10/2022 21:53:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000068 on epoch=199
03/10/2022 21:53:06 - INFO - __main__ - Global step 400 Train loss 0.000094 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 21:53:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000031 on epoch=204
03/10/2022 21:53:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001879 on epoch=209
03/10/2022 21:53:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000056 on epoch=214
03/10/2022 21:53:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000232 on epoch=219
03/10/2022 21:53:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000097 on epoch=224
03/10/2022 21:53:31 - INFO - __main__ - Global step 450 Train loss 0.000459 Classification-F1 0.9372549019607843 on epoch=224
03/10/2022 21:53:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000155 on epoch=229
03/10/2022 21:53:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000046 on epoch=234
03/10/2022 21:53:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000041 on epoch=239
03/10/2022 21:53:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000054 on epoch=244
03/10/2022 21:53:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000190 on epoch=249
03/10/2022 21:53:57 - INFO - __main__ - Global step 500 Train loss 0.000097 Classification-F1 0.9375 on epoch=249
03/10/2022 21:54:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000088 on epoch=254
03/10/2022 21:54:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000211 on epoch=259
03/10/2022 21:54:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000017 on epoch=264
03/10/2022 21:54:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000019 on epoch=269
03/10/2022 21:54:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000018 on epoch=274
03/10/2022 21:54:22 - INFO - __main__ - Global step 550 Train loss 0.000071 Classification-F1 0.9372549019607843 on epoch=274
03/10/2022 21:54:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000038 on epoch=279
03/10/2022 21:54:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000068 on epoch=284
03/10/2022 21:54:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000172 on epoch=289
03/10/2022 21:54:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000010 on epoch=294
03/10/2022 21:54:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000009 on epoch=299
03/10/2022 21:54:48 - INFO - __main__ - Global step 600 Train loss 0.000059 Classification-F1 0.9372549019607843 on epoch=299
03/10/2022 21:54:48 - INFO - __main__ - save last model!
03/10/2022 21:54:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:54:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:54:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:54:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:54:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:54:49 - INFO - __main__ - Printing 3 examples
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:54:49 - INFO - __main__ - ['negative']
03/10/2022 21:54:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:54:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:54:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:54:55 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 21:54:56 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:54:56 - INFO - __main__ - Printing 3 examples
03/10/2022 21:54:56 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:54:56 - INFO - __main__ - ['negative']
03/10/2022 21:54:56 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:54:56 - INFO - __main__ - ['negative']
03/10/2022 21:54:56 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:54:56 - INFO - __main__ - ['negative']
03/10/2022 21:54:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:54:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:54:57 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:54:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:55:00 - INFO - __main__ - Starting training!
03/10/2022 21:55:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0003_8_predictions.txt
03/10/2022 21:55:12 - INFO - __main__ - Classification-F1 on test data: 0.9098
03/10/2022 21:55:12 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9098409594524741
03/10/2022 21:55:12 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0002, bsz=8 ...
03/10/2022 21:55:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:55:13 - INFO - __main__ - Printing 3 examples
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:55:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:55:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:55:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:55:13 - INFO - __main__ - Printing 3 examples
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 21:55:13 - INFO - __main__ - ['negative']
03/10/2022 21:55:13 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:55:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:55:13 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:55:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 21:55:25 - INFO - __main__ - Starting training!
03/10/2022 21:55:29 - INFO - __main__ - Step 10 Global step 10 Train loss 23.860373 on epoch=4
03/10/2022 21:55:34 - INFO - __main__ - Step 20 Global step 20 Train loss 17.793711 on epoch=9
03/10/2022 21:55:39 - INFO - __main__ - Step 30 Global step 30 Train loss 16.316132 on epoch=14
03/10/2022 21:55:44 - INFO - __main__ - Step 40 Global step 40 Train loss 15.109976 on epoch=19
03/10/2022 21:55:50 - INFO - __main__ - Step 50 Global step 50 Train loss 14.917964 on epoch=24
03/10/2022 21:56:00 - INFO - __main__ - Global step 50 Train loss 17.599630 Classification-F1 0.0 on epoch=24
03/10/2022 21:56:06 - INFO - __main__ - Step 60 Global step 60 Train loss 13.591158 on epoch=29
03/10/2022 21:56:11 - INFO - __main__ - Step 70 Global step 70 Train loss 13.714678 on epoch=34
03/10/2022 21:56:16 - INFO - __main__ - Step 80 Global step 80 Train loss 13.165262 on epoch=39
03/10/2022 21:56:21 - INFO - __main__ - Step 90 Global step 90 Train loss 11.688146 on epoch=44
03/10/2022 21:56:26 - INFO - __main__ - Step 100 Global step 100 Train loss 10.676100 on epoch=49
03/10/2022 21:56:34 - INFO - __main__ - Global step 100 Train loss 12.567069 Classification-F1 0.02915601023017903 on epoch=49
03/10/2022 21:56:40 - INFO - __main__ - Step 110 Global step 110 Train loss 8.411632 on epoch=54
03/10/2022 21:56:45 - INFO - __main__ - Step 120 Global step 120 Train loss 2.792435 on epoch=59
03/10/2022 21:56:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.403059 on epoch=64
03/10/2022 21:56:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.244948 on epoch=69
03/10/2022 21:57:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.126165 on epoch=74
03/10/2022 21:57:00 - INFO - __main__ - Global step 150 Train loss 2.395648 Classification-F1 0.906158357771261 on epoch=74
03/10/2022 21:57:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.129094 on epoch=79
03/10/2022 21:57:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.059671 on epoch=84
03/10/2022 21:57:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.098063 on epoch=89
03/10/2022 21:57:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.035027 on epoch=94
03/10/2022 21:57:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.014268 on epoch=99
03/10/2022 21:57:27 - INFO - __main__ - Global step 200 Train loss 0.067224 Classification-F1 0.9375 on epoch=99
03/10/2022 21:57:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005691 on epoch=104
03/10/2022 21:57:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.008302 on epoch=109
03/10/2022 21:57:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.030871 on epoch=114
03/10/2022 21:57:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.005552 on epoch=119
03/10/2022 21:57:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.006916 on epoch=124
03/10/2022 21:57:53 - INFO - __main__ - Global step 250 Train loss 0.011466 Classification-F1 0.9375 on epoch=124
03/10/2022 21:57:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003948 on epoch=129
03/10/2022 21:58:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.004508 on epoch=134
03/10/2022 21:58:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.020001 on epoch=139
03/10/2022 21:58:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000661 on epoch=144
03/10/2022 21:58:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001497 on epoch=149
03/10/2022 21:58:18 - INFO - __main__ - Global step 300 Train loss 0.006123 Classification-F1 0.9375 on epoch=149
03/10/2022 21:58:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003789 on epoch=154
03/10/2022 21:58:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.019308 on epoch=159
03/10/2022 21:58:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001514 on epoch=164
03/10/2022 21:58:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001783 on epoch=169
03/10/2022 21:58:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000377 on epoch=174
03/10/2022 21:58:44 - INFO - __main__ - Global step 350 Train loss 0.005354 Classification-F1 0.9375 on epoch=174
03/10/2022 21:58:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000457 on epoch=179
03/10/2022 21:58:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000360 on epoch=184
03/10/2022 21:58:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000107 on epoch=189
03/10/2022 21:59:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000346 on epoch=194
03/10/2022 21:59:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000217 on epoch=199
03/10/2022 21:59:10 - INFO - __main__ - Global step 400 Train loss 0.000298 Classification-F1 0.9375 on epoch=199
03/10/2022 21:59:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.019902 on epoch=204
03/10/2022 21:59:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000300 on epoch=209
03/10/2022 21:59:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000169 on epoch=214
03/10/2022 21:59:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000044 on epoch=219
03/10/2022 21:59:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000055 on epoch=224
03/10/2022 21:59:35 - INFO - __main__ - Global step 450 Train loss 0.004094 Classification-F1 0.9372549019607843 on epoch=224
03/10/2022 21:59:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000101 on epoch=229
03/10/2022 21:59:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000299 on epoch=234
03/10/2022 21:59:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006415 on epoch=239
03/10/2022 21:59:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000193 on epoch=244
03/10/2022 22:00:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000154 on epoch=249
03/10/2022 22:00:01 - INFO - __main__ - Global step 500 Train loss 0.001432 Classification-F1 0.9375 on epoch=249
03/10/2022 22:00:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000623 on epoch=254
03/10/2022 22:00:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000158 on epoch=259
03/10/2022 22:00:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000029 on epoch=264
03/10/2022 22:00:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000032 on epoch=269
03/10/2022 22:00:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000040 on epoch=274
03/10/2022 22:00:27 - INFO - __main__ - Global step 550 Train loss 0.000177 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 22:00:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000048 on epoch=279
03/10/2022 22:00:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000042 on epoch=284
03/10/2022 22:00:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000033 on epoch=289
03/10/2022 22:00:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000020 on epoch=294
03/10/2022 22:00:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000168 on epoch=299
03/10/2022 22:00:53 - INFO - __main__ - Global step 600 Train loss 0.000062 Classification-F1 0.9375 on epoch=299
03/10/2022 22:00:53 - INFO - __main__ - save last model!
03/10/2022 22:00:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:00:53 - INFO - __main__ - Printing 3 examples
03/10/2022 22:00:53 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 22:00:53 - INFO - __main__ - ['negative']
03/10/2022 22:00:53 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 22:00:53 - INFO - __main__ - ['negative']
03/10/2022 22:00:53 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 22:00:53 - INFO - __main__ - ['negative']
03/10/2022 22:00:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:00:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:00:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:00:54 - INFO - __main__ - Printing 3 examples
03/10/2022 22:00:54 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 22:00:54 - INFO - __main__ - ['negative']
03/10/2022 22:00:54 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 22:00:54 - INFO - __main__ - ['negative']
03/10/2022 22:00:54 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 22:00:54 - INFO - __main__ - ['negative']
03/10/2022 22:00:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:00:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:00:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:01:00 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:01:00 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:01:00 - INFO - __main__ - Printing 3 examples
03/10/2022 22:01:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:01:00 - INFO - __main__ - ['negative']
03/10/2022 22:01:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:01:00 - INFO - __main__ - ['negative']
03/10/2022 22:01:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:01:00 - INFO - __main__ - ['negative']
03/10/2022 22:01:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:01:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:01:02 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:01:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:01:04 - INFO - __main__ - Starting training!
03/10/2022 22:01:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0002_8_predictions.txt
03/10/2022 22:01:17 - INFO - __main__ - Classification-F1 on test data: 0.9349
03/10/2022 22:01:17 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9348795923662854
03/10/2022 22:01:17 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0001, bsz=8 ...
03/10/2022 22:01:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:01:18 - INFO - __main__ - Printing 3 examples
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:01:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:01:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:01:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:01:18 - INFO - __main__ - Printing 3 examples
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 22:01:18 - INFO - __main__ - ['negative']
03/10/2022 22:01:18 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:01:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:01:18 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:01:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:01:28 - INFO - __main__ - Starting training!
03/10/2022 22:01:32 - INFO - __main__ - Step 10 Global step 10 Train loss 23.636469 on epoch=4
03/10/2022 22:01:37 - INFO - __main__ - Step 20 Global step 20 Train loss 20.991814 on epoch=9
03/10/2022 22:01:42 - INFO - __main__ - Step 30 Global step 30 Train loss 17.794104 on epoch=14
03/10/2022 22:01:47 - INFO - __main__ - Step 40 Global step 40 Train loss 17.421976 on epoch=19
03/10/2022 22:01:53 - INFO - __main__ - Step 50 Global step 50 Train loss 16.222311 on epoch=24
03/10/2022 22:02:02 - INFO - __main__ - Global step 50 Train loss 19.213333 Classification-F1 0.0 on epoch=24
03/10/2022 22:02:08 - INFO - __main__ - Step 60 Global step 60 Train loss 16.271488 on epoch=29
03/10/2022 22:02:13 - INFO - __main__ - Step 70 Global step 70 Train loss 15.681972 on epoch=34
03/10/2022 22:02:18 - INFO - __main__ - Step 80 Global step 80 Train loss 14.611334 on epoch=39
03/10/2022 22:02:23 - INFO - __main__ - Step 90 Global step 90 Train loss 13.850306 on epoch=44
03/10/2022 22:02:28 - INFO - __main__ - Step 100 Global step 100 Train loss 14.034871 on epoch=49
03/10/2022 22:02:38 - INFO - __main__ - Global step 100 Train loss 14.889994 Classification-F1 0.0 on epoch=49
03/10/2022 22:02:43 - INFO - __main__ - Step 110 Global step 110 Train loss 13.538152 on epoch=54
03/10/2022 22:02:48 - INFO - __main__ - Step 120 Global step 120 Train loss 13.525801 on epoch=59
03/10/2022 22:02:53 - INFO - __main__ - Step 130 Global step 130 Train loss 12.821051 on epoch=64
03/10/2022 22:02:58 - INFO - __main__ - Step 140 Global step 140 Train loss 12.504519 on epoch=69
03/10/2022 22:03:03 - INFO - __main__ - Step 150 Global step 150 Train loss 12.402614 on epoch=74
03/10/2022 22:03:12 - INFO - __main__ - Global step 150 Train loss 12.958426 Classification-F1 0.0 on epoch=74
03/10/2022 22:03:17 - INFO - __main__ - Step 160 Global step 160 Train loss 11.517245 on epoch=79
03/10/2022 22:03:22 - INFO - __main__ - Step 170 Global step 170 Train loss 10.160054 on epoch=84
03/10/2022 22:03:27 - INFO - __main__ - Step 180 Global step 180 Train loss 9.957245 on epoch=89
03/10/2022 22:03:32 - INFO - __main__ - Step 190 Global step 190 Train loss 7.441612 on epoch=94
03/10/2022 22:03:37 - INFO - __main__ - Step 200 Global step 200 Train loss 3.113474 on epoch=99
03/10/2022 22:03:38 - INFO - __main__ - Global step 200 Train loss 8.437926 Classification-F1 0.539313399778516 on epoch=99
03/10/2022 22:03:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.922860 on epoch=104
03/10/2022 22:03:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.486589 on epoch=109
03/10/2022 22:03:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.302556 on epoch=114
03/10/2022 22:03:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.316880 on epoch=119
03/10/2022 22:04:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.113904 on epoch=124
03/10/2022 22:04:04 - INFO - __main__ - Global step 250 Train loss 0.428558 Classification-F1 0.9375 on epoch=124
03/10/2022 22:04:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.104457 on epoch=129
03/10/2022 22:04:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.148595 on epoch=134
03/10/2022 22:04:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.072988 on epoch=139
03/10/2022 22:04:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.054976 on epoch=144
03/10/2022 22:04:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.023366 on epoch=149
03/10/2022 22:04:30 - INFO - __main__ - Global step 300 Train loss 0.080876 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 22:04:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.031931 on epoch=154
03/10/2022 22:04:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.036194 on epoch=159
03/10/2022 22:04:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.036948 on epoch=164
03/10/2022 22:04:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.015061 on epoch=169
03/10/2022 22:04:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.018843 on epoch=174
03/10/2022 22:04:56 - INFO - __main__ - Global step 350 Train loss 0.027795 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 22:05:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.005130 on epoch=179
03/10/2022 22:05:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009297 on epoch=184
03/10/2022 22:05:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.084081 on epoch=189
03/10/2022 22:05:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028788 on epoch=194
03/10/2022 22:05:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007343 on epoch=199
03/10/2022 22:05:22 - INFO - __main__ - Global step 400 Train loss 0.026928 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 22:05:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001847 on epoch=204
03/10/2022 22:05:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001108 on epoch=209
03/10/2022 22:05:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.101802 on epoch=214
03/10/2022 22:05:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011905 on epoch=219
03/10/2022 22:05:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.022686 on epoch=224
03/10/2022 22:05:47 - INFO - __main__ - Global step 450 Train loss 0.027870 Classification-F1 0.9375 on epoch=224
03/10/2022 22:05:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003783 on epoch=229
03/10/2022 22:05:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.005573 on epoch=234
03/10/2022 22:06:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001114 on epoch=239
03/10/2022 22:06:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005899 on epoch=244
03/10/2022 22:06:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001337 on epoch=249
03/10/2022 22:06:13 - INFO - __main__ - Global step 500 Train loss 0.003541 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 22:06:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000330 on epoch=254
03/10/2022 22:06:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002521 on epoch=259
03/10/2022 22:06:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001333 on epoch=264
03/10/2022 22:06:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000776 on epoch=269
03/10/2022 22:06:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000176 on epoch=274
03/10/2022 22:06:38 - INFO - __main__ - Global step 550 Train loss 0.001027 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 22:06:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000427 on epoch=279
03/10/2022 22:06:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000164 on epoch=284
03/10/2022 22:06:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000126 on epoch=289
03/10/2022 22:06:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000125 on epoch=294
03/10/2022 22:07:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000098 on epoch=299
03/10/2022 22:07:04 - INFO - __main__ - Global step 600 Train loss 0.000188 Classification-F1 0.9375 on epoch=299
03/10/2022 22:07:04 - INFO - __main__ - save last model!
03/10/2022 22:07:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:07:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:07:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:07:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:07:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:07:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:07:05 - INFO - __main__ - ['negative']
03/10/2022 22:07:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:07:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:07:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:07:11 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:07:11 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:07:11 - INFO - __main__ - Printing 3 examples
03/10/2022 22:07:11 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:07:11 - INFO - __main__ - ['negative']
03/10/2022 22:07:11 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:07:11 - INFO - __main__ - ['negative']
03/10/2022 22:07:11 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:07:11 - INFO - __main__ - ['negative']
03/10/2022 22:07:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:07:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:07:13 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:07:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:07:15 - INFO - __main__ - Starting training!
03/10/2022 22:07:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0001_8_predictions.txt
03/10/2022 22:07:29 - INFO - __main__ - Classification-F1 on test data: 0.6205
03/10/2022 22:07:29 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.6204981397493218
03/10/2022 22:07:29 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0005, bsz=8 ...
03/10/2022 22:07:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:07:30 - INFO - __main__ - Printing 3 examples
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:07:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:07:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:07:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:07:30 - INFO - __main__ - Printing 3 examples
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:07:30 - INFO - __main__ - ['negative']
03/10/2022 22:07:30 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:07:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:07:30 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:07:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:07:39 - INFO - __main__ - Starting training!
03/10/2022 22:07:43 - INFO - __main__ - Step 10 Global step 10 Train loss 22.251881 on epoch=4
03/10/2022 22:07:48 - INFO - __main__ - Step 20 Global step 20 Train loss 17.567472 on epoch=9
03/10/2022 22:07:53 - INFO - __main__ - Step 30 Global step 30 Train loss 15.551758 on epoch=14
03/10/2022 22:07:58 - INFO - __main__ - Step 40 Global step 40 Train loss 13.535991 on epoch=19
03/10/2022 22:08:03 - INFO - __main__ - Step 50 Global step 50 Train loss 11.469899 on epoch=24
03/10/2022 22:08:03 - INFO - __main__ - Global step 50 Train loss 16.075401 Classification-F1 0.037037037037037035 on epoch=24
03/10/2022 22:08:09 - INFO - __main__ - Step 60 Global step 60 Train loss 8.815944 on epoch=29
03/10/2022 22:08:13 - INFO - __main__ - Step 70 Global step 70 Train loss 2.821101 on epoch=34
03/10/2022 22:08:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.464259 on epoch=39
03/10/2022 22:08:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.401780 on epoch=44
03/10/2022 22:08:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.352990 on epoch=49
03/10/2022 22:08:28 - INFO - __main__ - Global step 100 Train loss 2.571215 Classification-F1 0.4589371980676329 on epoch=49
03/10/2022 22:08:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.321653 on epoch=54
03/10/2022 22:08:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.307997 on epoch=59
03/10/2022 22:08:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.299973 on epoch=64
03/10/2022 22:08:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.312559 on epoch=69
03/10/2022 22:08:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.229407 on epoch=74
03/10/2022 22:08:54 - INFO - __main__ - Global step 150 Train loss 0.294318 Classification-F1 0.5151515151515151 on epoch=74
03/10/2022 22:09:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.250126 on epoch=79
03/10/2022 22:09:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.214601 on epoch=84
03/10/2022 22:09:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.334930 on epoch=89
03/10/2022 22:09:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.231555 on epoch=94
03/10/2022 22:09:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.222284 on epoch=99
03/10/2022 22:09:20 - INFO - __main__ - Global step 200 Train loss 0.250699 Classification-F1 0.6190476190476191 on epoch=99
03/10/2022 22:09:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.190467 on epoch=104
03/10/2022 22:09:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.110164 on epoch=109
03/10/2022 22:09:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.182844 on epoch=114
03/10/2022 22:09:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.075221 on epoch=119
03/10/2022 22:09:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.033980 on epoch=124
03/10/2022 22:09:45 - INFO - __main__ - Global step 250 Train loss 0.118535 Classification-F1 0.6532019704433498 on epoch=124
03/10/2022 22:09:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.090568 on epoch=129
03/10/2022 22:09:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.032693 on epoch=134
03/10/2022 22:10:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.060853 on epoch=139
03/10/2022 22:10:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.033612 on epoch=144
03/10/2022 22:10:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.018579 on epoch=149
03/10/2022 22:10:11 - INFO - __main__ - Global step 300 Train loss 0.047261 Classification-F1 0.6825396825396826 on epoch=149
03/10/2022 22:10:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.019702 on epoch=154
03/10/2022 22:10:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.032616 on epoch=159
03/10/2022 22:10:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.015770 on epoch=164
03/10/2022 22:10:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.078444 on epoch=169
03/10/2022 22:10:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.637926 on epoch=174
03/10/2022 22:10:37 - INFO - __main__ - Global step 350 Train loss 0.156892 Classification-F1 0.49090909090909085 on epoch=174
03/10/2022 22:10:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.428656 on epoch=179
03/10/2022 22:10:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.373516 on epoch=184
03/10/2022 22:10:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.304782 on epoch=189
03/10/2022 22:10:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.287166 on epoch=194
03/10/2022 22:11:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.231985 on epoch=199
03/10/2022 22:11:02 - INFO - __main__ - Global step 400 Train loss 0.325221 Classification-F1 0.7793103448275862 on epoch=199
03/10/2022 22:11:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.239375 on epoch=204
03/10/2022 22:11:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.189758 on epoch=209
03/10/2022 22:11:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.194997 on epoch=214
03/10/2022 22:11:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.209693 on epoch=219
03/10/2022 22:11:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.209705 on epoch=224
03/10/2022 22:11:27 - INFO - __main__ - Global step 450 Train loss 0.208706 Classification-F1 0.7117117117117117 on epoch=224
03/10/2022 22:11:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.163781 on epoch=229
03/10/2022 22:11:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.136325 on epoch=234
03/10/2022 22:11:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.220499 on epoch=239
03/10/2022 22:11:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.148210 on epoch=244
03/10/2022 22:11:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.106060 on epoch=249
03/10/2022 22:11:52 - INFO - __main__ - Global step 500 Train loss 0.154975 Classification-F1 0.7810361681329424 on epoch=249
03/10/2022 22:11:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.056073 on epoch=254
03/10/2022 22:12:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.065716 on epoch=259
03/10/2022 22:12:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.068170 on epoch=264
03/10/2022 22:12:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.162447 on epoch=269
03/10/2022 22:12:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.071408 on epoch=274
03/10/2022 22:12:18 - INFO - __main__ - Global step 550 Train loss 0.084763 Classification-F1 0.8095238095238095 on epoch=274
03/10/2022 22:12:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.021615 on epoch=279
03/10/2022 22:12:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.178152 on epoch=284
03/10/2022 22:12:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.039852 on epoch=289
03/10/2022 22:12:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.083398 on epoch=294
03/10/2022 22:12:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037946 on epoch=299
03/10/2022 22:12:43 - INFO - __main__ - Global step 600 Train loss 0.072193 Classification-F1 0.6389743589743591 on epoch=299
03/10/2022 22:12:43 - INFO - __main__ - save last model!
03/10/2022 22:12:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:12:44 - INFO - __main__ - Printing 3 examples
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:12:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:12:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:12:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:12:44 - INFO - __main__ - Printing 3 examples
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:12:44 - INFO - __main__ - ['negative']
03/10/2022 22:12:44 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:12:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:12:44 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:12:50 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:12:51 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:12:51 - INFO - __main__ - Printing 3 examples
03/10/2022 22:12:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:12:51 - INFO - __main__ - ['negative']
03/10/2022 22:12:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:12:51 - INFO - __main__ - ['negative']
03/10/2022 22:12:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:12:51 - INFO - __main__ - ['negative']
03/10/2022 22:12:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:12:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:12:52 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:12:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:12:53 - INFO - __main__ - Starting training!
03/10/2022 22:13:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0005_8_predictions.txt
03/10/2022 22:13:07 - INFO - __main__ - Classification-F1 on test data: 0.6672
03/10/2022 22:13:07 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.8095238095238095, test_performance=0.6672077922077921
03/10/2022 22:13:07 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0003, bsz=8 ...
03/10/2022 22:13:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:13:08 - INFO - __main__ - Printing 3 examples
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:13:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:13:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:13:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:13:08 - INFO - __main__ - Printing 3 examples
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:13:08 - INFO - __main__ - ['negative']
03/10/2022 22:13:08 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:13:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:13:08 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:13:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:13:18 - INFO - __main__ - Starting training!
03/10/2022 22:13:25 - INFO - __main__ - Step 10 Global step 10 Train loss 21.368879 on epoch=4
03/10/2022 22:13:30 - INFO - __main__ - Step 20 Global step 20 Train loss 16.734297 on epoch=9
03/10/2022 22:13:34 - INFO - __main__ - Step 30 Global step 30 Train loss 15.040625 on epoch=14
03/10/2022 22:13:39 - INFO - __main__ - Step 40 Global step 40 Train loss 14.343817 on epoch=19
03/10/2022 22:13:44 - INFO - __main__ - Step 50 Global step 50 Train loss 13.159329 on epoch=24
03/10/2022 22:13:47 - INFO - __main__ - Global step 50 Train loss 16.129389 Classification-F1 0.0 on epoch=24
03/10/2022 22:13:52 - INFO - __main__ - Step 60 Global step 60 Train loss 11.777392 on epoch=29
03/10/2022 22:13:57 - INFO - __main__ - Step 70 Global step 70 Train loss 6.640307 on epoch=34
03/10/2022 22:14:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.903592 on epoch=39
03/10/2022 22:14:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.408728 on epoch=44
03/10/2022 22:14:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.427914 on epoch=49
03/10/2022 22:14:11 - INFO - __main__ - Global step 100 Train loss 4.231587 Classification-F1 0.3992490613266583 on epoch=49
03/10/2022 22:14:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.347317 on epoch=54
03/10/2022 22:14:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.375125 on epoch=59
03/10/2022 22:14:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.205902 on epoch=64
03/10/2022 22:14:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.175323 on epoch=69
03/10/2022 22:14:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.060962 on epoch=74
03/10/2022 22:14:37 - INFO - __main__ - Global step 150 Train loss 0.232926 Classification-F1 0.9687194525904204 on epoch=74
03/10/2022 22:14:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.165163 on epoch=79
03/10/2022 22:14:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.026812 on epoch=84
03/10/2022 22:14:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.007364 on epoch=89
03/10/2022 22:14:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.003591 on epoch=94
03/10/2022 22:15:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000737 on epoch=99
03/10/2022 22:15:02 - INFO - __main__ - Global step 200 Train loss 0.040733 Classification-F1 0.9687194525904204 on epoch=99
03/10/2022 22:15:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.016254 on epoch=104
03/10/2022 22:15:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000882 on epoch=109
03/10/2022 22:15:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000475 on epoch=114
03/10/2022 22:15:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000983 on epoch=119
03/10/2022 22:15:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000171 on epoch=124
03/10/2022 22:15:26 - INFO - __main__ - Global step 250 Train loss 0.003753 Classification-F1 0.9687194525904204 on epoch=124
03/10/2022 22:15:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001272 on epoch=129
03/10/2022 22:15:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000229 on epoch=134
03/10/2022 22:15:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000189 on epoch=139
03/10/2022 22:15:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.091454 on epoch=144
03/10/2022 22:15:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.020303 on epoch=149
03/10/2022 22:15:51 - INFO - __main__ - Global step 300 Train loss 0.022689 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 22:15:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.166255 on epoch=154
03/10/2022 22:16:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003675 on epoch=159
03/10/2022 22:16:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002011 on epoch=164
03/10/2022 22:16:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001172 on epoch=169
03/10/2022 22:16:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001479 on epoch=174
03/10/2022 22:16:15 - INFO - __main__ - Global step 350 Train loss 0.034918 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 22:16:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000633 on epoch=179
03/10/2022 22:16:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000361 on epoch=184
03/10/2022 22:16:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000231 on epoch=189
03/10/2022 22:16:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000111 on epoch=194
03/10/2022 22:16:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000161 on epoch=199
03/10/2022 22:16:39 - INFO - __main__ - Global step 400 Train loss 0.000300 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 22:16:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000114 on epoch=204
03/10/2022 22:16:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000081 on epoch=209
03/10/2022 22:16:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000074 on epoch=214
03/10/2022 22:16:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000084 on epoch=219
03/10/2022 22:17:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013712 on epoch=224
03/10/2022 22:17:04 - INFO - __main__ - Global step 450 Train loss 0.002813 Classification-F1 0.9372549019607843 on epoch=224
03/10/2022 22:17:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000869 on epoch=229
03/10/2022 22:17:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000177 on epoch=234
03/10/2022 22:17:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000102 on epoch=239
03/10/2022 22:17:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000325 on epoch=244
03/10/2022 22:17:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000743 on epoch=249
03/10/2022 22:17:28 - INFO - __main__ - Global step 500 Train loss 0.000443 Classification-F1 0.9372549019607843 on epoch=249
03/10/2022 22:17:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000361 on epoch=254
03/10/2022 22:17:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000151 on epoch=259
03/10/2022 22:17:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000459 on epoch=264
03/10/2022 22:17:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000279 on epoch=269
03/10/2022 22:17:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000050 on epoch=274
03/10/2022 22:17:52 - INFO - __main__ - Global step 550 Train loss 0.000260 Classification-F1 0.9372549019607843 on epoch=274
03/10/2022 22:17:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000065 on epoch=279
03/10/2022 22:18:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000132 on epoch=284
03/10/2022 22:18:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000401 on epoch=289
03/10/2022 22:18:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000101 on epoch=294
03/10/2022 22:18:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000013 on epoch=299
03/10/2022 22:18:17 - INFO - __main__ - Global step 600 Train loss 0.000143 Classification-F1 0.9372549019607843 on epoch=299
03/10/2022 22:18:17 - INFO - __main__ - save last model!
03/10/2022 22:18:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:18:18 - INFO - __main__ - Printing 3 examples
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:18:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:18:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:18:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:18:18 - INFO - __main__ - Printing 3 examples
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:18:18 - INFO - __main__ - ['negative']
03/10/2022 22:18:18 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:18:18 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:18:18 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:18:24 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:18:24 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:18:24 - INFO - __main__ - Printing 3 examples
03/10/2022 22:18:24 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:18:24 - INFO - __main__ - ['negative']
03/10/2022 22:18:24 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:18:24 - INFO - __main__ - ['negative']
03/10/2022 22:18:24 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:18:24 - INFO - __main__ - ['negative']
03/10/2022 22:18:24 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:18:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:18:26 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:18:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:18:28 - INFO - __main__ - Starting training!
03/10/2022 22:18:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0003_8_predictions.txt
03/10/2022 22:18:40 - INFO - __main__ - Classification-F1 on test data: 0.6270
03/10/2022 22:18:40 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.6269654371420562
03/10/2022 22:18:40 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0002, bsz=8 ...
03/10/2022 22:18:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:18:41 - INFO - __main__ - Printing 3 examples
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:18:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:18:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:18:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:18:41 - INFO - __main__ - Printing 3 examples
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:18:41 - INFO - __main__ - ['negative']
03/10/2022 22:18:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:18:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:18:41 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:18:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:18:52 - INFO - __main__ - Starting training!
03/10/2022 22:18:56 - INFO - __main__ - Step 10 Global step 10 Train loss 22.688114 on epoch=4
03/10/2022 22:19:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.796892 on epoch=9
03/10/2022 22:19:05 - INFO - __main__ - Step 30 Global step 30 Train loss 16.338926 on epoch=14
03/10/2022 22:19:10 - INFO - __main__ - Step 40 Global step 40 Train loss 15.414701 on epoch=19
03/10/2022 22:19:15 - INFO - __main__ - Step 50 Global step 50 Train loss 15.290901 on epoch=24
03/10/2022 22:19:24 - INFO - __main__ - Global step 50 Train loss 17.705908 Classification-F1 0.0 on epoch=24
03/10/2022 22:19:29 - INFO - __main__ - Step 60 Global step 60 Train loss 14.858658 on epoch=29
03/10/2022 22:19:34 - INFO - __main__ - Step 70 Global step 70 Train loss 14.334132 on epoch=34
03/10/2022 22:19:39 - INFO - __main__ - Step 80 Global step 80 Train loss 13.247999 on epoch=39
03/10/2022 22:19:44 - INFO - __main__ - Step 90 Global step 90 Train loss 12.469442 on epoch=44
03/10/2022 22:19:49 - INFO - __main__ - Step 100 Global step 100 Train loss 11.311871 on epoch=49
03/10/2022 22:19:56 - INFO - __main__ - Global step 100 Train loss 13.244420 Classification-F1 0.004901960784313725 on epoch=49
03/10/2022 22:20:01 - INFO - __main__ - Step 110 Global step 110 Train loss 11.051880 on epoch=54
03/10/2022 22:20:06 - INFO - __main__ - Step 120 Global step 120 Train loss 9.712967 on epoch=59
03/10/2022 22:20:11 - INFO - __main__ - Step 130 Global step 130 Train loss 6.024409 on epoch=64
03/10/2022 22:20:16 - INFO - __main__ - Step 140 Global step 140 Train loss 4.397892 on epoch=69
03/10/2022 22:20:21 - INFO - __main__ - Step 150 Global step 150 Train loss 2.989900 on epoch=74
03/10/2022 22:20:21 - INFO - __main__ - Global step 150 Train loss 6.835410 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 22:20:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.679665 on epoch=79
03/10/2022 22:20:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.840544 on epoch=84
03/10/2022 22:20:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.319929 on epoch=89
03/10/2022 22:20:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.222887 on epoch=94
03/10/2022 22:20:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.167890 on epoch=99
03/10/2022 22:20:46 - INFO - __main__ - Global step 200 Train loss 0.646183 Classification-F1 0.906158357771261 on epoch=99
03/10/2022 22:20:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.056543 on epoch=104
03/10/2022 22:20:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.068476 on epoch=109
03/10/2022 22:21:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.012718 on epoch=114
03/10/2022 22:21:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.022298 on epoch=119
03/10/2022 22:21:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.038339 on epoch=124
03/10/2022 22:21:12 - INFO - __main__ - Global step 250 Train loss 0.039675 Classification-F1 0.9687194525904204 on epoch=124
03/10/2022 22:21:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.008449 on epoch=129
03/10/2022 22:21:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.028437 on epoch=134
03/10/2022 22:21:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.038380 on epoch=139
03/10/2022 22:21:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.090657 on epoch=144
03/10/2022 22:21:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.014863 on epoch=149
03/10/2022 22:21:37 - INFO - __main__ - Global step 300 Train loss 0.036157 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 22:21:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.016899 on epoch=154
03/10/2022 22:21:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001072 on epoch=159
03/10/2022 22:21:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.037947 on epoch=164
03/10/2022 22:21:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.024001 on epoch=169
03/10/2022 22:22:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.088300 on epoch=174
03/10/2022 22:22:01 - INFO - __main__ - Global step 350 Train loss 0.033644 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 22:22:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006278 on epoch=179
03/10/2022 22:22:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.007735 on epoch=184
03/10/2022 22:22:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.005009 on epoch=189
03/10/2022 22:22:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025292 on epoch=194
03/10/2022 22:22:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.032200 on epoch=199
03/10/2022 22:22:26 - INFO - __main__ - Global step 400 Train loss 0.015303 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 22:22:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.031764 on epoch=204
03/10/2022 22:22:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.043734 on epoch=209
03/10/2022 22:22:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.035867 on epoch=214
03/10/2022 22:22:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008236 on epoch=219
03/10/2022 22:22:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.023809 on epoch=224
03/10/2022 22:22:50 - INFO - __main__ - Global step 450 Train loss 0.028682 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 22:22:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.010348 on epoch=229
03/10/2022 22:23:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.008600 on epoch=234
03/10/2022 22:23:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.043051 on epoch=239
03/10/2022 22:23:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.034173 on epoch=244
03/10/2022 22:23:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013083 on epoch=249
03/10/2022 22:23:15 - INFO - __main__ - Global step 500 Train loss 0.021851 Classification-F1 0.8745098039215686 on epoch=249
03/10/2022 22:23:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.030559 on epoch=254
03/10/2022 22:23:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.008252 on epoch=259
03/10/2022 22:23:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.022274 on epoch=264
03/10/2022 22:23:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003891 on epoch=269
03/10/2022 22:23:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.012610 on epoch=274
03/10/2022 22:23:39 - INFO - __main__ - Global step 550 Train loss 0.015517 Classification-F1 0.9372549019607843 on epoch=274
03/10/2022 22:23:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007819 on epoch=279
03/10/2022 22:23:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015584 on epoch=284
03/10/2022 22:23:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.025462 on epoch=289
03/10/2022 22:23:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006122 on epoch=294
03/10/2022 22:24:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.019564 on epoch=299
03/10/2022 22:24:04 - INFO - __main__ - Global step 600 Train loss 0.014910 Classification-F1 0.9687194525904204 on epoch=299
03/10/2022 22:24:04 - INFO - __main__ - save last model!
03/10/2022 22:24:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:24:04 - INFO - __main__ - Printing 3 examples
03/10/2022 22:24:04 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:24:04 - INFO - __main__ - ['negative']
03/10/2022 22:24:04 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:24:04 - INFO - __main__ - ['negative']
03/10/2022 22:24:04 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:24:04 - INFO - __main__ - ['negative']
03/10/2022 22:24:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:24:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:24:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:24:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:24:05 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:24:05 - INFO - __main__ - ['negative']
03/10/2022 22:24:05 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:24:05 - INFO - __main__ - ['negative']
03/10/2022 22:24:05 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:24:05 - INFO - __main__ - ['negative']
03/10/2022 22:24:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:24:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:24:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:24:11 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:24:11 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:24:11 - INFO - __main__ - Printing 3 examples
03/10/2022 22:24:11 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:24:11 - INFO - __main__ - ['negative']
03/10/2022 22:24:11 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:24:11 - INFO - __main__ - ['negative']
03/10/2022 22:24:11 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:24:11 - INFO - __main__ - ['negative']
03/10/2022 22:24:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:24:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:24:13 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:24:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:24:14 - INFO - __main__ - Starting training!
03/10/2022 22:24:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0002_8_predictions.txt
03/10/2022 22:24:29 - INFO - __main__ - Classification-F1 on test data: 0.8934
03/10/2022 22:24:29 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.89344763252702
03/10/2022 22:24:29 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0001, bsz=8 ...
03/10/2022 22:24:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:24:30 - INFO - __main__ - Printing 3 examples
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:24:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:24:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:24:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:24:30 - INFO - __main__ - Printing 3 examples
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/10/2022 22:24:30 - INFO - __main__ - ['negative']
03/10/2022 22:24:30 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:24:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:24:30 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:24:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:24:40 - INFO - __main__ - Starting training!
03/10/2022 22:24:44 - INFO - __main__ - Step 10 Global step 10 Train loss 22.171103 on epoch=4
03/10/2022 22:24:49 - INFO - __main__ - Step 20 Global step 20 Train loss 18.792810 on epoch=9
03/10/2022 22:24:54 - INFO - __main__ - Step 30 Global step 30 Train loss 19.388229 on epoch=14
03/10/2022 22:24:58 - INFO - __main__ - Step 40 Global step 40 Train loss 17.818981 on epoch=19
03/10/2022 22:25:03 - INFO - __main__ - Step 50 Global step 50 Train loss 16.639771 on epoch=24
03/10/2022 22:25:11 - INFO - __main__ - Global step 50 Train loss 18.962179 Classification-F1 0.0 on epoch=24
03/10/2022 22:25:17 - INFO - __main__ - Step 60 Global step 60 Train loss 16.090057 on epoch=29
03/10/2022 22:25:21 - INFO - __main__ - Step 70 Global step 70 Train loss 15.288417 on epoch=34
03/10/2022 22:25:26 - INFO - __main__ - Step 80 Global step 80 Train loss 15.155014 on epoch=39
03/10/2022 22:25:31 - INFO - __main__ - Step 90 Global step 90 Train loss 15.282814 on epoch=44
03/10/2022 22:25:36 - INFO - __main__ - Step 100 Global step 100 Train loss 13.854181 on epoch=49
03/10/2022 22:25:42 - INFO - __main__ - Global step 100 Train loss 15.134097 Classification-F1 0.0 on epoch=49
03/10/2022 22:25:47 - INFO - __main__ - Step 110 Global step 110 Train loss 13.961119 on epoch=54
03/10/2022 22:25:51 - INFO - __main__ - Step 120 Global step 120 Train loss 13.306704 on epoch=59
03/10/2022 22:25:56 - INFO - __main__ - Step 130 Global step 130 Train loss 13.020518 on epoch=64
03/10/2022 22:26:01 - INFO - __main__ - Step 140 Global step 140 Train loss 12.639707 on epoch=69
03/10/2022 22:26:06 - INFO - __main__ - Step 150 Global step 150 Train loss 12.619484 on epoch=74
03/10/2022 22:26:10 - INFO - __main__ - Global step 150 Train loss 13.109506 Classification-F1 0.0 on epoch=74
03/10/2022 22:26:14 - INFO - __main__ - Step 160 Global step 160 Train loss 11.827681 on epoch=79
03/10/2022 22:26:19 - INFO - __main__ - Step 170 Global step 170 Train loss 11.969781 on epoch=84
03/10/2022 22:26:24 - INFO - __main__ - Step 180 Global step 180 Train loss 11.126127 on epoch=89
03/10/2022 22:26:29 - INFO - __main__ - Step 190 Global step 190 Train loss 9.877627 on epoch=94
03/10/2022 22:26:34 - INFO - __main__ - Step 200 Global step 200 Train loss 9.762724 on epoch=99
03/10/2022 22:26:37 - INFO - __main__ - Global step 200 Train loss 10.912788 Classification-F1 0.0056022408963585435 on epoch=99
03/10/2022 22:26:43 - INFO - __main__ - Step 210 Global step 210 Train loss 8.789755 on epoch=104
03/10/2022 22:26:48 - INFO - __main__ - Step 220 Global step 220 Train loss 7.069996 on epoch=109
03/10/2022 22:26:52 - INFO - __main__ - Step 230 Global step 230 Train loss 2.689274 on epoch=114
03/10/2022 22:26:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.869983 on epoch=119
03/10/2022 22:27:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.502263 on epoch=124
03/10/2022 22:27:02 - INFO - __main__ - Global step 250 Train loss 3.984254 Classification-F1 0.9372549019607843 on epoch=124
03/10/2022 22:27:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.248116 on epoch=129
03/10/2022 22:27:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.148988 on epoch=134
03/10/2022 22:27:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.144560 on epoch=139
03/10/2022 22:27:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.059206 on epoch=144
03/10/2022 22:27:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.176871 on epoch=149
03/10/2022 22:27:27 - INFO - __main__ - Global step 300 Train loss 0.155548 Classification-F1 0.9687194525904204 on epoch=149
03/10/2022 22:27:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.042185 on epoch=154
03/10/2022 22:27:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040908 on epoch=159
03/10/2022 22:27:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.080848 on epoch=164
03/10/2022 22:27:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012471 on epoch=169
03/10/2022 22:27:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011343 on epoch=174
03/10/2022 22:27:53 - INFO - __main__ - Global step 350 Train loss 0.037551 Classification-F1 0.9687194525904204 on epoch=174
03/10/2022 22:27:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.014025 on epoch=179
03/10/2022 22:28:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009443 on epoch=184
03/10/2022 22:28:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.056932 on epoch=189
03/10/2022 22:28:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.022352 on epoch=194
03/10/2022 22:28:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009719 on epoch=199
03/10/2022 22:28:17 - INFO - __main__ - Global step 400 Train loss 0.022494 Classification-F1 0.9687194525904204 on epoch=199
03/10/2022 22:28:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.004688 on epoch=204
03/10/2022 22:28:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.077008 on epoch=209
03/10/2022 22:28:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011383 on epoch=214
03/10/2022 22:28:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.010205 on epoch=219
03/10/2022 22:28:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.219684 on epoch=224
03/10/2022 22:28:42 - INFO - __main__ - Global step 450 Train loss 0.064593 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 22:28:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.077457 on epoch=229
03/10/2022 22:28:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026050 on epoch=234
03/10/2022 22:28:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003592 on epoch=239
03/10/2022 22:29:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.082107 on epoch=244
03/10/2022 22:29:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005227 on epoch=249
03/10/2022 22:29:07 - INFO - __main__ - Global step 500 Train loss 0.038886 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 22:29:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001551 on epoch=254
03/10/2022 22:29:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.014485 on epoch=259
03/10/2022 22:29:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000494 on epoch=264
03/10/2022 22:29:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018314 on epoch=269
03/10/2022 22:29:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001013 on epoch=274
03/10/2022 22:29:32 - INFO - __main__ - Global step 550 Train loss 0.007172 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 22:29:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000756 on epoch=279
03/10/2022 22:29:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001092 on epoch=284
03/10/2022 22:29:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.059038 on epoch=289
03/10/2022 22:29:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001986 on epoch=294
03/10/2022 22:29:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005936 on epoch=299
03/10/2022 22:29:57 - INFO - __main__ - Global step 600 Train loss 0.013762 Classification-F1 0.9687194525904204 on epoch=299
03/10/2022 22:29:57 - INFO - __main__ - save last model!
03/10/2022 22:30:05 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:30:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:30:06 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:30:06 - INFO - __main__ - ['negative']
03/10/2022 22:30:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:30:06 - INFO - __main__ - ['negative']
03/10/2022 22:30:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:30:06 - INFO - __main__ - ['negative']
03/10/2022 22:30:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:30:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:07 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:30:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0001_8_predictions.txt
03/10/2022 22:30:22 - INFO - __main__ - Classification-F1 on test data: 0.9259
03/10/2022 22:30:22 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9258564581028872
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0005388259887695312 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15578", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 7027, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15579", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 7027, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 7027, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15666): No such process
Task: tweet_eval-irony, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_mbltigh3/none_zmm9eoo3
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mbltigh3/none_zmm9eoo3/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mbltigh3/none_zmm9eoo3/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/10/2022 22:30:28 - INFO - __main__ - Namespace(task_dir='data/tweet_eval-irony/', task_name='tweet_eval-irony', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-tweet_eval-irony', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 22:30:28 - INFO - __main__ - models/T5-large-ft-random/singletask-tweet_eval-irony
03/10/2022 22:30:28 - INFO - __main__ - Namespace(task_dir='data/tweet_eval-irony/', task_name='tweet_eval-irony', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-tweet_eval-irony', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/10/2022 22:30:28 - INFO - __main__ - models/T5-large-ft-random/singletask-tweet_eval-irony
03/10/2022 22:30:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 22:30:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 22:30:29 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 22:30:29 - INFO - __main__ - args.device: cuda:0
03/10/2022 22:30:29 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 22:30:29 - INFO - __main__ - Using 2 gpus
03/10/2022 22:30:29 - INFO - __main__ - args.device: cuda:1
03/10/2022 22:30:29 - INFO - __main__ - Using 2 gpus
03/10/2022 22:30:29 - INFO - __main__ - Fine-tuning the following samples: ['tweet_eval-irony_16_100', 'tweet_eval-irony_16_13', 'tweet_eval-irony_16_21', 'tweet_eval-irony_16_42', 'tweet_eval-irony_16_87']
03/10/2022 22:30:29 - INFO - __main__ - Fine-tuning the following samples: ['tweet_eval-irony_16_100', 'tweet_eval-irony_16_13', 'tweet_eval-irony_16_21', 'tweet_eval-irony_16_42', 'tweet_eval-irony_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 22:30:34 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.0005, bsz=8 ...
03/10/2022 22:30:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:34 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:34 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:34 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:30:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:35 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:30:35 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:30:35 - INFO - __main__ - ['hate']
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:30:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:30:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:30:46 - INFO - __main__ - Starting training!
03/10/2022 22:30:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:30:48 - INFO - __main__ - Starting training!
03/10/2022 22:30:54 - INFO - __main__ - Step 10 Global step 10 Train loss 20.807301 on epoch=4
03/10/2022 22:30:58 - INFO - __main__ - Step 20 Global step 20 Train loss 14.048007 on epoch=9
03/10/2022 22:31:03 - INFO - __main__ - Step 30 Global step 30 Train loss 10.732668 on epoch=14
03/10/2022 22:31:08 - INFO - __main__ - Step 40 Global step 40 Train loss 8.774923 on epoch=19
03/10/2022 22:31:13 - INFO - __main__ - Step 50 Global step 50 Train loss 7.691704 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 22:31:13 - INFO - __main__ - Global step 50 Train loss 12.410920 Classification-F1 0.0 on epoch=24
03/10/2022 22:31:18 - INFO - __main__ - Step 60 Global step 60 Train loss 5.236454 on epoch=29
03/10/2022 22:31:23 - INFO - __main__ - Step 70 Global step 70 Train loss 2.731668 on epoch=34
03/10/2022 22:31:28 - INFO - __main__ - Step 80 Global step 80 Train loss 2.179972 on epoch=39
03/10/2022 22:31:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.803983 on epoch=44
03/10/2022 22:31:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.611775 on epoch=49
03/10/2022 22:31:38 - INFO - __main__ - Global step 100 Train loss 2.712770 Classification-F1 0.2127659574468085 on epoch=49
03/10/2022 22:31:45 - INFO - __main__ - Step 110 Global step 110 Train loss 2.197354 on epoch=54
03/10/2022 22:31:50 - INFO - __main__ - Step 120 Global step 120 Train loss 1.770868 on epoch=59
03/10/2022 22:31:55 - INFO - __main__ - Step 130 Global step 130 Train loss 2.098098 on epoch=64
03/10/2022 22:32:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.715439 on epoch=69
03/10/2022 22:32:05 - INFO - __main__ - Step 150 Global step 150 Train loss 1.329832 on epoch=74
03/10/2022 22:32:05 - INFO - __main__ - Global step 150 Train loss 1.822318 Classification-F1 0.2222222222222222 on epoch=74
03/10/2022 22:32:11 - INFO - __main__ - Step 160 Global step 160 Train loss 1.702752 on epoch=79
03/10/2022 22:32:16 - INFO - __main__ - Step 170 Global step 170 Train loss 1.059563 on epoch=84
03/10/2022 22:32:21 - INFO - __main__ - Step 180 Global step 180 Train loss 1.298784 on epoch=89
03/10/2022 22:32:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.232153 on epoch=94
03/10/2022 22:32:31 - INFO - __main__ - Step 200 Global step 200 Train loss 1.291096 on epoch=99
03/10/2022 22:32:32 - INFO - __main__ - Global step 200 Train loss 1.316870 Classification-F1 0.2222222222222222 on epoch=99
03/10/2022 22:32:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.767817 on epoch=104
03/10/2022 22:32:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.963604 on epoch=109
03/10/2022 22:32:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.843812 on epoch=114
03/10/2022 22:32:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.888148 on epoch=119
03/10/2022 22:32:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.786579 on epoch=124
03/10/2022 22:32:57 - INFO - __main__ - Global step 250 Train loss 0.849992 Classification-F1 0.37915742793791574 on epoch=124
03/10/2022 22:33:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.699300 on epoch=129
03/10/2022 22:33:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.692336 on epoch=134
03/10/2022 22:33:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.482085 on epoch=139
03/10/2022 22:33:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.433862 on epoch=144
03/10/2022 22:33:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.451476 on epoch=149
03/10/2022 22:33:23 - INFO - __main__ - Global step 300 Train loss 0.551812 Classification-F1 0.2222222222222222 on epoch=149
03/10/2022 22:33:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.298216 on epoch=154
03/10/2022 22:33:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.459655 on epoch=159
03/10/2022 22:33:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.353478 on epoch=164
03/10/2022 22:33:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.377633 on epoch=169
03/10/2022 22:33:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.291199 on epoch=174
03/10/2022 22:33:48 - INFO - __main__ - Global step 350 Train loss 0.356036 Classification-F1 0.19290465631929044 on epoch=174
03/10/2022 22:33:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.308262 on epoch=179
03/10/2022 22:33:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.297643 on epoch=184
03/10/2022 22:34:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.326285 on epoch=189
03/10/2022 22:34:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.327183 on epoch=194
03/10/2022 22:34:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.283981 on epoch=199
03/10/2022 22:34:13 - INFO - __main__ - Global step 400 Train loss 0.308671 Classification-F1 0.2222222222222222 on epoch=199
03/10/2022 22:34:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.265726 on epoch=204
03/10/2022 22:34:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.282812 on epoch=209
03/10/2022 22:34:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.280622 on epoch=214
03/10/2022 22:34:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.303042 on epoch=219
03/10/2022 22:34:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.290925 on epoch=224
03/10/2022 22:34:37 - INFO - __main__ - Global step 450 Train loss 0.284625 Classification-F1 0.3595959595959595 on epoch=224
03/10/2022 22:34:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.239129 on epoch=229
03/10/2022 22:34:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.277817 on epoch=234
03/10/2022 22:34:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.255853 on epoch=239
03/10/2022 22:34:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.290960 on epoch=244
03/10/2022 22:35:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276679 on epoch=249
03/10/2022 22:35:02 - INFO - __main__ - Global step 500 Train loss 0.268087 Classification-F1 0.19290465631929044 on epoch=249
03/10/2022 22:35:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.268862 on epoch=254
03/10/2022 22:35:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.242897 on epoch=259
03/10/2022 22:35:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.255769 on epoch=264
03/10/2022 22:35:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.289251 on epoch=269
03/10/2022 22:35:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.215303 on epoch=274
03/10/2022 22:35:26 - INFO - __main__ - Global step 550 Train loss 0.254416 Classification-F1 0.19290465631929044 on epoch=274
03/10/2022 22:35:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.231192 on epoch=279
03/10/2022 22:35:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.266476 on epoch=284
03/10/2022 22:35:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.269282 on epoch=289
03/10/2022 22:35:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.210987 on epoch=294
03/10/2022 22:35:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.246161 on epoch=299
03/10/2022 22:35:51 - INFO - __main__ - Global step 600 Train loss 0.244820 Classification-F1 0.2821052631578947 on epoch=299
03/10/2022 22:35:51 - INFO - __main__ - save last model!
03/10/2022 22:35:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:35:52 - INFO - __main__ - Printing 3 examples
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:35:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:35:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:35:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:35:52 - INFO - __main__ - Printing 3 examples
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:35:52 - INFO - __main__ - ['hate']
03/10/2022 22:35:52 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:35:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:35:52 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:35:58 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:35:58 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 22:35:58 - INFO - __main__ - Printing 3 examples
03/10/2022 22:35:58 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 22:35:58 - INFO - __main__ - ['hate']
03/10/2022 22:35:58 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 22:35:58 - INFO - __main__ - ['non-irony']
03/10/2022 22:35:58 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 22:35:58 - INFO - __main__ - ['hate']
03/10/2022 22:35:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:35:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:35:59 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 22:36:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:36:01 - INFO - __main__ - Starting training!
03/10/2022 22:36:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.0005_8_predictions.txt
03/10/2022 22:36:13 - INFO - __main__ - Classification-F1 on test data: 0.4665
03/10/2022 22:36:14 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.0005, bsz=8, dev_performance=0.37915742793791574, test_performance=0.46651724137931033
03/10/2022 22:36:14 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.0003, bsz=8 ...
03/10/2022 22:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:36:15 - INFO - __main__ - Printing 3 examples
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:36:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:36:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:36:15 - INFO - __main__ - Printing 3 examples
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:36:15 - INFO - __main__ - ['hate']
03/10/2022 22:36:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:36:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:36:15 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:36:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:36:24 - INFO - __main__ - Starting training!
03/10/2022 22:36:28 - INFO - __main__ - Step 10 Global step 10 Train loss 19.982758 on epoch=4
03/10/2022 22:36:33 - INFO - __main__ - Step 20 Global step 20 Train loss 16.224554 on epoch=9
03/10/2022 22:36:38 - INFO - __main__ - Step 30 Global step 30 Train loss 10.491475 on epoch=14
03/10/2022 22:36:43 - INFO - __main__ - Step 40 Global step 40 Train loss 9.890181 on epoch=19
03/10/2022 22:36:48 - INFO - __main__ - Step 50 Global step 50 Train loss 9.050731 on epoch=24
03/10/2022 22:36:48 - INFO - __main__ - Global step 50 Train loss 13.127939 Classification-F1 0.0 on epoch=24
03/10/2022 22:36:54 - INFO - __main__ - Step 60 Global step 60 Train loss 8.177400 on epoch=29
03/10/2022 22:36:59 - INFO - __main__ - Step 70 Global step 70 Train loss 7.409161 on epoch=34
03/10/2022 22:37:03 - INFO - __main__ - Step 80 Global step 80 Train loss 6.991336 on epoch=39
03/10/2022 22:37:08 - INFO - __main__ - Step 90 Global step 90 Train loss 5.467896 on epoch=44
03/10/2022 22:37:13 - INFO - __main__ - Step 100 Global step 100 Train loss 3.060949 on epoch=49
03/10/2022 22:37:14 - INFO - __main__ - Global step 100 Train loss 6.221348 Classification-F1 0.007575757575757576 on epoch=49
03/10/2022 22:37:20 - INFO - __main__ - Step 110 Global step 110 Train loss 2.090330 on epoch=54
03/10/2022 22:37:25 - INFO - __main__ - Step 120 Global step 120 Train loss 2.117836 on epoch=59
03/10/2022 22:37:30 - INFO - __main__ - Step 130 Global step 130 Train loss 2.031667 on epoch=64
03/10/2022 22:37:35 - INFO - __main__ - Step 140 Global step 140 Train loss 2.202746 on epoch=69
03/10/2022 22:37:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.010458 on epoch=74
03/10/2022 22:37:40 - INFO - __main__ - Global step 150 Train loss 2.090607 Classification-F1 0.2222222222222222 on epoch=74
03/10/2022 22:37:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.905954 on epoch=79
03/10/2022 22:37:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.926552 on epoch=84
03/10/2022 22:37:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.632636 on epoch=89
03/10/2022 22:38:00 - INFO - __main__ - Step 190 Global step 190 Train loss 1.322684 on epoch=94
03/10/2022 22:38:04 - INFO - __main__ - Step 200 Global step 200 Train loss 1.776026 on epoch=99
03/10/2022 22:38:05 - INFO - __main__ - Global step 200 Train loss 1.712770 Classification-F1 0.2222222222222222 on epoch=99
03/10/2022 22:38:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.479289 on epoch=104
03/10/2022 22:38:14 - INFO - __main__ - Step 220 Global step 220 Train loss 1.289768 on epoch=109
03/10/2022 22:38:19 - INFO - __main__ - Step 230 Global step 230 Train loss 1.257563 on epoch=114
03/10/2022 22:38:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.631763 on epoch=119
03/10/2022 22:38:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.260205 on epoch=124
03/10/2022 22:38:29 - INFO - __main__ - Global step 250 Train loss 1.383718 Classification-F1 0.2222222222222222 on epoch=124
03/10/2022 22:38:34 - INFO - __main__ - Step 260 Global step 260 Train loss 1.223219 on epoch=129
03/10/2022 22:38:39 - INFO - __main__ - Step 270 Global step 270 Train loss 1.515529 on epoch=134
03/10/2022 22:38:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.331837 on epoch=139
03/10/2022 22:38:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.267360 on epoch=144
03/10/2022 22:38:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.265756 on epoch=149
03/10/2022 22:38:53 - INFO - __main__ - Global step 300 Train loss 1.320740 Classification-F1 0.2127659574468085 on epoch=149
03/10/2022 22:38:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.828113 on epoch=154
03/10/2022 22:39:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.812698 on epoch=159
03/10/2022 22:39:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.906675 on epoch=164
03/10/2022 22:39:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.699755 on epoch=169
03/10/2022 22:39:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.977659 on epoch=174
03/10/2022 22:39:18 - INFO - __main__ - Global step 350 Train loss 0.844980 Classification-F1 0.2127659574468085 on epoch=174
03/10/2022 22:39:22 - INFO - __main__ - Step 360 Global step 360 Train loss 1.218065 on epoch=179
03/10/2022 22:39:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.853781 on epoch=184
03/10/2022 22:39:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.689088 on epoch=189
03/10/2022 22:39:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.888836 on epoch=194
03/10/2022 22:39:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.665012 on epoch=199
03/10/2022 22:39:42 - INFO - __main__ - Global step 400 Train loss 0.862957 Classification-F1 0.4428571428571429 on epoch=199
03/10/2022 22:39:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.629030 on epoch=204
03/10/2022 22:39:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.654270 on epoch=209
03/10/2022 22:39:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.551398 on epoch=214
03/10/2022 22:40:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.489146 on epoch=219
03/10/2022 22:40:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.489509 on epoch=224
03/10/2022 22:40:07 - INFO - __main__ - Global step 450 Train loss 0.562670 Classification-F1 0.4444444444444444 on epoch=224
03/10/2022 22:40:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.358050 on epoch=229
03/10/2022 22:40:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.397222 on epoch=234
03/10/2022 22:40:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.269487 on epoch=239
03/10/2022 22:40:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.281804 on epoch=244
03/10/2022 22:40:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.277703 on epoch=249
03/10/2022 22:40:32 - INFO - __main__ - Global step 500 Train loss 0.316853 Classification-F1 0.4589473684210526 on epoch=249
03/10/2022 22:40:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.221785 on epoch=254
03/10/2022 22:40:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.179241 on epoch=259
03/10/2022 22:40:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.077200 on epoch=264
03/10/2022 22:40:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.139673 on epoch=269
03/10/2022 22:40:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.099889 on epoch=274
03/10/2022 22:40:57 - INFO - __main__ - Global step 550 Train loss 0.143558 Classification-F1 0.4528985507246377 on epoch=274
03/10/2022 22:41:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.295545 on epoch=279
03/10/2022 22:41:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.355130 on epoch=284
03/10/2022 22:41:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.317398 on epoch=289
03/10/2022 22:41:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.311237 on epoch=294
03/10/2022 22:41:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.337643 on epoch=299
03/10/2022 22:41:22 - INFO - __main__ - Global step 600 Train loss 0.323391 Classification-F1 0.4257206208425721 on epoch=299
03/10/2022 22:41:22 - INFO - __main__ - save last model!
03/10/2022 22:41:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:41:22 - INFO - __main__ - Printing 3 examples
03/10/2022 22:41:22 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:41:22 - INFO - __main__ - ['hate']
03/10/2022 22:41:22 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:41:22 - INFO - __main__ - ['hate']
03/10/2022 22:41:22 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:41:22 - INFO - __main__ - ['hate']
03/10/2022 22:41:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:41:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:41:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:41:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:41:23 - INFO - __main__ - Printing 3 examples
03/10/2022 22:41:23 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:41:23 - INFO - __main__ - ['hate']
03/10/2022 22:41:23 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:41:23 - INFO - __main__ - ['hate']
03/10/2022 22:41:23 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:41:23 - INFO - __main__ - ['hate']
03/10/2022 22:41:23 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:41:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:41:23 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:41:28 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:41:29 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 22:41:29 - INFO - __main__ - Printing 3 examples
03/10/2022 22:41:29 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 22:41:29 - INFO - __main__ - ['hate']
03/10/2022 22:41:29 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 22:41:29 - INFO - __main__ - ['non-irony']
03/10/2022 22:41:29 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 22:41:29 - INFO - __main__ - ['hate']
03/10/2022 22:41:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:41:30 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:41:30 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 22:41:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:41:33 - INFO - __main__ - Starting training!
03/10/2022 22:41:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.0003_8_predictions.txt
03/10/2022 22:41:45 - INFO - __main__ - Classification-F1 on test data: 0.5424
03/10/2022 22:41:45 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.0003, bsz=8, dev_performance=0.4589473684210526, test_performance=0.5424185015412062
03/10/2022 22:41:45 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.0002, bsz=8 ...
03/10/2022 22:41:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:41:46 - INFO - __main__ - Printing 3 examples
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:41:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:41:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:41:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:41:46 - INFO - __main__ - Printing 3 examples
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:41:46 - INFO - __main__ - ['hate']
03/10/2022 22:41:46 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:41:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:41:46 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:41:56 - INFO - __main__ - Starting training!
03/10/2022 22:42:00 - INFO - __main__ - Step 10 Global step 10 Train loss 20.447046 on epoch=4
03/10/2022 22:42:04 - INFO - __main__ - Step 20 Global step 20 Train loss 20.559607 on epoch=9
03/10/2022 22:42:09 - INFO - __main__ - Step 30 Global step 30 Train loss 15.981428 on epoch=14
03/10/2022 22:42:14 - INFO - __main__ - Step 40 Global step 40 Train loss 13.303591 on epoch=19
03/10/2022 22:42:19 - INFO - __main__ - Step 50 Global step 50 Train loss 11.953150 on epoch=24
03/10/2022 22:42:27 - INFO - __main__ - Global step 50 Train loss 16.448963 Classification-F1 0.0 on epoch=24
03/10/2022 22:42:33 - INFO - __main__ - Step 60 Global step 60 Train loss 10.843144 on epoch=29
03/10/2022 22:42:37 - INFO - __main__ - Step 70 Global step 70 Train loss 10.580980 on epoch=34
03/10/2022 22:42:42 - INFO - __main__ - Step 80 Global step 80 Train loss 9.490057 on epoch=39
03/10/2022 22:42:47 - INFO - __main__ - Step 90 Global step 90 Train loss 9.557350 on epoch=44
03/10/2022 22:42:52 - INFO - __main__ - Step 100 Global step 100 Train loss 8.253628 on epoch=49
03/10/2022 22:42:53 - INFO - __main__ - Global step 100 Train loss 9.745032 Classification-F1 0.0 on epoch=49
03/10/2022 22:42:57 - INFO - __main__ - Step 110 Global step 110 Train loss 7.451987 on epoch=54
03/10/2022 22:43:02 - INFO - __main__ - Step 120 Global step 120 Train loss 6.944131 on epoch=59
03/10/2022 22:43:07 - INFO - __main__ - Step 130 Global step 130 Train loss 5.814781 on epoch=64
03/10/2022 22:43:12 - INFO - __main__ - Step 140 Global step 140 Train loss 5.055125 on epoch=69
03/10/2022 22:43:17 - INFO - __main__ - Step 150 Global step 150 Train loss 4.342877 on epoch=74
03/10/2022 22:43:17 - INFO - __main__ - Global step 150 Train loss 5.921781 Classification-F1 0.0 on epoch=74
03/10/2022 22:43:22 - INFO - __main__ - Step 160 Global step 160 Train loss 4.159311 on epoch=79
03/10/2022 22:43:27 - INFO - __main__ - Step 170 Global step 170 Train loss 2.939414 on epoch=84
03/10/2022 22:43:32 - INFO - __main__ - Step 180 Global step 180 Train loss 2.628973 on epoch=89
03/10/2022 22:43:37 - INFO - __main__ - Step 190 Global step 190 Train loss 2.372613 on epoch=94
03/10/2022 22:43:41 - INFO - __main__ - Step 200 Global step 200 Train loss 2.371833 on epoch=99
03/10/2022 22:43:42 - INFO - __main__ - Global step 200 Train loss 2.894429 Classification-F1 0.2222222222222222 on epoch=99
03/10/2022 22:43:48 - INFO - __main__ - Step 210 Global step 210 Train loss 1.796120 on epoch=104
03/10/2022 22:43:53 - INFO - __main__ - Step 220 Global step 220 Train loss 2.030499 on epoch=109
03/10/2022 22:43:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.773049 on epoch=114
03/10/2022 22:44:02 - INFO - __main__ - Step 240 Global step 240 Train loss 1.392730 on epoch=119
03/10/2022 22:44:07 - INFO - __main__ - Step 250 Global step 250 Train loss 1.724370 on epoch=124
03/10/2022 22:44:08 - INFO - __main__ - Global step 250 Train loss 1.743354 Classification-F1 0.2222222222222222 on epoch=124
03/10/2022 22:44:12 - INFO - __main__ - Step 260 Global step 260 Train loss 1.874789 on epoch=129
03/10/2022 22:44:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.107248 on epoch=134
03/10/2022 22:44:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.255063 on epoch=139
03/10/2022 22:44:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.490289 on epoch=144
03/10/2022 22:44:32 - INFO - __main__ - Step 300 Global step 300 Train loss 1.342562 on epoch=149
03/10/2022 22:44:32 - INFO - __main__ - Global step 300 Train loss 1.413990 Classification-F1 0.2222222222222222 on epoch=149
03/10/2022 22:44:37 - INFO - __main__ - Step 310 Global step 310 Train loss 1.272567 on epoch=154
03/10/2022 22:44:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.432473 on epoch=159
03/10/2022 22:44:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.718682 on epoch=164
03/10/2022 22:44:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.557282 on epoch=169
03/10/2022 22:44:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.234312 on epoch=174
03/10/2022 22:44:57 - INFO - __main__ - Global step 350 Train loss 1.443063 Classification-F1 0.2222222222222222 on epoch=174
03/10/2022 22:45:01 - INFO - __main__ - Step 360 Global step 360 Train loss 1.126553 on epoch=179
03/10/2022 22:45:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.933851 on epoch=184
03/10/2022 22:45:11 - INFO - __main__ - Step 380 Global step 380 Train loss 1.531932 on epoch=189
03/10/2022 22:45:16 - INFO - __main__ - Step 390 Global step 390 Train loss 1.616188 on epoch=194
03/10/2022 22:45:20 - INFO - __main__ - Step 400 Global step 400 Train loss 1.139419 on epoch=199
03/10/2022 22:45:21 - INFO - __main__ - Global step 400 Train loss 1.269589 Classification-F1 0.2222222222222222 on epoch=199
03/10/2022 22:45:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.492373 on epoch=204
03/10/2022 22:45:31 - INFO - __main__ - Step 420 Global step 420 Train loss 1.218741 on epoch=209
03/10/2022 22:45:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.275194 on epoch=214
03/10/2022 22:45:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.286479 on epoch=219
03/10/2022 22:45:45 - INFO - __main__ - Step 450 Global step 450 Train loss 1.428834 on epoch=224
03/10/2022 22:45:45 - INFO - __main__ - Global step 450 Train loss 1.340324 Classification-F1 0.2222222222222222 on epoch=224
03/10/2022 22:45:50 - INFO - __main__ - Step 460 Global step 460 Train loss 1.314941 on epoch=229
03/10/2022 22:45:55 - INFO - __main__ - Step 470 Global step 470 Train loss 1.240019 on epoch=234
03/10/2022 22:46:00 - INFO - __main__ - Step 480 Global step 480 Train loss 1.205914 on epoch=239
03/10/2022 22:46:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.078479 on epoch=244
03/10/2022 22:46:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.938505 on epoch=249
03/10/2022 22:46:10 - INFO - __main__ - Global step 500 Train loss 1.155572 Classification-F1 0.4528985507246377 on epoch=249
03/10/2022 22:46:16 - INFO - __main__ - Step 510 Global step 510 Train loss 1.037969 on epoch=254
03/10/2022 22:46:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.091248 on epoch=259
03/10/2022 22:46:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.059868 on epoch=264
03/10/2022 22:46:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.228624 on epoch=269
03/10/2022 22:46:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.932603 on epoch=274
03/10/2022 22:46:35 - INFO - __main__ - Global step 550 Train loss 1.070062 Classification-F1 0.2222222222222222 on epoch=274
03/10/2022 22:46:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.781148 on epoch=279
03/10/2022 22:46:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.744552 on epoch=284
03/10/2022 22:46:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.953877 on epoch=289
03/10/2022 22:46:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.694614 on epoch=294
03/10/2022 22:47:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.555930 on epoch=299
03/10/2022 22:47:00 - INFO - __main__ - Global step 600 Train loss 0.746024 Classification-F1 0.2222222222222222 on epoch=299
03/10/2022 22:47:00 - INFO - __main__ - save last model!
03/10/2022 22:47:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:47:01 - INFO - __main__ - Printing 3 examples
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:47:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:47:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:47:01 - INFO - __main__ - Printing 3 examples
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:47:01 - INFO - __main__ - ['hate']
03/10/2022 22:47:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:47:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:47:07 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:47:08 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 22:47:08 - INFO - __main__ - Printing 3 examples
03/10/2022 22:47:08 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 22:47:08 - INFO - __main__ - ['hate']
03/10/2022 22:47:08 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 22:47:08 - INFO - __main__ - ['non-irony']
03/10/2022 22:47:08 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 22:47:08 - INFO - __main__ - ['hate']
03/10/2022 22:47:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:47:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:47:09 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 22:47:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:47:11 - INFO - __main__ - Starting training!
03/10/2022 22:47:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.0002_8_predictions.txt
03/10/2022 22:47:24 - INFO - __main__ - Classification-F1 on test data: 0.4798
03/10/2022 22:47:24 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.0002, bsz=8, dev_performance=0.4528985507246377, test_performance=0.4798027579484533
03/10/2022 22:47:25 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.0001, bsz=8 ...
03/10/2022 22:47:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:47:25 - INFO - __main__ - Printing 3 examples
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:47:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:47:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:47:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:47:25 - INFO - __main__ - Printing 3 examples
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/10/2022 22:47:25 - INFO - __main__ - ['hate']
03/10/2022 22:47:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:47:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:47:26 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:47:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:47:35 - INFO - __main__ - Starting training!
03/10/2022 22:47:39 - INFO - __main__ - Step 10 Global step 10 Train loss 20.814899 on epoch=4
03/10/2022 22:47:43 - INFO - __main__ - Step 20 Global step 20 Train loss 17.865183 on epoch=9
03/10/2022 22:47:48 - INFO - __main__ - Step 30 Global step 30 Train loss 14.969708 on epoch=14
03/10/2022 22:47:53 - INFO - __main__ - Step 40 Global step 40 Train loss 13.633466 on epoch=19
03/10/2022 22:47:58 - INFO - __main__ - Step 50 Global step 50 Train loss 12.175364 on epoch=24
03/10/2022 22:48:07 - INFO - __main__ - Global step 50 Train loss 15.891725 Classification-F1 0.0 on epoch=24
03/10/2022 22:48:13 - INFO - __main__ - Step 60 Global step 60 Train loss 11.545187 on epoch=29
03/10/2022 22:48:18 - INFO - __main__ - Step 70 Global step 70 Train loss 10.868177 on epoch=34
03/10/2022 22:48:22 - INFO - __main__ - Step 80 Global step 80 Train loss 10.298073 on epoch=39
03/10/2022 22:48:27 - INFO - __main__ - Step 90 Global step 90 Train loss 10.156212 on epoch=44
03/10/2022 22:48:32 - INFO - __main__ - Step 100 Global step 100 Train loss 9.800838 on epoch=49
03/10/2022 22:48:34 - INFO - __main__ - Global step 100 Train loss 10.533697 Classification-F1 0.0 on epoch=49
03/10/2022 22:48:39 - INFO - __main__ - Step 110 Global step 110 Train loss 9.338957 on epoch=54
03/10/2022 22:48:44 - INFO - __main__ - Step 120 Global step 120 Train loss 9.187551 on epoch=59
03/10/2022 22:48:48 - INFO - __main__ - Step 130 Global step 130 Train loss 9.417513 on epoch=64
03/10/2022 22:48:53 - INFO - __main__ - Step 140 Global step 140 Train loss 8.723214 on epoch=69
03/10/2022 22:48:58 - INFO - __main__ - Step 150 Global step 150 Train loss 8.534534 on epoch=74
03/10/2022 22:49:01 - INFO - __main__ - Global step 150 Train loss 9.040354 Classification-F1 0.0 on epoch=74
03/10/2022 22:49:05 - INFO - __main__ - Step 160 Global step 160 Train loss 8.154595 on epoch=79
03/10/2022 22:49:10 - INFO - __main__ - Step 170 Global step 170 Train loss 7.915943 on epoch=84
03/10/2022 22:49:15 - INFO - __main__ - Step 180 Global step 180 Train loss 7.881319 on epoch=89
03/10/2022 22:49:20 - INFO - __main__ - Step 190 Global step 190 Train loss 7.694581 on epoch=94
03/10/2022 22:49:25 - INFO - __main__ - Step 200 Global step 200 Train loss 6.781142 on epoch=99
03/10/2022 22:49:26 - INFO - __main__ - Global step 200 Train loss 7.685516 Classification-F1 0.0 on epoch=99
03/10/2022 22:49:31 - INFO - __main__ - Step 210 Global step 210 Train loss 6.204386 on epoch=104
03/10/2022 22:49:36 - INFO - __main__ - Step 220 Global step 220 Train loss 5.975565 on epoch=109
03/10/2022 22:49:41 - INFO - __main__ - Step 230 Global step 230 Train loss 5.169078 on epoch=114
03/10/2022 22:49:45 - INFO - __main__ - Step 240 Global step 240 Train loss 4.905515 on epoch=119
03/10/2022 22:49:50 - INFO - __main__ - Step 250 Global step 250 Train loss 4.080307 on epoch=124
03/10/2022 22:49:51 - INFO - __main__ - Global step 250 Train loss 5.266970 Classification-F1 0.0 on epoch=124
03/10/2022 22:49:56 - INFO - __main__ - Step 260 Global step 260 Train loss 3.735624 on epoch=129
03/10/2022 22:50:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.322578 on epoch=134
03/10/2022 22:50:06 - INFO - __main__ - Step 280 Global step 280 Train loss 3.123667 on epoch=139
03/10/2022 22:50:10 - INFO - __main__ - Step 290 Global step 290 Train loss 2.330102 on epoch=144
03/10/2022 22:50:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.910386 on epoch=149
03/10/2022 22:50:16 - INFO - __main__ - Global step 300 Train loss 2.884471 Classification-F1 0.2222222222222222 on epoch=149
03/10/2022 22:50:21 - INFO - __main__ - Step 310 Global step 310 Train loss 2.409132 on epoch=154
03/10/2022 22:50:26 - INFO - __main__ - Step 320 Global step 320 Train loss 2.000154 on epoch=159
03/10/2022 22:50:31 - INFO - __main__ - Step 330 Global step 330 Train loss 1.758261 on epoch=164
03/10/2022 22:50:35 - INFO - __main__ - Step 340 Global step 340 Train loss 2.060089 on epoch=169
03/10/2022 22:50:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.403297 on epoch=174
03/10/2022 22:50:41 - INFO - __main__ - Global step 350 Train loss 1.926187 Classification-F1 0.2222222222222222 on epoch=174
03/10/2022 22:50:46 - INFO - __main__ - Step 360 Global step 360 Train loss 2.092892 on epoch=179
03/10/2022 22:50:50 - INFO - __main__ - Step 370 Global step 370 Train loss 1.678470 on epoch=184
03/10/2022 22:50:55 - INFO - __main__ - Step 380 Global step 380 Train loss 2.093218 on epoch=189
03/10/2022 22:51:00 - INFO - __main__ - Step 390 Global step 390 Train loss 1.966716 on epoch=194
03/10/2022 22:51:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.805659 on epoch=199
03/10/2022 22:51:05 - INFO - __main__ - Global step 400 Train loss 1.927391 Classification-F1 0.2222222222222222 on epoch=199
03/10/2022 22:51:10 - INFO - __main__ - Step 410 Global step 410 Train loss 1.890272 on epoch=204
03/10/2022 22:51:15 - INFO - __main__ - Step 420 Global step 420 Train loss 1.601248 on epoch=209
03/10/2022 22:51:20 - INFO - __main__ - Step 430 Global step 430 Train loss 2.122287 on epoch=214
03/10/2022 22:51:24 - INFO - __main__ - Step 440 Global step 440 Train loss 1.771134 on epoch=219
03/10/2022 22:51:29 - INFO - __main__ - Step 450 Global step 450 Train loss 1.406956 on epoch=224
03/10/2022 22:51:30 - INFO - __main__ - Global step 450 Train loss 1.758379 Classification-F1 0.2222222222222222 on epoch=224
03/10/2022 22:51:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.767262 on epoch=229
03/10/2022 22:51:39 - INFO - __main__ - Step 470 Global step 470 Train loss 1.573195 on epoch=234
03/10/2022 22:51:44 - INFO - __main__ - Step 480 Global step 480 Train loss 1.287228 on epoch=239
03/10/2022 22:51:49 - INFO - __main__ - Step 490 Global step 490 Train loss 1.438895 on epoch=244
03/10/2022 22:51:54 - INFO - __main__ - Step 500 Global step 500 Train loss 2.249665 on epoch=249
03/10/2022 22:51:54 - INFO - __main__ - Global step 500 Train loss 1.663249 Classification-F1 0.2222222222222222 on epoch=249
03/10/2022 22:51:59 - INFO - __main__ - Step 510 Global step 510 Train loss 1.813539 on epoch=254
03/10/2022 22:52:04 - INFO - __main__ - Step 520 Global step 520 Train loss 1.920908 on epoch=259
03/10/2022 22:52:08 - INFO - __main__ - Step 530 Global step 530 Train loss 1.169859 on epoch=264
03/10/2022 22:52:13 - INFO - __main__ - Step 540 Global step 540 Train loss 1.427529 on epoch=269
03/10/2022 22:52:18 - INFO - __main__ - Step 550 Global step 550 Train loss 1.572335 on epoch=274
03/10/2022 22:52:19 - INFO - __main__ - Global step 550 Train loss 1.580834 Classification-F1 0.2222222222222222 on epoch=274
03/10/2022 22:52:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.734820 on epoch=279
03/10/2022 22:52:28 - INFO - __main__ - Step 570 Global step 570 Train loss 1.517842 on epoch=284
03/10/2022 22:52:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.468857 on epoch=289
03/10/2022 22:52:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.458403 on epoch=294
03/10/2022 22:52:43 - INFO - __main__ - Step 600 Global step 600 Train loss 1.072239 on epoch=299
03/10/2022 22:52:43 - INFO - __main__ - Global step 600 Train loss 1.450432 Classification-F1 0.2222222222222222 on epoch=299
03/10/2022 22:52:43 - INFO - __main__ - save last model!
03/10/2022 22:52:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:52:44 - INFO - __main__ - Printing 3 examples
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:52:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:52:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:52:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:52:44 - INFO - __main__ - Printing 3 examples
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 22:52:44 - INFO - __main__ - ['hate']
03/10/2022 22:52:44 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:52:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:52:44 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:52:50 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:52:51 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 22:52:51 - INFO - __main__ - Printing 3 examples
03/10/2022 22:52:51 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 22:52:51 - INFO - __main__ - ['hate']
03/10/2022 22:52:51 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 22:52:51 - INFO - __main__ - ['non-irony']
03/10/2022 22:52:51 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 22:52:51 - INFO - __main__ - ['hate']
03/10/2022 22:52:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:52:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:52:52 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 22:52:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:52:53 - INFO - __main__ - Starting training!
03/10/2022 22:53:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.0001_8_predictions.txt
03/10/2022 22:53:06 - INFO - __main__ - Classification-F1 on test data: 0.2221
03/10/2022 22:53:06 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.0001, bsz=8, dev_performance=0.2222222222222222, test_performance=0.22214903992233415
03/10/2022 22:53:06 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.0005, bsz=8 ...
03/10/2022 22:53:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:53:07 - INFO - __main__ - Printing 3 examples
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:53:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:53:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:53:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:53:07 - INFO - __main__ - Printing 3 examples
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 22:53:07 - INFO - __main__ - ['hate']
03/10/2022 22:53:07 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:53:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:53:07 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:53:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:53:16 - INFO - __main__ - Starting training!
03/10/2022 22:53:22 - INFO - __main__ - Step 10 Global step 10 Train loss 22.247074 on epoch=4
03/10/2022 22:53:27 - INFO - __main__ - Step 20 Global step 20 Train loss 15.631749 on epoch=9
03/10/2022 22:53:31 - INFO - __main__ - Step 30 Global step 30 Train loss 10.741741 on epoch=14
03/10/2022 22:53:36 - INFO - __main__ - Step 40 Global step 40 Train loss 9.113561 on epoch=19
03/10/2022 22:53:41 - INFO - __main__ - Step 50 Global step 50 Train loss 7.245436 on epoch=24
03/10/2022 22:53:43 - INFO - __main__ - Global step 50 Train loss 12.995914 Classification-F1 0.0 on epoch=24
03/10/2022 22:53:49 - INFO - __main__ - Step 60 Global step 60 Train loss 6.129313 on epoch=29
03/10/2022 22:53:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.247569 on epoch=34
03/10/2022 22:53:58 - INFO - __main__ - Step 80 Global step 80 Train loss 2.557194 on epoch=39
03/10/2022 22:54:03 - INFO - __main__ - Step 90 Global step 90 Train loss 1.696522 on epoch=44
03/10/2022 22:54:08 - INFO - __main__ - Step 100 Global step 100 Train loss 1.958889 on epoch=49
03/10/2022 22:54:08 - INFO - __main__ - Global step 100 Train loss 3.317898 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 22:54:14 - INFO - __main__ - Step 110 Global step 110 Train loss 2.099393 on epoch=54
03/10/2022 22:54:19 - INFO - __main__ - Step 120 Global step 120 Train loss 1.874665 on epoch=59
03/10/2022 22:54:24 - INFO - __main__ - Step 130 Global step 130 Train loss 1.763986 on epoch=64
03/10/2022 22:54:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.693639 on epoch=69
03/10/2022 22:54:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.487326 on epoch=74
03/10/2022 22:54:34 - INFO - __main__ - Global step 150 Train loss 1.783802 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 22:54:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.354041 on epoch=79
03/10/2022 22:54:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.225736 on epoch=84
03/10/2022 22:54:48 - INFO - __main__ - Step 180 Global step 180 Train loss 1.046500 on epoch=89
03/10/2022 22:54:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.011078 on epoch=94
03/10/2022 22:54:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.500471 on epoch=99
03/10/2022 22:54:58 - INFO - __main__ - Global step 200 Train loss 1.227565 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 22:55:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.035025 on epoch=104
03/10/2022 22:55:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.973997 on epoch=109
03/10/2022 22:55:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.595840 on epoch=114
03/10/2022 22:55:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.701186 on epoch=119
03/10/2022 22:55:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.630232 on epoch=124
03/10/2022 22:55:23 - INFO - __main__ - Global step 250 Train loss 0.787256 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 22:55:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.771844 on epoch=129
03/10/2022 22:55:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.665078 on epoch=134
03/10/2022 22:55:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.747583 on epoch=139
03/10/2022 22:55:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.504629 on epoch=144
03/10/2022 22:55:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.528051 on epoch=149
03/10/2022 22:55:47 - INFO - __main__ - Global step 300 Train loss 0.643437 Classification-F1 0.3992490613266583 on epoch=149
03/10/2022 22:55:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.482366 on epoch=154
03/10/2022 22:55:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.402979 on epoch=159
03/10/2022 22:56:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.407683 on epoch=164
03/10/2022 22:56:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.405804 on epoch=169
03/10/2022 22:56:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.398996 on epoch=174
03/10/2022 22:56:24 - INFO - __main__ - Global step 350 Train loss 0.419566 Classification-F1 0.08602484472049689 on epoch=174
03/10/2022 22:56:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.481603 on epoch=179
03/10/2022 22:56:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.336613 on epoch=184
03/10/2022 22:56:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.348573 on epoch=189
03/10/2022 22:56:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.343163 on epoch=194
03/10/2022 22:56:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.321290 on epoch=199
03/10/2022 22:56:48 - INFO - __main__ - Global step 400 Train loss 0.366249 Classification-F1 0.3191489361702127 on epoch=199
03/10/2022 22:56:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.332222 on epoch=204
03/10/2022 22:56:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.327038 on epoch=209
03/10/2022 22:57:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.266331 on epoch=214
03/10/2022 22:57:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.219920 on epoch=219
03/10/2022 22:57:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.254915 on epoch=224
03/10/2022 22:57:13 - INFO - __main__ - Global step 450 Train loss 0.280085 Classification-F1 0.3191489361702127 on epoch=224
03/10/2022 22:57:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.211310 on epoch=229
03/10/2022 22:57:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.202337 on epoch=234
03/10/2022 22:57:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.224984 on epoch=239
03/10/2022 22:57:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.197731 on epoch=244
03/10/2022 22:57:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.192312 on epoch=249
03/10/2022 22:57:37 - INFO - __main__ - Global step 500 Train loss 0.205735 Classification-F1 0.4909862142099682 on epoch=249
03/10/2022 22:57:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.217476 on epoch=254
03/10/2022 22:57:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.235936 on epoch=259
03/10/2022 22:57:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.194208 on epoch=264
03/10/2022 22:57:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.197793 on epoch=269
03/10/2022 22:58:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.177489 on epoch=274
03/10/2022 22:58:02 - INFO - __main__ - Global step 550 Train loss 0.204581 Classification-F1 0.4666666666666667 on epoch=274
03/10/2022 22:58:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.160174 on epoch=279
03/10/2022 22:58:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.157599 on epoch=284
03/10/2022 22:58:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.105281 on epoch=289
03/10/2022 22:58:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.102927 on epoch=294
03/10/2022 22:58:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.115276 on epoch=299
03/10/2022 22:58:27 - INFO - __main__ - Global step 600 Train loss 0.128251 Classification-F1 0.4181818181818182 on epoch=299
03/10/2022 22:58:27 - INFO - __main__ - save last model!
03/10/2022 22:58:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:58:27 - INFO - __main__ - Printing 3 examples
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:58:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:58:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:58:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:58:27 - INFO - __main__ - Printing 3 examples
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 22:58:27 - INFO - __main__ - ['hate']
03/10/2022 22:58:27 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:58:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:58:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:58:34 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 22:58:35 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 22:58:35 - INFO - __main__ - Printing 3 examples
03/10/2022 22:58:35 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 22:58:35 - INFO - __main__ - ['hate']
03/10/2022 22:58:35 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 22:58:35 - INFO - __main__ - ['non-irony']
03/10/2022 22:58:35 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 22:58:35 - INFO - __main__ - ['hate']
03/10/2022 22:58:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:58:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:58:36 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 22:58:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:58:38 - INFO - __main__ - Starting training!
03/10/2022 22:58:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.0005_8_predictions.txt
03/10/2022 22:58:50 - INFO - __main__ - Classification-F1 on test data: 0.5432
03/10/2022 22:58:52 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.0005, bsz=8, dev_performance=0.4909862142099682, test_performance=0.5431743585352627
03/10/2022 22:58:52 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.0003, bsz=8 ...
03/10/2022 22:58:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:58:53 - INFO - __main__ - Printing 3 examples
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:58:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:58:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:58:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:58:53 - INFO - __main__ - Printing 3 examples
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 22:58:53 - INFO - __main__ - ['hate']
03/10/2022 22:58:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:58:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:58:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:59:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 22:59:03 - INFO - __main__ - Starting training!
03/10/2022 22:59:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.708944 on epoch=4
03/10/2022 22:59:12 - INFO - __main__ - Step 20 Global step 20 Train loss 14.942629 on epoch=9
03/10/2022 22:59:17 - INFO - __main__ - Step 30 Global step 30 Train loss 12.072772 on epoch=14
03/10/2022 22:59:21 - INFO - __main__ - Step 40 Global step 40 Train loss 11.002584 on epoch=19
03/10/2022 22:59:26 - INFO - __main__ - Step 50 Global step 50 Train loss 9.427094 on epoch=24
03/10/2022 22:59:34 - INFO - __main__ - Global step 50 Train loss 13.830805 Classification-F1 0.0 on epoch=24
03/10/2022 22:59:39 - INFO - __main__ - Step 60 Global step 60 Train loss 9.277508 on epoch=29
03/10/2022 22:59:44 - INFO - __main__ - Step 70 Global step 70 Train loss 7.877503 on epoch=34
03/10/2022 22:59:49 - INFO - __main__ - Step 80 Global step 80 Train loss 7.158010 on epoch=39
03/10/2022 22:59:54 - INFO - __main__ - Step 90 Global step 90 Train loss 5.851930 on epoch=44
03/10/2022 22:59:59 - INFO - __main__ - Step 100 Global step 100 Train loss 4.541339 on epoch=49
03/10/2022 23:00:00 - INFO - __main__ - Global step 100 Train loss 6.941258 Classification-F1 0.0392156862745098 on epoch=49
03/10/2022 23:00:06 - INFO - __main__ - Step 110 Global step 110 Train loss 2.437611 on epoch=54
03/10/2022 23:00:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.751505 on epoch=59
03/10/2022 23:00:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.446720 on epoch=64
03/10/2022 23:00:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.491842 on epoch=69
03/10/2022 23:00:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.329968 on epoch=74
03/10/2022 23:00:26 - INFO - __main__ - Global step 150 Train loss 0.891529 Classification-F1 0.36374269005847953 on epoch=74
03/10/2022 23:00:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.284039 on epoch=79
03/10/2022 23:00:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.284511 on epoch=84
03/10/2022 23:00:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.193707 on epoch=89
03/10/2022 23:00:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.151470 on epoch=94
03/10/2022 23:00:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.088700 on epoch=99
03/10/2022 23:00:51 - INFO - __main__ - Global step 200 Train loss 0.200485 Classification-F1 0.4285714285714286 on epoch=99
03/10/2022 23:00:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.096935 on epoch=104
03/10/2022 23:01:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.066791 on epoch=109
03/10/2022 23:01:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.059992 on epoch=114
03/10/2022 23:01:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.043227 on epoch=119
03/10/2022 23:01:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.049856 on epoch=124
03/10/2022 23:01:17 - INFO - __main__ - Global step 250 Train loss 0.063360 Classification-F1 0.4231177094379639 on epoch=124
03/10/2022 23:01:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.049826 on epoch=129
03/10/2022 23:01:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.039605 on epoch=134
03/10/2022 23:01:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.012255 on epoch=139
03/10/2022 23:01:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.019410 on epoch=144
03/10/2022 23:01:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.011522 on epoch=149
03/10/2022 23:01:42 - INFO - __main__ - Global step 300 Train loss 0.026523 Classification-F1 0.37662337662337664 on epoch=149
03/10/2022 23:01:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.056398 on epoch=154
03/10/2022 23:01:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.005520 on epoch=159
03/10/2022 23:01:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.016667 on epoch=164
03/10/2022 23:02:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.020057 on epoch=169
03/10/2022 23:02:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021599 on epoch=174
03/10/2022 23:02:07 - INFO - __main__ - Global step 350 Train loss 0.024048 Classification-F1 0.4920634920634921 on epoch=174
03/10/2022 23:02:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.011534 on epoch=179
03/10/2022 23:02:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.021119 on epoch=184
03/10/2022 23:02:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008930 on epoch=189
03/10/2022 23:02:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002921 on epoch=194
03/10/2022 23:02:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001118 on epoch=199
03/10/2022 23:02:33 - INFO - __main__ - Global step 400 Train loss 0.009124 Classification-F1 0.39139139139139134 on epoch=199
03/10/2022 23:02:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.011335 on epoch=204
03/10/2022 23:02:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003086 on epoch=209
03/10/2022 23:02:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008873 on epoch=214
03/10/2022 23:02:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.030467 on epoch=219
03/10/2022 23:02:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005362 on epoch=224
03/10/2022 23:02:58 - INFO - __main__ - Global step 450 Train loss 0.011825 Classification-F1 0.41700404858299595 on epoch=224
03/10/2022 23:03:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000674 on epoch=229
03/10/2022 23:03:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002732 on epoch=234
03/10/2022 23:03:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000880 on epoch=239
03/10/2022 23:03:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000670 on epoch=244
03/10/2022 23:03:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003603 on epoch=249
03/10/2022 23:03:23 - INFO - __main__ - Global step 500 Train loss 0.001712 Classification-F1 0.4554554554554554 on epoch=249
03/10/2022 23:03:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000303 on epoch=254
03/10/2022 23:03:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000208 on epoch=259
03/10/2022 23:03:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004276 on epoch=264
03/10/2022 23:03:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000604 on epoch=269
03/10/2022 23:03:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000104 on epoch=274
03/10/2022 23:03:48 - INFO - __main__ - Global step 550 Train loss 0.001099 Classification-F1 0.39139139139139134 on epoch=274
03/10/2022 23:03:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000236 on epoch=279
03/10/2022 23:03:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000652 on epoch=284
03/10/2022 23:04:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007863 on epoch=289
03/10/2022 23:04:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000528 on epoch=294
03/10/2022 23:04:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000121 on epoch=299
03/10/2022 23:04:13 - INFO - __main__ - Global step 600 Train loss 0.001880 Classification-F1 0.39139139139139134 on epoch=299
03/10/2022 23:04:13 - INFO - __main__ - save last model!
03/10/2022 23:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:04:14 - INFO - __main__ - Printing 3 examples
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:04:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:04:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:04:14 - INFO - __main__ - Printing 3 examples
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 23:04:14 - INFO - __main__ - ['hate']
03/10/2022 23:04:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:04:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:04:14 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:04:20 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:04:21 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:04:21 - INFO - __main__ - Printing 3 examples
03/10/2022 23:04:21 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:04:21 - INFO - __main__ - ['hate']
03/10/2022 23:04:21 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:04:21 - INFO - __main__ - ['non-irony']
03/10/2022 23:04:21 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:04:21 - INFO - __main__ - ['hate']
03/10/2022 23:04:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:04:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:04:22 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:04:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:04:23 - INFO - __main__ - Starting training!
03/10/2022 23:04:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.0003_8_predictions.txt
03/10/2022 23:04:38 - INFO - __main__ - Classification-F1 on test data: 0.5729
03/10/2022 23:04:38 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.0003, bsz=8, dev_performance=0.4920634920634921, test_performance=0.5728638184245662
03/10/2022 23:04:38 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.0002, bsz=8 ...
03/10/2022 23:04:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:04:39 - INFO - __main__ - Printing 3 examples
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:04:39 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:04:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:04:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:04:39 - INFO - __main__ - Printing 3 examples
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 23:04:39 - INFO - __main__ - ['hate']
03/10/2022 23:04:39 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:04:39 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:04:39 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:04:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:04:49 - INFO - __main__ - Starting training!
03/10/2022 23:04:55 - INFO - __main__ - Step 10 Global step 10 Train loss 19.162144 on epoch=4
03/10/2022 23:04:59 - INFO - __main__ - Step 20 Global step 20 Train loss 16.470345 on epoch=9
03/10/2022 23:05:04 - INFO - __main__ - Step 30 Global step 30 Train loss 12.086081 on epoch=14
03/10/2022 23:05:09 - INFO - __main__ - Step 40 Global step 40 Train loss 10.887149 on epoch=19
03/10/2022 23:05:13 - INFO - __main__ - Step 50 Global step 50 Train loss 10.052541 on epoch=24
03/10/2022 23:05:14 - INFO - __main__ - Global step 50 Train loss 13.731651 Classification-F1 0.0 on epoch=24
03/10/2022 23:05:20 - INFO - __main__ - Step 60 Global step 60 Train loss 9.481692 on epoch=29
03/10/2022 23:05:25 - INFO - __main__ - Step 70 Global step 70 Train loss 8.589469 on epoch=34
03/10/2022 23:05:29 - INFO - __main__ - Step 80 Global step 80 Train loss 8.872289 on epoch=39
03/10/2022 23:05:34 - INFO - __main__ - Step 90 Global step 90 Train loss 7.835732 on epoch=44
03/10/2022 23:05:39 - INFO - __main__ - Step 100 Global step 100 Train loss 7.361096 on epoch=49
03/10/2022 23:05:41 - INFO - __main__ - Global step 100 Train loss 8.428057 Classification-F1 0.0 on epoch=49
03/10/2022 23:05:46 - INFO - __main__ - Step 110 Global step 110 Train loss 6.806789 on epoch=54
03/10/2022 23:05:50 - INFO - __main__ - Step 120 Global step 120 Train loss 6.146165 on epoch=59
03/10/2022 23:05:55 - INFO - __main__ - Step 130 Global step 130 Train loss 4.907298 on epoch=64
03/10/2022 23:06:00 - INFO - __main__ - Step 140 Global step 140 Train loss 4.023423 on epoch=69
03/10/2022 23:06:05 - INFO - __main__ - Step 150 Global step 150 Train loss 3.322477 on epoch=74
03/10/2022 23:06:05 - INFO - __main__ - Global step 150 Train loss 5.041231 Classification-F1 0.22695035460992907 on epoch=74
03/10/2022 23:06:11 - INFO - __main__ - Step 160 Global step 160 Train loss 2.215814 on epoch=79
03/10/2022 23:06:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.071153 on epoch=84
03/10/2022 23:06:21 - INFO - __main__ - Step 180 Global step 180 Train loss 2.014910 on epoch=89
03/10/2022 23:06:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.305466 on epoch=94
03/10/2022 23:06:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.059449 on epoch=99
03/10/2022 23:06:31 - INFO - __main__ - Global step 200 Train loss 1.933358 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:06:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.385530 on epoch=104
03/10/2022 23:06:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.950294 on epoch=109
03/10/2022 23:06:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.748753 on epoch=114
03/10/2022 23:06:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.486779 on epoch=119
03/10/2022 23:06:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.452910 on epoch=124
03/10/2022 23:06:56 - INFO - __main__ - Global step 250 Train loss 0.804853 Classification-F1 0.4385964912280702 on epoch=124
03/10/2022 23:07:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.449793 on epoch=129
03/10/2022 23:07:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.437307 on epoch=134
03/10/2022 23:07:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.296045 on epoch=139
03/10/2022 23:07:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.404959 on epoch=144
03/10/2022 23:07:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.445168 on epoch=149
03/10/2022 23:07:22 - INFO - __main__ - Global step 300 Train loss 0.406654 Classification-F1 0.4920634920634921 on epoch=149
03/10/2022 23:07:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.106320 on epoch=154
03/10/2022 23:07:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.073593 on epoch=159
03/10/2022 23:07:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.222409 on epoch=164
03/10/2022 23:07:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.070395 on epoch=169
03/10/2022 23:07:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.135497 on epoch=174
03/10/2022 23:07:47 - INFO - __main__ - Global step 350 Train loss 0.121643 Classification-F1 0.5151515151515151 on epoch=174
03/10/2022 23:07:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.059598 on epoch=179
03/10/2022 23:07:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.028589 on epoch=184
03/10/2022 23:08:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.130874 on epoch=189
03/10/2022 23:08:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.130805 on epoch=194
03/10/2022 23:08:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.023212 on epoch=199
03/10/2022 23:08:13 - INFO - __main__ - Global step 400 Train loss 0.074615 Classification-F1 0.4666666666666667 on epoch=199
03/10/2022 23:08:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015570 on epoch=204
03/10/2022 23:08:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.150667 on epoch=209
03/10/2022 23:08:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046696 on epoch=214
03/10/2022 23:08:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.025280 on epoch=219
03/10/2022 23:08:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.043842 on epoch=224
03/10/2022 23:08:38 - INFO - __main__ - Global step 450 Train loss 0.056411 Classification-F1 0.4231177094379639 on epoch=224
03/10/2022 23:08:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.010253 on epoch=229
03/10/2022 23:08:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010719 on epoch=234
03/10/2022 23:08:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012247 on epoch=239
03/10/2022 23:08:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026860 on epoch=244
03/10/2022 23:09:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.018321 on epoch=249
03/10/2022 23:09:03 - INFO - __main__ - Global step 500 Train loss 0.015680 Classification-F1 0.39139139139139134 on epoch=249
03/10/2022 23:09:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002564 on epoch=254
03/10/2022 23:09:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.010145 on epoch=259
03/10/2022 23:09:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016216 on epoch=264
03/10/2022 23:09:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.019999 on epoch=269
03/10/2022 23:09:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.012451 on epoch=274
03/10/2022 23:09:27 - INFO - __main__ - Global step 550 Train loss 0.012275 Classification-F1 0.4458874458874459 on epoch=274
03/10/2022 23:09:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.090323 on epoch=279
03/10/2022 23:09:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011186 on epoch=284
03/10/2022 23:09:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.054128 on epoch=289
03/10/2022 23:09:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.017457 on epoch=294
03/10/2022 23:09:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.025273 on epoch=299
03/10/2022 23:09:52 - INFO - __main__ - Global step 600 Train loss 0.039673 Classification-F1 0.4231177094379639 on epoch=299
03/10/2022 23:09:52 - INFO - __main__ - save last model!
03/10/2022 23:09:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:09:53 - INFO - __main__ - Printing 3 examples
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:09:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:09:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:09:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:09:53 - INFO - __main__ - Printing 3 examples
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 23:09:53 - INFO - __main__ - ['hate']
03/10/2022 23:09:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:09:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:09:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:09:59 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:10:00 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:10:00 - INFO - __main__ - Printing 3 examples
03/10/2022 23:10:00 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:10:00 - INFO - __main__ - ['hate']
03/10/2022 23:10:00 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:10:00 - INFO - __main__ - ['non-irony']
03/10/2022 23:10:00 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:10:00 - INFO - __main__ - ['hate']
03/10/2022 23:10:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:10:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:10:01 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:10:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:10:03 - INFO - __main__ - Starting training!
03/10/2022 23:10:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.0002_8_predictions.txt
03/10/2022 23:10:17 - INFO - __main__ - Classification-F1 on test data: 0.5223
03/10/2022 23:10:18 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.0002, bsz=8, dev_performance=0.5151515151515151, test_performance=0.5223089235694278
03/10/2022 23:10:18 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.0001, bsz=8 ...
03/10/2022 23:10:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:10:19 - INFO - __main__ - Printing 3 examples
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:10:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:10:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:10:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:10:19 - INFO - __main__ - Printing 3 examples
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/10/2022 23:10:19 - INFO - __main__ - ['hate']
03/10/2022 23:10:19 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:10:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:10:19 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:10:29 - INFO - __main__ - Starting training!
03/10/2022 23:10:33 - INFO - __main__ - Step 10 Global step 10 Train loss 21.099110 on epoch=4
03/10/2022 23:10:37 - INFO - __main__ - Step 20 Global step 20 Train loss 17.519917 on epoch=9
03/10/2022 23:10:42 - INFO - __main__ - Step 30 Global step 30 Train loss 14.707156 on epoch=14
03/10/2022 23:10:47 - INFO - __main__ - Step 40 Global step 40 Train loss 12.049538 on epoch=19
03/10/2022 23:10:52 - INFO - __main__ - Step 50 Global step 50 Train loss 11.848884 on epoch=24
03/10/2022 23:11:00 - INFO - __main__ - Global step 50 Train loss 15.444921 Classification-F1 0.0 on epoch=24
03/10/2022 23:11:06 - INFO - __main__ - Step 60 Global step 60 Train loss 10.933310 on epoch=29
03/10/2022 23:11:11 - INFO - __main__ - Step 70 Global step 70 Train loss 10.589939 on epoch=34
03/10/2022 23:11:16 - INFO - __main__ - Step 80 Global step 80 Train loss 10.187520 on epoch=39
03/10/2022 23:11:21 - INFO - __main__ - Step 90 Global step 90 Train loss 9.737957 on epoch=44
03/10/2022 23:11:26 - INFO - __main__ - Step 100 Global step 100 Train loss 9.605218 on epoch=49
03/10/2022 23:11:32 - INFO - __main__ - Global step 100 Train loss 10.210789 Classification-F1 0.0 on epoch=49
03/10/2022 23:11:37 - INFO - __main__ - Step 110 Global step 110 Train loss 9.285108 on epoch=54
03/10/2022 23:11:42 - INFO - __main__ - Step 120 Global step 120 Train loss 9.257916 on epoch=59
03/10/2022 23:11:47 - INFO - __main__ - Step 130 Global step 130 Train loss 9.073147 on epoch=64
03/10/2022 23:11:52 - INFO - __main__ - Step 140 Global step 140 Train loss 9.167657 on epoch=69
03/10/2022 23:11:57 - INFO - __main__ - Step 150 Global step 150 Train loss 8.477183 on epoch=74
03/10/2022 23:12:02 - INFO - __main__ - Global step 150 Train loss 9.052201 Classification-F1 0.0 on epoch=74
03/10/2022 23:12:07 - INFO - __main__ - Step 160 Global step 160 Train loss 8.335604 on epoch=79
03/10/2022 23:12:12 - INFO - __main__ - Step 170 Global step 170 Train loss 7.891934 on epoch=84
03/10/2022 23:12:17 - INFO - __main__ - Step 180 Global step 180 Train loss 8.007432 on epoch=89
03/10/2022 23:12:22 - INFO - __main__ - Step 190 Global step 190 Train loss 8.118544 on epoch=94
03/10/2022 23:12:27 - INFO - __main__ - Step 200 Global step 200 Train loss 7.658753 on epoch=99
03/10/2022 23:12:29 - INFO - __main__ - Global step 200 Train loss 8.002453 Classification-F1 0.0 on epoch=99
03/10/2022 23:12:34 - INFO - __main__ - Step 210 Global step 210 Train loss 6.854745 on epoch=104
03/10/2022 23:12:39 - INFO - __main__ - Step 220 Global step 220 Train loss 6.515909 on epoch=109
03/10/2022 23:12:44 - INFO - __main__ - Step 230 Global step 230 Train loss 5.859969 on epoch=114
03/10/2022 23:12:49 - INFO - __main__ - Step 240 Global step 240 Train loss 5.664558 on epoch=119
03/10/2022 23:12:54 - INFO - __main__ - Step 250 Global step 250 Train loss 5.105934 on epoch=124
03/10/2022 23:12:56 - INFO - __main__ - Global step 250 Train loss 6.000223 Classification-F1 0.0 on epoch=124
03/10/2022 23:13:01 - INFO - __main__ - Step 260 Global step 260 Train loss 4.673024 on epoch=129
03/10/2022 23:13:06 - INFO - __main__ - Step 270 Global step 270 Train loss 3.714607 on epoch=134
03/10/2022 23:13:11 - INFO - __main__ - Step 280 Global step 280 Train loss 3.497584 on epoch=139
03/10/2022 23:13:16 - INFO - __main__ - Step 290 Global step 290 Train loss 2.752149 on epoch=144
03/10/2022 23:13:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.741247 on epoch=149
03/10/2022 23:13:21 - INFO - __main__ - Global step 300 Train loss 3.275722 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:13:27 - INFO - __main__ - Step 310 Global step 310 Train loss 2.534621 on epoch=154
03/10/2022 23:13:32 - INFO - __main__ - Step 320 Global step 320 Train loss 2.842408 on epoch=159
03/10/2022 23:13:37 - INFO - __main__ - Step 330 Global step 330 Train loss 2.263562 on epoch=164
03/10/2022 23:13:42 - INFO - __main__ - Step 340 Global step 340 Train loss 1.598289 on epoch=169
03/10/2022 23:13:46 - INFO - __main__ - Step 350 Global step 350 Train loss 1.780889 on epoch=174
03/10/2022 23:13:47 - INFO - __main__ - Global step 350 Train loss 2.203954 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:13:52 - INFO - __main__ - Step 360 Global step 360 Train loss 2.769374 on epoch=179
03/10/2022 23:13:57 - INFO - __main__ - Step 370 Global step 370 Train loss 2.054097 on epoch=184
03/10/2022 23:14:02 - INFO - __main__ - Step 380 Global step 380 Train loss 1.623240 on epoch=189
03/10/2022 23:14:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.978053 on epoch=194
03/10/2022 23:14:12 - INFO - __main__ - Step 400 Global step 400 Train loss 2.068910 on epoch=199
03/10/2022 23:14:12 - INFO - __main__ - Global step 400 Train loss 2.098735 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 23:14:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.852736 on epoch=204
03/10/2022 23:14:22 - INFO - __main__ - Step 420 Global step 420 Train loss 2.241716 on epoch=209
03/10/2022 23:14:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.369828 on epoch=214
03/10/2022 23:14:32 - INFO - __main__ - Step 440 Global step 440 Train loss 1.771832 on epoch=219
03/10/2022 23:14:37 - INFO - __main__ - Step 450 Global step 450 Train loss 2.045182 on epoch=224
03/10/2022 23:14:37 - INFO - __main__ - Global step 450 Train loss 1.856259 Classification-F1 0.3333333333333333 on epoch=224
03/10/2022 23:14:42 - INFO - __main__ - Step 460 Global step 460 Train loss 1.782201 on epoch=229
03/10/2022 23:14:47 - INFO - __main__ - Step 470 Global step 470 Train loss 2.522679 on epoch=234
03/10/2022 23:14:52 - INFO - __main__ - Step 480 Global step 480 Train loss 2.060189 on epoch=239
03/10/2022 23:14:57 - INFO - __main__ - Step 490 Global step 490 Train loss 1.571765 on epoch=244
03/10/2022 23:15:02 - INFO - __main__ - Step 500 Global step 500 Train loss 1.604925 on epoch=249
03/10/2022 23:15:03 - INFO - __main__ - Global step 500 Train loss 1.908352 Classification-F1 0.3333333333333333 on epoch=249
03/10/2022 23:15:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.227224 on epoch=254
03/10/2022 23:15:12 - INFO - __main__ - Step 520 Global step 520 Train loss 1.425877 on epoch=259
03/10/2022 23:15:17 - INFO - __main__ - Step 530 Global step 530 Train loss 2.017572 on epoch=264
03/10/2022 23:15:22 - INFO - __main__ - Step 540 Global step 540 Train loss 1.459543 on epoch=269
03/10/2022 23:15:27 - INFO - __main__ - Step 550 Global step 550 Train loss 2.008282 on epoch=274
03/10/2022 23:15:28 - INFO - __main__ - Global step 550 Train loss 1.627699 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 23:15:33 - INFO - __main__ - Step 560 Global step 560 Train loss 1.500864 on epoch=279
03/10/2022 23:15:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.726971 on epoch=284
03/10/2022 23:15:42 - INFO - __main__ - Step 580 Global step 580 Train loss 1.679193 on epoch=289
03/10/2022 23:15:47 - INFO - __main__ - Step 590 Global step 590 Train loss 1.645600 on epoch=294
03/10/2022 23:15:52 - INFO - __main__ - Step 600 Global step 600 Train loss 1.392170 on epoch=299
03/10/2022 23:15:53 - INFO - __main__ - Global step 600 Train loss 1.588959 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 23:15:53 - INFO - __main__ - save last model!
03/10/2022 23:15:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:15:53 - INFO - __main__ - Printing 3 examples
03/10/2022 23:15:53 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:15:53 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:53 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:15:53 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:53 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:15:53 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:15:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:15:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:15:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:15:54 - INFO - __main__ - Printing 3 examples
03/10/2022 23:15:54 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:15:54 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:54 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:15:54 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:54 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:15:54 - INFO - __main__ - ['non-irony']
03/10/2022 23:15:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:15:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:15:54 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:16:00 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:16:01 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:16:01 - INFO - __main__ - Printing 3 examples
03/10/2022 23:16:01 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:16:01 - INFO - __main__ - ['hate']
03/10/2022 23:16:01 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:16:01 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:01 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:16:01 - INFO - __main__ - ['hate']
03/10/2022 23:16:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:16:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:16:02 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:16:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:16:03 - INFO - __main__ - Starting training!
03/10/2022 23:16:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.0001_8_predictions.txt
03/10/2022 23:16:15 - INFO - __main__ - Classification-F1 on test data: 0.3254
03/10/2022 23:16:16 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.32540425531914896
03/10/2022 23:16:16 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.0005, bsz=8 ...
03/10/2022 23:16:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:16:17 - INFO - __main__ - Printing 3 examples
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:16:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:16:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:16:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:16:17 - INFO - __main__ - Printing 3 examples
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:16:17 - INFO - __main__ - ['non-irony']
03/10/2022 23:16:17 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:16:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:16:17 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:16:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:16:27 - INFO - __main__ - Starting training!
03/10/2022 23:16:31 - INFO - __main__ - Step 10 Global step 10 Train loss 20.408684 on epoch=4
03/10/2022 23:16:35 - INFO - __main__ - Step 20 Global step 20 Train loss 15.035864 on epoch=9
03/10/2022 23:16:40 - INFO - __main__ - Step 30 Global step 30 Train loss 10.359385 on epoch=14
03/10/2022 23:16:45 - INFO - __main__ - Step 40 Global step 40 Train loss 8.011955 on epoch=19
03/10/2022 23:16:50 - INFO - __main__ - Step 50 Global step 50 Train loss 6.895583 on epoch=24
03/10/2022 23:16:50 - INFO - __main__ - Global step 50 Train loss 12.142294 Classification-F1 0.0 on epoch=24
03/10/2022 23:16:56 - INFO - __main__ - Step 60 Global step 60 Train loss 5.232360 on epoch=29
03/10/2022 23:17:01 - INFO - __main__ - Step 70 Global step 70 Train loss 2.920515 on epoch=34
03/10/2022 23:17:06 - INFO - __main__ - Step 80 Global step 80 Train loss 1.931496 on epoch=39
03/10/2022 23:17:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.704056 on epoch=44
03/10/2022 23:17:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.809947 on epoch=49
03/10/2022 23:17:16 - INFO - __main__ - Global step 100 Train loss 2.719674 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 23:17:22 - INFO - __main__ - Step 110 Global step 110 Train loss 1.585958 on epoch=54
03/10/2022 23:17:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.320042 on epoch=59
03/10/2022 23:17:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.246732 on epoch=64
03/10/2022 23:17:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.293090 on epoch=69
03/10/2022 23:17:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.223987 on epoch=74
03/10/2022 23:17:41 - INFO - __main__ - Global step 150 Train loss 0.533962 Classification-F1 0.5151515151515151 on epoch=74
03/10/2022 23:17:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.207686 on epoch=79
03/10/2022 23:17:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.217772 on epoch=84
03/10/2022 23:17:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.186853 on epoch=89
03/10/2022 23:18:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.274665 on epoch=94
03/10/2022 23:18:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.184050 on epoch=99
03/10/2022 23:18:06 - INFO - __main__ - Global step 200 Train loss 0.214205 Classification-F1 0.4909862142099682 on epoch=99
03/10/2022 23:18:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.203390 on epoch=104
03/10/2022 23:18:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.197319 on epoch=109
03/10/2022 23:18:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.191018 on epoch=114
03/10/2022 23:18:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.244526 on epoch=119
03/10/2022 23:18:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.225080 on epoch=124
03/10/2022 23:18:30 - INFO - __main__ - Global step 250 Train loss 0.212267 Classification-F1 0.6267232237539766 on epoch=124
03/10/2022 23:18:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.196757 on epoch=129
03/10/2022 23:18:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.203021 on epoch=134
03/10/2022 23:18:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.257779 on epoch=139
03/10/2022 23:18:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.190942 on epoch=144
03/10/2022 23:18:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.248040 on epoch=149
03/10/2022 23:18:56 - INFO - __main__ - Global step 300 Train loss 0.219308 Classification-F1 0.4231177094379639 on epoch=149
03/10/2022 23:19:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.248422 on epoch=154
03/10/2022 23:19:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.207442 on epoch=159
03/10/2022 23:19:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.188682 on epoch=164
03/10/2022 23:19:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.175612 on epoch=169
03/10/2022 23:19:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.144567 on epoch=174
03/10/2022 23:19:20 - INFO - __main__ - Global step 350 Train loss 0.192945 Classification-F1 0.46843853820598 on epoch=174
03/10/2022 23:19:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.157495 on epoch=179
03/10/2022 23:19:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.143446 on epoch=184
03/10/2022 23:19:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.126581 on epoch=189
03/10/2022 23:19:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.181397 on epoch=194
03/10/2022 23:19:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.163910 on epoch=199
03/10/2022 23:19:45 - INFO - __main__ - Global step 400 Train loss 0.154566 Classification-F1 0.464039408866995 on epoch=199
03/10/2022 23:19:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.153721 on epoch=204
03/10/2022 23:19:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.156313 on epoch=209
03/10/2022 23:19:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.102841 on epoch=214
03/10/2022 23:20:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.129916 on epoch=219
03/10/2022 23:20:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.080907 on epoch=224
03/10/2022 23:20:09 - INFO - __main__ - Global step 450 Train loss 0.124739 Classification-F1 0.4920634920634921 on epoch=224
03/10/2022 23:20:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.090292 on epoch=229
03/10/2022 23:20:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.084083 on epoch=234
03/10/2022 23:20:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.079771 on epoch=239
03/10/2022 23:20:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.105769 on epoch=244
03/10/2022 23:20:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.073311 on epoch=249
03/10/2022 23:20:34 - INFO - __main__ - Global step 500 Train loss 0.086645 Classification-F1 0.5333333333333333 on epoch=249
03/10/2022 23:20:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.038254 on epoch=254
03/10/2022 23:20:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.164626 on epoch=259
03/10/2022 23:20:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.062663 on epoch=264
03/10/2022 23:20:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.060070 on epoch=269
03/10/2022 23:20:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.033393 on epoch=274
03/10/2022 23:20:58 - INFO - __main__ - Global step 550 Train loss 0.071801 Classification-F1 0.46843853820598 on epoch=274
03/10/2022 23:21:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.043171 on epoch=279
03/10/2022 23:21:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.063748 on epoch=284
03/10/2022 23:21:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036020 on epoch=289
03/10/2022 23:21:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.137376 on epoch=294
03/10/2022 23:21:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.032638 on epoch=299
03/10/2022 23:21:23 - INFO - __main__ - Global step 600 Train loss 0.062591 Classification-F1 0.5076923076923077 on epoch=299
03/10/2022 23:21:23 - INFO - __main__ - save last model!
03/10/2022 23:21:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:21:23 - INFO - __main__ - Printing 3 examples
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:21:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:21:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:21:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:21:24 - INFO - __main__ - Printing 3 examples
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:21:24 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:24 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:21:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:21:24 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:21:30 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:21:30 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:21:30 - INFO - __main__ - Printing 3 examples
03/10/2022 23:21:30 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:21:30 - INFO - __main__ - ['hate']
03/10/2022 23:21:30 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:21:30 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:30 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:21:30 - INFO - __main__ - ['hate']
03/10/2022 23:21:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:21:31 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:21:32 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:21:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:21:34 - INFO - __main__ - Starting training!
03/10/2022 23:21:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.0005_8_predictions.txt
03/10/2022 23:21:45 - INFO - __main__ - Classification-F1 on test data: 0.4459
03/10/2022 23:21:45 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.0005, bsz=8, dev_performance=0.6267232237539766, test_performance=0.4458628292909365
03/10/2022 23:21:45 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.0003, bsz=8 ...
03/10/2022 23:21:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:21:46 - INFO - __main__ - Printing 3 examples
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:21:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:21:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:21:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:21:46 - INFO - __main__ - Printing 3 examples
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:21:46 - INFO - __main__ - ['non-irony']
03/10/2022 23:21:46 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:21:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:21:46 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:21:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:21:56 - INFO - __main__ - Starting training!
03/10/2022 23:22:00 - INFO - __main__ - Step 10 Global step 10 Train loss 20.614811 on epoch=4
03/10/2022 23:22:04 - INFO - __main__ - Step 20 Global step 20 Train loss 16.523304 on epoch=9
03/10/2022 23:22:09 - INFO - __main__ - Step 30 Global step 30 Train loss 10.885416 on epoch=14
03/10/2022 23:22:14 - INFO - __main__ - Step 40 Global step 40 Train loss 9.282953 on epoch=19
03/10/2022 23:22:19 - INFO - __main__ - Step 50 Global step 50 Train loss 8.151024 on epoch=24
03/10/2022 23:22:22 - INFO - __main__ - Global step 50 Train loss 13.091501 Classification-F1 0.0 on epoch=24
03/10/2022 23:22:27 - INFO - __main__ - Step 60 Global step 60 Train loss 7.860570 on epoch=29
03/10/2022 23:22:32 - INFO - __main__ - Step 70 Global step 70 Train loss 7.208632 on epoch=34
03/10/2022 23:22:37 - INFO - __main__ - Step 80 Global step 80 Train loss 5.961106 on epoch=39
03/10/2022 23:22:42 - INFO - __main__ - Step 90 Global step 90 Train loss 5.052328 on epoch=44
03/10/2022 23:22:46 - INFO - __main__ - Step 100 Global step 100 Train loss 3.611388 on epoch=49
03/10/2022 23:22:47 - INFO - __main__ - Global step 100 Train loss 5.938805 Classification-F1 0.0 on epoch=49
03/10/2022 23:22:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.608310 on epoch=54
03/10/2022 23:22:57 - INFO - __main__ - Step 120 Global step 120 Train loss 2.512814 on epoch=59
03/10/2022 23:23:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.281485 on epoch=64
03/10/2022 23:23:06 - INFO - __main__ - Step 140 Global step 140 Train loss 1.916525 on epoch=69
03/10/2022 23:23:11 - INFO - __main__ - Step 150 Global step 150 Train loss 2.207959 on epoch=74
03/10/2022 23:23:12 - INFO - __main__ - Global step 150 Train loss 2.305419 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:23:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.780994 on epoch=79
03/10/2022 23:23:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.473805 on epoch=84
03/10/2022 23:23:27 - INFO - __main__ - Step 180 Global step 180 Train loss 2.058642 on epoch=89
03/10/2022 23:23:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.137412 on epoch=94
03/10/2022 23:23:37 - INFO - __main__ - Step 200 Global step 200 Train loss 2.161673 on epoch=99
03/10/2022 23:23:37 - INFO - __main__ - Global step 200 Train loss 1.722505 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:23:42 - INFO - __main__ - Step 210 Global step 210 Train loss 1.491536 on epoch=104
03/10/2022 23:23:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.544046 on epoch=109
03/10/2022 23:23:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.496196 on epoch=114
03/10/2022 23:23:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.762723 on epoch=119
03/10/2022 23:24:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.867985 on epoch=124
03/10/2022 23:24:02 - INFO - __main__ - Global step 250 Train loss 1.432497 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 23:24:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.043950 on epoch=129
03/10/2022 23:24:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.930580 on epoch=134
03/10/2022 23:24:16 - INFO - __main__ - Step 280 Global step 280 Train loss 1.215003 on epoch=139
03/10/2022 23:24:21 - INFO - __main__ - Step 290 Global step 290 Train loss 1.277820 on epoch=144
03/10/2022 23:24:26 - INFO - __main__ - Step 300 Global step 300 Train loss 1.098081 on epoch=149
03/10/2022 23:24:26 - INFO - __main__ - Global step 300 Train loss 1.113087 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:24:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.817343 on epoch=154
03/10/2022 23:24:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.830326 on epoch=159
03/10/2022 23:24:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.933470 on epoch=164
03/10/2022 23:24:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.959881 on epoch=169
03/10/2022 23:24:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.820984 on epoch=174
03/10/2022 23:24:51 - INFO - __main__ - Global step 350 Train loss 0.872401 Classification-F1 0.3992490613266583 on epoch=174
03/10/2022 23:24:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.659544 on epoch=179
03/10/2022 23:25:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.749173 on epoch=184
03/10/2022 23:25:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.925546 on epoch=189
03/10/2022 23:25:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.570036 on epoch=194
03/10/2022 23:25:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.613026 on epoch=199
03/10/2022 23:25:16 - INFO - __main__ - Global step 400 Train loss 0.703465 Classification-F1 0.4458874458874459 on epoch=199
03/10/2022 23:25:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.569103 on epoch=204
03/10/2022 23:25:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.494346 on epoch=209
03/10/2022 23:25:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.480566 on epoch=214
03/10/2022 23:25:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.528827 on epoch=219
03/10/2022 23:25:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.536985 on epoch=224
03/10/2022 23:25:42 - INFO - __main__ - Global step 450 Train loss 0.521965 Classification-F1 0.4458874458874459 on epoch=224
03/10/2022 23:25:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.424836 on epoch=229
03/10/2022 23:25:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.269905 on epoch=234
03/10/2022 23:25:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.373599 on epoch=239
03/10/2022 23:26:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.484964 on epoch=244
03/10/2022 23:26:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.355961 on epoch=249
03/10/2022 23:26:06 - INFO - __main__ - Global step 500 Train loss 0.381853 Classification-F1 0.4458874458874459 on epoch=249
03/10/2022 23:26:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.323131 on epoch=254
03/10/2022 23:26:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.364035 on epoch=259
03/10/2022 23:26:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.273585 on epoch=264
03/10/2022 23:26:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.260755 on epoch=269
03/10/2022 23:26:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.270950 on epoch=274
03/10/2022 23:26:31 - INFO - __main__ - Global step 550 Train loss 0.298491 Classification-F1 0.4458874458874459 on epoch=274
03/10/2022 23:26:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.253971 on epoch=279
03/10/2022 23:26:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.208252 on epoch=284
03/10/2022 23:26:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.191299 on epoch=289
03/10/2022 23:26:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.536437 on epoch=294
03/10/2022 23:26:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.400953 on epoch=299
03/10/2022 23:26:56 - INFO - __main__ - Global step 600 Train loss 0.318183 Classification-F1 0.4458874458874459 on epoch=299
03/10/2022 23:26:56 - INFO - __main__ - save last model!
03/10/2022 23:26:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:26:56 - INFO - __main__ - Printing 3 examples
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:26:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:26:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:26:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:26:56 - INFO - __main__ - Printing 3 examples
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:26:56 - INFO - __main__ - ['non-irony']
03/10/2022 23:26:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:26:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:26:56 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:27:02 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:27:03 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:27:03 - INFO - __main__ - Printing 3 examples
03/10/2022 23:27:03 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:27:03 - INFO - __main__ - ['hate']
03/10/2022 23:27:03 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:27:03 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:03 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:27:03 - INFO - __main__ - ['hate']
03/10/2022 23:27:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:27:04 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:27:05 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:27:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:27:06 - INFO - __main__ - Starting training!
03/10/2022 23:27:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.0003_8_predictions.txt
03/10/2022 23:27:19 - INFO - __main__ - Classification-F1 on test data: 0.4675
03/10/2022 23:27:19 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.0003, bsz=8, dev_performance=0.4458874458874459, test_performance=0.4674620951879005
03/10/2022 23:27:19 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.0002, bsz=8 ...
03/10/2022 23:27:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:27:20 - INFO - __main__ - Printing 3 examples
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:27:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:27:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:27:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:27:20 - INFO - __main__ - Printing 3 examples
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:27:20 - INFO - __main__ - ['non-irony']
03/10/2022 23:27:20 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:27:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:27:20 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:27:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:27:30 - INFO - __main__ - Starting training!
03/10/2022 23:27:34 - INFO - __main__ - Step 10 Global step 10 Train loss 19.682629 on epoch=4
03/10/2022 23:27:39 - INFO - __main__ - Step 20 Global step 20 Train loss 15.469951 on epoch=9
03/10/2022 23:27:44 - INFO - __main__ - Step 30 Global step 30 Train loss 11.788306 on epoch=14
03/10/2022 23:27:49 - INFO - __main__ - Step 40 Global step 40 Train loss 9.799884 on epoch=19
03/10/2022 23:27:53 - INFO - __main__ - Step 50 Global step 50 Train loss 9.154011 on epoch=24
03/10/2022 23:27:58 - INFO - __main__ - Global step 50 Train loss 13.178956 Classification-F1 0.0 on epoch=24
03/10/2022 23:28:04 - INFO - __main__ - Step 60 Global step 60 Train loss 8.104300 on epoch=29
03/10/2022 23:28:09 - INFO - __main__ - Step 70 Global step 70 Train loss 7.985136 on epoch=34
03/10/2022 23:28:13 - INFO - __main__ - Step 80 Global step 80 Train loss 8.390733 on epoch=39
03/10/2022 23:28:18 - INFO - __main__ - Step 90 Global step 90 Train loss 7.278709 on epoch=44
03/10/2022 23:28:23 - INFO - __main__ - Step 100 Global step 100 Train loss 6.533175 on epoch=49
03/10/2022 23:28:26 - INFO - __main__ - Global step 100 Train loss 7.658411 Classification-F1 0.0 on epoch=49
03/10/2022 23:28:31 - INFO - __main__ - Step 110 Global step 110 Train loss 5.524539 on epoch=54
03/10/2022 23:28:36 - INFO - __main__ - Step 120 Global step 120 Train loss 5.202846 on epoch=59
03/10/2022 23:28:41 - INFO - __main__ - Step 130 Global step 130 Train loss 4.373723 on epoch=64
03/10/2022 23:28:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.782376 on epoch=69
03/10/2022 23:28:50 - INFO - __main__ - Step 150 Global step 150 Train loss 2.546757 on epoch=74
03/10/2022 23:28:52 - INFO - __main__ - Global step 150 Train loss 4.086048 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:28:58 - INFO - __main__ - Step 160 Global step 160 Train loss 2.613192 on epoch=79
03/10/2022 23:29:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.845130 on epoch=84
03/10/2022 23:29:08 - INFO - __main__ - Step 180 Global step 180 Train loss 1.962744 on epoch=89
03/10/2022 23:29:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.809727 on epoch=94
03/10/2022 23:29:18 - INFO - __main__ - Step 200 Global step 200 Train loss 1.848801 on epoch=99
03/10/2022 23:29:18 - INFO - __main__ - Global step 200 Train loss 2.015919 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:29:23 - INFO - __main__ - Step 210 Global step 210 Train loss 2.037377 on epoch=104
03/10/2022 23:29:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.745111 on epoch=109
03/10/2022 23:29:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.989661 on epoch=114
03/10/2022 23:29:38 - INFO - __main__ - Step 240 Global step 240 Train loss 2.006203 on epoch=119
03/10/2022 23:29:43 - INFO - __main__ - Step 250 Global step 250 Train loss 1.747976 on epoch=124
03/10/2022 23:29:43 - INFO - __main__ - Global step 250 Train loss 1.905266 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 23:29:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.951038 on epoch=129
03/10/2022 23:29:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.619215 on epoch=134
03/10/2022 23:29:58 - INFO - __main__ - Step 280 Global step 280 Train loss 1.181252 on epoch=139
03/10/2022 23:30:03 - INFO - __main__ - Step 290 Global step 290 Train loss 1.488515 on epoch=144
03/10/2022 23:30:07 - INFO - __main__ - Step 300 Global step 300 Train loss 1.527771 on epoch=149
03/10/2022 23:30:08 - INFO - __main__ - Global step 300 Train loss 1.553558 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:30:13 - INFO - __main__ - Step 310 Global step 310 Train loss 1.748315 on epoch=154
03/10/2022 23:30:18 - INFO - __main__ - Step 320 Global step 320 Train loss 1.760008 on epoch=159
03/10/2022 23:30:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.274418 on epoch=164
03/10/2022 23:30:27 - INFO - __main__ - Step 340 Global step 340 Train loss 1.481255 on epoch=169
03/10/2022 23:30:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.939836 on epoch=174
03/10/2022 23:30:33 - INFO - __main__ - Global step 350 Train loss 1.640767 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:30:37 - INFO - __main__ - Step 360 Global step 360 Train loss 1.484759 on epoch=179
03/10/2022 23:30:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.829793 on epoch=184
03/10/2022 23:30:47 - INFO - __main__ - Step 380 Global step 380 Train loss 1.516748 on epoch=189
03/10/2022 23:30:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.248256 on epoch=194
03/10/2022 23:30:57 - INFO - __main__ - Step 400 Global step 400 Train loss 1.085290 on epoch=199
03/10/2022 23:30:57 - INFO - __main__ - Global step 400 Train loss 1.432970 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 23:31:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.181236 on epoch=204
03/10/2022 23:31:07 - INFO - __main__ - Step 420 Global step 420 Train loss 1.202487 on epoch=209
03/10/2022 23:31:12 - INFO - __main__ - Step 430 Global step 430 Train loss 1.156886 on epoch=214
03/10/2022 23:31:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.915412 on epoch=219
03/10/2022 23:31:22 - INFO - __main__ - Step 450 Global step 450 Train loss 1.066619 on epoch=224
03/10/2022 23:31:23 - INFO - __main__ - Global step 450 Train loss 1.104528 Classification-F1 0.6190476190476191 on epoch=224
03/10/2022 23:31:28 - INFO - __main__ - Step 460 Global step 460 Train loss 1.049112 on epoch=229
03/10/2022 23:31:33 - INFO - __main__ - Step 470 Global step 470 Train loss 1.077771 on epoch=234
03/10/2022 23:31:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.848514 on epoch=239
03/10/2022 23:31:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.902903 on epoch=244
03/10/2022 23:31:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.958982 on epoch=249
03/10/2022 23:31:49 - INFO - __main__ - Global step 500 Train loss 0.967456 Classification-F1 0.4231177094379639 on epoch=249
03/10/2022 23:31:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.970150 on epoch=254
03/10/2022 23:31:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.635107 on epoch=259
03/10/2022 23:32:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.673291 on epoch=264
03/10/2022 23:32:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.652435 on epoch=269
03/10/2022 23:32:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.759717 on epoch=274
03/10/2022 23:32:14 - INFO - __main__ - Global step 550 Train loss 0.738140 Classification-F1 0.4554554554554554 on epoch=274
03/10/2022 23:32:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.641225 on epoch=279
03/10/2022 23:32:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.900286 on epoch=284
03/10/2022 23:32:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.874870 on epoch=289
03/10/2022 23:32:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.661276 on epoch=294
03/10/2022 23:32:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.529950 on epoch=299
03/10/2022 23:32:39 - INFO - __main__ - Global step 600 Train loss 0.721521 Classification-F1 0.6559139784946237 on epoch=299
03/10/2022 23:32:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:32:40 - INFO - __main__ - Printing 3 examples
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:32:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:32:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:32:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:32:40 - INFO - __main__ - Printing 3 examples
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:32:40 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:32:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:32:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:32:41 - INFO - __main__ - save last model!
03/10/2022 23:32:48 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:32:48 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:32:48 - INFO - __main__ - Printing 3 examples
03/10/2022 23:32:48 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:32:48 - INFO - __main__ - ['hate']
03/10/2022 23:32:48 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:32:48 - INFO - __main__ - ['non-irony']
03/10/2022 23:32:48 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:32:48 - INFO - __main__ - ['hate']
03/10/2022 23:32:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:32:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:32:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:32:49 - INFO - __main__ - Starting training!
03/10/2022 23:32:49 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:33:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.0002_8_predictions.txt
03/10/2022 23:33:04 - INFO - __main__ - Classification-F1 on test data: 0.5580
03/10/2022 23:33:04 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.0002, bsz=8, dev_performance=0.6559139784946237, test_performance=0.557997837183434
03/10/2022 23:33:04 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.0001, bsz=8 ...
03/10/2022 23:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:33:05 - INFO - __main__ - Printing 3 examples
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:33:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:33:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:33:05 - INFO - __main__ - Printing 3 examples
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/10/2022 23:33:05 - INFO - __main__ - ['non-irony']
03/10/2022 23:33:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:33:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:33:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:33:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:33:15 - INFO - __main__ - Starting training!
03/10/2022 23:33:19 - INFO - __main__ - Step 10 Global step 10 Train loss 20.720350 on epoch=4
03/10/2022 23:33:23 - INFO - __main__ - Step 20 Global step 20 Train loss 17.192352 on epoch=9
03/10/2022 23:33:28 - INFO - __main__ - Step 30 Global step 30 Train loss 14.796283 on epoch=14
03/10/2022 23:33:33 - INFO - __main__ - Step 40 Global step 40 Train loss 11.691977 on epoch=19
03/10/2022 23:33:38 - INFO - __main__ - Step 50 Global step 50 Train loss 10.941927 on epoch=24
03/10/2022 23:33:46 - INFO - __main__ - Global step 50 Train loss 15.068578 Classification-F1 0.0 on epoch=24
03/10/2022 23:33:52 - INFO - __main__ - Step 60 Global step 60 Train loss 10.236977 on epoch=29
03/10/2022 23:33:56 - INFO - __main__ - Step 70 Global step 70 Train loss 9.748652 on epoch=34
03/10/2022 23:34:01 - INFO - __main__ - Step 80 Global step 80 Train loss 10.041089 on epoch=39
03/10/2022 23:34:06 - INFO - __main__ - Step 90 Global step 90 Train loss 9.088427 on epoch=44
03/10/2022 23:34:11 - INFO - __main__ - Step 100 Global step 100 Train loss 9.051402 on epoch=49
03/10/2022 23:34:12 - INFO - __main__ - Global step 100 Train loss 9.633309 Classification-F1 0.0 on epoch=49
03/10/2022 23:34:17 - INFO - __main__ - Step 110 Global step 110 Train loss 9.018087 on epoch=54
03/10/2022 23:34:22 - INFO - __main__ - Step 120 Global step 120 Train loss 8.822378 on epoch=59
03/10/2022 23:34:27 - INFO - __main__ - Step 130 Global step 130 Train loss 8.635160 on epoch=64
03/10/2022 23:34:31 - INFO - __main__ - Step 140 Global step 140 Train loss 8.229464 on epoch=69
03/10/2022 23:34:36 - INFO - __main__ - Step 150 Global step 150 Train loss 8.614835 on epoch=74
03/10/2022 23:34:37 - INFO - __main__ - Global step 150 Train loss 8.663985 Classification-F1 0.0 on epoch=74
03/10/2022 23:34:42 - INFO - __main__ - Step 160 Global step 160 Train loss 7.881162 on epoch=79
03/10/2022 23:34:47 - INFO - __main__ - Step 170 Global step 170 Train loss 7.940585 on epoch=84
03/10/2022 23:34:52 - INFO - __main__ - Step 180 Global step 180 Train loss 7.561614 on epoch=89
03/10/2022 23:34:57 - INFO - __main__ - Step 190 Global step 190 Train loss 6.636990 on epoch=94
03/10/2022 23:35:02 - INFO - __main__ - Step 200 Global step 200 Train loss 7.186346 on epoch=99
03/10/2022 23:35:03 - INFO - __main__ - Global step 200 Train loss 7.441339 Classification-F1 0.0 on epoch=99
03/10/2022 23:35:07 - INFO - __main__ - Step 210 Global step 210 Train loss 7.000075 on epoch=104
03/10/2022 23:35:12 - INFO - __main__ - Step 220 Global step 220 Train loss 7.162010 on epoch=109
03/10/2022 23:35:17 - INFO - __main__ - Step 230 Global step 230 Train loss 6.277987 on epoch=114
03/10/2022 23:35:22 - INFO - __main__ - Step 240 Global step 240 Train loss 6.065404 on epoch=119
03/10/2022 23:35:27 - INFO - __main__ - Step 250 Global step 250 Train loss 5.406783 on epoch=124
03/10/2022 23:35:28 - INFO - __main__ - Global step 250 Train loss 6.382452 Classification-F1 0.0 on epoch=124
03/10/2022 23:35:32 - INFO - __main__ - Step 260 Global step 260 Train loss 4.975029 on epoch=129
03/10/2022 23:35:37 - INFO - __main__ - Step 270 Global step 270 Train loss 4.682971 on epoch=134
03/10/2022 23:35:42 - INFO - __main__ - Step 280 Global step 280 Train loss 3.581438 on epoch=139
03/10/2022 23:35:47 - INFO - __main__ - Step 290 Global step 290 Train loss 3.663974 on epoch=144
03/10/2022 23:35:52 - INFO - __main__ - Step 300 Global step 300 Train loss 3.052797 on epoch=149
03/10/2022 23:35:52 - INFO - __main__ - Global step 300 Train loss 3.991242 Classification-F1 0.0 on epoch=149
03/10/2022 23:35:57 - INFO - __main__ - Step 310 Global step 310 Train loss 2.559788 on epoch=154
03/10/2022 23:36:02 - INFO - __main__ - Step 320 Global step 320 Train loss 2.502043 on epoch=159
03/10/2022 23:36:06 - INFO - __main__ - Step 330 Global step 330 Train loss 2.424777 on epoch=164
03/10/2022 23:36:11 - INFO - __main__ - Step 340 Global step 340 Train loss 1.638576 on epoch=169
03/10/2022 23:36:16 - INFO - __main__ - Step 350 Global step 350 Train loss 2.157170 on epoch=174
03/10/2022 23:36:17 - INFO - __main__ - Global step 350 Train loss 2.256471 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:36:23 - INFO - __main__ - Step 360 Global step 360 Train loss 2.035688 on epoch=179
03/10/2022 23:36:27 - INFO - __main__ - Step 370 Global step 370 Train loss 1.976182 on epoch=184
03/10/2022 23:36:32 - INFO - __main__ - Step 380 Global step 380 Train loss 1.665867 on epoch=189
03/10/2022 23:36:37 - INFO - __main__ - Step 390 Global step 390 Train loss 1.722858 on epoch=194
03/10/2022 23:36:42 - INFO - __main__ - Step 400 Global step 400 Train loss 1.437599 on epoch=199
03/10/2022 23:36:42 - INFO - __main__ - Global step 400 Train loss 1.767639 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 23:36:47 - INFO - __main__ - Step 410 Global step 410 Train loss 1.556895 on epoch=204
03/10/2022 23:36:52 - INFO - __main__ - Step 420 Global step 420 Train loss 1.844933 on epoch=209
03/10/2022 23:36:57 - INFO - __main__ - Step 430 Global step 430 Train loss 1.945108 on epoch=214
03/10/2022 23:37:02 - INFO - __main__ - Step 440 Global step 440 Train loss 1.397186 on epoch=219
03/10/2022 23:37:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.956277 on epoch=224
03/10/2022 23:37:07 - INFO - __main__ - Global step 450 Train loss 1.740080 Classification-F1 0.3333333333333333 on epoch=224
03/10/2022 23:37:12 - INFO - __main__ - Step 460 Global step 460 Train loss 1.871328 on epoch=229
03/10/2022 23:37:16 - INFO - __main__ - Step 470 Global step 470 Train loss 1.545004 on epoch=234
03/10/2022 23:37:21 - INFO - __main__ - Step 480 Global step 480 Train loss 1.639592 on epoch=239
03/10/2022 23:37:26 - INFO - __main__ - Step 490 Global step 490 Train loss 1.136291 on epoch=244
03/10/2022 23:37:31 - INFO - __main__ - Step 500 Global step 500 Train loss 2.025117 on epoch=249
03/10/2022 23:37:31 - INFO - __main__ - Global step 500 Train loss 1.643467 Classification-F1 0.3333333333333333 on epoch=249
03/10/2022 23:37:36 - INFO - __main__ - Step 510 Global step 510 Train loss 1.715618 on epoch=254
03/10/2022 23:37:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.714109 on epoch=259
03/10/2022 23:37:46 - INFO - __main__ - Step 530 Global step 530 Train loss 1.697376 on epoch=264
03/10/2022 23:37:50 - INFO - __main__ - Step 540 Global step 540 Train loss 1.623144 on epoch=269
03/10/2022 23:37:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.670325 on epoch=274
03/10/2022 23:37:56 - INFO - __main__ - Global step 550 Train loss 1.684115 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 23:38:00 - INFO - __main__ - Step 560 Global step 560 Train loss 1.512496 on epoch=279
03/10/2022 23:38:05 - INFO - __main__ - Step 570 Global step 570 Train loss 1.263225 on epoch=284
03/10/2022 23:38:10 - INFO - __main__ - Step 580 Global step 580 Train loss 1.458253 on epoch=289
03/10/2022 23:38:15 - INFO - __main__ - Step 590 Global step 590 Train loss 1.464799 on epoch=294
03/10/2022 23:38:20 - INFO - __main__ - Step 600 Global step 600 Train loss 1.957992 on epoch=299
03/10/2022 23:38:20 - INFO - __main__ - Global step 600 Train loss 1.531353 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 23:38:20 - INFO - __main__ - save last model!
03/10/2022 23:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:38:21 - INFO - __main__ - Printing 3 examples
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:38:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:38:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:38:21 - INFO - __main__ - Printing 3 examples
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:38:21 - INFO - __main__ - ['hate']
03/10/2022 23:38:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:38:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:38:21 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:38:27 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:38:28 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:38:28 - INFO - __main__ - Printing 3 examples
03/10/2022 23:38:28 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:38:28 - INFO - __main__ - ['hate']
03/10/2022 23:38:28 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:38:28 - INFO - __main__ - ['non-irony']
03/10/2022 23:38:28 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:38:28 - INFO - __main__ - ['hate']
03/10/2022 23:38:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:38:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:38:29 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:38:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:38:31 - INFO - __main__ - Starting training!
03/10/2022 23:38:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.0001_8_predictions.txt
03/10/2022 23:38:41 - INFO - __main__ - Classification-F1 on test data: 0.3232
03/10/2022 23:38:42 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.32317505315379164
03/10/2022 23:38:42 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.0005, bsz=8 ...
03/10/2022 23:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:38:43 - INFO - __main__ - Printing 3 examples
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:38:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:38:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:38:43 - INFO - __main__ - Printing 3 examples
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:38:43 - INFO - __main__ - ['hate']
03/10/2022 23:38:43 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:38:43 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:38:43 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:38:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:38:52 - INFO - __main__ - Starting training!
03/10/2022 23:38:56 - INFO - __main__ - Step 10 Global step 10 Train loss 20.614956 on epoch=4
03/10/2022 23:39:01 - INFO - __main__ - Step 20 Global step 20 Train loss 14.664012 on epoch=9
03/10/2022 23:39:05 - INFO - __main__ - Step 30 Global step 30 Train loss 12.604796 on epoch=14
03/10/2022 23:39:10 - INFO - __main__ - Step 40 Global step 40 Train loss 9.116556 on epoch=19
03/10/2022 23:39:15 - INFO - __main__ - Step 50 Global step 50 Train loss 8.375484 on epoch=24
03/10/2022 23:39:18 - INFO - __main__ - Global step 50 Train loss 13.075161 Classification-F1 0.0 on epoch=24
03/10/2022 23:39:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.788846 on epoch=29
03/10/2022 23:39:28 - INFO - __main__ - Step 70 Global step 70 Train loss 4.598779 on epoch=34
03/10/2022 23:39:33 - INFO - __main__ - Step 80 Global step 80 Train loss 2.868620 on epoch=39
03/10/2022 23:39:37 - INFO - __main__ - Step 90 Global step 90 Train loss 2.528877 on epoch=44
03/10/2022 23:39:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.467818 on epoch=49
03/10/2022 23:39:43 - INFO - __main__ - Global step 100 Train loss 3.650588 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 23:39:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.956032 on epoch=54
03/10/2022 23:39:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.933194 on epoch=59
03/10/2022 23:39:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.405091 on epoch=64
03/10/2022 23:40:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.361221 on epoch=69
03/10/2022 23:40:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.354985 on epoch=74
03/10/2022 23:40:09 - INFO - __main__ - Global step 150 Train loss 1.602105 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:40:14 - INFO - __main__ - Step 160 Global step 160 Train loss 1.596099 on epoch=79
03/10/2022 23:40:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.544467 on epoch=84
03/10/2022 23:40:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.827458 on epoch=89
03/10/2022 23:40:29 - INFO - __main__ - Step 190 Global step 190 Train loss 1.099121 on epoch=94
03/10/2022 23:40:34 - INFO - __main__ - Step 200 Global step 200 Train loss 1.357895 on epoch=99
03/10/2022 23:40:34 - INFO - __main__ - Global step 200 Train loss 1.285008 Classification-F1 0.39756367663344405 on epoch=99
03/10/2022 23:40:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.191983 on epoch=104
03/10/2022 23:40:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.980387 on epoch=109
03/10/2022 23:40:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.074909 on epoch=114
03/10/2022 23:40:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.008976 on epoch=119
03/10/2022 23:41:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.721608 on epoch=124
03/10/2022 23:41:00 - INFO - __main__ - Global step 250 Train loss 0.995573 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 23:41:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.715169 on epoch=129
03/10/2022 23:41:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.693863 on epoch=134
03/10/2022 23:41:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.759107 on epoch=139
03/10/2022 23:41:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.599986 on epoch=144
03/10/2022 23:41:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.594659 on epoch=149
03/10/2022 23:41:25 - INFO - __main__ - Global step 300 Train loss 0.672557 Classification-F1 0.4181818181818182 on epoch=149
03/10/2022 23:41:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.523217 on epoch=154
03/10/2022 23:41:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.524541 on epoch=159
03/10/2022 23:41:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.455619 on epoch=164
03/10/2022 23:41:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.415753 on epoch=169
03/10/2022 23:41:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.380014 on epoch=174
03/10/2022 23:41:51 - INFO - __main__ - Global step 350 Train loss 0.459829 Classification-F1 0.5151515151515151 on epoch=174
03/10/2022 23:41:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.296639 on epoch=179
03/10/2022 23:42:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.250468 on epoch=184
03/10/2022 23:42:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.327621 on epoch=189
03/10/2022 23:42:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.521971 on epoch=194
03/10/2022 23:42:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.265674 on epoch=199
03/10/2022 23:42:17 - INFO - __main__ - Global step 400 Train loss 0.332475 Classification-F1 0.4458874458874459 on epoch=199
03/10/2022 23:42:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.309905 on epoch=204
03/10/2022 23:42:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.302747 on epoch=209
03/10/2022 23:42:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.331846 on epoch=214
03/10/2022 23:42:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.292629 on epoch=219
03/10/2022 23:42:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.341028 on epoch=224
03/10/2022 23:42:41 - INFO - __main__ - Global step 450 Train loss 0.315631 Classification-F1 0.5151515151515151 on epoch=224
03/10/2022 23:42:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.315610 on epoch=229
03/10/2022 23:42:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.252985 on epoch=234
03/10/2022 23:42:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.238723 on epoch=239
03/10/2022 23:43:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.238079 on epoch=244
03/10/2022 23:43:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.221973 on epoch=249
03/10/2022 23:43:06 - INFO - __main__ - Global step 500 Train loss 0.253474 Classification-F1 0.5151515151515151 on epoch=249
03/10/2022 23:43:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.266885 on epoch=254
03/10/2022 23:43:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.242961 on epoch=259
03/10/2022 23:43:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.246896 on epoch=264
03/10/2022 23:43:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.241966 on epoch=269
03/10/2022 23:43:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.230521 on epoch=274
03/10/2022 23:43:30 - INFO - __main__ - Global step 550 Train loss 0.245846 Classification-F1 0.4458874458874459 on epoch=274
03/10/2022 23:43:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.241872 on epoch=279
03/10/2022 23:43:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.251995 on epoch=284
03/10/2022 23:43:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.221465 on epoch=289
03/10/2022 23:43:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.228557 on epoch=294
03/10/2022 23:43:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.208112 on epoch=299
03/10/2022 23:43:55 - INFO - __main__ - Global step 600 Train loss 0.230400 Classification-F1 0.36374269005847953 on epoch=299
03/10/2022 23:43:55 - INFO - __main__ - save last model!
03/10/2022 23:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:43:55 - INFO - __main__ - Printing 3 examples
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:43:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:43:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:43:55 - INFO - __main__ - Printing 3 examples
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:43:55 - INFO - __main__ - ['hate']
03/10/2022 23:43:55 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:43:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:43:55 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:44:01 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:44:02 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:44:02 - INFO - __main__ - Printing 3 examples
03/10/2022 23:44:02 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:44:02 - INFO - __main__ - ['hate']
03/10/2022 23:44:02 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:44:02 - INFO - __main__ - ['non-irony']
03/10/2022 23:44:02 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:44:02 - INFO - __main__ - ['hate']
03/10/2022 23:44:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:44:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:44:03 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:44:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:44:06 - INFO - __main__ - Starting training!
03/10/2022 23:44:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.0005_8_predictions.txt
03/10/2022 23:44:18 - INFO - __main__ - Classification-F1 on test data: 0.4487
03/10/2022 23:44:18 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.0005, bsz=8, dev_performance=0.5151515151515151, test_performance=0.4487076129905546
03/10/2022 23:44:18 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.0003, bsz=8 ...
03/10/2022 23:44:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:44:19 - INFO - __main__ - Printing 3 examples
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:44:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:44:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:44:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:44:19 - INFO - __main__ - Printing 3 examples
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:44:19 - INFO - __main__ - ['hate']
03/10/2022 23:44:19 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:44:19 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:44:19 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:44:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:44:29 - INFO - __main__ - Starting training!
03/10/2022 23:44:33 - INFO - __main__ - Step 10 Global step 10 Train loss 21.259378 on epoch=4
03/10/2022 23:44:37 - INFO - __main__ - Step 20 Global step 20 Train loss 16.947117 on epoch=9
03/10/2022 23:44:42 - INFO - __main__ - Step 30 Global step 30 Train loss 12.276225 on epoch=14
03/10/2022 23:44:47 - INFO - __main__ - Step 40 Global step 40 Train loss 10.695898 on epoch=19
03/10/2022 23:44:51 - INFO - __main__ - Step 50 Global step 50 Train loss 9.417393 on epoch=24
03/10/2022 23:44:53 - INFO - __main__ - Global step 50 Train loss 14.119202 Classification-F1 0.0 on epoch=24
03/10/2022 23:44:58 - INFO - __main__ - Step 60 Global step 60 Train loss 9.000588 on epoch=29
03/10/2022 23:45:03 - INFO - __main__ - Step 70 Global step 70 Train loss 7.505569 on epoch=34
03/10/2022 23:45:07 - INFO - __main__ - Step 80 Global step 80 Train loss 6.646060 on epoch=39
03/10/2022 23:45:12 - INFO - __main__ - Step 90 Global step 90 Train loss 5.497392 on epoch=44
03/10/2022 23:45:17 - INFO - __main__ - Step 100 Global step 100 Train loss 3.833747 on epoch=49
03/10/2022 23:45:17 - INFO - __main__ - Global step 100 Train loss 6.496671 Classification-F1 0.026086956521739126 on epoch=49
03/10/2022 23:45:23 - INFO - __main__ - Step 110 Global step 110 Train loss 2.723712 on epoch=54
03/10/2022 23:45:27 - INFO - __main__ - Step 120 Global step 120 Train loss 2.258830 on epoch=59
03/10/2022 23:45:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.344620 on epoch=64
03/10/2022 23:45:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.559918 on epoch=69
03/10/2022 23:45:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.347746 on epoch=74
03/10/2022 23:45:42 - INFO - __main__ - Global step 150 Train loss 1.446965 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:45:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.264335 on epoch=79
03/10/2022 23:45:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.402434 on epoch=84
03/10/2022 23:45:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.245165 on epoch=89
03/10/2022 23:46:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.261210 on epoch=94
03/10/2022 23:46:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.293288 on epoch=99
03/10/2022 23:46:07 - INFO - __main__ - Global step 200 Train loss 0.293287 Classification-F1 0.5588547189819725 on epoch=99
03/10/2022 23:46:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.218068 on epoch=104
03/10/2022 23:46:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.234156 on epoch=109
03/10/2022 23:46:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.271492 on epoch=114
03/10/2022 23:46:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.215743 on epoch=119
03/10/2022 23:46:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.222411 on epoch=124
03/10/2022 23:46:32 - INFO - __main__ - Global step 250 Train loss 0.232374 Classification-F1 0.4385964912280702 on epoch=124
03/10/2022 23:46:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.202832 on epoch=129
03/10/2022 23:46:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.172364 on epoch=134
03/10/2022 23:46:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.192001 on epoch=139
03/10/2022 23:46:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.162632 on epoch=144
03/10/2022 23:46:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.148581 on epoch=149
03/10/2022 23:46:56 - INFO - __main__ - Global step 300 Train loss 0.175682 Classification-F1 0.5555555555555556 on epoch=149
03/10/2022 23:47:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.179566 on epoch=154
03/10/2022 23:47:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.155505 on epoch=159
03/10/2022 23:47:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.187268 on epoch=164
03/10/2022 23:47:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.092628 on epoch=169
03/10/2022 23:47:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.170383 on epoch=174
03/10/2022 23:47:20 - INFO - __main__ - Global step 350 Train loss 0.157070 Classification-F1 0.5555555555555556 on epoch=174
03/10/2022 23:47:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.096861 on epoch=179
03/10/2022 23:47:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.120123 on epoch=184
03/10/2022 23:47:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.105442 on epoch=189
03/10/2022 23:47:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.105732 on epoch=194
03/10/2022 23:47:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.098517 on epoch=199
03/10/2022 23:47:45 - INFO - __main__ - Global step 400 Train loss 0.105335 Classification-F1 0.5307917888563051 on epoch=199
03/10/2022 23:47:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.060780 on epoch=204
03/10/2022 23:47:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.080756 on epoch=209
03/10/2022 23:47:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.072054 on epoch=214
03/10/2022 23:48:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.047929 on epoch=219
03/10/2022 23:48:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.017338 on epoch=224
03/10/2022 23:48:09 - INFO - __main__ - Global step 450 Train loss 0.055771 Classification-F1 0.4682306940371457 on epoch=224
03/10/2022 23:48:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.044617 on epoch=229
03/10/2022 23:48:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.047263 on epoch=234
03/10/2022 23:48:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.019375 on epoch=239
03/10/2022 23:48:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.049942 on epoch=244
03/10/2022 23:48:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.036377 on epoch=249
03/10/2022 23:48:34 - INFO - __main__ - Global step 500 Train loss 0.039515 Classification-F1 0.5270935960591133 on epoch=249
03/10/2022 23:48:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.017727 on epoch=254
03/10/2022 23:48:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.017351 on epoch=259
03/10/2022 23:48:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.034787 on epoch=264
03/10/2022 23:48:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.015007 on epoch=269
03/10/2022 23:48:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.029088 on epoch=274
03/10/2022 23:48:58 - INFO - __main__ - Global step 550 Train loss 0.022792 Classification-F1 0.4980392156862745 on epoch=274
03/10/2022 23:49:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026821 on epoch=279
03/10/2022 23:49:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.132627 on epoch=284
03/10/2022 23:49:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.015744 on epoch=289
03/10/2022 23:49:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003428 on epoch=294
03/10/2022 23:49:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.033963 on epoch=299
03/10/2022 23:49:23 - INFO - __main__ - Global step 600 Train loss 0.042516 Classification-F1 0.5555555555555556 on epoch=299
03/10/2022 23:49:23 - INFO - __main__ - save last model!
03/10/2022 23:49:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:49:24 - INFO - __main__ - Printing 3 examples
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:49:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:49:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:49:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:49:24 - INFO - __main__ - Printing 3 examples
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:49:24 - INFO - __main__ - ['hate']
03/10/2022 23:49:24 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:49:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:49:24 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:49:30 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:49:30 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:49:30 - INFO - __main__ - Printing 3 examples
03/10/2022 23:49:30 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:49:30 - INFO - __main__ - ['hate']
03/10/2022 23:49:30 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:49:30 - INFO - __main__ - ['non-irony']
03/10/2022 23:49:30 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:49:30 - INFO - __main__ - ['hate']
03/10/2022 23:49:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:49:31 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:49:32 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:49:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:49:34 - INFO - __main__ - Starting training!
03/10/2022 23:49:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.0003_8_predictions.txt
03/10/2022 23:49:45 - INFO - __main__ - Classification-F1 on test data: 0.4864
03/10/2022 23:49:46 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.0003, bsz=8, dev_performance=0.5588547189819725, test_performance=0.48639129380252877
03/10/2022 23:49:46 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.0002, bsz=8 ...
03/10/2022 23:49:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:49:46 - INFO - __main__ - Printing 3 examples
03/10/2022 23:49:46 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:49:46 - INFO - __main__ - ['hate']
03/10/2022 23:49:46 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:49:46 - INFO - __main__ - ['hate']
03/10/2022 23:49:46 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:49:46 - INFO - __main__ - ['hate']
03/10/2022 23:49:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:49:47 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:49:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:49:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:49:47 - INFO - __main__ - Printing 3 examples
03/10/2022 23:49:47 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:49:47 - INFO - __main__ - ['hate']
03/10/2022 23:49:47 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:49:47 - INFO - __main__ - ['hate']
03/10/2022 23:49:47 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:49:47 - INFO - __main__ - ['hate']
03/10/2022 23:49:47 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:49:47 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:49:47 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:49:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:49:56 - INFO - __main__ - Starting training!
03/10/2022 23:50:01 - INFO - __main__ - Step 10 Global step 10 Train loss 21.183905 on epoch=4
03/10/2022 23:50:05 - INFO - __main__ - Step 20 Global step 20 Train loss 17.265350 on epoch=9
03/10/2022 23:50:10 - INFO - __main__ - Step 30 Global step 30 Train loss 12.284926 on epoch=14
03/10/2022 23:50:15 - INFO - __main__ - Step 40 Global step 40 Train loss 10.657813 on epoch=19
03/10/2022 23:50:19 - INFO - __main__ - Step 50 Global step 50 Train loss 9.904324 on epoch=24
03/10/2022 23:50:25 - INFO - __main__ - Global step 50 Train loss 14.259264 Classification-F1 0.0 on epoch=24
03/10/2022 23:50:30 - INFO - __main__ - Step 60 Global step 60 Train loss 9.419692 on epoch=29
03/10/2022 23:50:35 - INFO - __main__ - Step 70 Global step 70 Train loss 8.699842 on epoch=34
03/10/2022 23:50:40 - INFO - __main__ - Step 80 Global step 80 Train loss 8.633291 on epoch=39
03/10/2022 23:50:45 - INFO - __main__ - Step 90 Global step 90 Train loss 7.373891 on epoch=44
03/10/2022 23:50:50 - INFO - __main__ - Step 100 Global step 100 Train loss 7.019336 on epoch=49
03/10/2022 23:50:51 - INFO - __main__ - Global step 100 Train loss 8.229211 Classification-F1 0.0 on epoch=49
03/10/2022 23:50:56 - INFO - __main__ - Step 110 Global step 110 Train loss 6.459175 on epoch=54
03/10/2022 23:51:01 - INFO - __main__ - Step 120 Global step 120 Train loss 5.082000 on epoch=59
03/10/2022 23:51:06 - INFO - __main__ - Step 130 Global step 130 Train loss 4.301535 on epoch=64
03/10/2022 23:51:10 - INFO - __main__ - Step 140 Global step 140 Train loss 3.169549 on epoch=69
03/10/2022 23:51:15 - INFO - __main__ - Step 150 Global step 150 Train loss 2.478623 on epoch=74
03/10/2022 23:51:16 - INFO - __main__ - Global step 150 Train loss 4.298177 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:51:21 - INFO - __main__ - Step 160 Global step 160 Train loss 2.042440 on epoch=79
03/10/2022 23:51:26 - INFO - __main__ - Step 170 Global step 170 Train loss 2.294287 on epoch=84
03/10/2022 23:51:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.977213 on epoch=89
03/10/2022 23:51:36 - INFO - __main__ - Step 190 Global step 190 Train loss 2.540495 on epoch=94
03/10/2022 23:51:40 - INFO - __main__ - Step 200 Global step 200 Train loss 2.129149 on epoch=99
03/10/2022 23:51:41 - INFO - __main__ - Global step 200 Train loss 2.196717 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:51:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.961849 on epoch=104
03/10/2022 23:51:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.250474 on epoch=109
03/10/2022 23:51:55 - INFO - __main__ - Step 230 Global step 230 Train loss 2.054218 on epoch=114
03/10/2022 23:52:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.367722 on epoch=119
03/10/2022 23:52:05 - INFO - __main__ - Step 250 Global step 250 Train loss 2.024433 on epoch=124
03/10/2022 23:52:05 - INFO - __main__ - Global step 250 Train loss 1.731739 Classification-F1 0.4666666666666667 on epoch=124
03/10/2022 23:52:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.755608 on epoch=129
03/10/2022 23:52:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.698791 on epoch=134
03/10/2022 23:52:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.866014 on epoch=139
03/10/2022 23:52:25 - INFO - __main__ - Step 290 Global step 290 Train loss 1.597687 on epoch=144
03/10/2022 23:52:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.750201 on epoch=149
03/10/2022 23:52:31 - INFO - __main__ - Global step 300 Train loss 1.733660 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:52:35 - INFO - __main__ - Step 310 Global step 310 Train loss 1.717911 on epoch=154
03/10/2022 23:52:40 - INFO - __main__ - Step 320 Global step 320 Train loss 1.659175 on epoch=159
03/10/2022 23:52:45 - INFO - __main__ - Step 330 Global step 330 Train loss 1.511535 on epoch=164
03/10/2022 23:52:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.516606 on epoch=169
03/10/2022 23:52:55 - INFO - __main__ - Step 350 Global step 350 Train loss 1.337154 on epoch=174
03/10/2022 23:52:55 - INFO - __main__ - Global step 350 Train loss 1.548476 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:53:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.330892 on epoch=179
03/10/2022 23:53:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.415165 on epoch=184
03/10/2022 23:53:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.060013 on epoch=189
03/10/2022 23:53:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.153759 on epoch=194
03/10/2022 23:53:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.174702 on epoch=199
03/10/2022 23:53:20 - INFO - __main__ - Global step 400 Train loss 1.226906 Classification-F1 0.5636363636363637 on epoch=199
03/10/2022 23:53:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.274813 on epoch=204
03/10/2022 23:53:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.939372 on epoch=209
03/10/2022 23:53:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.422168 on epoch=214
03/10/2022 23:53:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.141479 on epoch=219
03/10/2022 23:53:45 - INFO - __main__ - Step 450 Global step 450 Train loss 1.153208 on epoch=224
03/10/2022 23:53:45 - INFO - __main__ - Global step 450 Train loss 1.186208 Classification-F1 0.5835835835835835 on epoch=224
03/10/2022 23:53:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.233804 on epoch=229
03/10/2022 23:53:56 - INFO - __main__ - Step 470 Global step 470 Train loss 1.050048 on epoch=234
03/10/2022 23:54:01 - INFO - __main__ - Step 480 Global step 480 Train loss 1.004753 on epoch=239
03/10/2022 23:54:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.861471 on epoch=244
03/10/2022 23:54:10 - INFO - __main__ - Step 500 Global step 500 Train loss 1.047324 on epoch=249
03/10/2022 23:54:11 - INFO - __main__ - Global step 500 Train loss 1.039480 Classification-F1 0.4909862142099682 on epoch=249
03/10/2022 23:54:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.894120 on epoch=254
03/10/2022 23:54:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.037033 on epoch=259
03/10/2022 23:54:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.731598 on epoch=264
03/10/2022 23:54:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.721393 on epoch=269
03/10/2022 23:54:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.526289 on epoch=274
03/10/2022 23:54:35 - INFO - __main__ - Global step 550 Train loss 0.782087 Classification-F1 0.4909862142099682 on epoch=274
03/10/2022 23:54:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.780183 on epoch=279
03/10/2022 23:54:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.866816 on epoch=284
03/10/2022 23:54:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.535079 on epoch=289
03/10/2022 23:54:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.501720 on epoch=294
03/10/2022 23:54:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.510591 on epoch=299
03/10/2022 23:55:00 - INFO - __main__ - Global step 600 Train loss 0.638878 Classification-F1 0.4909862142099682 on epoch=299
03/10/2022 23:55:00 - INFO - __main__ - save last model!
03/10/2022 23:55:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:55:00 - INFO - __main__ - Printing 3 examples
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:55:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:55:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:55:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:55:00 - INFO - __main__ - Printing 3 examples
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:55:00 - INFO - __main__ - ['hate']
03/10/2022 23:55:00 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:55:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:55:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:55:07 - INFO - __main__ - Loading checkpoint on the fly
03/10/2022 23:55:07 - INFO - __main__ - Start tokenizing ... 955 instances
03/10/2022 23:55:07 - INFO - __main__ - Printing 3 examples
03/10/2022 23:55:07 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/10/2022 23:55:07 - INFO - __main__ - ['hate']
03/10/2022 23:55:07 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/10/2022 23:55:07 - INFO - __main__ - ['non-irony']
03/10/2022 23:55:07 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/10/2022 23:55:07 - INFO - __main__ - ['hate']
03/10/2022 23:55:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:55:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:55:09 - INFO - __main__ - Loaded 955 examples from test data
03/10/2022 23:55:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:55:10 - INFO - __main__ - Starting training!
03/10/2022 23:55:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.0002_8_predictions.txt
03/10/2022 23:55:23 - INFO - __main__ - Classification-F1 on test data: 0.5328
03/10/2022 23:55:23 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.0002, bsz=8, dev_performance=0.5835835835835835, test_performance=0.532847481021394
03/10/2022 23:55:23 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.0001, bsz=8 ...
03/10/2022 23:55:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:55:24 - INFO - __main__ - Printing 3 examples
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:55:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:55:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:55:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:55:24 - INFO - __main__ - Printing 3 examples
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/10/2022 23:55:24 - INFO - __main__ - ['hate']
03/10/2022 23:55:24 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:55:24 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:55:24 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:55:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/10/2022 23:55:34 - INFO - __main__ - Starting training!
03/10/2022 23:55:38 - INFO - __main__ - Step 10 Global step 10 Train loss 21.795490 on epoch=4
03/10/2022 23:55:42 - INFO - __main__ - Step 20 Global step 20 Train loss 18.748253 on epoch=9
03/10/2022 23:55:47 - INFO - __main__ - Step 30 Global step 30 Train loss 16.191248 on epoch=14
03/10/2022 23:55:52 - INFO - __main__ - Step 40 Global step 40 Train loss 12.993675 on epoch=19
03/10/2022 23:55:57 - INFO - __main__ - Step 50 Global step 50 Train loss 12.819058 on epoch=24
03/10/2022 23:56:06 - INFO - __main__ - Global step 50 Train loss 16.509544 Classification-F1 0.0 on epoch=24
03/10/2022 23:56:11 - INFO - __main__ - Step 60 Global step 60 Train loss 10.923052 on epoch=29
03/10/2022 23:56:16 - INFO - __main__ - Step 70 Global step 70 Train loss 11.018202 on epoch=34
03/10/2022 23:56:21 - INFO - __main__ - Step 80 Global step 80 Train loss 10.230215 on epoch=39
03/10/2022 23:56:26 - INFO - __main__ - Step 90 Global step 90 Train loss 10.230695 on epoch=44
03/10/2022 23:56:31 - INFO - __main__ - Step 100 Global step 100 Train loss 9.535496 on epoch=49
03/10/2022 23:56:34 - INFO - __main__ - Global step 100 Train loss 10.387530 Classification-F1 0.0 on epoch=49
03/10/2022 23:56:39 - INFO - __main__ - Step 110 Global step 110 Train loss 9.162507 on epoch=54
03/10/2022 23:56:44 - INFO - __main__ - Step 120 Global step 120 Train loss 8.717035 on epoch=59
03/10/2022 23:56:49 - INFO - __main__ - Step 130 Global step 130 Train loss 9.190329 on epoch=64
03/10/2022 23:56:53 - INFO - __main__ - Step 140 Global step 140 Train loss 8.407043 on epoch=69
03/10/2022 23:56:58 - INFO - __main__ - Step 150 Global step 150 Train loss 8.474445 on epoch=74
03/10/2022 23:57:00 - INFO - __main__ - Global step 150 Train loss 8.790272 Classification-F1 0.0 on epoch=74
03/10/2022 23:57:05 - INFO - __main__ - Step 160 Global step 160 Train loss 8.146649 on epoch=79
03/10/2022 23:57:10 - INFO - __main__ - Step 170 Global step 170 Train loss 7.777999 on epoch=84
03/10/2022 23:57:14 - INFO - __main__ - Step 180 Global step 180 Train loss 7.755956 on epoch=89
03/10/2022 23:57:19 - INFO - __main__ - Step 190 Global step 190 Train loss 7.652648 on epoch=94
03/10/2022 23:57:24 - INFO - __main__ - Step 200 Global step 200 Train loss 6.417259 on epoch=99
03/10/2022 23:57:26 - INFO - __main__ - Global step 200 Train loss 7.550102 Classification-F1 0.0 on epoch=99
03/10/2022 23:57:31 - INFO - __main__ - Step 210 Global step 210 Train loss 6.534443 on epoch=104
03/10/2022 23:57:35 - INFO - __main__ - Step 220 Global step 220 Train loss 5.988568 on epoch=109
03/10/2022 23:57:40 - INFO - __main__ - Step 230 Global step 230 Train loss 5.399442 on epoch=114
03/10/2022 23:57:45 - INFO - __main__ - Step 240 Global step 240 Train loss 5.173712 on epoch=119
03/10/2022 23:57:50 - INFO - __main__ - Step 250 Global step 250 Train loss 5.167567 on epoch=124
03/10/2022 23:57:51 - INFO - __main__ - Global step 250 Train loss 5.652747 Classification-F1 0.0 on epoch=124
03/10/2022 23:57:56 - INFO - __main__ - Step 260 Global step 260 Train loss 4.381558 on epoch=129
03/10/2022 23:58:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.748325 on epoch=134
03/10/2022 23:58:05 - INFO - __main__ - Step 280 Global step 280 Train loss 3.114634 on epoch=139
03/10/2022 23:58:10 - INFO - __main__ - Step 290 Global step 290 Train loss 2.922327 on epoch=144
03/10/2022 23:58:15 - INFO - __main__ - Step 300 Global step 300 Train loss 2.107137 on epoch=149
03/10/2022 23:58:16 - INFO - __main__ - Global step 300 Train loss 3.254797 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:58:21 - INFO - __main__ - Step 310 Global step 310 Train loss 2.084963 on epoch=154
03/10/2022 23:58:26 - INFO - __main__ - Step 320 Global step 320 Train loss 2.287255 on epoch=159
03/10/2022 23:58:31 - INFO - __main__ - Step 330 Global step 330 Train loss 1.956794 on epoch=164
03/10/2022 23:58:36 - INFO - __main__ - Step 340 Global step 340 Train loss 2.227108 on epoch=169
03/10/2022 23:58:40 - INFO - __main__ - Step 350 Global step 350 Train loss 2.031144 on epoch=174
03/10/2022 23:58:41 - INFO - __main__ - Global step 350 Train loss 2.117453 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:58:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.602505 on epoch=179
03/10/2022 23:58:51 - INFO - __main__ - Step 370 Global step 370 Train loss 1.595896 on epoch=184
03/10/2022 23:58:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.625752 on epoch=189
03/10/2022 23:59:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.377298 on epoch=194
03/10/2022 23:59:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.304826 on epoch=199
03/10/2022 23:59:05 - INFO - __main__ - Global step 400 Train loss 0.901255 Classification-F1 0.3992490613266583 on epoch=199
03/10/2022 23:59:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.443221 on epoch=204
03/10/2022 23:59:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.282484 on epoch=209
03/10/2022 23:59:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.288926 on epoch=214
03/10/2022 23:59:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.270887 on epoch=219
03/10/2022 23:59:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.240362 on epoch=224
03/10/2022 23:59:31 - INFO - __main__ - Global step 450 Train loss 0.305176 Classification-F1 0.3992490613266583 on epoch=224
03/10/2022 23:59:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.334189 on epoch=229
03/10/2022 23:59:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.742314 on epoch=234
03/10/2022 23:59:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.379582 on epoch=239
03/10/2022 23:59:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.225953 on epoch=244
03/10/2022 23:59:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.456822 on epoch=249
03/10/2022 23:59:55 - INFO - __main__ - Global step 500 Train loss 0.427772 Classification-F1 0.49090909090909085 on epoch=249
03/11/2022 00:00:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.286278 on epoch=254
03/11/2022 00:00:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.221436 on epoch=259
03/11/2022 00:00:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.404858 on epoch=264
03/11/2022 00:00:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.161790 on epoch=269
03/11/2022 00:00:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.236839 on epoch=274
03/11/2022 00:00:20 - INFO - __main__ - Global step 550 Train loss 0.262240 Classification-F1 0.6761133603238867 on epoch=274
03/11/2022 00:00:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.283243 on epoch=279
03/11/2022 00:00:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.198512 on epoch=284
03/11/2022 00:00:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.169179 on epoch=289
03/11/2022 00:00:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.212746 on epoch=294
03/11/2022 00:00:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.153765 on epoch=299
03/11/2022 00:00:46 - INFO - __main__ - Global step 600 Train loss 0.203489 Classification-F1 0.5607843137254902 on epoch=299
03/11/2022 00:00:46 - INFO - __main__ - save last model!
03/11/2022 00:00:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:00:46 - INFO - __main__ - Printing 3 examples
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:00:46 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:00:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:00:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:00:46 - INFO - __main__ - Printing 3 examples
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:00:46 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:46 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:00:46 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:00:46 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:00:52 - INFO - __main__ - Loading checkpoint on the fly
03/11/2022 00:00:53 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 00:00:53 - INFO - __main__ - Printing 3 examples
03/11/2022 00:00:53 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 00:00:53 - INFO - __main__ - ['hate']
03/11/2022 00:00:53 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 00:00:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:00:53 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 00:00:53 - INFO - __main__ - ['hate']
03/11/2022 00:00:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:00:53 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:00:54 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 00:00:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:00:57 - INFO - __main__ - Starting training!
03/11/2022 00:01:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.0001_8_predictions.txt
03/11/2022 00:01:11 - INFO - __main__ - Classification-F1 on test data: 0.5043
03/11/2022 00:01:11 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.0001, bsz=8, dev_performance=0.6761133603238867, test_performance=0.5043040091132914
03/11/2022 00:01:11 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.0005, bsz=8 ...
03/11/2022 00:01:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:01:12 - INFO - __main__ - Printing 3 examples
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:01:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:01:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:01:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:01:12 - INFO - __main__ - Printing 3 examples
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 00:01:12 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:01:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:01:12 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:01:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:01:22 - INFO - __main__ - Starting training!
03/11/2022 00:01:26 - INFO - __main__ - Step 10 Global step 10 Train loss 20.363712 on epoch=4
03/11/2022 00:01:31 - INFO - __main__ - Step 20 Global step 20 Train loss 14.886113 on epoch=9
03/11/2022 00:01:35 - INFO - __main__ - Step 30 Global step 30 Train loss 10.119340 on epoch=14
03/11/2022 00:01:40 - INFO - __main__ - Step 40 Global step 40 Train loss 9.051565 on epoch=19
03/11/2022 00:01:45 - INFO - __main__ - Step 50 Global step 50 Train loss 7.184909 on epoch=24
03/11/2022 00:01:45 - INFO - __main__ - Global step 50 Train loss 12.321127 Classification-F1 0.0 on epoch=24
03/11/2022 00:01:51 - INFO - __main__ - Step 60 Global step 60 Train loss 6.565784 on epoch=29
03/11/2022 00:01:55 - INFO - __main__ - Step 70 Global step 70 Train loss 4.292195 on epoch=34
03/11/2022 00:02:00 - INFO - __main__ - Step 80 Global step 80 Train loss 2.693714 on epoch=39
03/11/2022 00:02:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.969872 on epoch=44
03/11/2022 00:02:10 - INFO - __main__ - Step 100 Global step 100 Train loss 1.605301 on epoch=49
03/11/2022 00:02:10 - INFO - __main__ - Global step 100 Train loss 3.425373 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 00:02:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.456704 on epoch=54
03/11/2022 00:02:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.415891 on epoch=59
03/11/2022 00:02:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.691632 on epoch=64
03/11/2022 00:02:30 - INFO - __main__ - Step 140 Global step 140 Train loss 1.172921 on epoch=69
03/11/2022 00:02:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.154862 on epoch=74
03/11/2022 00:02:36 - INFO - __main__ - Global step 150 Train loss 1.378402 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 00:02:41 - INFO - __main__ - Step 160 Global step 160 Train loss 1.590847 on epoch=79
03/11/2022 00:02:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.915749 on epoch=84
03/11/2022 00:02:50 - INFO - __main__ - Step 180 Global step 180 Train loss 1.575839 on epoch=89
03/11/2022 00:02:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.945904 on epoch=94
03/11/2022 00:03:00 - INFO - __main__ - Step 200 Global step 200 Train loss 1.433163 on epoch=99
03/11/2022 00:03:00 - INFO - __main__ - Global step 200 Train loss 1.292300 Classification-F1 0.4420512820512821 on epoch=99
03/11/2022 00:03:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.867510 on epoch=104
03/11/2022 00:03:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.871370 on epoch=109
03/11/2022 00:03:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.668035 on epoch=114
03/11/2022 00:03:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.840521 on epoch=119
03/11/2022 00:03:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.714728 on epoch=124
03/11/2022 00:03:25 - INFO - __main__ - Global step 250 Train loss 0.792433 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 00:03:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.899059 on epoch=129
03/11/2022 00:03:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.489992 on epoch=134
03/11/2022 00:03:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.625910 on epoch=139
03/11/2022 00:03:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.673098 on epoch=144
03/11/2022 00:03:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.487847 on epoch=149
03/11/2022 00:03:50 - INFO - __main__ - Global step 300 Train loss 0.635181 Classification-F1 0.3266888150609081 on epoch=149
03/11/2022 00:03:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.516514 on epoch=154
03/11/2022 00:04:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.474074 on epoch=159
03/11/2022 00:04:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.491984 on epoch=164
03/11/2022 00:04:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.454789 on epoch=169
03/11/2022 00:04:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.438338 on epoch=174
03/11/2022 00:04:14 - INFO - __main__ - Global step 350 Train loss 0.475140 Classification-F1 0.5555555555555556 on epoch=174
03/11/2022 00:04:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.334615 on epoch=179
03/11/2022 00:04:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.318308 on epoch=184
03/11/2022 00:04:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.326049 on epoch=189
03/11/2022 00:04:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.355247 on epoch=194
03/11/2022 00:04:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.247441 on epoch=199
03/11/2022 00:04:40 - INFO - __main__ - Global step 400 Train loss 0.316332 Classification-F1 0.3992490613266583 on epoch=199
03/11/2022 00:04:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.268379 on epoch=204
03/11/2022 00:04:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.340643 on epoch=209
03/11/2022 00:04:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.254527 on epoch=214
03/11/2022 00:05:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.223934 on epoch=219
03/11/2022 00:05:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.276265 on epoch=224
03/11/2022 00:05:05 - INFO - __main__ - Global step 450 Train loss 0.272749 Classification-F1 0.5151515151515151 on epoch=224
03/11/2022 00:05:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.266695 on epoch=229
03/11/2022 00:05:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.221612 on epoch=234
03/11/2022 00:05:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.242105 on epoch=239
03/11/2022 00:05:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.256531 on epoch=244
03/11/2022 00:05:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.232980 on epoch=249
03/11/2022 00:05:30 - INFO - __main__ - Global step 500 Train loss 0.243985 Classification-F1 0.5333333333333333 on epoch=249
03/11/2022 00:05:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.198164 on epoch=254
03/11/2022 00:05:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.255277 on epoch=259
03/11/2022 00:05:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.223404 on epoch=264
03/11/2022 00:05:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.244062 on epoch=269
03/11/2022 00:05:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.237737 on epoch=274
03/11/2022 00:05:55 - INFO - __main__ - Global step 550 Train loss 0.231729 Classification-F1 0.5151515151515151 on epoch=274
03/11/2022 00:06:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.242123 on epoch=279
03/11/2022 00:06:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.214384 on epoch=284
03/11/2022 00:06:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.242162 on epoch=289
03/11/2022 00:06:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.214695 on epoch=294
03/11/2022 00:06:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.202851 on epoch=299
03/11/2022 00:06:20 - INFO - __main__ - Global step 600 Train loss 0.223243 Classification-F1 0.5195195195195195 on epoch=299
03/11/2022 00:06:20 - INFO - __main__ - save last model!
03/11/2022 00:06:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:06:21 - INFO - __main__ - Printing 3 examples
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:06:21 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:06:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:06:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:06:21 - INFO - __main__ - Printing 3 examples
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:06:21 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:21 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:06:21 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:06:21 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:06:27 - INFO - __main__ - Loading checkpoint on the fly
03/11/2022 00:06:28 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 00:06:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:06:28 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 00:06:28 - INFO - __main__ - ['hate']
03/11/2022 00:06:28 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 00:06:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:28 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 00:06:28 - INFO - __main__ - ['hate']
03/11/2022 00:06:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:06:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:06:29 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 00:06:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:06:31 - INFO - __main__ - Starting training!
03/11/2022 00:06:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.0005_8_predictions.txt
03/11/2022 00:06:43 - INFO - __main__ - Classification-F1 on test data: 0.5049
03/11/2022 00:06:43 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.0005, bsz=8, dev_performance=0.5555555555555556, test_performance=0.5048518039463907
03/11/2022 00:06:43 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.0003, bsz=8 ...
03/11/2022 00:06:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:06:44 - INFO - __main__ - Printing 3 examples
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:06:44 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:06:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:06:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:06:44 - INFO - __main__ - Printing 3 examples
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:06:44 - INFO - __main__ - ['non-irony']
03/11/2022 00:06:44 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:06:44 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:06:44 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:06:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:06:54 - INFO - __main__ - Starting training!
03/11/2022 00:06:58 - INFO - __main__ - Step 10 Global step 10 Train loss 19.791759 on epoch=4
03/11/2022 00:07:03 - INFO - __main__ - Step 20 Global step 20 Train loss 14.478427 on epoch=9
03/11/2022 00:07:07 - INFO - __main__ - Step 30 Global step 30 Train loss 9.308816 on epoch=14
03/11/2022 00:07:12 - INFO - __main__ - Step 40 Global step 40 Train loss 8.421633 on epoch=19
03/11/2022 00:07:17 - INFO - __main__ - Step 50 Global step 50 Train loss 9.331362 on epoch=24
03/11/2022 00:07:20 - INFO - __main__ - Global step 50 Train loss 12.266399 Classification-F1 0.0 on epoch=24
03/11/2022 00:07:25 - INFO - __main__ - Step 60 Global step 60 Train loss 8.034971 on epoch=29
03/11/2022 00:07:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.391121 on epoch=34
03/11/2022 00:07:34 - INFO - __main__ - Step 80 Global step 80 Train loss 6.397795 on epoch=39
03/11/2022 00:07:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.602536 on epoch=44
03/11/2022 00:07:44 - INFO - __main__ - Step 100 Global step 100 Train loss 3.235636 on epoch=49
03/11/2022 00:07:44 - INFO - __main__ - Global step 100 Train loss 6.132411 Classification-F1 0.0 on epoch=49
03/11/2022 00:07:49 - INFO - __main__ - Step 110 Global step 110 Train loss 2.549417 on epoch=54
03/11/2022 00:07:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.612211 on epoch=59
03/11/2022 00:07:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.408787 on epoch=64
03/11/2022 00:08:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.289669 on epoch=69
03/11/2022 00:08:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.272229 on epoch=74
03/11/2022 00:08:09 - INFO - __main__ - Global step 150 Train loss 1.026463 Classification-F1 0.5 on epoch=74
03/11/2022 00:08:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.291478 on epoch=79
03/11/2022 00:08:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.240346 on epoch=84
03/11/2022 00:08:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.299828 on epoch=89
03/11/2022 00:08:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.227679 on epoch=94
03/11/2022 00:08:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.211784 on epoch=99
03/11/2022 00:08:34 - INFO - __main__ - Global step 200 Train loss 0.254223 Classification-F1 0.464039408866995 on epoch=99
03/11/2022 00:08:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.240685 on epoch=104
03/11/2022 00:08:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.222458 on epoch=109
03/11/2022 00:08:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.240794 on epoch=114
03/11/2022 00:08:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.219359 on epoch=119
03/11/2022 00:08:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.225053 on epoch=124
03/11/2022 00:08:59 - INFO - __main__ - Global step 250 Train loss 0.229670 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 00:09:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.204051 on epoch=129
03/11/2022 00:09:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.224442 on epoch=134
03/11/2022 00:09:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.160558 on epoch=139
03/11/2022 00:09:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.203545 on epoch=144
03/11/2022 00:09:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.181844 on epoch=149
03/11/2022 00:09:24 - INFO - __main__ - Global step 300 Train loss 0.194888 Classification-F1 0.4375 on epoch=149
03/11/2022 00:09:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.185392 on epoch=154
03/11/2022 00:09:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.153866 on epoch=159
03/11/2022 00:09:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.181564 on epoch=164
03/11/2022 00:09:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.157659 on epoch=169
03/11/2022 00:09:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.221220 on epoch=174
03/11/2022 00:09:48 - INFO - __main__ - Global step 350 Train loss 0.179940 Classification-F1 0.4231177094379639 on epoch=174
03/11/2022 00:09:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.144305 on epoch=179
03/11/2022 00:09:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.119946 on epoch=184
03/11/2022 00:10:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.242955 on epoch=189
03/11/2022 00:10:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.149231 on epoch=194
03/11/2022 00:10:12 - INFO - __main__ - Step 400 Global step 400 Train loss 1.736057 on epoch=199
03/11/2022 00:10:12 - INFO - __main__ - Global step 400 Train loss 0.478499 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 00:10:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.153488 on epoch=204
03/11/2022 00:10:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.126499 on epoch=209
03/11/2022 00:10:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.113545 on epoch=214
03/11/2022 00:10:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.060499 on epoch=219
03/11/2022 00:10:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.119109 on epoch=224
03/11/2022 00:10:37 - INFO - __main__ - Global step 450 Train loss 0.314628 Classification-F1 0.3816425120772947 on epoch=224
03/11/2022 00:10:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.122383 on epoch=229
03/11/2022 00:10:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.056814 on epoch=234
03/11/2022 00:10:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.078036 on epoch=239
03/11/2022 00:10:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.067670 on epoch=244
03/11/2022 00:11:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.083655 on epoch=249
03/11/2022 00:11:01 - INFO - __main__ - Global step 500 Train loss 0.081711 Classification-F1 0.464039408866995 on epoch=249
03/11/2022 00:11:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.051683 on epoch=254
03/11/2022 00:11:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.046989 on epoch=259
03/11/2022 00:11:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.048737 on epoch=264
03/11/2022 00:11:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.030897 on epoch=269
03/11/2022 00:11:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.111370 on epoch=274
03/11/2022 00:11:25 - INFO - __main__ - Global step 550 Train loss 0.057935 Classification-F1 0.4909862142099682 on epoch=274
03/11/2022 00:11:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.024042 on epoch=279
03/11/2022 00:11:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.031017 on epoch=284
03/11/2022 00:11:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016675 on epoch=289
03/11/2022 00:11:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010080 on epoch=294
03/11/2022 00:11:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.026461 on epoch=299
03/11/2022 00:11:50 - INFO - __main__ - Global step 600 Train loss 0.021655 Classification-F1 0.4920634920634921 on epoch=299
03/11/2022 00:11:50 - INFO - __main__ - save last model!
03/11/2022 00:11:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:11:50 - INFO - __main__ - Printing 3 examples
03/11/2022 00:11:50 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:11:50 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:50 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:11:50 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:50 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:11:50 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:11:50 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:11:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:11:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:11:51 - INFO - __main__ - Printing 3 examples
03/11/2022 00:11:51 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:11:51 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:51 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:11:51 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:51 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:11:51 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:51 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:11:51 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:11:51 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:11:56 - INFO - __main__ - Loading checkpoint on the fly
03/11/2022 00:11:57 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 00:11:57 - INFO - __main__ - Printing 3 examples
03/11/2022 00:11:57 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 00:11:57 - INFO - __main__ - ['hate']
03/11/2022 00:11:57 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 00:11:57 - INFO - __main__ - ['non-irony']
03/11/2022 00:11:57 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 00:11:57 - INFO - __main__ - ['hate']
03/11/2022 00:11:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:11:57 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:11:58 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 00:12:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:12:01 - INFO - __main__ - Starting training!
03/11/2022 00:12:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.0003_8_predictions.txt
03/11/2022 00:12:14 - INFO - __main__ - Classification-F1 on test data: 0.5078
03/11/2022 00:12:14 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.0003, bsz=8, dev_performance=0.5, test_performance=0.5078269603586059
03/11/2022 00:12:14 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.0002, bsz=8 ...
03/11/2022 00:12:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:12:15 - INFO - __main__ - Printing 3 examples
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:12:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:12:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:12:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:12:15 - INFO - __main__ - Printing 3 examples
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:12:15 - INFO - __main__ - ['non-irony']
03/11/2022 00:12:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:12:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:12:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:12:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:12:24 - INFO - __main__ - Starting training!
03/11/2022 00:12:28 - INFO - __main__ - Step 10 Global step 10 Train loss 20.129534 on epoch=4
03/11/2022 00:12:33 - INFO - __main__ - Step 20 Global step 20 Train loss 20.001438 on epoch=9
03/11/2022 00:12:38 - INFO - __main__ - Step 30 Global step 30 Train loss 14.662372 on epoch=14
03/11/2022 00:12:43 - INFO - __main__ - Step 40 Global step 40 Train loss 11.786219 on epoch=19
03/11/2022 00:12:47 - INFO - __main__ - Step 50 Global step 50 Train loss 10.359506 on epoch=24
03/11/2022 00:12:53 - INFO - __main__ - Global step 50 Train loss 15.387813 Classification-F1 0.0 on epoch=24
03/11/2022 00:12:59 - INFO - __main__ - Step 60 Global step 60 Train loss 9.667496 on epoch=29
03/11/2022 00:13:04 - INFO - __main__ - Step 70 Global step 70 Train loss 8.743563 on epoch=34
03/11/2022 00:13:09 - INFO - __main__ - Step 80 Global step 80 Train loss 8.063285 on epoch=39
03/11/2022 00:13:14 - INFO - __main__ - Step 90 Global step 90 Train loss 7.883893 on epoch=44
03/11/2022 00:13:18 - INFO - __main__ - Step 100 Global step 100 Train loss 6.596321 on epoch=49
03/11/2022 00:13:19 - INFO - __main__ - Global step 100 Train loss 8.190911 Classification-F1 0.0 on epoch=49
03/11/2022 00:13:24 - INFO - __main__ - Step 110 Global step 110 Train loss 6.270878 on epoch=54
03/11/2022 00:13:29 - INFO - __main__ - Step 120 Global step 120 Train loss 6.137141 on epoch=59
03/11/2022 00:13:34 - INFO - __main__ - Step 130 Global step 130 Train loss 5.653725 on epoch=64
03/11/2022 00:13:39 - INFO - __main__ - Step 140 Global step 140 Train loss 4.929652 on epoch=69
03/11/2022 00:13:44 - INFO - __main__ - Step 150 Global step 150 Train loss 3.840330 on epoch=74
03/11/2022 00:13:44 - INFO - __main__ - Global step 150 Train loss 5.366345 Classification-F1 0.0 on epoch=74
03/11/2022 00:13:49 - INFO - __main__ - Step 160 Global step 160 Train loss 3.446260 on epoch=79
03/11/2022 00:13:54 - INFO - __main__ - Step 170 Global step 170 Train loss 2.644625 on epoch=84
03/11/2022 00:13:59 - INFO - __main__ - Step 180 Global step 180 Train loss 2.466580 on epoch=89
03/11/2022 00:14:04 - INFO - __main__ - Step 190 Global step 190 Train loss 2.383486 on epoch=94
03/11/2022 00:14:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.539392 on epoch=99
03/11/2022 00:14:09 - INFO - __main__ - Global step 200 Train loss 2.496068 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 00:14:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.642578 on epoch=104
03/11/2022 00:14:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.759090 on epoch=109
03/11/2022 00:14:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.508655 on epoch=114
03/11/2022 00:14:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.833387 on epoch=119
03/11/2022 00:14:34 - INFO - __main__ - Step 250 Global step 250 Train loss 1.510751 on epoch=124
03/11/2022 00:14:35 - INFO - __main__ - Global step 250 Train loss 1.650892 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 00:14:39 - INFO - __main__ - Step 260 Global step 260 Train loss 1.887515 on epoch=129
03/11/2022 00:14:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.301245 on epoch=134
03/11/2022 00:14:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.568037 on epoch=139
03/11/2022 00:14:54 - INFO - __main__ - Step 290 Global step 290 Train loss 1.656604 on epoch=144
03/11/2022 00:14:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.783822 on epoch=149
03/11/2022 00:14:59 - INFO - __main__ - Global step 300 Train loss 1.639445 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 00:15:04 - INFO - __main__ - Step 310 Global step 310 Train loss 1.761383 on epoch=154
03/11/2022 00:15:09 - INFO - __main__ - Step 320 Global step 320 Train loss 1.301330 on epoch=159
03/11/2022 00:15:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.621030 on epoch=164
03/11/2022 00:15:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.488120 on epoch=169
03/11/2022 00:15:24 - INFO - __main__ - Step 350 Global step 350 Train loss 1.339087 on epoch=174
03/11/2022 00:15:24 - INFO - __main__ - Global step 350 Train loss 1.502190 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 00:15:29 - INFO - __main__ - Step 360 Global step 360 Train loss 1.604442 on epoch=179
03/11/2022 00:15:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.325837 on epoch=184
03/11/2022 00:15:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.956505 on epoch=189
03/11/2022 00:15:43 - INFO - __main__ - Step 390 Global step 390 Train loss 1.114463 on epoch=194
03/11/2022 00:15:48 - INFO - __main__ - Step 400 Global step 400 Train loss 1.182546 on epoch=199
03/11/2022 00:15:49 - INFO - __main__ - Global step 400 Train loss 1.236759 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 00:15:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.040695 on epoch=204
03/11/2022 00:15:58 - INFO - __main__ - Step 420 Global step 420 Train loss 1.089770 on epoch=209
03/11/2022 00:16:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.017386 on epoch=214
03/11/2022 00:16:08 - INFO - __main__ - Step 440 Global step 440 Train loss 1.027907 on epoch=219
03/11/2022 00:16:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.948385 on epoch=224
03/11/2022 00:16:13 - INFO - __main__ - Global step 450 Train loss 1.024829 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 00:16:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.235570 on epoch=229
03/11/2022 00:16:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.825508 on epoch=234
03/11/2022 00:16:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.905807 on epoch=239
03/11/2022 00:16:32 - INFO - __main__ - Step 490 Global step 490 Train loss 1.114640 on epoch=244
03/11/2022 00:16:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.745323 on epoch=249
03/11/2022 00:16:38 - INFO - __main__ - Global step 500 Train loss 0.965370 Classification-F1 0.5195195195195195 on epoch=249
03/11/2022 00:16:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.889382 on epoch=254
03/11/2022 00:16:48 - INFO - __main__ - Step 520 Global step 520 Train loss 1.001894 on epoch=259
03/11/2022 00:16:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.762122 on epoch=264
03/11/2022 00:16:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.941818 on epoch=269
03/11/2022 00:17:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.686282 on epoch=274
03/11/2022 00:17:03 - INFO - __main__ - Global step 550 Train loss 0.856300 Classification-F1 0.5076923076923077 on epoch=274
03/11/2022 00:17:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.722834 on epoch=279
03/11/2022 00:17:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.675127 on epoch=284
03/11/2022 00:17:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.808776 on epoch=289
03/11/2022 00:17:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.586184 on epoch=294
03/11/2022 00:17:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.414790 on epoch=299
03/11/2022 00:17:27 - INFO - __main__ - Global step 600 Train loss 0.641542 Classification-F1 0.4920634920634921 on epoch=299
03/11/2022 00:17:27 - INFO - __main__ - save last model!
03/11/2022 00:17:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:17:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:17:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:17:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:17:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:17:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:17:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:28 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:17:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:17:28 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:17:34 - INFO - __main__ - Loading checkpoint on the fly
03/11/2022 00:17:35 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 00:17:35 - INFO - __main__ - Printing 3 examples
03/11/2022 00:17:35 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 00:17:35 - INFO - __main__ - ['hate']
03/11/2022 00:17:35 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 00:17:35 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:35 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 00:17:35 - INFO - __main__ - ['hate']
03/11/2022 00:17:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:17:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:17:37 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 00:17:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:17:38 - INFO - __main__ - Starting training!
03/11/2022 00:17:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.0002_8_predictions.txt
03/11/2022 00:17:50 - INFO - __main__ - Classification-F1 on test data: 0.5206
03/11/2022 00:17:52 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.0002, bsz=8, dev_performance=0.5195195195195195, test_performance=0.5205742166567956
03/11/2022 00:17:52 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.0001, bsz=8 ...
03/11/2022 00:17:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:17:53 - INFO - __main__ - Printing 3 examples
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:17:53 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:17:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:17:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:17:53 - INFO - __main__ - Printing 3 examples
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 00:17:53 - INFO - __main__ - ['non-irony']
03/11/2022 00:17:53 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:17:53 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:17:53 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:18:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/11/2022 00:18:02 - INFO - __main__ - Starting training!
03/11/2022 00:18:06 - INFO - __main__ - Step 10 Global step 10 Train loss 20.607006 on epoch=4
03/11/2022 00:18:11 - INFO - __main__ - Step 20 Global step 20 Train loss 19.447866 on epoch=9
03/11/2022 00:18:16 - INFO - __main__ - Step 30 Global step 30 Train loss 15.803116 on epoch=14
03/11/2022 00:18:21 - INFO - __main__ - Step 40 Global step 40 Train loss 13.738432 on epoch=19
03/11/2022 00:18:25 - INFO - __main__ - Step 50 Global step 50 Train loss 12.696719 on epoch=24
03/11/2022 00:18:35 - INFO - __main__ - Global step 50 Train loss 16.458626 Classification-F1 0.0 on epoch=24
03/11/2022 00:18:41 - INFO - __main__ - Step 60 Global step 60 Train loss 11.467452 on epoch=29
03/11/2022 00:18:46 - INFO - __main__ - Step 70 Global step 70 Train loss 9.991031 on epoch=34
03/11/2022 00:18:50 - INFO - __main__ - Step 80 Global step 80 Train loss 9.936329 on epoch=39
03/11/2022 00:18:55 - INFO - __main__ - Step 90 Global step 90 Train loss 9.598551 on epoch=44
03/11/2022 00:19:00 - INFO - __main__ - Step 100 Global step 100 Train loss 9.514595 on epoch=49
03/11/2022 00:19:07 - INFO - __main__ - Global step 100 Train loss 10.101592 Classification-F1 0.0 on epoch=49
03/11/2022 00:19:12 - INFO - __main__ - Step 110 Global step 110 Train loss 8.871162 on epoch=54
03/11/2022 00:19:17 - INFO - __main__ - Step 120 Global step 120 Train loss 8.857110 on epoch=59
03/11/2022 00:19:22 - INFO - __main__ - Step 130 Global step 130 Train loss 8.776287 on epoch=64
03/11/2022 00:19:27 - INFO - __main__ - Step 140 Global step 140 Train loss 7.989751 on epoch=69
03/11/2022 00:19:32 - INFO - __main__ - Step 150 Global step 150 Train loss 7.747084 on epoch=74
03/11/2022 00:19:34 - INFO - __main__ - Global step 150 Train loss 8.448279 Classification-F1 0.0 on epoch=74
03/11/2022 00:19:39 - INFO - __main__ - Step 160 Global step 160 Train loss 7.591475 on epoch=79
03/11/2022 00:19:44 - INFO - __main__ - Step 170 Global step 170 Train loss 7.224196 on epoch=84
03/11/2022 00:19:49 - INFO - __main__ - Step 180 Global step 180 Train loss 7.403098 on epoch=89
03/11/2022 00:19:53 - INFO - __main__ - Step 190 Global step 190 Train loss 6.898175 on epoch=94
03/11/2022 00:19:58 - INFO - __main__ - Step 200 Global step 200 Train loss 6.115990 on epoch=99
03/11/2022 00:20:00 - INFO - __main__ - Global step 200 Train loss 7.046587 Classification-F1 0.0 on epoch=99
03/11/2022 00:20:05 - INFO - __main__ - Step 210 Global step 210 Train loss 6.450140 on epoch=104
03/11/2022 00:20:09 - INFO - __main__ - Step 220 Global step 220 Train loss 6.146097 on epoch=109
03/11/2022 00:20:14 - INFO - __main__ - Step 230 Global step 230 Train loss 5.922750 on epoch=114
03/11/2022 00:20:19 - INFO - __main__ - Step 240 Global step 240 Train loss 5.405436 on epoch=119
03/11/2022 00:20:24 - INFO - __main__ - Step 250 Global step 250 Train loss 5.119991 on epoch=124
03/11/2022 00:20:25 - INFO - __main__ - Global step 250 Train loss 5.808883 Classification-F1 0.0 on epoch=124
03/11/2022 00:20:30 - INFO - __main__ - Step 260 Global step 260 Train loss 5.373790 on epoch=129
03/11/2022 00:20:35 - INFO - __main__ - Step 270 Global step 270 Train loss 4.269189 on epoch=134
03/11/2022 00:20:40 - INFO - __main__ - Step 280 Global step 280 Train loss 4.220625 on epoch=139
03/11/2022 00:20:45 - INFO - __main__ - Step 290 Global step 290 Train loss 3.811117 on epoch=144
03/11/2022 00:20:50 - INFO - __main__ - Step 300 Global step 300 Train loss 3.688955 on epoch=149
03/11/2022 00:20:51 - INFO - __main__ - Global step 300 Train loss 4.272735 Classification-F1 0.0 on epoch=149
03/11/2022 00:20:56 - INFO - __main__ - Step 310 Global step 310 Train loss 2.816863 on epoch=154
03/11/2022 00:21:01 - INFO - __main__ - Step 320 Global step 320 Train loss 2.942852 on epoch=159
03/11/2022 00:21:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.660986 on epoch=164
03/11/2022 00:21:10 - INFO - __main__ - Step 340 Global step 340 Train loss 2.577397 on epoch=169
03/11/2022 00:21:15 - INFO - __main__ - Step 350 Global step 350 Train loss 2.314197 on epoch=174
03/11/2022 00:21:16 - INFO - __main__ - Global step 350 Train loss 2.462459 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 00:21:21 - INFO - __main__ - Step 360 Global step 360 Train loss 2.071114 on epoch=179
03/11/2022 00:21:26 - INFO - __main__ - Step 370 Global step 370 Train loss 1.425273 on epoch=184
03/11/2022 00:21:31 - INFO - __main__ - Step 380 Global step 380 Train loss 2.042022 on epoch=189
03/11/2022 00:21:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.924385 on epoch=194
03/11/2022 00:21:41 - INFO - __main__ - Step 400 Global step 400 Train loss 1.768464 on epoch=199
03/11/2022 00:21:41 - INFO - __main__ - Global step 400 Train loss 1.846251 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 00:21:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.547581 on epoch=204
03/11/2022 00:21:51 - INFO - __main__ - Step 420 Global step 420 Train loss 1.552547 on epoch=209
03/11/2022 00:21:56 - INFO - __main__ - Step 430 Global step 430 Train loss 1.580378 on epoch=214
03/11/2022 00:22:01 - INFO - __main__ - Step 440 Global step 440 Train loss 1.766760 on epoch=219
03/11/2022 00:22:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.635499 on epoch=224
03/11/2022 00:22:06 - INFO - __main__ - Global step 450 Train loss 1.616553 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 00:22:11 - INFO - __main__ - Step 460 Global step 460 Train loss 1.442251 on epoch=229
03/11/2022 00:22:16 - INFO - __main__ - Step 470 Global step 470 Train loss 1.707379 on epoch=234
03/11/2022 00:22:21 - INFO - __main__ - Step 480 Global step 480 Train loss 1.602708 on epoch=239
03/11/2022 00:22:26 - INFO - __main__ - Step 490 Global step 490 Train loss 1.823526 on epoch=244
03/11/2022 00:22:31 - INFO - __main__ - Step 500 Global step 500 Train loss 1.449165 on epoch=249
03/11/2022 00:22:31 - INFO - __main__ - Global step 500 Train loss 1.605006 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 00:22:36 - INFO - __main__ - Step 510 Global step 510 Train loss 1.372160 on epoch=254
03/11/2022 00:22:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.750390 on epoch=259
03/11/2022 00:22:46 - INFO - __main__ - Step 530 Global step 530 Train loss 1.270308 on epoch=264
03/11/2022 00:22:51 - INFO - __main__ - Step 540 Global step 540 Train loss 1.437178 on epoch=269
03/11/2022 00:22:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.665905 on epoch=274
03/11/2022 00:22:56 - INFO - __main__ - Global step 550 Train loss 1.499188 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 00:23:01 - INFO - __main__ - Step 560 Global step 560 Train loss 1.603978 on epoch=279
03/11/2022 00:23:06 - INFO - __main__ - Step 570 Global step 570 Train loss 1.584075 on epoch=284
03/11/2022 00:23:11 - INFO - __main__ - Step 580 Global step 580 Train loss 1.861300 on epoch=289
03/11/2022 00:23:16 - INFO - __main__ - Step 590 Global step 590 Train loss 1.661378 on epoch=294
03/11/2022 00:23:20 - INFO - __main__ - Step 600 Global step 600 Train loss 1.514713 on epoch=299
03/11/2022 00:23:21 - INFO - __main__ - Global step 600 Train loss 1.645089 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 00:23:21 - INFO - __main__ - save last model!
03/11/2022 00:23:28 - INFO - __main__ - Loading checkpoint on the fly
03/11/2022 00:23:28 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 00:23:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:23:28 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 00:23:28 - INFO - __main__ - ['hate']
03/11/2022 00:23:28 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 00:23:28 - INFO - __main__ - ['non-irony']
03/11/2022 00:23:28 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 00:23:28 - INFO - __main__ - ['hate']
03/11/2022 00:23:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:23:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:23:30 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 00:23:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.0001_8_predictions.txt
03/11/2022 00:23:51 - INFO - __main__ - Classification-F1 on test data: 0.3232
03/11/2022 00:23:52 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.32317505315379164
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0004112720489501953 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15673", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6812, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15674", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6812, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 6812, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15697): No such process
