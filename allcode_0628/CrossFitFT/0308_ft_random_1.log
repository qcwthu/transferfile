nohup: ignoring input
Task: crawl_domain, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_wajbakup/none_6ynf7qf3
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_wajbakup/none_6ynf7qf3/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_wajbakup/none_6ynf7qf3/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/08/2022 16:01:01 - INFO - __main__ - Namespace(task_dir='data/crawl_domain/', task_name='crawl_domain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-crawl_domain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 16:01:01 - INFO - __main__ - models/T5-large-ft-random/singletask-crawl_domain
03/08/2022 16:01:01 - INFO - __main__ - Namespace(task_dir='data/crawl_domain/', task_name='crawl_domain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-crawl_domain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 16:01:01 - INFO - __main__ - models/T5-large-ft-random/singletask-crawl_domain
03/08/2022 16:01:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/08/2022 16:01:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/08/2022 16:01:02 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/08/2022 16:01:02 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/08/2022 16:01:02 - INFO - __main__ - args.device: cuda:0
03/08/2022 16:01:02 - INFO - __main__ - args.device: cuda:1
03/08/2022 16:01:02 - INFO - __main__ - Using 2 gpus
03/08/2022 16:01:02 - INFO - __main__ - Using 2 gpus
03/08/2022 16:01:02 - INFO - __main__ - Fine-tuning the following samples: ['crawl_domain_32_100', 'crawl_domain_32_13', 'crawl_domain_32_21', 'crawl_domain_32_42', 'crawl_domain_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/08/2022 16:01:02 - INFO - __main__ - Fine-tuning the following samples: ['crawl_domain_32_100', 'crawl_domain_32_13', 'crawl_domain_32_21', 'crawl_domain_32_42', 'crawl_domain_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/08/2022 16:01:07 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0005, bsz=8 ...
03/08/2022 16:01:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:01:08 - INFO - __main__ - Printing 3 examples
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:01:08 - INFO - __main__ - ['List Cast']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:01:08 - INFO - __main__ - ['Leidinger']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:01:08 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:01:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:01:08 - INFO - __main__ - Printing 3 examples
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:01:08 - INFO - __main__ - ['List Cast']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:01:08 - INFO - __main__ - ['Leidinger']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:01:08 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 16:01:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:01:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:01:08 - INFO - __main__ - Printing 3 examples
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:01:08 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:01:08 - INFO - __main__ - ['Friend']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:01:08 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:01:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:01:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:01:08 - INFO - __main__ - Printing 3 examples
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:01:08 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:01:08 - INFO - __main__ - ['Friend']
03/08/2022 16:01:08 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:01:08 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:01:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:01:08 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:01:08 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:01:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:01:20 - INFO - __main__ - Starting training!
03/08/2022 16:01:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:01:21 - INFO - __main__ - Starting training!
03/08/2022 16:01:25 - INFO - __main__ - Step 10 Global step 10 Train loss 19.770868 on epoch=4
03/08/2022 16:01:30 - INFO - __main__ - Step 20 Global step 20 Train loss 17.858011 on epoch=9
03/08/2022 16:01:34 - INFO - __main__ - Step 30 Global step 30 Train loss 11.591599 on epoch=14
03/08/2022 16:01:39 - INFO - __main__ - Step 40 Global step 40 Train loss 10.363333 on epoch=19
03/08/2022 16:01:43 - INFO - __main__ - Step 50 Global step 50 Train loss 9.019181 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/08/2022 16:01:44 - INFO - __main__ - Global step 50 Train loss 13.720598 EM 0.0 on epoch=24
03/08/2022 16:02:16 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 16:02:21 - INFO - __main__ - Step 60 Global step 60 Train loss 7.347328 on epoch=29
03/08/2022 16:02:25 - INFO - __main__ - Step 70 Global step 70 Train loss 5.937170 on epoch=34
03/08/2022 16:02:30 - INFO - __main__ - Step 80 Global step 80 Train loss 4.976209 on epoch=39
03/08/2022 16:02:34 - INFO - __main__ - Step 90 Global step 90 Train loss 4.079841 on epoch=44
03/08/2022 16:02:39 - INFO - __main__ - Step 100 Global step 100 Train loss 3.090210 on epoch=49
03/08/2022 16:02:39 - INFO - __main__ - Global step 100 Train loss 5.086152 EM 0.0 on epoch=49
03/08/2022 16:02:44 - INFO - __main__ - Step 110 Global step 110 Train loss 2.594819 on epoch=54
03/08/2022 16:02:49 - INFO - __main__ - Step 120 Global step 120 Train loss 2.209362 on epoch=59
03/08/2022 16:02:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.099404 on epoch=64
03/08/2022 16:02:58 - INFO - __main__ - Step 140 Global step 140 Train loss 2.077057 on epoch=69
03/08/2022 16:03:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.863445 on epoch=74
03/08/2022 16:03:03 - INFO - __main__ - Global step 150 Train loss 2.168818 EM 0.0 on epoch=74
03/08/2022 16:03:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.926301 on epoch=79
03/08/2022 16:03:12 - INFO - __main__ - Step 170 Global step 170 Train loss 2.012275 on epoch=84
03/08/2022 16:03:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.606894 on epoch=89
03/08/2022 16:03:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.542193 on epoch=94
03/08/2022 16:03:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.354919 on epoch=99
03/08/2022 16:03:27 - INFO - __main__ - Global step 200 Train loss 1.688516 EM 0.0 on epoch=99
03/08/2022 16:03:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.469077 on epoch=104
03/08/2022 16:03:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.434195 on epoch=109
03/08/2022 16:03:41 - INFO - __main__ - Step 230 Global step 230 Train loss 1.369336 on epoch=114
03/08/2022 16:03:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.205539 on epoch=119
03/08/2022 16:03:50 - INFO - __main__ - Step 250 Global step 250 Train loss 1.233317 on epoch=124
03/08/2022 16:03:50 - INFO - __main__ - Global step 250 Train loss 1.342293 EM 0.0 on epoch=124
03/08/2022 16:03:55 - INFO - __main__ - Step 260 Global step 260 Train loss 1.366744 on epoch=129
03/08/2022 16:04:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.153974 on epoch=134
03/08/2022 16:04:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.239811 on epoch=139
03/08/2022 16:04:09 - INFO - __main__ - Step 290 Global step 290 Train loss 1.066404 on epoch=144
03/08/2022 16:04:13 - INFO - __main__ - Step 300 Global step 300 Train loss 1.012691 on epoch=149
03/08/2022 16:04:14 - INFO - __main__ - Global step 300 Train loss 1.167925 EM 0.0 on epoch=149
03/08/2022 16:04:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.968545 on epoch=154
03/08/2022 16:04:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.970211 on epoch=159
03/08/2022 16:04:28 - INFO - __main__ - Step 330 Global step 330 Train loss 1.006533 on epoch=164
03/08/2022 16:04:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.984134 on epoch=169
03/08/2022 16:04:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.947628 on epoch=174
03/08/2022 16:04:37 - INFO - __main__ - Global step 350 Train loss 0.975410 EM 0.0 on epoch=174
03/08/2022 16:04:42 - INFO - __main__ - Step 360 Global step 360 Train loss 1.019074 on epoch=179
03/08/2022 16:04:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.967517 on epoch=184
03/08/2022 16:04:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.860765 on epoch=189
03/08/2022 16:04:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.842457 on epoch=194
03/08/2022 16:05:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.852712 on epoch=199
03/08/2022 16:05:01 - INFO - __main__ - Global step 400 Train loss 0.908505 EM 0.0 on epoch=199
03/08/2022 16:05:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.873724 on epoch=204
03/08/2022 16:05:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.817979 on epoch=209
03/08/2022 16:05:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.759369 on epoch=214
03/08/2022 16:05:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.754455 on epoch=219
03/08/2022 16:05:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.790121 on epoch=224
03/08/2022 16:05:24 - INFO - __main__ - Global step 450 Train loss 0.799130 EM 0.0 on epoch=224
03/08/2022 16:05:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.827518 on epoch=229
03/08/2022 16:05:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.858680 on epoch=234
03/08/2022 16:05:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.761981 on epoch=239
03/08/2022 16:05:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.751088 on epoch=244
03/08/2022 16:05:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.708959 on epoch=249
03/08/2022 16:05:47 - INFO - __main__ - Global step 500 Train loss 0.781645 EM 0.0 on epoch=249
03/08/2022 16:05:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.737379 on epoch=254
03/08/2022 16:05:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.707467 on epoch=259
03/08/2022 16:06:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.749701 on epoch=264
03/08/2022 16:06:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.720468 on epoch=269
03/08/2022 16:06:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.736473 on epoch=274
03/08/2022 16:06:11 - INFO - __main__ - Global step 550 Train loss 0.730298 EM 0.0 on epoch=274
03/08/2022 16:06:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.734786 on epoch=279
03/08/2022 16:06:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.723345 on epoch=284
03/08/2022 16:06:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.735425 on epoch=289
03/08/2022 16:06:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.729330 on epoch=294
03/08/2022 16:06:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.687948 on epoch=299
03/08/2022 16:06:34 - INFO - __main__ - Global step 600 Train loss 0.722167 EM 0.0 on epoch=299
03/08/2022 16:06:34 - INFO - __main__ - save last model!
03/08/2022 16:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:06:35 - INFO - __main__ - Printing 3 examples
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:06:35 - INFO - __main__ - ['List Cast']
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:06:35 - INFO - __main__ - ['Leidinger']
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:06:35 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:06:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 16:06:35 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:06:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:06:35 - INFO - __main__ - Printing 3 examples
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:06:35 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:06:35 - INFO - __main__ - ['Friend']
03/08/2022 16:06:35 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:06:35 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:06:35 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:06:35 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:06:35 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:06:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:06:45 - INFO - __main__ - Starting training!
03/08/2022 16:07:16 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 16:07:17 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 16:07:17 - INFO - __main__ - Printing 3 examples
03/08/2022 16:07:17 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 16:07:17 - INFO - __main__ - ['Whole S M']
03/08/2022 16:07:17 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 16:07:17 - INFO - __main__ - ['pushin Daisies']
03/08/2022 16:07:17 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 16:07:17 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 16:07:17 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:07:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:07:19 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 16:10:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0005_8_predictions.txt
03/08/2022 16:10:06 - INFO - __main__ - EM on test data: 0.0041
03/08/2022 16:10:06 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.00409626216077829
03/08/2022 16:10:06 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0003, bsz=8 ...
03/08/2022 16:10:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:10:07 - INFO - __main__ - Printing 3 examples
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:10:07 - INFO - __main__ - ['List Cast']
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:10:07 - INFO - __main__ - ['Leidinger']
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:10:07 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:10:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 16:10:07 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:10:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:10:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:10:07 - INFO - __main__ - Printing 3 examples
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:10:07 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:10:07 - INFO - __main__ - ['Friend']
03/08/2022 16:10:07 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:10:07 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:10:07 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:10:07 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:10:08 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:10:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:10:18 - INFO - __main__ - Starting training!
03/08/2022 16:10:23 - INFO - __main__ - Step 10 Global step 10 Train loss 20.058319 on epoch=4
03/08/2022 16:10:27 - INFO - __main__ - Step 20 Global step 20 Train loss 17.285984 on epoch=9
03/08/2022 16:10:32 - INFO - __main__ - Step 30 Global step 30 Train loss 12.834514 on epoch=14
03/08/2022 16:10:37 - INFO - __main__ - Step 40 Global step 40 Train loss 10.951889 on epoch=19
03/08/2022 16:10:42 - INFO - __main__ - Step 50 Global step 50 Train loss 10.133731 on epoch=24
03/08/2022 16:10:54 - INFO - __main__ - Global step 50 Train loss 14.252887 EM 0.03125 on epoch=24
03/08/2022 16:11:24 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/08/2022 16:11:29 - INFO - __main__ - Step 60 Global step 60 Train loss 9.018760 on epoch=29
03/08/2022 16:11:34 - INFO - __main__ - Step 70 Global step 70 Train loss 8.067491 on epoch=34
03/08/2022 16:11:38 - INFO - __main__ - Step 80 Global step 80 Train loss 7.192767 on epoch=39
03/08/2022 16:11:43 - INFO - __main__ - Step 90 Global step 90 Train loss 5.996198 on epoch=44
03/08/2022 16:11:47 - INFO - __main__ - Step 100 Global step 100 Train loss 5.789332 on epoch=49
03/08/2022 16:11:48 - INFO - __main__ - Global step 100 Train loss 7.212910 EM 0.0 on epoch=49
03/08/2022 16:11:53 - INFO - __main__ - Step 110 Global step 110 Train loss 5.290674 on epoch=54
03/08/2022 16:11:58 - INFO - __main__ - Step 120 Global step 120 Train loss 4.636157 on epoch=59
03/08/2022 16:12:02 - INFO - __main__ - Step 130 Global step 130 Train loss 4.244074 on epoch=64
03/08/2022 16:12:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.428479 on epoch=69
03/08/2022 16:12:12 - INFO - __main__ - Step 150 Global step 150 Train loss 3.956909 on epoch=74
03/08/2022 16:12:12 - INFO - __main__ - Global step 150 Train loss 4.511259 EM 0.03125 on epoch=74
03/08/2022 16:12:17 - INFO - __main__ - Step 160 Global step 160 Train loss 3.421632 on epoch=79
03/08/2022 16:12:22 - INFO - __main__ - Step 170 Global step 170 Train loss 3.098789 on epoch=84
03/08/2022 16:12:26 - INFO - __main__ - Step 180 Global step 180 Train loss 2.777063 on epoch=89
03/08/2022 16:12:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.598073 on epoch=94
03/08/2022 16:12:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.312686 on epoch=99
03/08/2022 16:12:36 - INFO - __main__ - Global step 200 Train loss 2.841649 EM 0.0 on epoch=99
03/08/2022 16:12:41 - INFO - __main__ - Step 210 Global step 210 Train loss 2.723462 on epoch=104
03/08/2022 16:12:45 - INFO - __main__ - Step 220 Global step 220 Train loss 2.109910 on epoch=109
03/08/2022 16:12:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.845263 on epoch=114
03/08/2022 16:12:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.840023 on epoch=119
03/08/2022 16:12:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.845993 on epoch=124
03/08/2022 16:13:00 - INFO - __main__ - Global step 250 Train loss 2.072930 EM 0.0 on epoch=124
03/08/2022 16:13:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.664130 on epoch=129
03/08/2022 16:13:09 - INFO - __main__ - Step 270 Global step 270 Train loss 1.814436 on epoch=134
03/08/2022 16:13:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.787267 on epoch=139
03/08/2022 16:13:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.486624 on epoch=144
03/08/2022 16:13:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.567119 on epoch=149
03/08/2022 16:13:23 - INFO - __main__ - Global step 300 Train loss 1.663915 EM 0.0 on epoch=149
03/08/2022 16:13:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.553427 on epoch=154
03/08/2022 16:13:33 - INFO - __main__ - Step 320 Global step 320 Train loss 1.681809 on epoch=159
03/08/2022 16:13:38 - INFO - __main__ - Step 330 Global step 330 Train loss 1.619184 on epoch=164
03/08/2022 16:13:42 - INFO - __main__ - Step 340 Global step 340 Train loss 1.467288 on epoch=169
03/08/2022 16:13:47 - INFO - __main__ - Step 350 Global step 350 Train loss 1.403820 on epoch=174
03/08/2022 16:13:47 - INFO - __main__ - Global step 350 Train loss 1.545106 EM 0.0 on epoch=174
03/08/2022 16:13:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.301231 on epoch=179
03/08/2022 16:13:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.393660 on epoch=184
03/08/2022 16:14:02 - INFO - __main__ - Step 380 Global step 380 Train loss 1.140334 on epoch=189
03/08/2022 16:14:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.256243 on epoch=194
03/08/2022 16:14:11 - INFO - __main__ - Step 400 Global step 400 Train loss 1.291202 on epoch=199
03/08/2022 16:14:12 - INFO - __main__ - Global step 400 Train loss 1.276534 EM 0.0 on epoch=199
03/08/2022 16:14:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.248139 on epoch=204
03/08/2022 16:14:21 - INFO - __main__ - Step 420 Global step 420 Train loss 1.390286 on epoch=209
03/08/2022 16:14:26 - INFO - __main__ - Step 430 Global step 430 Train loss 1.125880 on epoch=214
03/08/2022 16:14:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.204214 on epoch=219
03/08/2022 16:14:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.172266 on epoch=224
03/08/2022 16:14:36 - INFO - __main__ - Global step 450 Train loss 1.228157 EM 0.0 on epoch=224
03/08/2022 16:14:41 - INFO - __main__ - Step 460 Global step 460 Train loss 1.149371 on epoch=229
03/08/2022 16:14:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.099946 on epoch=234
03/08/2022 16:14:50 - INFO - __main__ - Step 480 Global step 480 Train loss 1.016441 on epoch=239
03/08/2022 16:14:55 - INFO - __main__ - Step 490 Global step 490 Train loss 1.018435 on epoch=244
03/08/2022 16:15:00 - INFO - __main__ - Step 500 Global step 500 Train loss 1.084572 on epoch=249
03/08/2022 16:15:00 - INFO - __main__ - Global step 500 Train loss 1.073753 EM 0.0 on epoch=249
03/08/2022 16:15:05 - INFO - __main__ - Step 510 Global step 510 Train loss 1.090947 on epoch=254
03/08/2022 16:15:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.936741 on epoch=259
03/08/2022 16:15:14 - INFO - __main__ - Step 530 Global step 530 Train loss 1.028027 on epoch=264
03/08/2022 16:15:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.911998 on epoch=269
03/08/2022 16:15:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.910192 on epoch=274
03/08/2022 16:15:25 - INFO - __main__ - Global step 550 Train loss 0.975581 EM 0.0 on epoch=274
03/08/2022 16:15:29 - INFO - __main__ - Step 560 Global step 560 Train loss 1.053294 on epoch=279
03/08/2022 16:15:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.996756 on epoch=284
03/08/2022 16:15:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.982844 on epoch=289
03/08/2022 16:15:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.845429 on epoch=294
03/08/2022 16:15:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.937841 on epoch=299
03/08/2022 16:15:49 - INFO - __main__ - Global step 600 Train loss 0.963233 EM 0.0 on epoch=299
03/08/2022 16:15:49 - INFO - __main__ - save last model!
03/08/2022 16:15:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:15:49 - INFO - __main__ - Printing 3 examples
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:15:49 - INFO - __main__ - ['List Cast']
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:15:49 - INFO - __main__ - ['Leidinger']
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:15:49 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:15:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 16:15:49 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:15:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:15:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:15:49 - INFO - __main__ - Printing 3 examples
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:15:49 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:15:49 - INFO - __main__ - ['Friend']
03/08/2022 16:15:49 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:15:49 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:15:49 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:15:49 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:15:49 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:16:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:16:00 - INFO - __main__ - Starting training!
03/08/2022 16:16:28 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 16:16:29 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 16:16:29 - INFO - __main__ - Printing 3 examples
03/08/2022 16:16:29 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 16:16:29 - INFO - __main__ - ['Whole S M']
03/08/2022 16:16:29 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 16:16:29 - INFO - __main__ - ['pushin Daisies']
03/08/2022 16:16:29 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 16:16:29 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 16:16:29 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:16:30 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:16:32 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 16:24:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0003_8_predictions.txt
03/08/2022 16:24:58 - INFO - __main__ - EM on test data: 0.0189
03/08/2022 16:24:59 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.01894521249359959
03/08/2022 16:24:59 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0002, bsz=8 ...
03/08/2022 16:25:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:25:00 - INFO - __main__ - Printing 3 examples
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:25:00 - INFO - __main__ - ['List Cast']
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:25:00 - INFO - __main__ - ['Leidinger']
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:25:00 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:25:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 16:25:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:25:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:25:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:25:00 - INFO - __main__ - Printing 3 examples
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:25:00 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:25:00 - INFO - __main__ - ['Friend']
03/08/2022 16:25:00 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:25:00 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:25:00 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:25:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:25:00 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:25:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:25:09 - INFO - __main__ - Starting training!
03/08/2022 16:25:13 - INFO - __main__ - Step 10 Global step 10 Train loss 20.302624 on epoch=4
03/08/2022 16:25:18 - INFO - __main__ - Step 20 Global step 20 Train loss 17.525249 on epoch=9
03/08/2022 16:25:22 - INFO - __main__ - Step 30 Global step 30 Train loss 15.431314 on epoch=14
03/08/2022 16:25:27 - INFO - __main__ - Step 40 Global step 40 Train loss 13.426900 on epoch=19
03/08/2022 16:25:32 - INFO - __main__ - Step 50 Global step 50 Train loss 11.655492 on epoch=24
03/08/2022 16:25:40 - INFO - __main__ - Global step 50 Train loss 15.668314 EM 0.0 on epoch=24
03/08/2022 16:26:11 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 16:26:16 - INFO - __main__ - Step 60 Global step 60 Train loss 11.400643 on epoch=29
03/08/2022 16:26:20 - INFO - __main__ - Step 70 Global step 70 Train loss 10.374076 on epoch=34
03/08/2022 16:26:25 - INFO - __main__ - Step 80 Global step 80 Train loss 9.824293 on epoch=39
03/08/2022 16:26:30 - INFO - __main__ - Step 90 Global step 90 Train loss 8.790123 on epoch=44
03/08/2022 16:26:35 - INFO - __main__ - Step 100 Global step 100 Train loss 8.288466 on epoch=49
03/08/2022 16:26:36 - INFO - __main__ - Global step 100 Train loss 9.735520 EM 0.0 on epoch=49
03/08/2022 16:26:41 - INFO - __main__ - Step 110 Global step 110 Train loss 7.993811 on epoch=54
03/08/2022 16:26:45 - INFO - __main__ - Step 120 Global step 120 Train loss 6.720711 on epoch=59
03/08/2022 16:26:50 - INFO - __main__ - Step 130 Global step 130 Train loss 7.594251 on epoch=64
03/08/2022 16:26:55 - INFO - __main__ - Step 140 Global step 140 Train loss 6.837809 on epoch=69
03/08/2022 16:26:59 - INFO - __main__ - Step 150 Global step 150 Train loss 6.374009 on epoch=74
03/08/2022 16:27:00 - INFO - __main__ - Global step 150 Train loss 7.104118 EM 0.0 on epoch=74
03/08/2022 16:27:05 - INFO - __main__ - Step 160 Global step 160 Train loss 5.804210 on epoch=79
03/08/2022 16:27:10 - INFO - __main__ - Step 170 Global step 170 Train loss 5.467048 on epoch=84
03/08/2022 16:27:15 - INFO - __main__ - Step 180 Global step 180 Train loss 5.053056 on epoch=89
03/08/2022 16:27:19 - INFO - __main__ - Step 190 Global step 190 Train loss 4.673263 on epoch=94
03/08/2022 16:27:24 - INFO - __main__ - Step 200 Global step 200 Train loss 4.323789 on epoch=99
03/08/2022 16:27:25 - INFO - __main__ - Global step 200 Train loss 5.064273 EM 0.0 on epoch=99
03/08/2022 16:27:30 - INFO - __main__ - Step 210 Global step 210 Train loss 4.073206 on epoch=104
03/08/2022 16:27:34 - INFO - __main__ - Step 220 Global step 220 Train loss 3.823708 on epoch=109
03/08/2022 16:27:39 - INFO - __main__ - Step 230 Global step 230 Train loss 3.769143 on epoch=114
03/08/2022 16:27:44 - INFO - __main__ - Step 240 Global step 240 Train loss 3.427969 on epoch=119
03/08/2022 16:27:49 - INFO - __main__ - Step 250 Global step 250 Train loss 3.460960 on epoch=124
03/08/2022 16:27:50 - INFO - __main__ - Global step 250 Train loss 3.710997 EM 0.0 on epoch=124
03/08/2022 16:27:55 - INFO - __main__ - Step 260 Global step 260 Train loss 3.094649 on epoch=129
03/08/2022 16:27:59 - INFO - __main__ - Step 270 Global step 270 Train loss 2.637827 on epoch=134
03/08/2022 16:28:04 - INFO - __main__ - Step 280 Global step 280 Train loss 2.386196 on epoch=139
03/08/2022 16:28:09 - INFO - __main__ - Step 290 Global step 290 Train loss 2.814116 on epoch=144
03/08/2022 16:28:14 - INFO - __main__ - Step 300 Global step 300 Train loss 2.234683 on epoch=149
03/08/2022 16:28:14 - INFO - __main__ - Global step 300 Train loss 2.633494 EM 0.0 on epoch=149
03/08/2022 16:28:19 - INFO - __main__ - Step 310 Global step 310 Train loss 2.205093 on epoch=154
03/08/2022 16:28:24 - INFO - __main__ - Step 320 Global step 320 Train loss 1.900340 on epoch=159
03/08/2022 16:28:29 - INFO - __main__ - Step 330 Global step 330 Train loss 1.918033 on epoch=164
03/08/2022 16:28:34 - INFO - __main__ - Step 340 Global step 340 Train loss 2.145755 on epoch=169
03/08/2022 16:28:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.994842 on epoch=174
03/08/2022 16:28:39 - INFO - __main__ - Global step 350 Train loss 2.032812 EM 0.0 on epoch=174
03/08/2022 16:28:44 - INFO - __main__ - Step 360 Global step 360 Train loss 1.883506 on epoch=179
03/08/2022 16:28:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.581956 on epoch=184
03/08/2022 16:28:53 - INFO - __main__ - Step 380 Global step 380 Train loss 1.663433 on epoch=189
03/08/2022 16:28:58 - INFO - __main__ - Step 390 Global step 390 Train loss 1.657005 on epoch=194
03/08/2022 16:29:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.704031 on epoch=199
03/08/2022 16:29:03 - INFO - __main__ - Global step 400 Train loss 1.697986 EM 0.0 on epoch=199
03/08/2022 16:29:08 - INFO - __main__ - Step 410 Global step 410 Train loss 1.581368 on epoch=204
03/08/2022 16:29:13 - INFO - __main__ - Step 420 Global step 420 Train loss 1.722572 on epoch=209
03/08/2022 16:29:18 - INFO - __main__ - Step 430 Global step 430 Train loss 1.583027 on epoch=214
03/08/2022 16:29:23 - INFO - __main__ - Step 440 Global step 440 Train loss 1.617220 on epoch=219
03/08/2022 16:29:28 - INFO - __main__ - Step 450 Global step 450 Train loss 1.540220 on epoch=224
03/08/2022 16:29:28 - INFO - __main__ - Global step 450 Train loss 1.608881 EM 0.0 on epoch=224
03/08/2022 16:29:33 - INFO - __main__ - Step 460 Global step 460 Train loss 1.593560 on epoch=229
03/08/2022 16:29:38 - INFO - __main__ - Step 470 Global step 470 Train loss 1.433217 on epoch=234
03/08/2022 16:29:42 - INFO - __main__ - Step 480 Global step 480 Train loss 1.461754 on epoch=239
03/08/2022 16:29:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.406774 on epoch=244
03/08/2022 16:29:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.486869 on epoch=249
03/08/2022 16:29:53 - INFO - __main__ - Global step 500 Train loss 1.476435 EM 0.0 on epoch=249
03/08/2022 16:29:57 - INFO - __main__ - Step 510 Global step 510 Train loss 1.395399 on epoch=254
03/08/2022 16:30:02 - INFO - __main__ - Step 520 Global step 520 Train loss 1.526172 on epoch=259
03/08/2022 16:30:07 - INFO - __main__ - Step 530 Global step 530 Train loss 1.281569 on epoch=264
03/08/2022 16:30:12 - INFO - __main__ - Step 540 Global step 540 Train loss 1.433830 on epoch=269
03/08/2022 16:30:17 - INFO - __main__ - Step 550 Global step 550 Train loss 1.227060 on epoch=274
03/08/2022 16:30:17 - INFO - __main__ - Global step 550 Train loss 1.372806 EM 0.0 on epoch=274
03/08/2022 16:30:22 - INFO - __main__ - Step 560 Global step 560 Train loss 1.215128 on epoch=279
03/08/2022 16:30:27 - INFO - __main__ - Step 570 Global step 570 Train loss 1.311141 on epoch=284
03/08/2022 16:30:32 - INFO - __main__ - Step 580 Global step 580 Train loss 1.235101 on epoch=289
03/08/2022 16:30:37 - INFO - __main__ - Step 590 Global step 590 Train loss 1.238240 on epoch=294
03/08/2022 16:30:41 - INFO - __main__ - Step 600 Global step 600 Train loss 1.282243 on epoch=299
03/08/2022 16:30:42 - INFO - __main__ - Global step 600 Train loss 1.256371 EM 0.0 on epoch=299
03/08/2022 16:30:42 - INFO - __main__ - save last model!
03/08/2022 16:30:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:30:42 - INFO - __main__ - Printing 3 examples
03/08/2022 16:30:42 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:30:42 - INFO - __main__ - ['List Cast']
03/08/2022 16:30:42 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:30:42 - INFO - __main__ - ['Leidinger']
03/08/2022 16:30:42 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:30:42 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:30:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 16:30:42 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:30:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:30:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:30:42 - INFO - __main__ - Printing 3 examples
03/08/2022 16:30:42 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:30:43 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:30:43 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:30:43 - INFO - __main__ - ['Friend']
03/08/2022 16:30:43 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:30:43 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:30:43 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:30:43 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:30:43 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:30:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:30:52 - INFO - __main__ - Starting training!
03/08/2022 16:31:22 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 16:31:23 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 16:31:23 - INFO - __main__ - Printing 3 examples
03/08/2022 16:31:23 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 16:31:23 - INFO - __main__ - ['Whole S M']
03/08/2022 16:31:23 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 16:31:23 - INFO - __main__ - ['pushin Daisies']
03/08/2022 16:31:23 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 16:31:23 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 16:31:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:31:24 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:31:26 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 16:38:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0002_8_predictions.txt
03/08/2022 16:38:54 - INFO - __main__ - EM on test data: 0.0010
03/08/2022 16:38:55 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0010240655401945725
03/08/2022 16:38:55 - INFO - __main__ - Running ... prefix=crawl_domain_32_100, lr=0.0001, bsz=8 ...
03/08/2022 16:38:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:38:56 - INFO - __main__ - Printing 3 examples
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] listcast
03/08/2022 16:38:56 - INFO - __main__ - ['List Cast']
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] leidinger
03/08/2022 16:38:56 - INFO - __main__ - ['Leidinger']
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] successfulmeetings
03/08/2022 16:38:56 - INFO - __main__ - ['Successful Meetings']
03/08/2022 16:38:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 16:38:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:38:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:38:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:38:56 - INFO - __main__ - Printing 3 examples
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] touchscreenhardware
03/08/2022 16:38:56 - INFO - __main__ - ['Touchscreen hardware']
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] friend
03/08/2022 16:38:56 - INFO - __main__ - ['Friend']
03/08/2022 16:38:56 - INFO - __main__ -  [crawl_domain] peterbutlerproperties
03/08/2022 16:38:56 - INFO - __main__ - ['Peter Butler Properties']
03/08/2022 16:38:56 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:38:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:38:56 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:39:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:39:07 - INFO - __main__ - Starting training!
03/08/2022 16:39:10 - INFO - __main__ - Step 10 Global step 10 Train loss 19.892750 on epoch=4
03/08/2022 16:39:15 - INFO - __main__ - Step 20 Global step 20 Train loss 19.873722 on epoch=9
03/08/2022 16:39:20 - INFO - __main__ - Step 30 Global step 30 Train loss 18.152744 on epoch=14
03/08/2022 16:39:24 - INFO - __main__ - Step 40 Global step 40 Train loss 17.566187 on epoch=19
03/08/2022 16:39:29 - INFO - __main__ - Step 50 Global step 50 Train loss 15.593717 on epoch=24
03/08/2022 16:39:39 - INFO - __main__ - Global step 50 Train loss 18.215822 EM 0.0 on epoch=24
03/08/2022 16:40:09 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 16:40:14 - INFO - __main__ - Step 60 Global step 60 Train loss 15.093424 on epoch=29
03/08/2022 16:40:18 - INFO - __main__ - Step 70 Global step 70 Train loss 14.112646 on epoch=34
03/08/2022 16:40:23 - INFO - __main__ - Step 80 Global step 80 Train loss 13.822725 on epoch=39
03/08/2022 16:40:28 - INFO - __main__ - Step 90 Global step 90 Train loss 12.845247 on epoch=44
03/08/2022 16:40:33 - INFO - __main__ - Step 100 Global step 100 Train loss 12.912407 on epoch=49
03/08/2022 16:40:40 - INFO - __main__ - Global step 100 Train loss 13.757290 EM 0.0 on epoch=49
03/08/2022 16:40:45 - INFO - __main__ - Step 110 Global step 110 Train loss 11.940862 on epoch=54
03/08/2022 16:40:50 - INFO - __main__ - Step 120 Global step 120 Train loss 11.783564 on epoch=59
03/08/2022 16:40:55 - INFO - __main__ - Step 130 Global step 130 Train loss 11.784189 on epoch=64
03/08/2022 16:41:00 - INFO - __main__ - Step 140 Global step 140 Train loss 10.742000 on epoch=69
03/08/2022 16:41:04 - INFO - __main__ - Step 150 Global step 150 Train loss 11.082103 on epoch=74
03/08/2022 16:41:05 - INFO - __main__ - Global step 150 Train loss 11.466544 EM 0.0 on epoch=74
03/08/2022 16:41:10 - INFO - __main__ - Step 160 Global step 160 Train loss 10.430242 on epoch=79
03/08/2022 16:41:15 - INFO - __main__ - Step 170 Global step 170 Train loss 10.046401 on epoch=84
03/08/2022 16:41:20 - INFO - __main__ - Step 180 Global step 180 Train loss 9.588122 on epoch=89
03/08/2022 16:41:24 - INFO - __main__ - Step 190 Global step 190 Train loss 9.593474 on epoch=94
03/08/2022 16:41:29 - INFO - __main__ - Step 200 Global step 200 Train loss 9.218088 on epoch=99
03/08/2022 16:41:30 - INFO - __main__ - Global step 200 Train loss 9.775266 EM 0.0 on epoch=99
03/08/2022 16:41:35 - INFO - __main__ - Step 210 Global step 210 Train loss 9.105588 on epoch=104
03/08/2022 16:41:39 - INFO - __main__ - Step 220 Global step 220 Train loss 8.880101 on epoch=109
03/08/2022 16:41:44 - INFO - __main__ - Step 230 Global step 230 Train loss 8.684797 on epoch=114
03/08/2022 16:41:49 - INFO - __main__ - Step 240 Global step 240 Train loss 8.299627 on epoch=119
03/08/2022 16:41:53 - INFO - __main__ - Step 250 Global step 250 Train loss 8.267752 on epoch=124
03/08/2022 16:41:54 - INFO - __main__ - Global step 250 Train loss 8.647573 EM 0.0 on epoch=124
03/08/2022 16:41:59 - INFO - __main__ - Step 260 Global step 260 Train loss 7.667945 on epoch=129
03/08/2022 16:42:03 - INFO - __main__ - Step 270 Global step 270 Train loss 7.801826 on epoch=134
03/08/2022 16:42:08 - INFO - __main__ - Step 280 Global step 280 Train loss 7.277096 on epoch=139
03/08/2022 16:42:13 - INFO - __main__ - Step 290 Global step 290 Train loss 6.948691 on epoch=144
03/08/2022 16:42:18 - INFO - __main__ - Step 300 Global step 300 Train loss 7.253336 on epoch=149
03/08/2022 16:42:18 - INFO - __main__ - Global step 300 Train loss 7.389779 EM 0.0 on epoch=149
03/08/2022 16:42:23 - INFO - __main__ - Step 310 Global step 310 Train loss 7.075575 on epoch=154
03/08/2022 16:42:28 - INFO - __main__ - Step 320 Global step 320 Train loss 6.702263 on epoch=159
03/08/2022 16:42:33 - INFO - __main__ - Step 330 Global step 330 Train loss 6.518688 on epoch=164
03/08/2022 16:42:37 - INFO - __main__ - Step 340 Global step 340 Train loss 6.456491 on epoch=169
03/08/2022 16:42:42 - INFO - __main__ - Step 350 Global step 350 Train loss 6.543900 on epoch=174
03/08/2022 16:42:43 - INFO - __main__ - Global step 350 Train loss 6.659384 EM 0.0 on epoch=174
03/08/2022 16:42:47 - INFO - __main__ - Step 360 Global step 360 Train loss 6.289783 on epoch=179
03/08/2022 16:42:52 - INFO - __main__ - Step 370 Global step 370 Train loss 5.911571 on epoch=184
03/08/2022 16:42:57 - INFO - __main__ - Step 380 Global step 380 Train loss 5.887015 on epoch=189
03/08/2022 16:43:01 - INFO - __main__ - Step 390 Global step 390 Train loss 5.432060 on epoch=194
03/08/2022 16:43:06 - INFO - __main__ - Step 400 Global step 400 Train loss 5.341794 on epoch=199
03/08/2022 16:43:07 - INFO - __main__ - Global step 400 Train loss 5.772444 EM 0.0 on epoch=199
03/08/2022 16:43:11 - INFO - __main__ - Step 410 Global step 410 Train loss 5.309854 on epoch=204
03/08/2022 16:43:16 - INFO - __main__ - Step 420 Global step 420 Train loss 4.925790 on epoch=209
03/08/2022 16:43:21 - INFO - __main__ - Step 430 Global step 430 Train loss 4.971354 on epoch=214
03/08/2022 16:43:26 - INFO - __main__ - Step 440 Global step 440 Train loss 4.645549 on epoch=219
03/08/2022 16:43:30 - INFO - __main__ - Step 450 Global step 450 Train loss 4.207684 on epoch=224
03/08/2022 16:43:31 - INFO - __main__ - Global step 450 Train loss 4.812046 EM 0.0 on epoch=224
03/08/2022 16:43:36 - INFO - __main__ - Step 460 Global step 460 Train loss 4.185947 on epoch=229
03/08/2022 16:43:40 - INFO - __main__ - Step 470 Global step 470 Train loss 4.090515 on epoch=234
03/08/2022 16:43:45 - INFO - __main__ - Step 480 Global step 480 Train loss 3.888356 on epoch=239
03/08/2022 16:43:50 - INFO - __main__ - Step 490 Global step 490 Train loss 3.729445 on epoch=244
03/08/2022 16:43:55 - INFO - __main__ - Step 500 Global step 500 Train loss 3.697469 on epoch=249
03/08/2022 16:43:56 - INFO - __main__ - Global step 500 Train loss 3.918346 EM 0.0 on epoch=249
03/08/2022 16:44:00 - INFO - __main__ - Step 510 Global step 510 Train loss 3.292602 on epoch=254
03/08/2022 16:44:05 - INFO - __main__ - Step 520 Global step 520 Train loss 3.118775 on epoch=259
03/08/2022 16:44:10 - INFO - __main__ - Step 530 Global step 530 Train loss 3.084145 on epoch=264
03/08/2022 16:44:15 - INFO - __main__ - Step 540 Global step 540 Train loss 2.871497 on epoch=269
03/08/2022 16:44:19 - INFO - __main__ - Step 550 Global step 550 Train loss 2.825990 on epoch=274
03/08/2022 16:44:20 - INFO - __main__ - Global step 550 Train loss 3.038602 EM 0.0 on epoch=274
03/08/2022 16:44:25 - INFO - __main__ - Step 560 Global step 560 Train loss 2.658493 on epoch=279
03/08/2022 16:44:29 - INFO - __main__ - Step 570 Global step 570 Train loss 2.639215 on epoch=284
03/08/2022 16:44:34 - INFO - __main__ - Step 580 Global step 580 Train loss 2.546841 on epoch=289
03/08/2022 16:44:39 - INFO - __main__ - Step 590 Global step 590 Train loss 2.159204 on epoch=294
03/08/2022 16:44:44 - INFO - __main__ - Step 600 Global step 600 Train loss 2.319166 on epoch=299
03/08/2022 16:44:44 - INFO - __main__ - Global step 600 Train loss 2.464584 EM 0.0 on epoch=299
03/08/2022 16:44:44 - INFO - __main__ - save last model!
03/08/2022 16:44:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:44:45 - INFO - __main__ - Printing 3 examples
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 16:44:45 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 16:44:45 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 16:44:45 - INFO - __main__ - ['Olio']
03/08/2022 16:44:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 16:44:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:44:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:44:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:44:45 - INFO - __main__ - Printing 3 examples
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 16:44:45 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 16:44:45 - INFO - __main__ - ['Acuren']
03/08/2022 16:44:45 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 16:44:45 - INFO - __main__ - ['J for Justice']
03/08/2022 16:44:45 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:44:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:44:45 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:44:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:44:54 - INFO - __main__ - Starting training!
03/08/2022 16:45:25 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 16:45:26 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 16:45:26 - INFO - __main__ - Printing 3 examples
03/08/2022 16:45:26 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 16:45:26 - INFO - __main__ - ['Whole S M']
03/08/2022 16:45:26 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 16:45:26 - INFO - __main__ - ['pushin Daisies']
03/08/2022 16:45:26 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 16:45:26 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 16:45:26 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:45:26 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:45:28 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 16:55:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_100_0.0001_8_predictions.txt
03/08/2022 16:55:29 - INFO - __main__ - EM on test data: 0.0046
03/08/2022 16:55:29 - INFO - __main__ - prefix=crawl_domain_32_100, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.004608294930875576
03/08/2022 16:55:29 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0005, bsz=8 ...
03/08/2022 16:55:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:55:30 - INFO - __main__ - Printing 3 examples
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 16:55:30 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 16:55:30 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 16:55:30 - INFO - __main__ - ['Olio']
03/08/2022 16:55:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 16:55:30 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:55:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 16:55:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 16:55:30 - INFO - __main__ - Printing 3 examples
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 16:55:30 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 16:55:30 - INFO - __main__ - ['Acuren']
03/08/2022 16:55:30 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 16:55:30 - INFO - __main__ - ['J for Justice']
03/08/2022 16:55:30 - INFO - __main__ - Tokenizing Input ...
03/08/2022 16:55:30 - INFO - __main__ - Tokenizing Output ...
03/08/2022 16:55:30 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 16:55:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 16:55:40 - INFO - __main__ - Starting training!
03/08/2022 16:55:44 - INFO - __main__ - Step 10 Global step 10 Train loss 22.169977 on epoch=4
03/08/2022 16:55:49 - INFO - __main__ - Step 20 Global step 20 Train loss 20.221508 on epoch=9
03/08/2022 16:55:53 - INFO - __main__ - Step 30 Global step 30 Train loss 14.238935 on epoch=14
03/08/2022 16:55:58 - INFO - __main__ - Step 40 Global step 40 Train loss 12.513116 on epoch=19
03/08/2022 16:56:03 - INFO - __main__ - Step 50 Global step 50 Train loss 10.752254 on epoch=24
03/08/2022 16:56:05 - INFO - __main__ - Global step 50 Train loss 15.979157 EM 0.03125 on epoch=24
03/08/2022 16:56:49 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/08/2022 16:56:53 - INFO - __main__ - Step 60 Global step 60 Train loss 9.530540 on epoch=29
03/08/2022 16:56:58 - INFO - __main__ - Step 70 Global step 70 Train loss 8.314863 on epoch=34
03/08/2022 16:57:03 - INFO - __main__ - Step 80 Global step 80 Train loss 7.085550 on epoch=39
03/08/2022 16:57:07 - INFO - __main__ - Step 90 Global step 90 Train loss 6.243340 on epoch=44
03/08/2022 16:57:12 - INFO - __main__ - Step 100 Global step 100 Train loss 5.203651 on epoch=49
03/08/2022 16:57:13 - INFO - __main__ - Global step 100 Train loss 7.275589 EM 0.0 on epoch=49
03/08/2022 16:57:18 - INFO - __main__ - Step 110 Global step 110 Train loss 4.820553 on epoch=54
03/08/2022 16:57:22 - INFO - __main__ - Step 120 Global step 120 Train loss 3.838403 on epoch=59
03/08/2022 16:57:27 - INFO - __main__ - Step 130 Global step 130 Train loss 3.060731 on epoch=64
03/08/2022 16:57:32 - INFO - __main__ - Step 140 Global step 140 Train loss 2.863333 on epoch=69
03/08/2022 16:57:36 - INFO - __main__ - Step 150 Global step 150 Train loss 3.000518 on epoch=74
03/08/2022 16:57:37 - INFO - __main__ - Global step 150 Train loss 3.516708 EM 0.0 on epoch=74
03/08/2022 16:57:42 - INFO - __main__ - Step 160 Global step 160 Train loss 2.357654 on epoch=79
03/08/2022 16:57:46 - INFO - __main__ - Step 170 Global step 170 Train loss 2.844096 on epoch=84
03/08/2022 16:57:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.449019 on epoch=89
03/08/2022 16:57:56 - INFO - __main__ - Step 190 Global step 190 Train loss 2.348243 on epoch=94
03/08/2022 16:58:01 - INFO - __main__ - Step 200 Global step 200 Train loss 2.144969 on epoch=99
03/08/2022 16:58:01 - INFO - __main__ - Global step 200 Train loss 2.428796 EM 0.0 on epoch=99
03/08/2022 16:58:06 - INFO - __main__ - Step 210 Global step 210 Train loss 2.064688 on epoch=104
03/08/2022 16:58:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.799168 on epoch=109
03/08/2022 16:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 1.841351 on epoch=114
03/08/2022 16:58:20 - INFO - __main__ - Step 240 Global step 240 Train loss 1.955490 on epoch=119
03/08/2022 16:58:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.487027 on epoch=124
03/08/2022 16:58:25 - INFO - __main__ - Global step 250 Train loss 1.829545 EM 0.0 on epoch=124
03/08/2022 16:58:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.884562 on epoch=129
03/08/2022 16:58:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.604274 on epoch=134
03/08/2022 16:58:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.743397 on epoch=139
03/08/2022 16:58:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.692279 on epoch=144
03/08/2022 16:58:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.797992 on epoch=149
03/08/2022 16:58:50 - INFO - __main__ - Global step 300 Train loss 1.744501 EM 0.0 on epoch=149
03/08/2022 16:58:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.600315 on epoch=154
03/08/2022 16:58:59 - INFO - __main__ - Step 320 Global step 320 Train loss 1.424097 on epoch=159
03/08/2022 16:59:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.351904 on epoch=164
03/08/2022 16:59:09 - INFO - __main__ - Step 340 Global step 340 Train loss 1.367917 on epoch=169
03/08/2022 16:59:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.255611 on epoch=174
03/08/2022 16:59:15 - INFO - __main__ - Global step 350 Train loss 1.399969 EM 0.0 on epoch=174
03/08/2022 16:59:19 - INFO - __main__ - Step 360 Global step 360 Train loss 1.290851 on epoch=179
03/08/2022 16:59:24 - INFO - __main__ - Step 370 Global step 370 Train loss 1.101979 on epoch=184
03/08/2022 16:59:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.288876 on epoch=189
03/08/2022 16:59:34 - INFO - __main__ - Step 390 Global step 390 Train loss 1.223155 on epoch=194
03/08/2022 16:59:38 - INFO - __main__ - Step 400 Global step 400 Train loss 1.294296 on epoch=199
03/08/2022 16:59:39 - INFO - __main__ - Global step 400 Train loss 1.239831 EM 0.0 on epoch=199
03/08/2022 16:59:44 - INFO - __main__ - Step 410 Global step 410 Train loss 1.084398 on epoch=204
03/08/2022 16:59:49 - INFO - __main__ - Step 420 Global step 420 Train loss 1.067977 on epoch=209
03/08/2022 16:59:54 - INFO - __main__ - Step 430 Global step 430 Train loss 1.128812 on epoch=214
03/08/2022 16:59:58 - INFO - __main__ - Step 440 Global step 440 Train loss 1.102931 on epoch=219
03/08/2022 17:00:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.030616 on epoch=224
03/08/2022 17:00:04 - INFO - __main__ - Global step 450 Train loss 1.082947 EM 0.0 on epoch=224
03/08/2022 17:00:09 - INFO - __main__ - Step 460 Global step 460 Train loss 1.072186 on epoch=229
03/08/2022 17:00:13 - INFO - __main__ - Step 470 Global step 470 Train loss 1.074908 on epoch=234
03/08/2022 17:00:18 - INFO - __main__ - Step 480 Global step 480 Train loss 1.018913 on epoch=239
03/08/2022 17:00:23 - INFO - __main__ - Step 490 Global step 490 Train loss 1.054736 on epoch=244
03/08/2022 17:00:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.986788 on epoch=249
03/08/2022 17:00:29 - INFO - __main__ - Global step 500 Train loss 1.041506 EM 0.0 on epoch=249
03/08/2022 17:00:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.929797 on epoch=254
03/08/2022 17:00:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.933387 on epoch=259
03/08/2022 17:00:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.964878 on epoch=264
03/08/2022 17:00:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.977489 on epoch=269
03/08/2022 17:00:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.914367 on epoch=274
03/08/2022 17:00:53 - INFO - __main__ - Global step 550 Train loss 0.943984 EM 0.0 on epoch=274
03/08/2022 17:00:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.970450 on epoch=279
03/08/2022 17:01:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.965549 on epoch=284
03/08/2022 17:01:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.897421 on epoch=289
03/08/2022 17:01:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.915446 on epoch=294
03/08/2022 17:01:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.924659 on epoch=299
03/08/2022 17:01:18 - INFO - __main__ - Global step 600 Train loss 0.934705 EM 0.0 on epoch=299
03/08/2022 17:01:18 - INFO - __main__ - save last model!
03/08/2022 17:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:01:19 - INFO - __main__ - Printing 3 examples
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:01:19 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:01:19 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:01:19 - INFO - __main__ - ['Olio']
03/08/2022 17:01:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:01:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:01:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:01:19 - INFO - __main__ - Printing 3 examples
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:01:19 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:01:19 - INFO - __main__ - ['Acuren']
03/08/2022 17:01:19 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:01:19 - INFO - __main__ - ['J for Justice']
03/08/2022 17:01:19 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:01:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:01:19 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:01:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:01:30 - INFO - __main__ - Starting training!
03/08/2022 17:01:58 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:01:59 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:01:59 - INFO - __main__ - Printing 3 examples
03/08/2022 17:01:59 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:01:59 - INFO - __main__ - ['Whole S M']
03/08/2022 17:01:59 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:01:59 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:01:59 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:01:59 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:01:59 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:01:59 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:02:01 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:04:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0005_8_predictions.txt
03/08/2022 17:04:15 - INFO - __main__ - EM on test data: 0.0097
03/08/2022 17:04:15 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.00972862263184844
03/08/2022 17:04:15 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0003, bsz=8 ...
03/08/2022 17:04:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:04:16 - INFO - __main__ - Printing 3 examples
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:04:16 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:04:16 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:04:16 - INFO - __main__ - ['Olio']
03/08/2022 17:04:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:04:16 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:04:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:04:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:04:16 - INFO - __main__ - Printing 3 examples
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:04:16 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:04:16 - INFO - __main__ - ['Acuren']
03/08/2022 17:04:16 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:04:16 - INFO - __main__ - ['J for Justice']
03/08/2022 17:04:16 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:04:16 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:04:16 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:04:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:04:27 - INFO - __main__ - Starting training!
03/08/2022 17:04:30 - INFO - __main__ - Step 10 Global step 10 Train loss 21.703747 on epoch=4
03/08/2022 17:04:35 - INFO - __main__ - Step 20 Global step 20 Train loss 20.342865 on epoch=9
03/08/2022 17:04:40 - INFO - __main__ - Step 30 Global step 30 Train loss 15.632631 on epoch=14
03/08/2022 17:04:45 - INFO - __main__ - Step 40 Global step 40 Train loss 13.674799 on epoch=19
03/08/2022 17:04:49 - INFO - __main__ - Step 50 Global step 50 Train loss 12.034645 on epoch=24
03/08/2022 17:04:56 - INFO - __main__ - Global step 50 Train loss 16.677738 EM 0.0 on epoch=24
03/08/2022 17:05:35 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 17:05:39 - INFO - __main__ - Step 60 Global step 60 Train loss 10.185628 on epoch=29
03/08/2022 17:05:44 - INFO - __main__ - Step 70 Global step 70 Train loss 9.018618 on epoch=34
03/08/2022 17:05:49 - INFO - __main__ - Step 80 Global step 80 Train loss 8.424570 on epoch=39
03/08/2022 17:05:54 - INFO - __main__ - Step 90 Global step 90 Train loss 7.483356 on epoch=44
03/08/2022 17:05:58 - INFO - __main__ - Step 100 Global step 100 Train loss 7.374215 on epoch=49
03/08/2022 17:05:59 - INFO - __main__ - Global step 100 Train loss 8.497278 EM 0.03125 on epoch=49
03/08/2022 17:06:30 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 17:06:34 - INFO - __main__ - Step 110 Global step 110 Train loss 6.834193 on epoch=54
03/08/2022 17:06:39 - INFO - __main__ - Step 120 Global step 120 Train loss 6.132937 on epoch=59
03/08/2022 17:06:44 - INFO - __main__ - Step 130 Global step 130 Train loss 6.532866 on epoch=64
03/08/2022 17:06:49 - INFO - __main__ - Step 140 Global step 140 Train loss 5.625341 on epoch=69
03/08/2022 17:06:54 - INFO - __main__ - Step 150 Global step 150 Train loss 5.442763 on epoch=74
03/08/2022 17:06:54 - INFO - __main__ - Global step 150 Train loss 6.113620 EM 0.0 on epoch=74
03/08/2022 17:06:59 - INFO - __main__ - Step 160 Global step 160 Train loss 4.902516 on epoch=79
03/08/2022 17:07:04 - INFO - __main__ - Step 170 Global step 170 Train loss 4.453235 on epoch=84
03/08/2022 17:07:09 - INFO - __main__ - Step 180 Global step 180 Train loss 3.758096 on epoch=89
03/08/2022 17:07:13 - INFO - __main__ - Step 190 Global step 190 Train loss 3.604235 on epoch=94
03/08/2022 17:07:18 - INFO - __main__ - Step 200 Global step 200 Train loss 3.052634 on epoch=99
03/08/2022 17:07:19 - INFO - __main__ - Global step 200 Train loss 3.954143 EM 0.0 on epoch=99
03/08/2022 17:07:24 - INFO - __main__ - Step 210 Global step 210 Train loss 3.054484 on epoch=104
03/08/2022 17:07:28 - INFO - __main__ - Step 220 Global step 220 Train loss 2.924891 on epoch=109
03/08/2022 17:07:33 - INFO - __main__ - Step 230 Global step 230 Train loss 2.460902 on epoch=114
03/08/2022 17:07:38 - INFO - __main__ - Step 240 Global step 240 Train loss 2.600865 on epoch=119
03/08/2022 17:07:43 - INFO - __main__ - Step 250 Global step 250 Train loss 2.601402 on epoch=124
03/08/2022 17:07:43 - INFO - __main__ - Global step 250 Train loss 2.728509 EM 0.0 on epoch=124
03/08/2022 17:07:48 - INFO - __main__ - Step 260 Global step 260 Train loss 2.389353 on epoch=129
03/08/2022 17:07:53 - INFO - __main__ - Step 270 Global step 270 Train loss 2.154083 on epoch=134
03/08/2022 17:07:58 - INFO - __main__ - Step 280 Global step 280 Train loss 2.019657 on epoch=139
03/08/2022 17:08:03 - INFO - __main__ - Step 290 Global step 290 Train loss 2.340535 on epoch=144
03/08/2022 17:08:07 - INFO - __main__ - Step 300 Global step 300 Train loss 2.067037 on epoch=149
03/08/2022 17:08:08 - INFO - __main__ - Global step 300 Train loss 2.194133 EM 0.0 on epoch=149
03/08/2022 17:08:13 - INFO - __main__ - Step 310 Global step 310 Train loss 2.083169 on epoch=154
03/08/2022 17:08:18 - INFO - __main__ - Step 320 Global step 320 Train loss 2.170076 on epoch=159
03/08/2022 17:08:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.887007 on epoch=164
03/08/2022 17:08:27 - INFO - __main__ - Step 340 Global step 340 Train loss 1.918579 on epoch=169
03/08/2022 17:08:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.520724 on epoch=174
03/08/2022 17:08:33 - INFO - __main__ - Global step 350 Train loss 1.915911 EM 0.0 on epoch=174
03/08/2022 17:08:38 - INFO - __main__ - Step 360 Global step 360 Train loss 2.038957 on epoch=179
03/08/2022 17:08:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.722996 on epoch=184
03/08/2022 17:08:47 - INFO - __main__ - Step 380 Global step 380 Train loss 1.502611 on epoch=189
03/08/2022 17:08:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.663976 on epoch=194
03/08/2022 17:08:57 - INFO - __main__ - Step 400 Global step 400 Train loss 1.565831 on epoch=199
03/08/2022 17:08:58 - INFO - __main__ - Global step 400 Train loss 1.698874 EM 0.0 on epoch=199
03/08/2022 17:09:03 - INFO - __main__ - Step 410 Global step 410 Train loss 1.662265 on epoch=204
03/08/2022 17:09:07 - INFO - __main__ - Step 420 Global step 420 Train loss 1.475981 on epoch=209
03/08/2022 17:09:12 - INFO - __main__ - Step 430 Global step 430 Train loss 1.637490 on epoch=214
03/08/2022 17:09:17 - INFO - __main__ - Step 440 Global step 440 Train loss 1.417440 on epoch=219
03/08/2022 17:09:22 - INFO - __main__ - Step 450 Global step 450 Train loss 1.281596 on epoch=224
03/08/2022 17:09:23 - INFO - __main__ - Global step 450 Train loss 1.494954 EM 0.0 on epoch=224
03/08/2022 17:09:28 - INFO - __main__ - Step 460 Global step 460 Train loss 1.391114 on epoch=229
03/08/2022 17:09:32 - INFO - __main__ - Step 470 Global step 470 Train loss 1.449098 on epoch=234
03/08/2022 17:09:37 - INFO - __main__ - Step 480 Global step 480 Train loss 1.482256 on epoch=239
03/08/2022 17:09:42 - INFO - __main__ - Step 490 Global step 490 Train loss 1.401990 on epoch=244
03/08/2022 17:09:47 - INFO - __main__ - Step 500 Global step 500 Train loss 1.421142 on epoch=249
03/08/2022 17:09:48 - INFO - __main__ - Global step 500 Train loss 1.429120 EM 0.0 on epoch=249
03/08/2022 17:09:53 - INFO - __main__ - Step 510 Global step 510 Train loss 1.308004 on epoch=254
03/08/2022 17:09:57 - INFO - __main__ - Step 520 Global step 520 Train loss 1.384291 on epoch=259
03/08/2022 17:10:02 - INFO - __main__ - Step 530 Global step 530 Train loss 1.217751 on epoch=264
03/08/2022 17:10:07 - INFO - __main__ - Step 540 Global step 540 Train loss 1.228646 on epoch=269
03/08/2022 17:10:12 - INFO - __main__ - Step 550 Global step 550 Train loss 1.264971 on epoch=274
03/08/2022 17:10:13 - INFO - __main__ - Global step 550 Train loss 1.280732 EM 0.0 on epoch=274
03/08/2022 17:10:18 - INFO - __main__ - Step 560 Global step 560 Train loss 1.168584 on epoch=279
03/08/2022 17:10:22 - INFO - __main__ - Step 570 Global step 570 Train loss 1.098431 on epoch=284
03/08/2022 17:10:27 - INFO - __main__ - Step 580 Global step 580 Train loss 1.058475 on epoch=289
03/08/2022 17:10:32 - INFO - __main__ - Step 590 Global step 590 Train loss 1.049651 on epoch=294
03/08/2022 17:10:37 - INFO - __main__ - Step 600 Global step 600 Train loss 1.122995 on epoch=299
03/08/2022 17:10:38 - INFO - __main__ - Global step 600 Train loss 1.099627 EM 0.0 on epoch=299
03/08/2022 17:10:38 - INFO - __main__ - save last model!
03/08/2022 17:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:10:38 - INFO - __main__ - Printing 3 examples
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:10:38 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:10:38 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:10:38 - INFO - __main__ - ['Olio']
03/08/2022 17:10:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:10:38 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:10:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:10:38 - INFO - __main__ - Printing 3 examples
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:10:38 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:10:38 - INFO - __main__ - ['Acuren']
03/08/2022 17:10:38 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:10:38 - INFO - __main__ - ['J for Justice']
03/08/2022 17:10:38 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:10:38 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:10:38 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:10:49 - INFO - __main__ - Starting training!
03/08/2022 17:11:18 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:11:18 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:11:18 - INFO - __main__ - Printing 3 examples
03/08/2022 17:11:18 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:11:18 - INFO - __main__ - ['Whole S M']
03/08/2022 17:11:18 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:11:18 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:11:18 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:11:18 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:11:18 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:11:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:11:21 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:12:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0003_8_predictions.txt
03/08/2022 17:12:10 - INFO - __main__ - EM on test data: 0.0205
03/08/2022 17:12:10 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.02048131080389145
03/08/2022 17:12:10 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0002, bsz=8 ...
03/08/2022 17:12:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:12:11 - INFO - __main__ - Printing 3 examples
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:12:11 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:12:11 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:12:11 - INFO - __main__ - ['Olio']
03/08/2022 17:12:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:12:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:12:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:12:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:12:11 - INFO - __main__ - Printing 3 examples
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:12:11 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:12:11 - INFO - __main__ - ['Acuren']
03/08/2022 17:12:11 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:12:11 - INFO - __main__ - ['J for Justice']
03/08/2022 17:12:11 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:12:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:12:11 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:12:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:12:21 - INFO - __main__ - Starting training!
03/08/2022 17:12:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.339066 on epoch=4
03/08/2022 17:12:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.970472 on epoch=9
03/08/2022 17:12:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.356356 on epoch=14
03/08/2022 17:12:39 - INFO - __main__ - Step 40 Global step 40 Train loss 14.057981 on epoch=19
03/08/2022 17:12:44 - INFO - __main__ - Step 50 Global step 50 Train loss 12.080463 on epoch=24
03/08/2022 17:12:46 - INFO - __main__ - Global step 50 Train loss 16.760868 EM 0.0 on epoch=24
03/08/2022 17:13:18 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 17:13:23 - INFO - __main__ - Step 60 Global step 60 Train loss 11.749080 on epoch=29
03/08/2022 17:13:27 - INFO - __main__ - Step 70 Global step 70 Train loss 10.995880 on epoch=34
03/08/2022 17:13:32 - INFO - __main__ - Step 80 Global step 80 Train loss 9.478316 on epoch=39
03/08/2022 17:13:37 - INFO - __main__ - Step 90 Global step 90 Train loss 8.558725 on epoch=44
03/08/2022 17:13:41 - INFO - __main__ - Step 100 Global step 100 Train loss 8.480043 on epoch=49
03/08/2022 17:13:42 - INFO - __main__ - Global step 100 Train loss 9.852409 EM 0.03125 on epoch=49
03/08/2022 17:14:15 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 17:14:20 - INFO - __main__ - Step 110 Global step 110 Train loss 7.974475 on epoch=54
03/08/2022 17:14:24 - INFO - __main__ - Step 120 Global step 120 Train loss 7.924251 on epoch=59
03/08/2022 17:14:29 - INFO - __main__ - Step 130 Global step 130 Train loss 7.617412 on epoch=64
03/08/2022 17:14:34 - INFO - __main__ - Step 140 Global step 140 Train loss 7.459931 on epoch=69
03/08/2022 17:14:39 - INFO - __main__ - Step 150 Global step 150 Train loss 6.741387 on epoch=74
03/08/2022 17:14:39 - INFO - __main__ - Global step 150 Train loss 7.543491 EM 0.03125 on epoch=74
03/08/2022 17:14:44 - INFO - __main__ - Step 160 Global step 160 Train loss 6.587377 on epoch=79
03/08/2022 17:14:49 - INFO - __main__ - Step 170 Global step 170 Train loss 6.303933 on epoch=84
03/08/2022 17:14:53 - INFO - __main__ - Step 180 Global step 180 Train loss 5.998719 on epoch=89
03/08/2022 17:14:58 - INFO - __main__ - Step 190 Global step 190 Train loss 5.927079 on epoch=94
03/08/2022 17:15:03 - INFO - __main__ - Step 200 Global step 200 Train loss 6.034231 on epoch=99
03/08/2022 17:15:04 - INFO - __main__ - Global step 200 Train loss 6.170267 EM 0.03125 on epoch=99
03/08/2022 17:15:08 - INFO - __main__ - Step 210 Global step 210 Train loss 5.526314 on epoch=104
03/08/2022 17:15:13 - INFO - __main__ - Step 220 Global step 220 Train loss 4.861691 on epoch=109
03/08/2022 17:15:18 - INFO - __main__ - Step 230 Global step 230 Train loss 4.819733 on epoch=114
03/08/2022 17:15:23 - INFO - __main__ - Step 240 Global step 240 Train loss 4.682811 on epoch=119
03/08/2022 17:15:27 - INFO - __main__ - Step 250 Global step 250 Train loss 3.706438 on epoch=124
03/08/2022 17:15:28 - INFO - __main__ - Global step 250 Train loss 4.719398 EM 0.0 on epoch=124
03/08/2022 17:15:33 - INFO - __main__ - Step 260 Global step 260 Train loss 3.796453 on epoch=129
03/08/2022 17:15:38 - INFO - __main__ - Step 270 Global step 270 Train loss 3.707843 on epoch=134
03/08/2022 17:15:42 - INFO - __main__ - Step 280 Global step 280 Train loss 3.334915 on epoch=139
03/08/2022 17:15:47 - INFO - __main__ - Step 290 Global step 290 Train loss 3.414559 on epoch=144
03/08/2022 17:15:52 - INFO - __main__ - Step 300 Global step 300 Train loss 2.785232 on epoch=149
03/08/2022 17:15:52 - INFO - __main__ - Global step 300 Train loss 3.407801 EM 0.0 on epoch=149
03/08/2022 17:15:57 - INFO - __main__ - Step 310 Global step 310 Train loss 2.717520 on epoch=154
03/08/2022 17:16:02 - INFO - __main__ - Step 320 Global step 320 Train loss 3.053342 on epoch=159
03/08/2022 17:16:07 - INFO - __main__ - Step 330 Global step 330 Train loss 2.681825 on epoch=164
03/08/2022 17:16:11 - INFO - __main__ - Step 340 Global step 340 Train loss 2.378758 on epoch=169
03/08/2022 17:16:16 - INFO - __main__ - Step 350 Global step 350 Train loss 2.522587 on epoch=174
03/08/2022 17:16:17 - INFO - __main__ - Global step 350 Train loss 2.670806 EM 0.0 on epoch=174
03/08/2022 17:16:22 - INFO - __main__ - Step 360 Global step 360 Train loss 2.444372 on epoch=179
03/08/2022 17:16:26 - INFO - __main__ - Step 370 Global step 370 Train loss 2.261226 on epoch=184
03/08/2022 17:16:31 - INFO - __main__ - Step 380 Global step 380 Train loss 2.207701 on epoch=189
03/08/2022 17:16:36 - INFO - __main__ - Step 390 Global step 390 Train loss 2.361298 on epoch=194
03/08/2022 17:16:41 - INFO - __main__ - Step 400 Global step 400 Train loss 2.405033 on epoch=199
03/08/2022 17:16:41 - INFO - __main__ - Global step 400 Train loss 2.335926 EM 0.0 on epoch=199
03/08/2022 17:16:46 - INFO - __main__ - Step 410 Global step 410 Train loss 2.371053 on epoch=204
03/08/2022 17:16:51 - INFO - __main__ - Step 420 Global step 420 Train loss 2.193228 on epoch=209
03/08/2022 17:16:55 - INFO - __main__ - Step 430 Global step 430 Train loss 2.345191 on epoch=214
03/08/2022 17:17:00 - INFO - __main__ - Step 440 Global step 440 Train loss 2.121703 on epoch=219
03/08/2022 17:17:05 - INFO - __main__ - Step 450 Global step 450 Train loss 2.218090 on epoch=224
03/08/2022 17:17:05 - INFO - __main__ - Global step 450 Train loss 2.249853 EM 0.0 on epoch=224
03/08/2022 17:17:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.968992 on epoch=229
03/08/2022 17:17:15 - INFO - __main__ - Step 470 Global step 470 Train loss 2.098025 on epoch=234
03/08/2022 17:17:19 - INFO - __main__ - Step 480 Global step 480 Train loss 1.971522 on epoch=239
03/08/2022 17:17:24 - INFO - __main__ - Step 490 Global step 490 Train loss 1.806972 on epoch=244
03/08/2022 17:17:29 - INFO - __main__ - Step 500 Global step 500 Train loss 1.725082 on epoch=249
03/08/2022 17:17:29 - INFO - __main__ - Global step 500 Train loss 1.914119 EM 0.0 on epoch=249
03/08/2022 17:17:34 - INFO - __main__ - Step 510 Global step 510 Train loss 2.061547 on epoch=254
03/08/2022 17:17:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.983790 on epoch=259
03/08/2022 17:17:43 - INFO - __main__ - Step 530 Global step 530 Train loss 1.715874 on epoch=264
03/08/2022 17:17:48 - INFO - __main__ - Step 540 Global step 540 Train loss 1.672368 on epoch=269
03/08/2022 17:17:53 - INFO - __main__ - Step 550 Global step 550 Train loss 1.842315 on epoch=274
03/08/2022 17:17:54 - INFO - __main__ - Global step 550 Train loss 1.855179 EM 0.0 on epoch=274
03/08/2022 17:17:58 - INFO - __main__ - Step 560 Global step 560 Train loss 1.933135 on epoch=279
03/08/2022 17:18:03 - INFO - __main__ - Step 570 Global step 570 Train loss 1.663825 on epoch=284
03/08/2022 17:18:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.718590 on epoch=289
03/08/2022 17:18:12 - INFO - __main__ - Step 590 Global step 590 Train loss 1.486512 on epoch=294
03/08/2022 17:18:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.716027 on epoch=299
03/08/2022 17:18:18 - INFO - __main__ - Global step 600 Train loss 1.703618 EM 0.0 on epoch=299
03/08/2022 17:18:18 - INFO - __main__ - save last model!
03/08/2022 17:18:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:18:18 - INFO - __main__ - Printing 3 examples
03/08/2022 17:18:18 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:18:18 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:18:18 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:18:18 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:18:18 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:18:18 - INFO - __main__ - ['Olio']
03/08/2022 17:18:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:18:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:18:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:18:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:18:19 - INFO - __main__ - Printing 3 examples
03/08/2022 17:18:19 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:18:19 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:18:19 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:18:19 - INFO - __main__ - ['Acuren']
03/08/2022 17:18:19 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:18:19 - INFO - __main__ - ['J for Justice']
03/08/2022 17:18:19 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:18:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:18:19 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:18:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:18:29 - INFO - __main__ - Starting training!
03/08/2022 17:18:56 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:18:57 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:18:57 - INFO - __main__ - Printing 3 examples
03/08/2022 17:18:57 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:18:57 - INFO - __main__ - ['Whole S M']
03/08/2022 17:18:57 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:18:57 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:18:57 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:18:57 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:18:57 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:18:58 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:19:00 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:19:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0002_8_predictions.txt
03/08/2022 17:19:47 - INFO - __main__ - EM on test data: 0.0261
03/08/2022 17:19:47 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.026113671274961597
03/08/2022 17:19:47 - INFO - __main__ - Running ... prefix=crawl_domain_32_13, lr=0.0001, bsz=8 ...
03/08/2022 17:19:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:19:48 - INFO - __main__ - Printing 3 examples
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] grallrealtygroup
03/08/2022 17:19:48 - INFO - __main__ - ['Grall Realty Group']
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] elocalplumbers
03/08/2022 17:19:48 - INFO - __main__ - ['e Local Plumbers']
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] olio
03/08/2022 17:19:48 - INFO - __main__ - ['Olio']
03/08/2022 17:19:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:19:48 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:19:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:19:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:19:48 - INFO - __main__ - Printing 3 examples
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] muscatinearc
03/08/2022 17:19:48 - INFO - __main__ - ['Muscatine A R C']
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] acuren
03/08/2022 17:19:48 - INFO - __main__ - ['Acuren']
03/08/2022 17:19:48 - INFO - __main__ -  [crawl_domain] jforjustice
03/08/2022 17:19:48 - INFO - __main__ - ['J for Justice']
03/08/2022 17:19:48 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:19:48 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:19:49 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:19:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:19:59 - INFO - __main__ - Starting training!
03/08/2022 17:20:02 - INFO - __main__ - Step 10 Global step 10 Train loss 22.333233 on epoch=4
03/08/2022 17:20:07 - INFO - __main__ - Step 20 Global step 20 Train loss 20.904930 on epoch=9
03/08/2022 17:20:12 - INFO - __main__ - Step 30 Global step 30 Train loss 18.848286 on epoch=14
03/08/2022 17:20:16 - INFO - __main__ - Step 40 Global step 40 Train loss 16.661087 on epoch=19
03/08/2022 17:20:21 - INFO - __main__ - Step 50 Global step 50 Train loss 17.004063 on epoch=24
03/08/2022 17:20:30 - INFO - __main__ - Global step 50 Train loss 19.150322 EM 0.0 on epoch=24
03/08/2022 17:21:02 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 17:21:06 - INFO - __main__ - Step 60 Global step 60 Train loss 15.261194 on epoch=29
03/08/2022 17:21:11 - INFO - __main__ - Step 70 Global step 70 Train loss 13.981600 on epoch=34
03/08/2022 17:21:16 - INFO - __main__ - Step 80 Global step 80 Train loss 13.090628 on epoch=39
03/08/2022 17:21:21 - INFO - __main__ - Step 90 Global step 90 Train loss 12.458365 on epoch=44
03/08/2022 17:21:25 - INFO - __main__ - Step 100 Global step 100 Train loss 11.763903 on epoch=49
03/08/2022 17:21:27 - INFO - __main__ - Global step 100 Train loss 13.311139 EM 0.03125 on epoch=49
03/08/2022 17:22:00 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 17:22:04 - INFO - __main__ - Step 110 Global step 110 Train loss 11.525214 on epoch=54
03/08/2022 17:22:09 - INFO - __main__ - Step 120 Global step 120 Train loss 11.082205 on epoch=59
03/08/2022 17:22:14 - INFO - __main__ - Step 130 Global step 130 Train loss 10.214021 on epoch=64
03/08/2022 17:22:18 - INFO - __main__ - Step 140 Global step 140 Train loss 9.554288 on epoch=69
03/08/2022 17:22:23 - INFO - __main__ - Step 150 Global step 150 Train loss 9.426364 on epoch=74
03/08/2022 17:22:24 - INFO - __main__ - Global step 150 Train loss 10.360418 EM 0.0625 on epoch=74
03/08/2022 17:22:56 - INFO - __main__ - Saving model with best EM: 0.03125 -> 0.0625 on epoch=74, global_step=150
03/08/2022 17:23:01 - INFO - __main__ - Step 160 Global step 160 Train loss 8.962543 on epoch=79
03/08/2022 17:23:06 - INFO - __main__ - Step 170 Global step 170 Train loss 8.710011 on epoch=84
03/08/2022 17:23:10 - INFO - __main__ - Step 180 Global step 180 Train loss 8.504992 on epoch=89
03/08/2022 17:23:15 - INFO - __main__ - Step 190 Global step 190 Train loss 8.596883 on epoch=94
03/08/2022 17:23:20 - INFO - __main__ - Step 200 Global step 200 Train loss 8.488923 on epoch=99
03/08/2022 17:23:21 - INFO - __main__ - Global step 200 Train loss 8.652671 EM 0.0625 on epoch=99
03/08/2022 17:23:25 - INFO - __main__ - Step 210 Global step 210 Train loss 7.659068 on epoch=104
03/08/2022 17:23:30 - INFO - __main__ - Step 220 Global step 220 Train loss 7.805391 on epoch=109
03/08/2022 17:23:35 - INFO - __main__ - Step 230 Global step 230 Train loss 7.506803 on epoch=114
03/08/2022 17:23:40 - INFO - __main__ - Step 240 Global step 240 Train loss 7.491287 on epoch=119
03/08/2022 17:23:44 - INFO - __main__ - Step 250 Global step 250 Train loss 7.480849 on epoch=124
03/08/2022 17:23:45 - INFO - __main__ - Global step 250 Train loss 7.588680 EM 0.0 on epoch=124
03/08/2022 17:23:50 - INFO - __main__ - Step 260 Global step 260 Train loss 7.241086 on epoch=129
03/08/2022 17:23:55 - INFO - __main__ - Step 270 Global step 270 Train loss 6.874936 on epoch=134
03/08/2022 17:23:59 - INFO - __main__ - Step 280 Global step 280 Train loss 6.890382 on epoch=139
03/08/2022 17:24:04 - INFO - __main__ - Step 290 Global step 290 Train loss 6.985963 on epoch=144
03/08/2022 17:24:09 - INFO - __main__ - Step 300 Global step 300 Train loss 6.809590 on epoch=149
03/08/2022 17:24:10 - INFO - __main__ - Global step 300 Train loss 6.960391 EM 0.0 on epoch=149
03/08/2022 17:24:14 - INFO - __main__ - Step 310 Global step 310 Train loss 6.614006 on epoch=154
03/08/2022 17:24:19 - INFO - __main__ - Step 320 Global step 320 Train loss 6.581793 on epoch=159
03/08/2022 17:24:24 - INFO - __main__ - Step 330 Global step 330 Train loss 6.206095 on epoch=164
03/08/2022 17:24:29 - INFO - __main__ - Step 340 Global step 340 Train loss 6.565237 on epoch=169
03/08/2022 17:24:34 - INFO - __main__ - Step 350 Global step 350 Train loss 6.119951 on epoch=174
03/08/2022 17:24:34 - INFO - __main__ - Global step 350 Train loss 6.417417 EM 0.0 on epoch=174
03/08/2022 17:24:39 - INFO - __main__ - Step 360 Global step 360 Train loss 6.000666 on epoch=179
03/08/2022 17:24:44 - INFO - __main__ - Step 370 Global step 370 Train loss 5.894360 on epoch=184
03/08/2022 17:24:48 - INFO - __main__ - Step 380 Global step 380 Train loss 5.569457 on epoch=189
03/08/2022 17:24:53 - INFO - __main__ - Step 390 Global step 390 Train loss 5.834033 on epoch=194
03/08/2022 17:24:58 - INFO - __main__ - Step 400 Global step 400 Train loss 5.363996 on epoch=199
03/08/2022 17:24:59 - INFO - __main__ - Global step 400 Train loss 5.732503 EM 0.0 on epoch=199
03/08/2022 17:25:03 - INFO - __main__ - Step 410 Global step 410 Train loss 5.241941 on epoch=204
03/08/2022 17:25:08 - INFO - __main__ - Step 420 Global step 420 Train loss 4.984397 on epoch=209
03/08/2022 17:25:13 - INFO - __main__ - Step 430 Global step 430 Train loss 4.899800 on epoch=214
03/08/2022 17:25:18 - INFO - __main__ - Step 440 Global step 440 Train loss 5.382716 on epoch=219
03/08/2022 17:25:23 - INFO - __main__ - Step 450 Global step 450 Train loss 4.673515 on epoch=224
03/08/2022 17:25:23 - INFO - __main__ - Global step 450 Train loss 5.036474 EM 0.0 on epoch=224
03/08/2022 17:25:28 - INFO - __main__ - Step 460 Global step 460 Train loss 4.436614 on epoch=229
03/08/2022 17:25:33 - INFO - __main__ - Step 470 Global step 470 Train loss 4.438901 on epoch=234
03/08/2022 17:25:38 - INFO - __main__ - Step 480 Global step 480 Train loss 3.983407 on epoch=239
03/08/2022 17:25:42 - INFO - __main__ - Step 490 Global step 490 Train loss 4.192550 on epoch=244
03/08/2022 17:25:47 - INFO - __main__ - Step 500 Global step 500 Train loss 3.728811 on epoch=249
03/08/2022 17:25:48 - INFO - __main__ - Global step 500 Train loss 4.156057 EM 0.0 on epoch=249
03/08/2022 17:25:52 - INFO - __main__ - Step 510 Global step 510 Train loss 3.730157 on epoch=254
03/08/2022 17:25:57 - INFO - __main__ - Step 520 Global step 520 Train loss 3.460644 on epoch=259
03/08/2022 17:26:02 - INFO - __main__ - Step 530 Global step 530 Train loss 3.549564 on epoch=264
03/08/2022 17:26:07 - INFO - __main__ - Step 540 Global step 540 Train loss 3.132710 on epoch=269
03/08/2022 17:26:12 - INFO - __main__ - Step 550 Global step 550 Train loss 3.200030 on epoch=274
03/08/2022 17:26:12 - INFO - __main__ - Global step 550 Train loss 3.414621 EM 0.0 on epoch=274
03/08/2022 17:26:17 - INFO - __main__ - Step 560 Global step 560 Train loss 3.268746 on epoch=279
03/08/2022 17:26:22 - INFO - __main__ - Step 570 Global step 570 Train loss 2.940121 on epoch=284
03/08/2022 17:26:26 - INFO - __main__ - Step 580 Global step 580 Train loss 2.708812 on epoch=289
03/08/2022 17:26:31 - INFO - __main__ - Step 590 Global step 590 Train loss 2.686605 on epoch=294
03/08/2022 17:26:36 - INFO - __main__ - Step 600 Global step 600 Train loss 3.074431 on epoch=299
03/08/2022 17:26:37 - INFO - __main__ - Global step 600 Train loss 2.935743 EM 0.0 on epoch=299
03/08/2022 17:26:37 - INFO - __main__ - save last model!
03/08/2022 17:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:26:37 - INFO - __main__ - Printing 3 examples
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:26:37 - INFO - __main__ - ['Rapamune']
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:26:37 - INFO - __main__ - ['G B O G H']
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:26:37 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:26:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:26:37 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:26:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:26:37 - INFO - __main__ - Printing 3 examples
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:26:37 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:26:37 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:26:37 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:26:37 - INFO - __main__ - ['Dick For President']
03/08/2022 17:26:37 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:26:37 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:26:37 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:26:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:26:48 - INFO - __main__ - Starting training!
03/08/2022 17:27:17 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:27:18 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:27:18 - INFO - __main__ - Printing 3 examples
03/08/2022 17:27:18 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:27:18 - INFO - __main__ - ['Whole S M']
03/08/2022 17:27:18 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:27:18 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:27:18 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:27:18 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:27:18 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:27:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:27:20 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:28:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_13_0.0001_8_predictions.txt
03/08/2022 17:28:04 - INFO - __main__ - EM on test data: 0.0317
03/08/2022 17:28:04 - INFO - __main__ - prefix=crawl_domain_32_13, lr=0.0001, bsz=8, dev_performance=0.0625, test_performance=0.031746031746031744
03/08/2022 17:28:10 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0005, bsz=8 ...
03/08/2022 17:28:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:28:10 - INFO - __main__ - Printing 3 examples
03/08/2022 17:28:10 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:28:11 - INFO - __main__ - ['Rapamune']
03/08/2022 17:28:11 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:28:11 - INFO - __main__ - ['G B O G H']
03/08/2022 17:28:11 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:28:11 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:28:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:28:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:28:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:28:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:28:11 - INFO - __main__ - Printing 3 examples
03/08/2022 17:28:11 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:28:11 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:28:11 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:28:11 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:28:11 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:28:11 - INFO - __main__ - ['Dick For President']
03/08/2022 17:28:11 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:28:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:28:11 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:28:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:28:20 - INFO - __main__ - Starting training!
03/08/2022 17:28:24 - INFO - __main__ - Step 10 Global step 10 Train loss 20.473562 on epoch=4
03/08/2022 17:28:29 - INFO - __main__ - Step 20 Global step 20 Train loss 17.102077 on epoch=9
03/08/2022 17:28:34 - INFO - __main__ - Step 30 Global step 30 Train loss 12.600191 on epoch=14
03/08/2022 17:28:38 - INFO - __main__ - Step 40 Global step 40 Train loss 9.944158 on epoch=19
03/08/2022 17:28:43 - INFO - __main__ - Step 50 Global step 50 Train loss 8.714159 on epoch=24
03/08/2022 17:28:44 - INFO - __main__ - Global step 50 Train loss 13.766831 EM 0.03125 on epoch=24
03/08/2022 17:29:14 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/08/2022 17:29:19 - INFO - __main__ - Step 60 Global step 60 Train loss 7.580853 on epoch=29
03/08/2022 17:29:23 - INFO - __main__ - Step 70 Global step 70 Train loss 6.704120 on epoch=34
03/08/2022 17:29:28 - INFO - __main__ - Step 80 Global step 80 Train loss 5.839250 on epoch=39
03/08/2022 17:29:33 - INFO - __main__ - Step 90 Global step 90 Train loss 4.572320 on epoch=44
03/08/2022 17:29:37 - INFO - __main__ - Step 100 Global step 100 Train loss 3.786701 on epoch=49
03/08/2022 17:29:38 - INFO - __main__ - Global step 100 Train loss 5.696649 EM 0.0 on epoch=49
03/08/2022 17:29:43 - INFO - __main__ - Step 110 Global step 110 Train loss 2.921597 on epoch=54
03/08/2022 17:29:47 - INFO - __main__ - Step 120 Global step 120 Train loss 2.581199 on epoch=59
03/08/2022 17:29:52 - INFO - __main__ - Step 130 Global step 130 Train loss 2.095411 on epoch=64
03/08/2022 17:29:57 - INFO - __main__ - Step 140 Global step 140 Train loss 2.078347 on epoch=69
03/08/2022 17:30:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.930353 on epoch=74
03/08/2022 17:30:02 - INFO - __main__ - Global step 150 Train loss 2.321381 EM 0.0 on epoch=74
03/08/2022 17:30:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.836854 on epoch=79
03/08/2022 17:30:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.808818 on epoch=84
03/08/2022 17:30:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.559372 on epoch=89
03/08/2022 17:30:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.534971 on epoch=94
03/08/2022 17:30:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.379954 on epoch=99
03/08/2022 17:30:26 - INFO - __main__ - Global step 200 Train loss 1.623994 EM 0.0 on epoch=99
03/08/2022 17:30:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.654553 on epoch=104
03/08/2022 17:30:35 - INFO - __main__ - Step 220 Global step 220 Train loss 1.416261 on epoch=109
03/08/2022 17:30:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.224980 on epoch=114
03/08/2022 17:30:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.332610 on epoch=119
03/08/2022 17:30:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.295149 on epoch=124
03/08/2022 17:30:50 - INFO - __main__ - Global step 250 Train loss 1.384711 EM 0.0 on epoch=124
03/08/2022 17:30:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.090281 on epoch=129
03/08/2022 17:30:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.076682 on epoch=134
03/08/2022 17:31:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.070308 on epoch=139
03/08/2022 17:31:08 - INFO - __main__ - Step 290 Global step 290 Train loss 1.058923 on epoch=144
03/08/2022 17:31:13 - INFO - __main__ - Step 300 Global step 300 Train loss 1.167037 on epoch=149
03/08/2022 17:31:14 - INFO - __main__ - Global step 300 Train loss 1.092646 EM 0.0 on epoch=149
03/08/2022 17:31:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.950481 on epoch=154
03/08/2022 17:31:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.949804 on epoch=159
03/08/2022 17:31:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.858291 on epoch=164
03/08/2022 17:31:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.863899 on epoch=169
03/08/2022 17:31:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.974849 on epoch=174
03/08/2022 17:31:38 - INFO - __main__ - Global step 350 Train loss 0.919465 EM 0.0 on epoch=174
03/08/2022 17:31:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.848649 on epoch=179
03/08/2022 17:31:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.876758 on epoch=184
03/08/2022 17:31:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.913612 on epoch=189
03/08/2022 17:31:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.973451 on epoch=194
03/08/2022 17:32:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.776810 on epoch=199
03/08/2022 17:32:01 - INFO - __main__ - Global step 400 Train loss 0.877856 EM 0.0 on epoch=199
03/08/2022 17:32:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.011209 on epoch=204
03/08/2022 17:32:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.171334 on epoch=209
03/08/2022 17:32:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.889129 on epoch=214
03/08/2022 17:32:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.752456 on epoch=219
03/08/2022 17:32:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.789873 on epoch=224
03/08/2022 17:32:25 - INFO - __main__ - Global step 450 Train loss 0.922800 EM 0.0 on epoch=224
03/08/2022 17:32:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.763665 on epoch=229
03/08/2022 17:32:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.837603 on epoch=234
03/08/2022 17:32:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.804668 on epoch=239
03/08/2022 17:32:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.734627 on epoch=244
03/08/2022 17:32:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.776882 on epoch=249
03/08/2022 17:32:49 - INFO - __main__ - Global step 500 Train loss 0.783489 EM 0.0 on epoch=249
03/08/2022 17:32:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.754056 on epoch=254
03/08/2022 17:32:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.727317 on epoch=259
03/08/2022 17:33:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.677725 on epoch=264
03/08/2022 17:33:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.720338 on epoch=269
03/08/2022 17:33:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.689257 on epoch=274
03/08/2022 17:33:13 - INFO - __main__ - Global step 550 Train loss 0.713739 EM 0.0 on epoch=274
03/08/2022 17:33:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.682198 on epoch=279
03/08/2022 17:33:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.702137 on epoch=284
03/08/2022 17:33:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.698568 on epoch=289
03/08/2022 17:33:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.690437 on epoch=294
03/08/2022 17:33:37 - INFO - __main__ - Step 600 Global step 600 Train loss 1.317943 on epoch=299
03/08/2022 17:33:37 - INFO - __main__ - Global step 600 Train loss 0.818256 EM 0.0 on epoch=299
03/08/2022 17:33:37 - INFO - __main__ - save last model!
03/08/2022 17:33:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:33:38 - INFO - __main__ - Printing 3 examples
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:33:38 - INFO - __main__ - ['Rapamune']
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:33:38 - INFO - __main__ - ['G B O G H']
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:33:38 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:33:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:33:38 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:33:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:33:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:33:38 - INFO - __main__ - Printing 3 examples
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:33:38 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:33:38 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:33:38 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:33:38 - INFO - __main__ - ['Dick For President']
03/08/2022 17:33:38 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:33:38 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:33:38 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:33:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:33:48 - INFO - __main__ - Starting training!
03/08/2022 17:34:18 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:34:19 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:34:19 - INFO - __main__ - Printing 3 examples
03/08/2022 17:34:19 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:34:19 - INFO - __main__ - ['Whole S M']
03/08/2022 17:34:19 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:34:19 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:34:19 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:34:19 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:34:19 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:34:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:34:21 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:34:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0005_8_predictions.txt
03/08/2022 17:34:55 - INFO - __main__ - EM on test data: 0.0108
03/08/2022 17:34:55 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.010752688172043012
03/08/2022 17:34:55 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0003, bsz=8 ...
03/08/2022 17:34:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:34:56 - INFO - __main__ - Printing 3 examples
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:34:56 - INFO - __main__ - ['Rapamune']
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:34:56 - INFO - __main__ - ['G B O G H']
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:34:56 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:34:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:34:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:34:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:34:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:34:56 - INFO - __main__ - Printing 3 examples
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:34:56 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:34:56 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:34:56 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:34:56 - INFO - __main__ - ['Dick For President']
03/08/2022 17:34:56 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:34:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:34:56 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:35:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:35:05 - INFO - __main__ - Starting training!
03/08/2022 17:35:09 - INFO - __main__ - Step 10 Global step 10 Train loss 20.998993 on epoch=4
03/08/2022 17:35:13 - INFO - __main__ - Step 20 Global step 20 Train loss 19.478764 on epoch=9
03/08/2022 17:35:18 - INFO - __main__ - Step 30 Global step 30 Train loss 14.617435 on epoch=14
03/08/2022 17:35:23 - INFO - __main__ - Step 40 Global step 40 Train loss 12.790887 on epoch=19
03/08/2022 17:35:28 - INFO - __main__ - Step 50 Global step 50 Train loss 11.374367 on epoch=24
03/08/2022 17:35:34 - INFO - __main__ - Global step 50 Train loss 15.852090 EM 0.0 on epoch=24
03/08/2022 17:36:05 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 17:36:09 - INFO - __main__ - Step 60 Global step 60 Train loss 10.205836 on epoch=29
03/08/2022 17:36:14 - INFO - __main__ - Step 70 Global step 70 Train loss 8.568349 on epoch=34
03/08/2022 17:36:19 - INFO - __main__ - Step 80 Global step 80 Train loss 7.681559 on epoch=39
03/08/2022 17:36:24 - INFO - __main__ - Step 90 Global step 90 Train loss 6.640673 on epoch=44
03/08/2022 17:36:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.040024 on epoch=49
03/08/2022 17:36:29 - INFO - __main__ - Global step 100 Train loss 7.827288 EM 0.0 on epoch=49
03/08/2022 17:36:34 - INFO - __main__ - Step 110 Global step 110 Train loss 5.362897 on epoch=54
03/08/2022 17:36:38 - INFO - __main__ - Step 120 Global step 120 Train loss 5.205953 on epoch=59
03/08/2022 17:36:43 - INFO - __main__ - Step 130 Global step 130 Train loss 4.447089 on epoch=64
03/08/2022 17:36:48 - INFO - __main__ - Step 140 Global step 140 Train loss 4.109331 on epoch=69
03/08/2022 17:36:53 - INFO - __main__ - Step 150 Global step 150 Train loss 3.556246 on epoch=74
03/08/2022 17:36:53 - INFO - __main__ - Global step 150 Train loss 4.536303 EM 0.0 on epoch=74
03/08/2022 17:36:58 - INFO - __main__ - Step 160 Global step 160 Train loss 3.427744 on epoch=79
03/08/2022 17:37:02 - INFO - __main__ - Step 170 Global step 170 Train loss 3.107492 on epoch=84
03/08/2022 17:37:07 - INFO - __main__ - Step 180 Global step 180 Train loss 2.775034 on epoch=89
03/08/2022 17:37:12 - INFO - __main__ - Step 190 Global step 190 Train loss 2.785329 on epoch=94
03/08/2022 17:37:17 - INFO - __main__ - Step 200 Global step 200 Train loss 2.367526 on epoch=99
03/08/2022 17:37:17 - INFO - __main__ - Global step 200 Train loss 2.892625 EM 0.0 on epoch=99
03/08/2022 17:37:22 - INFO - __main__ - Step 210 Global step 210 Train loss 2.068147 on epoch=104
03/08/2022 17:37:26 - INFO - __main__ - Step 220 Global step 220 Train loss 2.003557 on epoch=109
03/08/2022 17:37:31 - INFO - __main__ - Step 230 Global step 230 Train loss 2.001660 on epoch=114
03/08/2022 17:37:36 - INFO - __main__ - Step 240 Global step 240 Train loss 2.180093 on epoch=119
03/08/2022 17:37:41 - INFO - __main__ - Step 250 Global step 250 Train loss 2.078785 on epoch=124
03/08/2022 17:37:41 - INFO - __main__ - Global step 250 Train loss 2.066448 EM 0.0 on epoch=124
03/08/2022 17:37:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.671244 on epoch=129
03/08/2022 17:37:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.738765 on epoch=134
03/08/2022 17:37:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.882882 on epoch=139
03/08/2022 17:38:00 - INFO - __main__ - Step 290 Global step 290 Train loss 1.781144 on epoch=144
03/08/2022 17:38:05 - INFO - __main__ - Step 300 Global step 300 Train loss 1.684622 on epoch=149
03/08/2022 17:38:05 - INFO - __main__ - Global step 300 Train loss 1.751732 EM 0.0 on epoch=149
03/08/2022 17:38:10 - INFO - __main__ - Step 310 Global step 310 Train loss 1.735126 on epoch=154
03/08/2022 17:38:15 - INFO - __main__ - Step 320 Global step 320 Train loss 1.758482 on epoch=159
03/08/2022 17:38:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.683275 on epoch=164
03/08/2022 17:38:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.540267 on epoch=169
03/08/2022 17:38:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.080065 on epoch=174
03/08/2022 17:38:30 - INFO - __main__ - Global step 350 Train loss 1.559443 EM 0.0 on epoch=174
03/08/2022 17:38:34 - INFO - __main__ - Step 360 Global step 360 Train loss 1.457415 on epoch=179
03/08/2022 17:38:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.378126 on epoch=184
03/08/2022 17:38:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.369893 on epoch=189
03/08/2022 17:38:49 - INFO - __main__ - Step 390 Global step 390 Train loss 1.314355 on epoch=194
03/08/2022 17:38:53 - INFO - __main__ - Step 400 Global step 400 Train loss 1.206287 on epoch=199
03/08/2022 17:38:54 - INFO - __main__ - Global step 400 Train loss 1.345215 EM 0.0 on epoch=199
03/08/2022 17:38:59 - INFO - __main__ - Step 410 Global step 410 Train loss 1.222011 on epoch=204
03/08/2022 17:39:03 - INFO - __main__ - Step 420 Global step 420 Train loss 1.213051 on epoch=209
03/08/2022 17:39:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.282299 on epoch=214
03/08/2022 17:39:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.206775 on epoch=219
03/08/2022 17:39:18 - INFO - __main__ - Step 450 Global step 450 Train loss 1.175566 on epoch=224
03/08/2022 17:39:18 - INFO - __main__ - Global step 450 Train loss 1.219941 EM 0.0 on epoch=224
03/08/2022 17:39:23 - INFO - __main__ - Step 460 Global step 460 Train loss 1.243715 on epoch=229
03/08/2022 17:39:28 - INFO - __main__ - Step 470 Global step 470 Train loss 1.174916 on epoch=234
03/08/2022 17:39:33 - INFO - __main__ - Step 480 Global step 480 Train loss 1.166954 on epoch=239
03/08/2022 17:39:37 - INFO - __main__ - Step 490 Global step 490 Train loss 1.096593 on epoch=244
03/08/2022 17:39:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.956705 on epoch=249
03/08/2022 17:39:43 - INFO - __main__ - Global step 500 Train loss 1.127777 EM 0.0 on epoch=249
03/08/2022 17:39:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.140281 on epoch=254
03/08/2022 17:39:52 - INFO - __main__ - Step 520 Global step 520 Train loss 1.043778 on epoch=259
03/08/2022 17:39:57 - INFO - __main__ - Step 530 Global step 530 Train loss 1.038207 on epoch=264
03/08/2022 17:40:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.954810 on epoch=269
03/08/2022 17:40:06 - INFO - __main__ - Step 550 Global step 550 Train loss 1.002023 on epoch=274
03/08/2022 17:40:07 - INFO - __main__ - Global step 550 Train loss 1.035820 EM 0.0 on epoch=274
03/08/2022 17:40:12 - INFO - __main__ - Step 560 Global step 560 Train loss 1.007635 on epoch=279
03/08/2022 17:40:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.916458 on epoch=284
03/08/2022 17:40:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.883709 on epoch=289
03/08/2022 17:40:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.929211 on epoch=294
03/08/2022 17:40:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.840987 on epoch=299
03/08/2022 17:40:31 - INFO - __main__ - Global step 600 Train loss 0.915600 EM 0.0 on epoch=299
03/08/2022 17:40:31 - INFO - __main__ - save last model!
03/08/2022 17:40:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:40:32 - INFO - __main__ - Printing 3 examples
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:40:32 - INFO - __main__ - ['Rapamune']
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:40:32 - INFO - __main__ - ['G B O G H']
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:40:32 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:40:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:40:32 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:40:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:40:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:40:32 - INFO - __main__ - Printing 3 examples
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:40:32 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:40:32 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:40:32 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:40:32 - INFO - __main__ - ['Dick For President']
03/08/2022 17:40:32 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:40:32 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:40:32 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:40:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:40:42 - INFO - __main__ - Starting training!
03/08/2022 17:41:12 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:41:12 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:41:12 - INFO - __main__ - Printing 3 examples
03/08/2022 17:41:12 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:41:12 - INFO - __main__ - ['Whole S M']
03/08/2022 17:41:12 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:41:12 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:41:12 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:41:12 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:41:12 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:41:13 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:41:15 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 17:47:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0003_8_predictions.txt
03/08/2022 17:47:13 - INFO - __main__ - EM on test data: 0.0061
03/08/2022 17:47:14 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.006144393241167435
03/08/2022 17:47:14 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0002, bsz=8 ...
03/08/2022 17:47:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:47:15 - INFO - __main__ - Printing 3 examples
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:47:15 - INFO - __main__ - ['Rapamune']
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:47:15 - INFO - __main__ - ['G B O G H']
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:47:15 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:47:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 17:47:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:47:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:47:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:47:15 - INFO - __main__ - Printing 3 examples
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:47:15 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:47:15 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:47:15 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:47:15 - INFO - __main__ - ['Dick For President']
03/08/2022 17:47:15 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:47:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:47:15 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:47:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:47:25 - INFO - __main__ - Starting training!
03/08/2022 17:47:28 - INFO - __main__ - Step 10 Global step 10 Train loss 21.057262 on epoch=4
03/08/2022 17:47:33 - INFO - __main__ - Step 20 Global step 20 Train loss 18.237810 on epoch=9
03/08/2022 17:47:38 - INFO - __main__ - Step 30 Global step 30 Train loss 15.540550 on epoch=14
03/08/2022 17:47:42 - INFO - __main__ - Step 40 Global step 40 Train loss 13.283406 on epoch=19
03/08/2022 17:47:47 - INFO - __main__ - Step 50 Global step 50 Train loss 12.134954 on epoch=24
03/08/2022 17:47:55 - INFO - __main__ - Global step 50 Train loss 16.050797 EM 0.0 on epoch=24
03/08/2022 17:48:30 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 17:48:35 - INFO - __main__ - Step 60 Global step 60 Train loss 10.863177 on epoch=29
03/08/2022 17:48:39 - INFO - __main__ - Step 70 Global step 70 Train loss 9.969539 on epoch=34
03/08/2022 17:48:44 - INFO - __main__ - Step 80 Global step 80 Train loss 9.352541 on epoch=39
03/08/2022 17:48:49 - INFO - __main__ - Step 90 Global step 90 Train loss 8.724901 on epoch=44
03/08/2022 17:48:54 - INFO - __main__ - Step 100 Global step 100 Train loss 7.826723 on epoch=49
03/08/2022 17:48:54 - INFO - __main__ - Global step 100 Train loss 9.347376 EM 0.0 on epoch=49
03/08/2022 17:48:59 - INFO - __main__ - Step 110 Global step 110 Train loss 6.990656 on epoch=54
03/08/2022 17:49:04 - INFO - __main__ - Step 120 Global step 120 Train loss 6.749289 on epoch=59
03/08/2022 17:49:08 - INFO - __main__ - Step 130 Global step 130 Train loss 6.109398 on epoch=64
03/08/2022 17:49:13 - INFO - __main__ - Step 140 Global step 140 Train loss 5.594159 on epoch=69
03/08/2022 17:49:18 - INFO - __main__ - Step 150 Global step 150 Train loss 5.053899 on epoch=74
03/08/2022 17:49:18 - INFO - __main__ - Global step 150 Train loss 6.099480 EM 0.0 on epoch=74
03/08/2022 17:49:23 - INFO - __main__ - Step 160 Global step 160 Train loss 5.311047 on epoch=79
03/08/2022 17:49:28 - INFO - __main__ - Step 170 Global step 170 Train loss 4.852649 on epoch=84
03/08/2022 17:49:33 - INFO - __main__ - Step 180 Global step 180 Train loss 4.445070 on epoch=89
03/08/2022 17:49:37 - INFO - __main__ - Step 190 Global step 190 Train loss 4.443383 on epoch=94
03/08/2022 17:49:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.770454 on epoch=99
03/08/2022 17:49:42 - INFO - __main__ - Global step 200 Train loss 4.564520 EM 0.0 on epoch=99
03/08/2022 17:49:47 - INFO - __main__ - Step 210 Global step 210 Train loss 3.621825 on epoch=104
03/08/2022 17:49:52 - INFO - __main__ - Step 220 Global step 220 Train loss 3.455277 on epoch=109
03/08/2022 17:49:57 - INFO - __main__ - Step 230 Global step 230 Train loss 3.414426 on epoch=114
03/08/2022 17:50:01 - INFO - __main__ - Step 240 Global step 240 Train loss 3.057835 on epoch=119
03/08/2022 17:50:06 - INFO - __main__ - Step 250 Global step 250 Train loss 3.038542 on epoch=124
03/08/2022 17:50:07 - INFO - __main__ - Global step 250 Train loss 3.317581 EM 0.0 on epoch=124
03/08/2022 17:50:11 - INFO - __main__ - Step 260 Global step 260 Train loss 2.753289 on epoch=129
03/08/2022 17:50:16 - INFO - __main__ - Step 270 Global step 270 Train loss 2.775823 on epoch=134
03/08/2022 17:50:21 - INFO - __main__ - Step 280 Global step 280 Train loss 2.352251 on epoch=139
03/08/2022 17:50:26 - INFO - __main__ - Step 290 Global step 290 Train loss 2.442210 on epoch=144
03/08/2022 17:50:30 - INFO - __main__ - Step 300 Global step 300 Train loss 2.124836 on epoch=149
03/08/2022 17:50:31 - INFO - __main__ - Global step 300 Train loss 2.489682 EM 0.0 on epoch=149
03/08/2022 17:50:36 - INFO - __main__ - Step 310 Global step 310 Train loss 1.833335 on epoch=154
03/08/2022 17:50:41 - INFO - __main__ - Step 320 Global step 320 Train loss 1.969305 on epoch=159
03/08/2022 17:50:46 - INFO - __main__ - Step 330 Global step 330 Train loss 2.102699 on epoch=164
03/08/2022 17:50:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.733040 on epoch=169
03/08/2022 17:50:55 - INFO - __main__ - Step 350 Global step 350 Train loss 2.051501 on epoch=174
03/08/2022 17:50:56 - INFO - __main__ - Global step 350 Train loss 1.937976 EM 0.0 on epoch=174
03/08/2022 17:51:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.905054 on epoch=179
03/08/2022 17:51:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.916871 on epoch=184
03/08/2022 17:51:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.794221 on epoch=189
03/08/2022 17:51:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.605260 on epoch=194
03/08/2022 17:51:20 - INFO - __main__ - Step 400 Global step 400 Train loss 1.729269 on epoch=199
03/08/2022 17:51:20 - INFO - __main__ - Global step 400 Train loss 1.790135 EM 0.0 on epoch=199
03/08/2022 17:51:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.580611 on epoch=204
03/08/2022 17:51:30 - INFO - __main__ - Step 420 Global step 420 Train loss 1.529434 on epoch=209
03/08/2022 17:51:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.590841 on epoch=214
03/08/2022 17:51:39 - INFO - __main__ - Step 440 Global step 440 Train loss 1.652608 on epoch=219
03/08/2022 17:51:44 - INFO - __main__ - Step 450 Global step 450 Train loss 1.369171 on epoch=224
03/08/2022 17:51:45 - INFO - __main__ - Global step 450 Train loss 1.544533 EM 0.0 on epoch=224
03/08/2022 17:51:49 - INFO - __main__ - Step 460 Global step 460 Train loss 1.429484 on epoch=229
03/08/2022 17:51:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.329444 on epoch=234
03/08/2022 17:51:59 - INFO - __main__ - Step 480 Global step 480 Train loss 1.368961 on epoch=239
03/08/2022 17:52:04 - INFO - __main__ - Step 490 Global step 490 Train loss 1.457696 on epoch=244
03/08/2022 17:52:09 - INFO - __main__ - Step 500 Global step 500 Train loss 1.422102 on epoch=249
03/08/2022 17:52:09 - INFO - __main__ - Global step 500 Train loss 1.401537 EM 0.0 on epoch=249
03/08/2022 17:52:14 - INFO - __main__ - Step 510 Global step 510 Train loss 1.551998 on epoch=254
03/08/2022 17:52:19 - INFO - __main__ - Step 520 Global step 520 Train loss 1.274534 on epoch=259
03/08/2022 17:52:23 - INFO - __main__ - Step 530 Global step 530 Train loss 1.197560 on epoch=264
03/08/2022 17:52:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.237410 on epoch=269
03/08/2022 17:52:33 - INFO - __main__ - Step 550 Global step 550 Train loss 1.380481 on epoch=274
03/08/2022 17:52:34 - INFO - __main__ - Global step 550 Train loss 1.328397 EM 0.0 on epoch=274
03/08/2022 17:52:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.473024 on epoch=279
03/08/2022 17:52:43 - INFO - __main__ - Step 570 Global step 570 Train loss 1.329017 on epoch=284
03/08/2022 17:52:48 - INFO - __main__ - Step 580 Global step 580 Train loss 1.335759 on epoch=289
03/08/2022 17:52:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.279342 on epoch=294
03/08/2022 17:52:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.291241 on epoch=299
03/08/2022 17:52:58 - INFO - __main__ - Global step 600 Train loss 1.341676 EM 0.0 on epoch=299
03/08/2022 17:52:58 - INFO - __main__ - save last model!
03/08/2022 17:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:52:58 - INFO - __main__ - Printing 3 examples
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 17:52:58 - INFO - __main__ - ['Rapamune']
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 17:52:58 - INFO - __main__ - ['G B O G H']
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 17:52:58 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 17:52:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 17:52:58 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:52:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 17:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 17:52:58 - INFO - __main__ - Printing 3 examples
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 17:52:58 - INFO - __main__ - ['Seating Renovators']
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 17:52:58 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 17:52:58 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 17:52:58 - INFO - __main__ - ['Dick For President']
03/08/2022 17:52:58 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:52:58 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:52:59 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 17:53:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 17:53:08 - INFO - __main__ - Starting training!
03/08/2022 17:53:38 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 17:53:38 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 17:53:38 - INFO - __main__ - Printing 3 examples
03/08/2022 17:53:38 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 17:53:38 - INFO - __main__ - ['Whole S M']
03/08/2022 17:53:38 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 17:53:38 - INFO - __main__ - ['pushin Daisies']
03/08/2022 17:53:38 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 17:53:38 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 17:53:38 - INFO - __main__ - Tokenizing Input ...
03/08/2022 17:53:39 - INFO - __main__ - Tokenizing Output ...
03/08/2022 17:53:41 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:01:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0002_8_predictions.txt
03/08/2022 18:01:09 - INFO - __main__ - EM on test data: 0.0041
03/08/2022 18:01:10 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.00409626216077829
03/08/2022 18:01:10 - INFO - __main__ - Running ... prefix=crawl_domain_32_21, lr=0.0001, bsz=8 ...
03/08/2022 18:01:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:01:11 - INFO - __main__ - Printing 3 examples
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] rapamune
03/08/2022 18:01:11 - INFO - __main__ - ['Rapamune']
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] gbogh
03/08/2022 18:01:11 - INFO - __main__ - ['G B O G H']
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] carshieldcareers
03/08/2022 18:01:11 - INFO - __main__ - ['Car Shield Careers']
03/08/2022 18:01:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:01:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:01:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:01:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:01:11 - INFO - __main__ - Printing 3 examples
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] seatingrenovators
03/08/2022 18:01:11 - INFO - __main__ - ['Seating Renovators']
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] pinballheaven
03/08/2022 18:01:11 - INFO - __main__ - ['Pinball Heaven']
03/08/2022 18:01:11 - INFO - __main__ -  [crawl_domain] dickforpresident
03/08/2022 18:01:11 - INFO - __main__ - ['Dick For President']
03/08/2022 18:01:11 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:01:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:01:11 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:01:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:01:21 - INFO - __main__ - Starting training!
03/08/2022 18:01:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.320641 on epoch=4
03/08/2022 18:01:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.066389 on epoch=9
03/08/2022 18:01:35 - INFO - __main__ - Step 30 Global step 30 Train loss 15.111023 on epoch=14
03/08/2022 18:01:39 - INFO - __main__ - Step 40 Global step 40 Train loss 14.936064 on epoch=19
03/08/2022 18:01:44 - INFO - __main__ - Step 50 Global step 50 Train loss 13.413083 on epoch=24
03/08/2022 18:01:54 - INFO - __main__ - Global step 50 Train loss 16.569441 EM 0.0 on epoch=24
03/08/2022 18:02:46 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 18:02:51 - INFO - __main__ - Step 60 Global step 60 Train loss 12.184072 on epoch=29
03/08/2022 18:02:55 - INFO - __main__ - Step 70 Global step 70 Train loss 11.289807 on epoch=34
03/08/2022 18:03:00 - INFO - __main__ - Step 80 Global step 80 Train loss 10.339511 on epoch=39
03/08/2022 18:03:05 - INFO - __main__ - Step 90 Global step 90 Train loss 9.615134 on epoch=44
03/08/2022 18:03:09 - INFO - __main__ - Step 100 Global step 100 Train loss 9.155445 on epoch=49
03/08/2022 18:03:11 - INFO - __main__ - Global step 100 Train loss 10.516794 EM 0.03125 on epoch=49
03/08/2022 18:04:02 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 18:04:07 - INFO - __main__ - Step 110 Global step 110 Train loss 8.371181 on epoch=54
03/08/2022 18:04:11 - INFO - __main__ - Step 120 Global step 120 Train loss 8.291022 on epoch=59
03/08/2022 18:04:16 - INFO - __main__ - Step 130 Global step 130 Train loss 8.082277 on epoch=64
03/08/2022 18:04:21 - INFO - __main__ - Step 140 Global step 140 Train loss 7.389915 on epoch=69
03/08/2022 18:04:26 - INFO - __main__ - Step 150 Global step 150 Train loss 7.037613 on epoch=74
03/08/2022 18:04:26 - INFO - __main__ - Global step 150 Train loss 7.834401 EM 0.03125 on epoch=74
03/08/2022 18:04:31 - INFO - __main__ - Step 160 Global step 160 Train loss 6.774835 on epoch=79
03/08/2022 18:04:36 - INFO - __main__ - Step 170 Global step 170 Train loss 6.680477 on epoch=84
03/08/2022 18:04:41 - INFO - __main__ - Step 180 Global step 180 Train loss 6.450836 on epoch=89
03/08/2022 18:04:46 - INFO - __main__ - Step 190 Global step 190 Train loss 5.921579 on epoch=94
03/08/2022 18:04:50 - INFO - __main__ - Step 200 Global step 200 Train loss 5.807946 on epoch=99
03/08/2022 18:04:51 - INFO - __main__ - Global step 200 Train loss 6.327135 EM 0.0625 on epoch=99
03/08/2022 18:05:35 - INFO - __main__ - Saving model with best EM: 0.03125 -> 0.0625 on epoch=99, global_step=200
03/08/2022 18:05:40 - INFO - __main__ - Step 210 Global step 210 Train loss 5.913124 on epoch=104
03/08/2022 18:05:45 - INFO - __main__ - Step 220 Global step 220 Train loss 5.435072 on epoch=109
03/08/2022 18:05:50 - INFO - __main__ - Step 230 Global step 230 Train loss 5.787513 on epoch=114
03/08/2022 18:05:54 - INFO - __main__ - Step 240 Global step 240 Train loss 5.189694 on epoch=119
03/08/2022 18:05:59 - INFO - __main__ - Step 250 Global step 250 Train loss 5.420630 on epoch=124
03/08/2022 18:06:00 - INFO - __main__ - Global step 250 Train loss 5.549207 EM 0.0625 on epoch=124
03/08/2022 18:06:05 - INFO - __main__ - Step 260 Global step 260 Train loss 5.253611 on epoch=129
03/08/2022 18:06:09 - INFO - __main__ - Step 270 Global step 270 Train loss 5.359959 on epoch=134
03/08/2022 18:06:14 - INFO - __main__ - Step 280 Global step 280 Train loss 4.960279 on epoch=139
03/08/2022 18:06:19 - INFO - __main__ - Step 290 Global step 290 Train loss 5.099278 on epoch=144
03/08/2022 18:06:24 - INFO - __main__ - Step 300 Global step 300 Train loss 4.839833 on epoch=149
03/08/2022 18:06:24 - INFO - __main__ - Global step 300 Train loss 5.102592 EM 0.0 on epoch=149
03/08/2022 18:06:29 - INFO - __main__ - Step 310 Global step 310 Train loss 4.986241 on epoch=154
03/08/2022 18:06:34 - INFO - __main__ - Step 320 Global step 320 Train loss 4.696243 on epoch=159
03/08/2022 18:06:39 - INFO - __main__ - Step 330 Global step 330 Train loss 4.264941 on epoch=164
03/08/2022 18:06:44 - INFO - __main__ - Step 340 Global step 340 Train loss 4.474205 on epoch=169
03/08/2022 18:06:49 - INFO - __main__ - Step 350 Global step 350 Train loss 4.266383 on epoch=174
03/08/2022 18:06:50 - INFO - __main__ - Global step 350 Train loss 4.537603 EM 0.03125 on epoch=174
03/08/2022 18:06:54 - INFO - __main__ - Step 360 Global step 360 Train loss 4.223494 on epoch=179
03/08/2022 18:06:59 - INFO - __main__ - Step 370 Global step 370 Train loss 4.076279 on epoch=184
03/08/2022 18:07:04 - INFO - __main__ - Step 380 Global step 380 Train loss 4.136218 on epoch=189
03/08/2022 18:07:09 - INFO - __main__ - Step 390 Global step 390 Train loss 4.053675 on epoch=194
03/08/2022 18:07:14 - INFO - __main__ - Step 400 Global step 400 Train loss 3.818007 on epoch=199
03/08/2022 18:07:14 - INFO - __main__ - Global step 400 Train loss 4.061534 EM 0.0 on epoch=199
03/08/2022 18:07:19 - INFO - __main__ - Step 410 Global step 410 Train loss 3.518750 on epoch=204
03/08/2022 18:07:24 - INFO - __main__ - Step 420 Global step 420 Train loss 3.704357 on epoch=209
03/08/2022 18:07:29 - INFO - __main__ - Step 430 Global step 430 Train loss 3.606122 on epoch=214
03/08/2022 18:07:34 - INFO - __main__ - Step 440 Global step 440 Train loss 3.290643 on epoch=219
03/08/2022 18:07:38 - INFO - __main__ - Step 450 Global step 450 Train loss 2.996598 on epoch=224
03/08/2022 18:07:39 - INFO - __main__ - Global step 450 Train loss 3.423294 EM 0.0 on epoch=224
03/08/2022 18:07:44 - INFO - __main__ - Step 460 Global step 460 Train loss 2.975445 on epoch=229
03/08/2022 18:07:49 - INFO - __main__ - Step 470 Global step 470 Train loss 2.736679 on epoch=234
03/08/2022 18:07:53 - INFO - __main__ - Step 480 Global step 480 Train loss 2.568830 on epoch=239
03/08/2022 18:07:58 - INFO - __main__ - Step 490 Global step 490 Train loss 2.490206 on epoch=244
03/08/2022 18:08:03 - INFO - __main__ - Step 500 Global step 500 Train loss 2.655273 on epoch=249
03/08/2022 18:08:04 - INFO - __main__ - Global step 500 Train loss 2.685287 EM 0.0 on epoch=249
03/08/2022 18:08:08 - INFO - __main__ - Step 510 Global step 510 Train loss 2.689381 on epoch=254
03/08/2022 18:08:13 - INFO - __main__ - Step 520 Global step 520 Train loss 2.343803 on epoch=259
03/08/2022 18:08:18 - INFO - __main__ - Step 530 Global step 530 Train loss 2.348356 on epoch=264
03/08/2022 18:08:23 - INFO - __main__ - Step 540 Global step 540 Train loss 2.169055 on epoch=269
03/08/2022 18:08:28 - INFO - __main__ - Step 550 Global step 550 Train loss 2.068954 on epoch=274
03/08/2022 18:08:28 - INFO - __main__ - Global step 550 Train loss 2.323910 EM 0.0 on epoch=274
03/08/2022 18:08:33 - INFO - __main__ - Step 560 Global step 560 Train loss 2.442138 on epoch=279
03/08/2022 18:08:38 - INFO - __main__ - Step 570 Global step 570 Train loss 2.056323 on epoch=284
03/08/2022 18:08:43 - INFO - __main__ - Step 580 Global step 580 Train loss 2.197450 on epoch=289
03/08/2022 18:08:48 - INFO - __main__ - Step 590 Global step 590 Train loss 1.927421 on epoch=294
03/08/2022 18:08:53 - INFO - __main__ - Step 600 Global step 600 Train loss 2.120018 on epoch=299
03/08/2022 18:08:53 - INFO - __main__ - Global step 600 Train loss 2.148669 EM 0.0 on epoch=299
03/08/2022 18:08:53 - INFO - __main__ - save last model!
03/08/2022 18:08:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:08:54 - INFO - __main__ - Printing 3 examples
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:08:54 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:08:54 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:08:54 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:08:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 18:08:54 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:08:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:08:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:08:54 - INFO - __main__ - Printing 3 examples
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:08:54 - INFO - __main__ - ['N F M S']
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:08:54 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:08:54 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:08:54 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:08:54 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:08:54 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:08:54 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:09:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:09:04 - INFO - __main__ - Starting training!
03/08/2022 18:09:33 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 18:09:33 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 18:09:33 - INFO - __main__ - Printing 3 examples
03/08/2022 18:09:33 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 18:09:33 - INFO - __main__ - ['Whole S M']
03/08/2022 18:09:34 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 18:09:34 - INFO - __main__ - ['pushin Daisies']
03/08/2022 18:09:34 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 18:09:34 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 18:09:34 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:09:34 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:09:36 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:10:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_21_0.0001_8_predictions.txt
03/08/2022 18:10:13 - INFO - __main__ - EM on test data: 0.0123
03/08/2022 18:10:14 - INFO - __main__ - prefix=crawl_domain_32_21, lr=0.0001, bsz=8, dev_performance=0.0625, test_performance=0.01228878648233487
03/08/2022 18:10:14 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0005, bsz=8 ...
03/08/2022 18:10:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:10:15 - INFO - __main__ - Printing 3 examples
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:10:15 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:10:15 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:10:15 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:10:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:10:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:10:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:10:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:10:15 - INFO - __main__ - Printing 3 examples
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:10:15 - INFO - __main__ - ['N F M S']
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:10:15 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:10:15 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:10:15 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:10:15 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:10:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:10:15 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:10:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:10:24 - INFO - __main__ - Starting training!
03/08/2022 18:10:28 - INFO - __main__ - Step 10 Global step 10 Train loss 20.641169 on epoch=4
03/08/2022 18:10:33 - INFO - __main__ - Step 20 Global step 20 Train loss 16.783310 on epoch=9
03/08/2022 18:10:37 - INFO - __main__ - Step 30 Global step 30 Train loss 12.391582 on epoch=14
03/08/2022 18:10:42 - INFO - __main__ - Step 40 Global step 40 Train loss 10.307453 on epoch=19
03/08/2022 18:10:47 - INFO - __main__ - Step 50 Global step 50 Train loss 9.002833 on epoch=24
03/08/2022 18:10:48 - INFO - __main__ - Global step 50 Train loss 13.825269 EM 0.0 on epoch=24
03/08/2022 18:11:23 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 18:11:28 - INFO - __main__ - Step 60 Global step 60 Train loss 8.050835 on epoch=29
03/08/2022 18:11:33 - INFO - __main__ - Step 70 Global step 70 Train loss 7.041064 on epoch=34
03/08/2022 18:11:38 - INFO - __main__ - Step 80 Global step 80 Train loss 5.928793 on epoch=39
03/08/2022 18:11:42 - INFO - __main__ - Step 90 Global step 90 Train loss 4.777258 on epoch=44
03/08/2022 18:11:47 - INFO - __main__ - Step 100 Global step 100 Train loss 3.437063 on epoch=49
03/08/2022 18:11:47 - INFO - __main__ - Global step 100 Train loss 5.847003 EM 0.0 on epoch=49
03/08/2022 18:11:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.633318 on epoch=54
03/08/2022 18:11:57 - INFO - __main__ - Step 120 Global step 120 Train loss 2.439102 on epoch=59
03/08/2022 18:12:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.134680 on epoch=64
03/08/2022 18:12:06 - INFO - __main__ - Step 140 Global step 140 Train loss 1.906890 on epoch=69
03/08/2022 18:12:11 - INFO - __main__ - Step 150 Global step 150 Train loss 1.791299 on epoch=74
03/08/2022 18:12:12 - INFO - __main__ - Global step 150 Train loss 2.181058 EM 0.0 on epoch=74
03/08/2022 18:12:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.909026 on epoch=79
03/08/2022 18:12:21 - INFO - __main__ - Step 170 Global step 170 Train loss 1.768615 on epoch=84
03/08/2022 18:12:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.865551 on epoch=89
03/08/2022 18:12:30 - INFO - __main__ - Step 190 Global step 190 Train loss 1.569129 on epoch=94
03/08/2022 18:12:35 - INFO - __main__ - Step 200 Global step 200 Train loss 1.332790 on epoch=99
03/08/2022 18:12:35 - INFO - __main__ - Global step 200 Train loss 1.689022 EM 0.0 on epoch=99
03/08/2022 18:12:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.601101 on epoch=104
03/08/2022 18:12:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.414912 on epoch=109
03/08/2022 18:12:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.837583 on epoch=114
03/08/2022 18:12:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.467316 on epoch=119
03/08/2022 18:12:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.318700 on epoch=124
03/08/2022 18:12:59 - INFO - __main__ - Global step 250 Train loss 1.527923 EM 0.0 on epoch=124
03/08/2022 18:13:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.514723 on epoch=129
03/08/2022 18:13:09 - INFO - __main__ - Step 270 Global step 270 Train loss 1.120717 on epoch=134
03/08/2022 18:13:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.337132 on epoch=139
03/08/2022 18:13:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.069124 on epoch=144
03/08/2022 18:13:22 - INFO - __main__ - Step 300 Global step 300 Train loss 1.098876 on epoch=149
03/08/2022 18:13:23 - INFO - __main__ - Global step 300 Train loss 1.228114 EM 0.0 on epoch=149
03/08/2022 18:13:27 - INFO - __main__ - Step 310 Global step 310 Train loss 1.144178 on epoch=154
03/08/2022 18:13:32 - INFO - __main__ - Step 320 Global step 320 Train loss 1.127544 on epoch=159
03/08/2022 18:13:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.106681 on epoch=164
03/08/2022 18:13:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.111224 on epoch=169
03/08/2022 18:13:46 - INFO - __main__ - Step 350 Global step 350 Train loss 1.038609 on epoch=174
03/08/2022 18:13:46 - INFO - __main__ - Global step 350 Train loss 1.105647 EM 0.0 on epoch=174
03/08/2022 18:13:51 - INFO - __main__ - Step 360 Global step 360 Train loss 1.007262 on epoch=179
03/08/2022 18:13:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.008098 on epoch=184
03/08/2022 18:14:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.026545 on epoch=189
03/08/2022 18:14:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.971649 on epoch=194
03/08/2022 18:14:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.993139 on epoch=199
03/08/2022 18:14:10 - INFO - __main__ - Global step 400 Train loss 1.001339 EM 0.0 on epoch=199
03/08/2022 18:14:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.877580 on epoch=204
03/08/2022 18:14:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.932770 on epoch=209
03/08/2022 18:14:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.823703 on epoch=214
03/08/2022 18:14:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.844447 on epoch=219
03/08/2022 18:14:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.835919 on epoch=224
03/08/2022 18:14:33 - INFO - __main__ - Global step 450 Train loss 0.862884 EM 0.0 on epoch=224
03/08/2022 18:14:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.826338 on epoch=229
03/08/2022 18:14:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.741522 on epoch=234
03/08/2022 18:14:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.820622 on epoch=239
03/08/2022 18:14:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.858702 on epoch=244
03/08/2022 18:14:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.839448 on epoch=249
03/08/2022 18:14:57 - INFO - __main__ - Global step 500 Train loss 0.817327 EM 0.0 on epoch=249
03/08/2022 18:15:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.866404 on epoch=254
03/08/2022 18:15:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.842654 on epoch=259
03/08/2022 18:15:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.836167 on epoch=264
03/08/2022 18:15:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.810071 on epoch=269
03/08/2022 18:15:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.715684 on epoch=274
03/08/2022 18:15:21 - INFO - __main__ - Global step 550 Train loss 0.814196 EM 0.0 on epoch=274
03/08/2022 18:15:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.811716 on epoch=279
03/08/2022 18:15:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.856362 on epoch=284
03/08/2022 18:15:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.883111 on epoch=289
03/08/2022 18:15:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.748538 on epoch=294
03/08/2022 18:15:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.806697 on epoch=299
03/08/2022 18:15:45 - INFO - __main__ - Global step 600 Train loss 0.821285 EM 0.0 on epoch=299
03/08/2022 18:15:45 - INFO - __main__ - save last model!
03/08/2022 18:15:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:15:46 - INFO - __main__ - Printing 3 examples
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:15:46 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:15:46 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:15:46 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:15:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 18:15:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:15:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:15:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:15:46 - INFO - __main__ - Printing 3 examples
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:15:46 - INFO - __main__ - ['N F M S']
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:15:46 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:15:46 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:15:46 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:15:46 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:15:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:15:46 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:15:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:15:57 - INFO - __main__ - Starting training!
03/08/2022 18:16:25 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 18:16:26 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 18:16:26 - INFO - __main__ - Printing 3 examples
03/08/2022 18:16:26 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 18:16:26 - INFO - __main__ - ['Whole S M']
03/08/2022 18:16:26 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 18:16:26 - INFO - __main__ - ['pushin Daisies']
03/08/2022 18:16:26 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 18:16:26 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 18:16:26 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:16:26 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:16:28 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:19:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0005_8_predictions.txt
03/08/2022 18:19:43 - INFO - __main__ - EM on test data: 0.0015
03/08/2022 18:19:44 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0015360983102918587
03/08/2022 18:19:44 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0003, bsz=8 ...
03/08/2022 18:19:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:19:45 - INFO - __main__ - Printing 3 examples
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:19:45 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:19:45 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:19:45 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:19:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:19:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:19:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:19:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:19:45 - INFO - __main__ - Printing 3 examples
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:19:45 - INFO - __main__ - ['N F M S']
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:19:45 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:19:45 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:19:45 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:19:45 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:19:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:19:45 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:19:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:19:54 - INFO - __main__ - Starting training!
03/08/2022 18:19:58 - INFO - __main__ - Step 10 Global step 10 Train loss 20.168402 on epoch=4
03/08/2022 18:20:02 - INFO - __main__ - Step 20 Global step 20 Train loss 17.639172 on epoch=9
03/08/2022 18:20:07 - INFO - __main__ - Step 30 Global step 30 Train loss 13.303424 on epoch=14
03/08/2022 18:20:12 - INFO - __main__ - Step 40 Global step 40 Train loss 11.372456 on epoch=19
03/08/2022 18:20:17 - INFO - __main__ - Step 50 Global step 50 Train loss 10.179957 on epoch=24
03/08/2022 18:20:22 - INFO - __main__ - Global step 50 Train loss 14.532681 EM 0.0 on epoch=24
03/08/2022 18:20:57 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 18:21:02 - INFO - __main__ - Step 60 Global step 60 Train loss 9.073816 on epoch=29
03/08/2022 18:21:07 - INFO - __main__ - Step 70 Global step 70 Train loss 7.949934 on epoch=34
03/08/2022 18:21:11 - INFO - __main__ - Step 80 Global step 80 Train loss 6.856830 on epoch=39
03/08/2022 18:21:16 - INFO - __main__ - Step 90 Global step 90 Train loss 6.110747 on epoch=44
03/08/2022 18:21:21 - INFO - __main__ - Step 100 Global step 100 Train loss 5.445693 on epoch=49
03/08/2022 18:21:22 - INFO - __main__ - Global step 100 Train loss 7.087404 EM 0.03125 on epoch=49
03/08/2022 18:21:54 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 18:21:59 - INFO - __main__ - Step 110 Global step 110 Train loss 4.862219 on epoch=54
03/08/2022 18:22:04 - INFO - __main__ - Step 120 Global step 120 Train loss 4.351862 on epoch=59
03/08/2022 18:22:09 - INFO - __main__ - Step 130 Global step 130 Train loss 4.400249 on epoch=64
03/08/2022 18:22:13 - INFO - __main__ - Step 140 Global step 140 Train loss 4.070715 on epoch=69
03/08/2022 18:22:18 - INFO - __main__ - Step 150 Global step 150 Train loss 3.343708 on epoch=74
03/08/2022 18:22:19 - INFO - __main__ - Global step 150 Train loss 4.205751 EM 0.0 on epoch=74
03/08/2022 18:22:23 - INFO - __main__ - Step 160 Global step 160 Train loss 3.276582 on epoch=79
03/08/2022 18:22:28 - INFO - __main__ - Step 170 Global step 170 Train loss 2.763104 on epoch=84
03/08/2022 18:22:33 - INFO - __main__ - Step 180 Global step 180 Train loss 2.287498 on epoch=89
03/08/2022 18:22:37 - INFO - __main__ - Step 190 Global step 190 Train loss 2.158873 on epoch=94
03/08/2022 18:22:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.989051 on epoch=99
03/08/2022 18:22:43 - INFO - __main__ - Global step 200 Train loss 2.495022 EM 0.0 on epoch=99
03/08/2022 18:22:47 - INFO - __main__ - Step 210 Global step 210 Train loss 1.930729 on epoch=104
03/08/2022 18:22:52 - INFO - __main__ - Step 220 Global step 220 Train loss 1.829607 on epoch=109
03/08/2022 18:22:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.840060 on epoch=114
03/08/2022 18:23:02 - INFO - __main__ - Step 240 Global step 240 Train loss 2.100725 on epoch=119
03/08/2022 18:23:06 - INFO - __main__ - Step 250 Global step 250 Train loss 1.544070 on epoch=124
03/08/2022 18:23:07 - INFO - __main__ - Global step 250 Train loss 1.849038 EM 0.0 on epoch=124
03/08/2022 18:23:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.785022 on epoch=129
03/08/2022 18:23:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.835312 on epoch=134
03/08/2022 18:23:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.821734 on epoch=139
03/08/2022 18:23:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.528181 on epoch=144
03/08/2022 18:23:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.793028 on epoch=149
03/08/2022 18:23:31 - INFO - __main__ - Global step 300 Train loss 1.752655 EM 0.0 on epoch=149
03/08/2022 18:23:36 - INFO - __main__ - Step 310 Global step 310 Train loss 1.541242 on epoch=154
03/08/2022 18:23:40 - INFO - __main__ - Step 320 Global step 320 Train loss 1.503420 on epoch=159
03/08/2022 18:23:45 - INFO - __main__ - Step 330 Global step 330 Train loss 1.523582 on epoch=164
03/08/2022 18:23:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.506121 on epoch=169
03/08/2022 18:23:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.361735 on epoch=174
03/08/2022 18:23:55 - INFO - __main__ - Global step 350 Train loss 1.487220 EM 0.0 on epoch=174
03/08/2022 18:24:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.340684 on epoch=179
03/08/2022 18:24:04 - INFO - __main__ - Step 370 Global step 370 Train loss 1.297813 on epoch=184
03/08/2022 18:24:09 - INFO - __main__ - Step 380 Global step 380 Train loss 1.310400 on epoch=189
03/08/2022 18:24:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.249434 on epoch=194
03/08/2022 18:24:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.408894 on epoch=199
03/08/2022 18:24:19 - INFO - __main__ - Global step 400 Train loss 1.321445 EM 0.0 on epoch=199
03/08/2022 18:24:24 - INFO - __main__ - Step 410 Global step 410 Train loss 1.349543 on epoch=204
03/08/2022 18:24:29 - INFO - __main__ - Step 420 Global step 420 Train loss 1.403485 on epoch=209
03/08/2022 18:24:33 - INFO - __main__ - Step 430 Global step 430 Train loss 1.210877 on epoch=214
03/08/2022 18:24:38 - INFO - __main__ - Step 440 Global step 440 Train loss 1.247156 on epoch=219
03/08/2022 18:24:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.272632 on epoch=224
03/08/2022 18:24:43 - INFO - __main__ - Global step 450 Train loss 1.296739 EM 0.0 on epoch=224
03/08/2022 18:24:48 - INFO - __main__ - Step 460 Global step 460 Train loss 1.149818 on epoch=229
03/08/2022 18:24:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.169174 on epoch=234
03/08/2022 18:24:58 - INFO - __main__ - Step 480 Global step 480 Train loss 1.075751 on epoch=239
03/08/2022 18:25:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.057661 on epoch=244
03/08/2022 18:25:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.031940 on epoch=249
03/08/2022 18:25:08 - INFO - __main__ - Global step 500 Train loss 1.096869 EM 0.0 on epoch=249
03/08/2022 18:25:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.022718 on epoch=254
03/08/2022 18:25:17 - INFO - __main__ - Step 520 Global step 520 Train loss 1.048243 on epoch=259
03/08/2022 18:25:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.920996 on epoch=264
03/08/2022 18:25:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.043845 on epoch=269
03/08/2022 18:25:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.901838 on epoch=274
03/08/2022 18:25:32 - INFO - __main__ - Global step 550 Train loss 0.987528 EM 0.0 on epoch=274
03/08/2022 18:25:36 - INFO - __main__ - Step 560 Global step 560 Train loss 1.042765 on epoch=279
03/08/2022 18:25:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956232 on epoch=284
03/08/2022 18:25:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.954260 on epoch=289
03/08/2022 18:25:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.877376 on epoch=294
03/08/2022 18:25:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.958155 on epoch=299
03/08/2022 18:25:56 - INFO - __main__ - Global step 600 Train loss 0.957758 EM 0.0 on epoch=299
03/08/2022 18:25:56 - INFO - __main__ - save last model!
03/08/2022 18:25:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:25:56 - INFO - __main__ - Printing 3 examples
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:25:56 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:25:56 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:25:56 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:25:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 18:25:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:25:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:25:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:25:56 - INFO - __main__ - Printing 3 examples
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:25:56 - INFO - __main__ - ['N F M S']
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:25:56 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:25:56 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:25:56 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:25:56 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:25:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:25:56 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:26:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:26:07 - INFO - __main__ - Starting training!
03/08/2022 18:26:36 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 18:26:36 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 18:26:36 - INFO - __main__ - Printing 3 examples
03/08/2022 18:26:36 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 18:26:36 - INFO - __main__ - ['Whole S M']
03/08/2022 18:26:36 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 18:26:36 - INFO - __main__ - ['pushin Daisies']
03/08/2022 18:26:36 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 18:26:36 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 18:26:36 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:26:37 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:26:39 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:27:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0003_8_predictions.txt
03/08/2022 18:27:25 - INFO - __main__ - EM on test data: 0.0169
03/08/2022 18:27:26 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.016897081413210446
03/08/2022 18:27:26 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0002, bsz=8 ...
03/08/2022 18:27:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:27:26 - INFO - __main__ - Printing 3 examples
03/08/2022 18:27:26 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:27:26 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:27:26 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:27:26 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:27:26 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:27:26 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:27:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:27:26 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:27:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:27:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:27:27 - INFO - __main__ - Printing 3 examples
03/08/2022 18:27:27 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:27:27 - INFO - __main__ - ['N F M S']
03/08/2022 18:27:27 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:27:27 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:27:27 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:27:27 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:27:27 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:27:27 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:27:27 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:27:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:27:37 - INFO - __main__ - Starting training!
03/08/2022 18:27:41 - INFO - __main__ - Step 10 Global step 10 Train loss 21.066547 on epoch=4
03/08/2022 18:27:45 - INFO - __main__ - Step 20 Global step 20 Train loss 18.077887 on epoch=9
03/08/2022 18:27:50 - INFO - __main__ - Step 30 Global step 30 Train loss 16.788851 on epoch=14
03/08/2022 18:27:54 - INFO - __main__ - Step 40 Global step 40 Train loss 14.024854 on epoch=19
03/08/2022 18:27:59 - INFO - __main__ - Step 50 Global step 50 Train loss 13.551190 on epoch=24
03/08/2022 18:28:08 - INFO - __main__ - Global step 50 Train loss 16.701866 EM 0.0 on epoch=24
03/08/2022 18:28:40 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 18:28:45 - INFO - __main__ - Step 60 Global step 60 Train loss 12.869360 on epoch=29
03/08/2022 18:28:49 - INFO - __main__ - Step 70 Global step 70 Train loss 12.629152 on epoch=34
03/08/2022 18:28:54 - INFO - __main__ - Step 80 Global step 80 Train loss 11.377266 on epoch=39
03/08/2022 18:28:59 - INFO - __main__ - Step 90 Global step 90 Train loss 10.488613 on epoch=44
03/08/2022 18:29:04 - INFO - __main__ - Step 100 Global step 100 Train loss 9.891205 on epoch=49
03/08/2022 18:29:04 - INFO - __main__ - Global step 100 Train loss 11.451119 EM 0.0 on epoch=49
03/08/2022 18:29:09 - INFO - __main__ - Step 110 Global step 110 Train loss 9.038453 on epoch=54
03/08/2022 18:29:14 - INFO - __main__ - Step 120 Global step 120 Train loss 8.493065 on epoch=59
03/08/2022 18:29:19 - INFO - __main__ - Step 130 Global step 130 Train loss 8.021463 on epoch=64
03/08/2022 18:29:23 - INFO - __main__ - Step 140 Global step 140 Train loss 7.621083 on epoch=69
03/08/2022 18:29:28 - INFO - __main__ - Step 150 Global step 150 Train loss 7.253566 on epoch=74
03/08/2022 18:29:29 - INFO - __main__ - Global step 150 Train loss 8.085526 EM 0.0 on epoch=74
03/08/2022 18:29:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.442595 on epoch=79
03/08/2022 18:29:38 - INFO - __main__ - Step 170 Global step 170 Train loss 6.386209 on epoch=84
03/08/2022 18:29:43 - INFO - __main__ - Step 180 Global step 180 Train loss 5.939996 on epoch=89
03/08/2022 18:29:47 - INFO - __main__ - Step 190 Global step 190 Train loss 5.515197 on epoch=94
03/08/2022 18:29:52 - INFO - __main__ - Step 200 Global step 200 Train loss 5.170688 on epoch=99
03/08/2022 18:29:53 - INFO - __main__ - Global step 200 Train loss 5.890937 EM 0.0 on epoch=99
03/08/2022 18:29:57 - INFO - __main__ - Step 210 Global step 210 Train loss 4.539303 on epoch=104
03/08/2022 18:30:02 - INFO - __main__ - Step 220 Global step 220 Train loss 4.334866 on epoch=109
03/08/2022 18:30:07 - INFO - __main__ - Step 230 Global step 230 Train loss 3.652869 on epoch=114
03/08/2022 18:30:12 - INFO - __main__ - Step 240 Global step 240 Train loss 3.405354 on epoch=119
03/08/2022 18:30:16 - INFO - __main__ - Step 250 Global step 250 Train loss 3.222134 on epoch=124
03/08/2022 18:30:17 - INFO - __main__ - Global step 250 Train loss 3.830905 EM 0.0 on epoch=124
03/08/2022 18:30:22 - INFO - __main__ - Step 260 Global step 260 Train loss 3.076876 on epoch=129
03/08/2022 18:30:26 - INFO - __main__ - Step 270 Global step 270 Train loss 2.705405 on epoch=134
03/08/2022 18:30:31 - INFO - __main__ - Step 280 Global step 280 Train loss 2.462991 on epoch=139
03/08/2022 18:30:36 - INFO - __main__ - Step 290 Global step 290 Train loss 2.391930 on epoch=144
03/08/2022 18:30:40 - INFO - __main__ - Step 300 Global step 300 Train loss 2.074502 on epoch=149
03/08/2022 18:30:41 - INFO - __main__ - Global step 300 Train loss 2.542341 EM 0.0 on epoch=149
03/08/2022 18:30:46 - INFO - __main__ - Step 310 Global step 310 Train loss 2.169536 on epoch=154
03/08/2022 18:30:50 - INFO - __main__ - Step 320 Global step 320 Train loss 2.063700 on epoch=159
03/08/2022 18:30:55 - INFO - __main__ - Step 330 Global step 330 Train loss 2.306985 on epoch=164
03/08/2022 18:31:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.817906 on epoch=169
03/08/2022 18:31:04 - INFO - __main__ - Step 350 Global step 350 Train loss 1.791309 on epoch=174
03/08/2022 18:31:05 - INFO - __main__ - Global step 350 Train loss 2.029887 EM 0.0 on epoch=174
03/08/2022 18:31:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.665859 on epoch=179
03/08/2022 18:31:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.899434 on epoch=184
03/08/2022 18:31:19 - INFO - __main__ - Step 380 Global step 380 Train loss 1.905968 on epoch=189
03/08/2022 18:31:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.716524 on epoch=194
03/08/2022 18:31:29 - INFO - __main__ - Step 400 Global step 400 Train loss 1.982593 on epoch=199
03/08/2022 18:31:29 - INFO - __main__ - Global step 400 Train loss 1.834076 EM 0.0 on epoch=199
03/08/2022 18:31:34 - INFO - __main__ - Step 410 Global step 410 Train loss 1.670531 on epoch=204
03/08/2022 18:31:39 - INFO - __main__ - Step 420 Global step 420 Train loss 1.726192 on epoch=209
03/08/2022 18:31:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.619317 on epoch=214
03/08/2022 18:31:49 - INFO - __main__ - Step 440 Global step 440 Train loss 1.657575 on epoch=219
03/08/2022 18:31:53 - INFO - __main__ - Step 450 Global step 450 Train loss 1.834209 on epoch=224
03/08/2022 18:31:54 - INFO - __main__ - Global step 450 Train loss 1.701565 EM 0.0 on epoch=224
03/08/2022 18:31:59 - INFO - __main__ - Step 460 Global step 460 Train loss 1.759073 on epoch=229
03/08/2022 18:32:03 - INFO - __main__ - Step 470 Global step 470 Train loss 1.552463 on epoch=234
03/08/2022 18:32:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.558401 on epoch=239
03/08/2022 18:32:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.459815 on epoch=244
03/08/2022 18:32:17 - INFO - __main__ - Step 500 Global step 500 Train loss 1.562851 on epoch=249
03/08/2022 18:32:18 - INFO - __main__ - Global step 500 Train loss 1.578521 EM 0.0 on epoch=249
03/08/2022 18:32:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.578943 on epoch=254
03/08/2022 18:32:28 - INFO - __main__ - Step 520 Global step 520 Train loss 1.434775 on epoch=259
03/08/2022 18:32:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.292102 on epoch=264
03/08/2022 18:32:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.462906 on epoch=269
03/08/2022 18:32:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.425776 on epoch=274
03/08/2022 18:32:42 - INFO - __main__ - Global step 550 Train loss 1.438900 EM 0.0 on epoch=274
03/08/2022 18:32:47 - INFO - __main__ - Step 560 Global step 560 Train loss 1.323335 on epoch=279
03/08/2022 18:32:52 - INFO - __main__ - Step 570 Global step 570 Train loss 1.569090 on epoch=284
03/08/2022 18:32:56 - INFO - __main__ - Step 580 Global step 580 Train loss 1.529825 on epoch=289
03/08/2022 18:33:01 - INFO - __main__ - Step 590 Global step 590 Train loss 1.537436 on epoch=294
03/08/2022 18:33:06 - INFO - __main__ - Step 600 Global step 600 Train loss 1.212131 on epoch=299
03/08/2022 18:33:06 - INFO - __main__ - Global step 600 Train loss 1.434363 EM 0.0 on epoch=299
03/08/2022 18:33:06 - INFO - __main__ - save last model!
03/08/2022 18:33:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:33:07 - INFO - __main__ - Printing 3 examples
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:33:07 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:33:07 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:33:07 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:33:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 18:33:07 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:33:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:33:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:33:07 - INFO - __main__ - Printing 3 examples
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:33:07 - INFO - __main__ - ['N F M S']
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:33:07 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:33:07 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:33:07 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:33:07 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:33:07 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:33:07 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:33:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:33:18 - INFO - __main__ - Starting training!
03/08/2022 18:33:47 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 18:33:48 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 18:33:48 - INFO - __main__ - Printing 3 examples
03/08/2022 18:33:48 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 18:33:48 - INFO - __main__ - ['Whole S M']
03/08/2022 18:33:48 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 18:33:48 - INFO - __main__ - ['pushin Daisies']
03/08/2022 18:33:48 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 18:33:48 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 18:33:48 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:33:48 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:33:50 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:42:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0002_8_predictions.txt
03/08/2022 18:42:43 - INFO - __main__ - EM on test data: 0.0010
03/08/2022 18:42:44 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0010240655401945725
03/08/2022 18:42:44 - INFO - __main__ - Running ... prefix=crawl_domain_32_42, lr=0.0001, bsz=8 ...
03/08/2022 18:42:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:42:45 - INFO - __main__ - Printing 3 examples
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] dwglassmarkerboards
03/08/2022 18:42:45 - INFO - __main__ - ['D W glass Markerboards']
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] aromacomposer
03/08/2022 18:42:45 - INFO - __main__ - ['aroma Composer']
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] jmbrodrick
03/08/2022 18:42:45 - INFO - __main__ - ['J M Brodrick']
03/08/2022 18:42:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:42:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:42:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:42:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:42:45 - INFO - __main__ - Printing 3 examples
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] nfms
03/08/2022 18:42:45 - INFO - __main__ - ['N F M S']
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] loveworldtelevisionministry
03/08/2022 18:42:45 - INFO - __main__ - ['Love world television ministry']
03/08/2022 18:42:45 - INFO - __main__ -  [crawl_domain] shirkmusic
03/08/2022 18:42:45 - INFO - __main__ - ['Shirk Music']
03/08/2022 18:42:45 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:42:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:42:45 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:42:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:42:55 - INFO - __main__ - Starting training!
03/08/2022 18:42:59 - INFO - __main__ - Step 10 Global step 10 Train loss 20.202028 on epoch=4
03/08/2022 18:43:04 - INFO - __main__ - Step 20 Global step 20 Train loss 17.969721 on epoch=9
03/08/2022 18:43:08 - INFO - __main__ - Step 30 Global step 30 Train loss 17.160624 on epoch=14
03/08/2022 18:43:13 - INFO - __main__ - Step 40 Global step 40 Train loss 15.182925 on epoch=19
03/08/2022 18:43:17 - INFO - __main__ - Step 50 Global step 50 Train loss 14.754292 on epoch=24
03/08/2022 18:43:27 - INFO - __main__ - Global step 50 Train loss 17.053917 EM 0.0 on epoch=24
03/08/2022 18:44:03 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 18:44:08 - INFO - __main__ - Step 60 Global step 60 Train loss 13.774915 on epoch=29
03/08/2022 18:44:13 - INFO - __main__ - Step 70 Global step 70 Train loss 12.489703 on epoch=34
03/08/2022 18:44:17 - INFO - __main__ - Step 80 Global step 80 Train loss 12.845505 on epoch=39
03/08/2022 18:44:22 - INFO - __main__ - Step 90 Global step 90 Train loss 11.520163 on epoch=44
03/08/2022 18:44:27 - INFO - __main__ - Step 100 Global step 100 Train loss 11.157951 on epoch=49
03/08/2022 18:44:34 - INFO - __main__ - Global step 100 Train loss 12.357648 EM 0.0 on epoch=49
03/08/2022 18:44:39 - INFO - __main__ - Step 110 Global step 110 Train loss 11.627232 on epoch=54
03/08/2022 18:44:44 - INFO - __main__ - Step 120 Global step 120 Train loss 11.143096 on epoch=59
03/08/2022 18:44:48 - INFO - __main__ - Step 130 Global step 130 Train loss 10.753961 on epoch=64
03/08/2022 18:44:53 - INFO - __main__ - Step 140 Global step 140 Train loss 10.560368 on epoch=69
03/08/2022 18:44:58 - INFO - __main__ - Step 150 Global step 150 Train loss 10.026731 on epoch=74
03/08/2022 18:44:59 - INFO - __main__ - Global step 150 Train loss 10.822278 EM 0.0 on epoch=74
03/08/2022 18:45:04 - INFO - __main__ - Step 160 Global step 160 Train loss 9.666493 on epoch=79
03/08/2022 18:45:09 - INFO - __main__ - Step 170 Global step 170 Train loss 9.385073 on epoch=84
03/08/2022 18:45:14 - INFO - __main__ - Step 180 Global step 180 Train loss 8.980004 on epoch=89
03/08/2022 18:45:18 - INFO - __main__ - Step 190 Global step 190 Train loss 8.846939 on epoch=94
03/08/2022 18:45:23 - INFO - __main__ - Step 200 Global step 200 Train loss 8.307763 on epoch=99
03/08/2022 18:45:24 - INFO - __main__ - Global step 200 Train loss 9.037254 EM 0.0 on epoch=99
03/08/2022 18:45:28 - INFO - __main__ - Step 210 Global step 210 Train loss 8.607520 on epoch=104
03/08/2022 18:45:33 - INFO - __main__ - Step 220 Global step 220 Train loss 7.680250 on epoch=109
03/08/2022 18:45:38 - INFO - __main__ - Step 230 Global step 230 Train loss 7.735811 on epoch=114
03/08/2022 18:45:43 - INFO - __main__ - Step 240 Global step 240 Train loss 7.435762 on epoch=119
03/08/2022 18:45:47 - INFO - __main__ - Step 250 Global step 250 Train loss 6.982578 on epoch=124
03/08/2022 18:45:48 - INFO - __main__ - Global step 250 Train loss 7.688385 EM 0.0 on epoch=124
03/08/2022 18:45:53 - INFO - __main__ - Step 260 Global step 260 Train loss 6.997376 on epoch=129
03/08/2022 18:45:58 - INFO - __main__ - Step 270 Global step 270 Train loss 6.727931 on epoch=134
03/08/2022 18:46:02 - INFO - __main__ - Step 280 Global step 280 Train loss 6.713638 on epoch=139
03/08/2022 18:46:07 - INFO - __main__ - Step 290 Global step 290 Train loss 6.452781 on epoch=144
03/08/2022 18:46:12 - INFO - __main__ - Step 300 Global step 300 Train loss 6.463544 on epoch=149
03/08/2022 18:46:13 - INFO - __main__ - Global step 300 Train loss 6.671054 EM 0.0 on epoch=149
03/08/2022 18:46:17 - INFO - __main__ - Step 310 Global step 310 Train loss 6.157945 on epoch=154
03/08/2022 18:46:22 - INFO - __main__ - Step 320 Global step 320 Train loss 6.032699 on epoch=159
03/08/2022 18:46:27 - INFO - __main__ - Step 330 Global step 330 Train loss 5.535845 on epoch=164
03/08/2022 18:46:32 - INFO - __main__ - Step 340 Global step 340 Train loss 5.379111 on epoch=169
03/08/2022 18:46:36 - INFO - __main__ - Step 350 Global step 350 Train loss 4.844237 on epoch=174
03/08/2022 18:46:37 - INFO - __main__ - Global step 350 Train loss 5.589967 EM 0.0 on epoch=174
03/08/2022 18:46:42 - INFO - __main__ - Step 360 Global step 360 Train loss 4.899179 on epoch=179
03/08/2022 18:46:47 - INFO - __main__ - Step 370 Global step 370 Train loss 4.791266 on epoch=184
03/08/2022 18:46:51 - INFO - __main__ - Step 380 Global step 380 Train loss 4.394280 on epoch=189
03/08/2022 18:46:56 - INFO - __main__ - Step 390 Global step 390 Train loss 4.233732 on epoch=194
03/08/2022 18:47:01 - INFO - __main__ - Step 400 Global step 400 Train loss 4.316030 on epoch=199
03/08/2022 18:47:01 - INFO - __main__ - Global step 400 Train loss 4.526897 EM 0.0 on epoch=199
03/08/2022 18:47:06 - INFO - __main__ - Step 410 Global step 410 Train loss 3.868865 on epoch=204
03/08/2022 18:47:11 - INFO - __main__ - Step 420 Global step 420 Train loss 3.753970 on epoch=209
03/08/2022 18:47:16 - INFO - __main__ - Step 430 Global step 430 Train loss 3.643304 on epoch=214
03/08/2022 18:47:21 - INFO - __main__ - Step 440 Global step 440 Train loss 3.358409 on epoch=219
03/08/2022 18:47:25 - INFO - __main__ - Step 450 Global step 450 Train loss 3.138919 on epoch=224
03/08/2022 18:47:26 - INFO - __main__ - Global step 450 Train loss 3.552693 EM 0.0 on epoch=224
03/08/2022 18:47:31 - INFO - __main__ - Step 460 Global step 460 Train loss 2.999937 on epoch=229
03/08/2022 18:47:36 - INFO - __main__ - Step 470 Global step 470 Train loss 3.166857 on epoch=234
03/08/2022 18:47:41 - INFO - __main__ - Step 480 Global step 480 Train loss 2.546609 on epoch=239
03/08/2022 18:47:45 - INFO - __main__ - Step 490 Global step 490 Train loss 2.647700 on epoch=244
03/08/2022 18:47:50 - INFO - __main__ - Step 500 Global step 500 Train loss 2.667511 on epoch=249
03/08/2022 18:47:51 - INFO - __main__ - Global step 500 Train loss 2.805723 EM 0.0 on epoch=249
03/08/2022 18:47:56 - INFO - __main__ - Step 510 Global step 510 Train loss 2.717798 on epoch=254
03/08/2022 18:48:01 - INFO - __main__ - Step 520 Global step 520 Train loss 2.415378 on epoch=259
03/08/2022 18:48:05 - INFO - __main__ - Step 530 Global step 530 Train loss 2.323331 on epoch=264
03/08/2022 18:48:10 - INFO - __main__ - Step 540 Global step 540 Train loss 2.080748 on epoch=269
03/08/2022 18:48:15 - INFO - __main__ - Step 550 Global step 550 Train loss 2.333452 on epoch=274
03/08/2022 18:48:16 - INFO - __main__ - Global step 550 Train loss 2.374141 EM 0.0 on epoch=274
03/08/2022 18:48:20 - INFO - __main__ - Step 560 Global step 560 Train loss 2.279894 on epoch=279
03/08/2022 18:48:25 - INFO - __main__ - Step 570 Global step 570 Train loss 2.481958 on epoch=284
03/08/2022 18:48:30 - INFO - __main__ - Step 580 Global step 580 Train loss 2.332525 on epoch=289
03/08/2022 18:48:35 - INFO - __main__ - Step 590 Global step 590 Train loss 2.182814 on epoch=294
03/08/2022 18:48:40 - INFO - __main__ - Step 600 Global step 600 Train loss 2.358635 on epoch=299
03/08/2022 18:48:40 - INFO - __main__ - Global step 600 Train loss 2.327165 EM 0.0 on epoch=299
03/08/2022 18:48:40 - INFO - __main__ - save last model!
03/08/2022 18:48:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:48:41 - INFO - __main__ - Printing 3 examples
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 18:48:41 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 18:48:41 - INFO - __main__ - ['Jim fargiano']
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 18:48:41 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 18:48:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 18:48:41 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:48:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:48:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:48:41 - INFO - __main__ - Printing 3 examples
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 18:48:41 - INFO - __main__ - ['Find Local Electric']
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 18:48:41 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 18:48:41 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 18:48:41 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 18:48:41 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:48:41 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:48:41 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:48:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:48:51 - INFO - __main__ - Starting training!
03/08/2022 18:49:19 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 18:49:20 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 18:49:20 - INFO - __main__ - Printing 3 examples
03/08/2022 18:49:20 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 18:49:20 - INFO - __main__ - ['Whole S M']
03/08/2022 18:49:20 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 18:49:20 - INFO - __main__ - ['pushin Daisies']
03/08/2022 18:49:20 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 18:49:20 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 18:49:20 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:49:20 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:49:22 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 18:59:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_42_0.0001_8_predictions.txt
03/08/2022 18:59:21 - INFO - __main__ - EM on test data: 0.0000
03/08/2022 18:59:22 - INFO - __main__ - prefix=crawl_domain_32_42, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/08/2022 18:59:22 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0005, bsz=8 ...
03/08/2022 18:59:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:59:23 - INFO - __main__ - Printing 3 examples
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 18:59:23 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 18:59:23 - INFO - __main__ - ['Jim fargiano']
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 18:59:23 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 18:59:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 18:59:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:59:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 18:59:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 18:59:23 - INFO - __main__ - Printing 3 examples
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 18:59:23 - INFO - __main__ - ['Find Local Electric']
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 18:59:23 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 18:59:23 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 18:59:23 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 18:59:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 18:59:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 18:59:23 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 18:59:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 18:59:32 - INFO - __main__ - Starting training!
03/08/2022 18:59:36 - INFO - __main__ - Step 10 Global step 10 Train loss 21.392817 on epoch=4
03/08/2022 18:59:41 - INFO - __main__ - Step 20 Global step 20 Train loss 15.385191 on epoch=9
03/08/2022 18:59:45 - INFO - __main__ - Step 30 Global step 30 Train loss 12.623703 on epoch=14
03/08/2022 18:59:50 - INFO - __main__ - Step 40 Global step 40 Train loss 10.054950 on epoch=19
03/08/2022 18:59:55 - INFO - __main__ - Step 50 Global step 50 Train loss 8.689352 on epoch=24
03/08/2022 18:59:58 - INFO - __main__ - Global step 50 Train loss 13.629202 EM 0.0 on epoch=24
03/08/2022 19:00:34 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 19:00:39 - INFO - __main__ - Step 60 Global step 60 Train loss 7.242790 on epoch=29
03/08/2022 19:00:44 - INFO - __main__ - Step 70 Global step 70 Train loss 6.372913 on epoch=34
03/08/2022 19:00:48 - INFO - __main__ - Step 80 Global step 80 Train loss 5.375432 on epoch=39
03/08/2022 19:00:53 - INFO - __main__ - Step 90 Global step 90 Train loss 4.711534 on epoch=44
03/08/2022 19:00:58 - INFO - __main__ - Step 100 Global step 100 Train loss 3.930841 on epoch=49
03/08/2022 19:00:59 - INFO - __main__ - Global step 100 Train loss 5.526703 EM 0.0 on epoch=49
03/08/2022 19:01:04 - INFO - __main__ - Step 110 Global step 110 Train loss 3.264167 on epoch=54
03/08/2022 19:01:08 - INFO - __main__ - Step 120 Global step 120 Train loss 2.736776 on epoch=59
03/08/2022 19:01:13 - INFO - __main__ - Step 130 Global step 130 Train loss 2.644321 on epoch=64
03/08/2022 19:01:18 - INFO - __main__ - Step 140 Global step 140 Train loss 2.552358 on epoch=69
03/08/2022 19:01:23 - INFO - __main__ - Step 150 Global step 150 Train loss 2.536237 on epoch=74
03/08/2022 19:01:23 - INFO - __main__ - Global step 150 Train loss 2.746772 EM 0.0 on epoch=74
03/08/2022 19:01:28 - INFO - __main__ - Step 160 Global step 160 Train loss 2.396605 on epoch=79
03/08/2022 19:01:33 - INFO - __main__ - Step 170 Global step 170 Train loss 2.070202 on epoch=84
03/08/2022 19:01:38 - INFO - __main__ - Step 180 Global step 180 Train loss 2.136462 on epoch=89
03/08/2022 19:01:42 - INFO - __main__ - Step 190 Global step 190 Train loss 2.067081 on epoch=94
03/08/2022 19:01:47 - INFO - __main__ - Step 200 Global step 200 Train loss 2.175739 on epoch=99
03/08/2022 19:01:48 - INFO - __main__ - Global step 200 Train loss 2.169218 EM 0.0 on epoch=99
03/08/2022 19:01:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.945720 on epoch=104
03/08/2022 19:01:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.664150 on epoch=109
03/08/2022 19:02:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.724065 on epoch=114
03/08/2022 19:02:07 - INFO - __main__ - Step 240 Global step 240 Train loss 1.592459 on epoch=119
03/08/2022 19:02:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.523502 on epoch=124
03/08/2022 19:02:12 - INFO - __main__ - Global step 250 Train loss 1.689979 EM 0.0 on epoch=124
03/08/2022 19:02:17 - INFO - __main__ - Step 260 Global step 260 Train loss 1.344600 on epoch=129
03/08/2022 19:02:21 - INFO - __main__ - Step 270 Global step 270 Train loss 1.617836 on epoch=134
03/08/2022 19:02:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.499631 on epoch=139
03/08/2022 19:02:31 - INFO - __main__ - Step 290 Global step 290 Train loss 1.329859 on epoch=144
03/08/2022 19:02:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.399770 on epoch=149
03/08/2022 19:02:36 - INFO - __main__ - Global step 300 Train loss 1.438339 EM 0.0 on epoch=149
03/08/2022 19:02:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.359706 on epoch=154
03/08/2022 19:02:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.316322 on epoch=159
03/08/2022 19:02:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.241067 on epoch=164
03/08/2022 19:02:55 - INFO - __main__ - Step 340 Global step 340 Train loss 1.168339 on epoch=169
03/08/2022 19:03:00 - INFO - __main__ - Step 350 Global step 350 Train loss 1.294265 on epoch=174
03/08/2022 19:03:00 - INFO - __main__ - Global step 350 Train loss 1.275940 EM 0.0 on epoch=174
03/08/2022 19:03:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.190042 on epoch=179
03/08/2022 19:03:10 - INFO - __main__ - Step 370 Global step 370 Train loss 1.148447 on epoch=184
03/08/2022 19:03:15 - INFO - __main__ - Step 380 Global step 380 Train loss 1.067672 on epoch=189
03/08/2022 19:03:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.975841 on epoch=194
03/08/2022 19:03:24 - INFO - __main__ - Step 400 Global step 400 Train loss 1.127317 on epoch=199
03/08/2022 19:03:25 - INFO - __main__ - Global step 400 Train loss 1.101864 EM 0.0 on epoch=199
03/08/2022 19:03:30 - INFO - __main__ - Step 410 Global step 410 Train loss 1.025315 on epoch=204
03/08/2022 19:03:34 - INFO - __main__ - Step 420 Global step 420 Train loss 1.017051 on epoch=209
03/08/2022 19:03:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.060579 on epoch=214
03/08/2022 19:03:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.961013 on epoch=219
03/08/2022 19:03:49 - INFO - __main__ - Step 450 Global step 450 Train loss 1.006285 on epoch=224
03/08/2022 19:03:49 - INFO - __main__ - Global step 450 Train loss 1.014049 EM 0.0 on epoch=224
03/08/2022 19:03:54 - INFO - __main__ - Step 460 Global step 460 Train loss 1.022498 on epoch=229
03/08/2022 19:03:59 - INFO - __main__ - Step 470 Global step 470 Train loss 1.008595 on epoch=234
03/08/2022 19:04:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.990247 on epoch=239
03/08/2022 19:04:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.917587 on epoch=244
03/08/2022 19:04:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.823242 on epoch=249
03/08/2022 19:04:13 - INFO - __main__ - Global step 500 Train loss 0.952434 EM 0.0 on epoch=249
03/08/2022 19:04:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.945854 on epoch=254
03/08/2022 19:04:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.835644 on epoch=259
03/08/2022 19:04:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.877343 on epoch=264
03/08/2022 19:04:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.835283 on epoch=269
03/08/2022 19:04:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.788883 on epoch=274
03/08/2022 19:04:37 - INFO - __main__ - Global step 550 Train loss 0.856602 EM 0.0 on epoch=274
03/08/2022 19:04:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.793078 on epoch=279
03/08/2022 19:04:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.776926 on epoch=284
03/08/2022 19:04:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.843690 on epoch=289
03/08/2022 19:04:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.649935 on epoch=294
03/08/2022 19:05:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.772904 on epoch=299
03/08/2022 19:05:01 - INFO - __main__ - Global step 600 Train loss 0.767307 EM 0.0 on epoch=299
03/08/2022 19:05:01 - INFO - __main__ - save last model!
03/08/2022 19:05:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:05:02 - INFO - __main__ - Printing 3 examples
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:05:02 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:05:02 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:05:02 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:05:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:05:02 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:05:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:05:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:05:02 - INFO - __main__ - Printing 3 examples
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:05:02 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:05:02 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:05:02 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:05:02 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:05:02 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:05:02 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:05:02 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:05:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:05:12 - INFO - __main__ - Starting training!
03/08/2022 19:05:42 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:05:42 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 19:05:42 - INFO - __main__ - Printing 3 examples
03/08/2022 19:05:42 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 19:05:42 - INFO - __main__ - ['Whole S M']
03/08/2022 19:05:42 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 19:05:42 - INFO - __main__ - ['pushin Daisies']
03/08/2022 19:05:42 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 19:05:42 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 19:05:42 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:05:43 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:05:45 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 19:07:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0005_8_predictions.txt
03/08/2022 19:07:40 - INFO - __main__ - EM on test data: 0.0143
03/08/2022 19:07:41 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.014336917562724014
03/08/2022 19:07:41 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0003, bsz=8 ...
03/08/2022 19:07:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:07:42 - INFO - __main__ - Printing 3 examples
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:07:42 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:07:42 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:07:42 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:07:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:07:42 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:07:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:07:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:07:42 - INFO - __main__ - Printing 3 examples
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:07:42 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:07:42 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:07:42 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:07:42 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:07:42 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:07:42 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:07:42 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:07:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:07:52 - INFO - __main__ - Starting training!
03/08/2022 19:07:56 - INFO - __main__ - Step 10 Global step 10 Train loss 22.239128 on epoch=4
03/08/2022 19:08:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.652187 on epoch=9
03/08/2022 19:08:05 - INFO - __main__ - Step 30 Global step 30 Train loss 16.428905 on epoch=14
03/08/2022 19:08:10 - INFO - __main__ - Step 40 Global step 40 Train loss 14.145081 on epoch=19
03/08/2022 19:08:15 - INFO - __main__ - Step 50 Global step 50 Train loss 12.248583 on epoch=24
03/08/2022 19:08:25 - INFO - __main__ - Global step 50 Train loss 16.742777 EM 0.0 on epoch=24
03/08/2022 19:08:59 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 19:09:04 - INFO - __main__ - Step 60 Global step 60 Train loss 11.215422 on epoch=29
03/08/2022 19:09:09 - INFO - __main__ - Step 70 Global step 70 Train loss 9.937567 on epoch=34
03/08/2022 19:09:14 - INFO - __main__ - Step 80 Global step 80 Train loss 9.538462 on epoch=39
03/08/2022 19:09:19 - INFO - __main__ - Step 90 Global step 90 Train loss 8.449139 on epoch=44
03/08/2022 19:09:23 - INFO - __main__ - Step 100 Global step 100 Train loss 8.095617 on epoch=49
03/08/2022 19:09:24 - INFO - __main__ - Global step 100 Train loss 9.447242 EM 0.03125 on epoch=49
03/08/2022 19:09:58 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 19:10:03 - INFO - __main__ - Step 110 Global step 110 Train loss 6.836401 on epoch=54
03/08/2022 19:10:08 - INFO - __main__ - Step 120 Global step 120 Train loss 6.284898 on epoch=59
03/08/2022 19:10:13 - INFO - __main__ - Step 130 Global step 130 Train loss 5.489193 on epoch=64
03/08/2022 19:10:18 - INFO - __main__ - Step 140 Global step 140 Train loss 5.362124 on epoch=69
03/08/2022 19:10:22 - INFO - __main__ - Step 150 Global step 150 Train loss 5.037354 on epoch=74
03/08/2022 19:10:23 - INFO - __main__ - Global step 150 Train loss 5.801994 EM 0.0 on epoch=74
03/08/2022 19:10:28 - INFO - __main__ - Step 160 Global step 160 Train loss 4.193202 on epoch=79
03/08/2022 19:10:32 - INFO - __main__ - Step 170 Global step 170 Train loss 3.943006 on epoch=84
03/08/2022 19:10:37 - INFO - __main__ - Step 180 Global step 180 Train loss 3.913870 on epoch=89
03/08/2022 19:10:42 - INFO - __main__ - Step 190 Global step 190 Train loss 3.088883 on epoch=94
03/08/2022 19:10:47 - INFO - __main__ - Step 200 Global step 200 Train loss 3.114126 on epoch=99
03/08/2022 19:10:47 - INFO - __main__ - Global step 200 Train loss 3.650617 EM 0.0 on epoch=99
03/08/2022 19:10:52 - INFO - __main__ - Step 210 Global step 210 Train loss 2.648342 on epoch=104
03/08/2022 19:10:57 - INFO - __main__ - Step 220 Global step 220 Train loss 2.276562 on epoch=109
03/08/2022 19:11:02 - INFO - __main__ - Step 230 Global step 230 Train loss 2.377223 on epoch=114
03/08/2022 19:11:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.577532 on epoch=119
03/08/2022 19:11:11 - INFO - __main__ - Step 250 Global step 250 Train loss 2.365644 on epoch=124
03/08/2022 19:11:12 - INFO - __main__ - Global step 250 Train loss 2.449061 EM 0.0 on epoch=124
03/08/2022 19:11:17 - INFO - __main__ - Step 260 Global step 260 Train loss 2.104185 on epoch=129
03/08/2022 19:11:21 - INFO - __main__ - Step 270 Global step 270 Train loss 2.079000 on epoch=134
03/08/2022 19:11:26 - INFO - __main__ - Step 280 Global step 280 Train loss 2.116897 on epoch=139
03/08/2022 19:11:31 - INFO - __main__ - Step 290 Global step 290 Train loss 2.007790 on epoch=144
03/08/2022 19:11:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.812988 on epoch=149
03/08/2022 19:11:36 - INFO - __main__ - Global step 300 Train loss 2.024172 EM 0.0 on epoch=149
03/08/2022 19:11:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.798351 on epoch=154
03/08/2022 19:11:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.979113 on epoch=159
03/08/2022 19:11:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.954975 on epoch=164
03/08/2022 19:11:55 - INFO - __main__ - Step 340 Global step 340 Train loss 1.669406 on epoch=169
03/08/2022 19:12:00 - INFO - __main__ - Step 350 Global step 350 Train loss 1.779647 on epoch=174
03/08/2022 19:12:01 - INFO - __main__ - Global step 350 Train loss 1.836298 EM 0.0 on epoch=174
03/08/2022 19:12:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.699513 on epoch=179
03/08/2022 19:12:10 - INFO - __main__ - Step 370 Global step 370 Train loss 1.733286 on epoch=184
03/08/2022 19:12:15 - INFO - __main__ - Step 380 Global step 380 Train loss 1.625175 on epoch=189
03/08/2022 19:12:20 - INFO - __main__ - Step 390 Global step 390 Train loss 1.666866 on epoch=194
03/08/2022 19:12:25 - INFO - __main__ - Step 400 Global step 400 Train loss 1.530119 on epoch=199
03/08/2022 19:12:25 - INFO - __main__ - Global step 400 Train loss 1.650992 EM 0.0 on epoch=199
03/08/2022 19:12:30 - INFO - __main__ - Step 410 Global step 410 Train loss 1.461432 on epoch=204
03/08/2022 19:12:35 - INFO - __main__ - Step 420 Global step 420 Train loss 1.580910 on epoch=209
03/08/2022 19:12:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.468972 on epoch=214
03/08/2022 19:12:44 - INFO - __main__ - Step 440 Global step 440 Train loss 1.403731 on epoch=219
03/08/2022 19:12:49 - INFO - __main__ - Step 450 Global step 450 Train loss 1.531794 on epoch=224
03/08/2022 19:12:49 - INFO - __main__ - Global step 450 Train loss 1.489368 EM 0.0 on epoch=224
03/08/2022 19:12:54 - INFO - __main__ - Step 460 Global step 460 Train loss 1.347290 on epoch=229
03/08/2022 19:12:59 - INFO - __main__ - Step 470 Global step 470 Train loss 1.357488 on epoch=234
03/08/2022 19:13:04 - INFO - __main__ - Step 480 Global step 480 Train loss 1.358803 on epoch=239
03/08/2022 19:13:08 - INFO - __main__ - Step 490 Global step 490 Train loss 1.260397 on epoch=244
03/08/2022 19:13:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.377137 on epoch=249
03/08/2022 19:13:14 - INFO - __main__ - Global step 500 Train loss 1.340223 EM 0.0 on epoch=249
03/08/2022 19:13:19 - INFO - __main__ - Step 510 Global step 510 Train loss 1.392755 on epoch=254
03/08/2022 19:13:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.380357 on epoch=259
03/08/2022 19:13:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.395746 on epoch=264
03/08/2022 19:13:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.140144 on epoch=269
03/08/2022 19:13:38 - INFO - __main__ - Step 550 Global step 550 Train loss 1.226855 on epoch=274
03/08/2022 19:13:38 - INFO - __main__ - Global step 550 Train loss 1.307171 EM 0.0 on epoch=274
03/08/2022 19:13:43 - INFO - __main__ - Step 560 Global step 560 Train loss 1.203625 on epoch=279
03/08/2022 19:13:47 - INFO - __main__ - Step 570 Global step 570 Train loss 1.167034 on epoch=284
03/08/2022 19:13:52 - INFO - __main__ - Step 580 Global step 580 Train loss 1.062467 on epoch=289
03/08/2022 19:13:57 - INFO - __main__ - Step 590 Global step 590 Train loss 1.101015 on epoch=294
03/08/2022 19:14:02 - INFO - __main__ - Step 600 Global step 600 Train loss 1.171195 on epoch=299
03/08/2022 19:14:02 - INFO - __main__ - Global step 600 Train loss 1.141067 EM 0.0 on epoch=299
03/08/2022 19:14:02 - INFO - __main__ - save last model!
03/08/2022 19:14:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:14:03 - INFO - __main__ - Printing 3 examples
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:14:03 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:14:03 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:14:03 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:14:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:14:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:14:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:14:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:14:03 - INFO - __main__ - Printing 3 examples
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:14:03 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:14:03 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:14:03 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:14:03 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:14:03 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:14:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:14:03 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:14:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:14:12 - INFO - __main__ - Starting training!
03/08/2022 19:14:43 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:14:44 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 19:14:44 - INFO - __main__ - Printing 3 examples
03/08/2022 19:14:44 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 19:14:44 - INFO - __main__ - ['Whole S M']
03/08/2022 19:14:44 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 19:14:44 - INFO - __main__ - ['pushin Daisies']
03/08/2022 19:14:44 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 19:14:44 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 19:14:44 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:14:44 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:14:46 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 19:15:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0003_8_predictions.txt
03/08/2022 19:15:20 - INFO - __main__ - EM on test data: 0.0041
03/08/2022 19:15:22 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.00409626216077829
03/08/2022 19:15:22 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0002, bsz=8 ...
03/08/2022 19:15:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:15:23 - INFO - __main__ - Printing 3 examples
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:15:23 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:15:23 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:15:23 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:15:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:15:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:15:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:15:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:15:23 - INFO - __main__ - Printing 3 examples
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:15:23 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:15:23 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:15:23 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:15:23 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:15:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:15:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:15:23 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:15:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:15:32 - INFO - __main__ - Starting training!
03/08/2022 19:15:36 - INFO - __main__ - Step 10 Global step 10 Train loss 21.617632 on epoch=4
03/08/2022 19:15:41 - INFO - __main__ - Step 20 Global step 20 Train loss 18.362301 on epoch=9
03/08/2022 19:15:45 - INFO - __main__ - Step 30 Global step 30 Train loss 14.898984 on epoch=14
03/08/2022 19:15:50 - INFO - __main__ - Step 40 Global step 40 Train loss 13.371157 on epoch=19
03/08/2022 19:15:55 - INFO - __main__ - Step 50 Global step 50 Train loss 11.465695 on epoch=24
03/08/2022 19:16:03 - INFO - __main__ - Global step 50 Train loss 15.943153 EM 0.0 on epoch=24
03/08/2022 19:16:37 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 19:16:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.655437 on epoch=29
03/08/2022 19:16:46 - INFO - __main__ - Step 70 Global step 70 Train loss 9.883430 on epoch=34
03/08/2022 19:16:51 - INFO - __main__ - Step 80 Global step 80 Train loss 9.279242 on epoch=39
03/08/2022 19:16:56 - INFO - __main__ - Step 90 Global step 90 Train loss 8.270137 on epoch=44
03/08/2022 19:17:01 - INFO - __main__ - Step 100 Global step 100 Train loss 7.313438 on epoch=49
03/08/2022 19:17:01 - INFO - __main__ - Global step 100 Train loss 9.080337 EM 0.03125 on epoch=49
03/08/2022 19:17:35 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/08/2022 19:17:40 - INFO - __main__ - Step 110 Global step 110 Train loss 7.006866 on epoch=54
03/08/2022 19:17:45 - INFO - __main__ - Step 120 Global step 120 Train loss 6.698251 on epoch=59
03/08/2022 19:17:49 - INFO - __main__ - Step 130 Global step 130 Train loss 6.651324 on epoch=64
03/08/2022 19:17:54 - INFO - __main__ - Step 140 Global step 140 Train loss 6.132501 on epoch=69
03/08/2022 19:17:59 - INFO - __main__ - Step 150 Global step 150 Train loss 5.976411 on epoch=74
03/08/2022 19:18:00 - INFO - __main__ - Global step 150 Train loss 6.493071 EM 0.03125 on epoch=74
03/08/2022 19:18:04 - INFO - __main__ - Step 160 Global step 160 Train loss 5.732610 on epoch=79
03/08/2022 19:18:09 - INFO - __main__ - Step 170 Global step 170 Train loss 5.585776 on epoch=84
03/08/2022 19:18:14 - INFO - __main__ - Step 180 Global step 180 Train loss 4.990995 on epoch=89
03/08/2022 19:18:18 - INFO - __main__ - Step 190 Global step 190 Train loss 4.749539 on epoch=94
03/08/2022 19:18:23 - INFO - __main__ - Step 200 Global step 200 Train loss 4.853705 on epoch=99
03/08/2022 19:18:24 - INFO - __main__ - Global step 200 Train loss 5.182525 EM 0.03125 on epoch=99
03/08/2022 19:18:28 - INFO - __main__ - Step 210 Global step 210 Train loss 4.077638 on epoch=104
03/08/2022 19:18:33 - INFO - __main__ - Step 220 Global step 220 Train loss 3.726320 on epoch=109
03/08/2022 19:18:38 - INFO - __main__ - Step 230 Global step 230 Train loss 3.760906 on epoch=114
03/08/2022 19:18:43 - INFO - __main__ - Step 240 Global step 240 Train loss 3.227713 on epoch=119
03/08/2022 19:18:47 - INFO - __main__ - Step 250 Global step 250 Train loss 3.092629 on epoch=124
03/08/2022 19:18:48 - INFO - __main__ - Global step 250 Train loss 3.577041 EM 0.03125 on epoch=124
03/08/2022 19:18:53 - INFO - __main__ - Step 260 Global step 260 Train loss 2.724927 on epoch=129
03/08/2022 19:18:57 - INFO - __main__ - Step 270 Global step 270 Train loss 2.722033 on epoch=134
03/08/2022 19:19:02 - INFO - __main__ - Step 280 Global step 280 Train loss 2.613707 on epoch=139
03/08/2022 19:19:07 - INFO - __main__ - Step 290 Global step 290 Train loss 2.666839 on epoch=144
03/08/2022 19:19:12 - INFO - __main__ - Step 300 Global step 300 Train loss 2.723284 on epoch=149
03/08/2022 19:19:12 - INFO - __main__ - Global step 300 Train loss 2.690157 EM 0.0 on epoch=149
03/08/2022 19:19:17 - INFO - __main__ - Step 310 Global step 310 Train loss 2.631853 on epoch=154
03/08/2022 19:19:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.293093 on epoch=159
03/08/2022 19:19:26 - INFO - __main__ - Step 330 Global step 330 Train loss 2.091580 on epoch=164
03/08/2022 19:19:31 - INFO - __main__ - Step 340 Global step 340 Train loss 2.446470 on epoch=169
03/08/2022 19:19:36 - INFO - __main__ - Step 350 Global step 350 Train loss 2.029450 on epoch=174
03/08/2022 19:19:37 - INFO - __main__ - Global step 350 Train loss 2.298489 EM 0.0 on epoch=174
03/08/2022 19:19:41 - INFO - __main__ - Step 360 Global step 360 Train loss 2.346496 on epoch=179
03/08/2022 19:19:46 - INFO - __main__ - Step 370 Global step 370 Train loss 2.120278 on epoch=184
03/08/2022 19:19:51 - INFO - __main__ - Step 380 Global step 380 Train loss 1.981319 on epoch=189
03/08/2022 19:19:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.904577 on epoch=194
03/08/2022 19:20:00 - INFO - __main__ - Step 400 Global step 400 Train loss 1.915955 on epoch=199
03/08/2022 19:20:01 - INFO - __main__ - Global step 400 Train loss 2.053725 EM 0.03125 on epoch=199
03/08/2022 19:20:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.877183 on epoch=204
03/08/2022 19:20:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.772429 on epoch=209
03/08/2022 19:20:16 - INFO - __main__ - Step 430 Global step 430 Train loss 1.657562 on epoch=214
03/08/2022 19:20:20 - INFO - __main__ - Step 440 Global step 440 Train loss 2.038433 on epoch=219
03/08/2022 19:20:25 - INFO - __main__ - Step 450 Global step 450 Train loss 1.949430 on epoch=224
03/08/2022 19:20:26 - INFO - __main__ - Global step 450 Train loss 1.859007 EM 0.03125 on epoch=224
03/08/2022 19:20:31 - INFO - __main__ - Step 460 Global step 460 Train loss 1.718562 on epoch=229
03/08/2022 19:20:35 - INFO - __main__ - Step 470 Global step 470 Train loss 1.887143 on epoch=234
03/08/2022 19:20:40 - INFO - __main__ - Step 480 Global step 480 Train loss 1.896161 on epoch=239
03/08/2022 19:20:45 - INFO - __main__ - Step 490 Global step 490 Train loss 1.590579 on epoch=244
03/08/2022 19:20:50 - INFO - __main__ - Step 500 Global step 500 Train loss 1.668683 on epoch=249
03/08/2022 19:20:51 - INFO - __main__ - Global step 500 Train loss 1.752226 EM 0.03125 on epoch=249
03/08/2022 19:20:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.621604 on epoch=254
03/08/2022 19:21:00 - INFO - __main__ - Step 520 Global step 520 Train loss 1.651054 on epoch=259
03/08/2022 19:21:05 - INFO - __main__ - Step 530 Global step 530 Train loss 1.569062 on epoch=264
03/08/2022 19:21:10 - INFO - __main__ - Step 540 Global step 540 Train loss 1.539647 on epoch=269
03/08/2022 19:21:15 - INFO - __main__ - Step 550 Global step 550 Train loss 1.548244 on epoch=274
03/08/2022 19:21:15 - INFO - __main__ - Global step 550 Train loss 1.585922 EM 0.03125 on epoch=274
03/08/2022 19:21:20 - INFO - __main__ - Step 560 Global step 560 Train loss 1.697539 on epoch=279
03/08/2022 19:21:25 - INFO - __main__ - Step 570 Global step 570 Train loss 1.488209 on epoch=284
03/08/2022 19:21:30 - INFO - __main__ - Step 580 Global step 580 Train loss 1.307869 on epoch=289
03/08/2022 19:21:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.493213 on epoch=294
03/08/2022 19:21:39 - INFO - __main__ - Step 600 Global step 600 Train loss 1.470385 on epoch=299
03/08/2022 19:21:40 - INFO - __main__ - Global step 600 Train loss 1.491443 EM 0.0 on epoch=299
03/08/2022 19:21:40 - INFO - __main__ - save last model!
03/08/2022 19:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:21:41 - INFO - __main__ - Printing 3 examples
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:21:41 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:21:41 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:21:41 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:21:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:21:41 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:21:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:21:41 - INFO - __main__ - Printing 3 examples
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:21:41 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:21:41 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:21:41 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:21:41 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:21:41 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:21:41 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:21:41 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:21:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:21:51 - INFO - __main__ - Starting training!
03/08/2022 19:22:20 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:22:21 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 19:22:21 - INFO - __main__ - Printing 3 examples
03/08/2022 19:22:21 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 19:22:21 - INFO - __main__ - ['Whole S M']
03/08/2022 19:22:21 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 19:22:21 - INFO - __main__ - ['pushin Daisies']
03/08/2022 19:22:21 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 19:22:21 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 19:22:21 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:22:22 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:22:24 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 19:23:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0002_8_predictions.txt
03/08/2022 19:23:11 - INFO - __main__ - EM on test data: 0.0466
03/08/2022 19:23:11 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.04659498207885305
03/08/2022 19:23:11 - INFO - __main__ - Running ... prefix=crawl_domain_32_87, lr=0.0001, bsz=8 ...
03/08/2022 19:23:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:23:12 - INFO - __main__ - Printing 3 examples
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] thefunhouseseattle
03/08/2022 19:23:12 - INFO - __main__ - ['The Fun house Seattle']
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] jimfargiano
03/08/2022 19:23:12 - INFO - __main__ - ['Jim fargiano']
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] inspiredwordnyc
03/08/2022 19:23:12 - INFO - __main__ - ['Inspired Word N Y C']
03/08/2022 19:23:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:23:12 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:23:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:23:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:23:12 - INFO - __main__ - Printing 3 examples
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] findlocalelectric
03/08/2022 19:23:12 - INFO - __main__ - ['Find Local Electric']
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] dilenabrothers
03/08/2022 19:23:12 - INFO - __main__ - ['Dilena Brothers']
03/08/2022 19:23:12 - INFO - __main__ -  [crawl_domain] redboypizza
03/08/2022 19:23:12 - INFO - __main__ - ['Red Boy Pizza']
03/08/2022 19:23:12 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:23:12 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:23:12 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:23:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:23:21 - INFO - __main__ - Starting training!
03/08/2022 19:23:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.520102 on epoch=4
03/08/2022 19:23:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.896761 on epoch=9
03/08/2022 19:23:35 - INFO - __main__ - Step 30 Global step 30 Train loss 18.185671 on epoch=14
03/08/2022 19:23:39 - INFO - __main__ - Step 40 Global step 40 Train loss 16.612915 on epoch=19
03/08/2022 19:23:44 - INFO - __main__ - Step 50 Global step 50 Train loss 16.769011 on epoch=24
03/08/2022 19:23:55 - INFO - __main__ - Global step 50 Train loss 18.796892 EM 0.0 on epoch=24
03/08/2022 19:24:30 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 19:24:34 - INFO - __main__ - Step 60 Global step 60 Train loss 15.120436 on epoch=29
03/08/2022 19:24:39 - INFO - __main__ - Step 70 Global step 70 Train loss 13.858754 on epoch=34
03/08/2022 19:24:44 - INFO - __main__ - Step 80 Global step 80 Train loss 13.124771 on epoch=39
03/08/2022 19:24:49 - INFO - __main__ - Step 90 Global step 90 Train loss 12.307360 on epoch=44
03/08/2022 19:24:53 - INFO - __main__ - Step 100 Global step 100 Train loss 11.270405 on epoch=49
03/08/2022 19:25:01 - INFO - __main__ - Global step 100 Train loss 13.136344 EM 0.0 on epoch=49
03/08/2022 19:25:06 - INFO - __main__ - Step 110 Global step 110 Train loss 11.090390 on epoch=54
03/08/2022 19:25:10 - INFO - __main__ - Step 120 Global step 120 Train loss 10.631172 on epoch=59
03/08/2022 19:25:15 - INFO - __main__ - Step 130 Global step 130 Train loss 10.486858 on epoch=64
03/08/2022 19:25:20 - INFO - __main__ - Step 140 Global step 140 Train loss 9.430625 on epoch=69
03/08/2022 19:25:24 - INFO - __main__ - Step 150 Global step 150 Train loss 9.186206 on epoch=74
03/08/2022 19:25:28 - INFO - __main__ - Global step 150 Train loss 10.165051 EM 0.03125 on epoch=74
03/08/2022 19:26:02 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=74, global_step=150
03/08/2022 19:26:06 - INFO - __main__ - Step 160 Global step 160 Train loss 8.963963 on epoch=79
03/08/2022 19:26:11 - INFO - __main__ - Step 170 Global step 170 Train loss 8.772748 on epoch=84
03/08/2022 19:26:16 - INFO - __main__ - Step 180 Global step 180 Train loss 8.103705 on epoch=89
03/08/2022 19:26:20 - INFO - __main__ - Step 190 Global step 190 Train loss 7.855959 on epoch=94
03/08/2022 19:26:25 - INFO - __main__ - Step 200 Global step 200 Train loss 7.394065 on epoch=99
03/08/2022 19:26:27 - INFO - __main__ - Global step 200 Train loss 8.218088 EM 0.03125 on epoch=99
03/08/2022 19:26:31 - INFO - __main__ - Step 210 Global step 210 Train loss 7.367898 on epoch=104
03/08/2022 19:26:36 - INFO - __main__ - Step 220 Global step 220 Train loss 7.086570 on epoch=109
03/08/2022 19:26:41 - INFO - __main__ - Step 230 Global step 230 Train loss 6.779267 on epoch=114
03/08/2022 19:26:45 - INFO - __main__ - Step 240 Global step 240 Train loss 6.530344 on epoch=119
03/08/2022 19:26:50 - INFO - __main__ - Step 250 Global step 250 Train loss 6.336271 on epoch=124
03/08/2022 19:26:51 - INFO - __main__ - Global step 250 Train loss 6.820070 EM 0.03125 on epoch=124
03/08/2022 19:26:56 - INFO - __main__ - Step 260 Global step 260 Train loss 6.309571 on epoch=129
03/08/2022 19:27:00 - INFO - __main__ - Step 270 Global step 270 Train loss 6.344987 on epoch=134
03/08/2022 19:27:05 - INFO - __main__ - Step 280 Global step 280 Train loss 6.071633 on epoch=139
03/08/2022 19:27:10 - INFO - __main__ - Step 290 Global step 290 Train loss 5.658174 on epoch=144
03/08/2022 19:27:14 - INFO - __main__ - Step 300 Global step 300 Train loss 5.988915 on epoch=149
03/08/2022 19:27:15 - INFO - __main__ - Global step 300 Train loss 6.074656 EM 0.03125 on epoch=149
03/08/2022 19:27:20 - INFO - __main__ - Step 310 Global step 310 Train loss 5.443411 on epoch=154
03/08/2022 19:27:24 - INFO - __main__ - Step 320 Global step 320 Train loss 5.522728 on epoch=159
03/08/2022 19:27:29 - INFO - __main__ - Step 330 Global step 330 Train loss 5.614560 on epoch=164
03/08/2022 19:27:34 - INFO - __main__ - Step 340 Global step 340 Train loss 5.133795 on epoch=169
03/08/2022 19:27:39 - INFO - __main__ - Step 350 Global step 350 Train loss 5.708201 on epoch=174
03/08/2022 19:27:39 - INFO - __main__ - Global step 350 Train loss 5.484539 EM 0.03125 on epoch=174
03/08/2022 19:27:44 - INFO - __main__ - Step 360 Global step 360 Train loss 5.263837 on epoch=179
03/08/2022 19:27:49 - INFO - __main__ - Step 370 Global step 370 Train loss 4.960139 on epoch=184
03/08/2022 19:27:53 - INFO - __main__ - Step 380 Global step 380 Train loss 5.276629 on epoch=189
03/08/2022 19:27:58 - INFO - __main__ - Step 390 Global step 390 Train loss 5.091897 on epoch=194
03/08/2022 19:28:03 - INFO - __main__ - Step 400 Global step 400 Train loss 4.658868 on epoch=199
03/08/2022 19:28:04 - INFO - __main__ - Global step 400 Train loss 5.050274 EM 0.03125 on epoch=199
03/08/2022 19:28:08 - INFO - __main__ - Step 410 Global step 410 Train loss 4.531845 on epoch=204
03/08/2022 19:28:13 - INFO - __main__ - Step 420 Global step 420 Train loss 4.361402 on epoch=209
03/08/2022 19:28:18 - INFO - __main__ - Step 430 Global step 430 Train loss 4.154398 on epoch=214
03/08/2022 19:28:23 - INFO - __main__ - Step 440 Global step 440 Train loss 3.490924 on epoch=219
03/08/2022 19:28:27 - INFO - __main__ - Step 450 Global step 450 Train loss 3.706483 on epoch=224
03/08/2022 19:28:28 - INFO - __main__ - Global step 450 Train loss 4.049010 EM 0.03125 on epoch=224
03/08/2022 19:28:33 - INFO - __main__ - Step 460 Global step 460 Train loss 3.368670 on epoch=229
03/08/2022 19:28:37 - INFO - __main__ - Step 470 Global step 470 Train loss 3.633094 on epoch=234
03/08/2022 19:28:42 - INFO - __main__ - Step 480 Global step 480 Train loss 3.327130 on epoch=239
03/08/2022 19:28:47 - INFO - __main__ - Step 490 Global step 490 Train loss 3.350820 on epoch=244
03/08/2022 19:28:51 - INFO - __main__ - Step 500 Global step 500 Train loss 3.384992 on epoch=249
03/08/2022 19:28:52 - INFO - __main__ - Global step 500 Train loss 3.412941 EM 0.03125 on epoch=249
03/08/2022 19:28:57 - INFO - __main__ - Step 510 Global step 510 Train loss 2.747145 on epoch=254
03/08/2022 19:29:02 - INFO - __main__ - Step 520 Global step 520 Train loss 3.093906 on epoch=259
03/08/2022 19:29:06 - INFO - __main__ - Step 530 Global step 530 Train loss 2.899033 on epoch=264
03/08/2022 19:29:11 - INFO - __main__ - Step 540 Global step 540 Train loss 2.692406 on epoch=269
03/08/2022 19:29:16 - INFO - __main__ - Step 550 Global step 550 Train loss 2.447490 on epoch=274
03/08/2022 19:29:16 - INFO - __main__ - Global step 550 Train loss 2.775996 EM 0.03125 on epoch=274
03/08/2022 19:29:21 - INFO - __main__ - Step 560 Global step 560 Train loss 2.362051 on epoch=279
03/08/2022 19:29:26 - INFO - __main__ - Step 570 Global step 570 Train loss 2.366635 on epoch=284
03/08/2022 19:29:31 - INFO - __main__ - Step 580 Global step 580 Train loss 2.743466 on epoch=289
03/08/2022 19:29:36 - INFO - __main__ - Step 590 Global step 590 Train loss 2.585457 on epoch=294
03/08/2022 19:29:40 - INFO - __main__ - Step 600 Global step 600 Train loss 2.743617 on epoch=299
03/08/2022 19:29:41 - INFO - __main__ - Global step 600 Train loss 2.560245 EM 0.0 on epoch=299
03/08/2022 19:29:41 - INFO - __main__ - save last model!
03/08/2022 19:30:21 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:30:21 - INFO - __main__ - Start tokenizing ... 1953 instances
03/08/2022 19:30:21 - INFO - __main__ - Printing 3 examples
03/08/2022 19:30:21 - INFO - __main__ -  [crawl_domain] wholesm
03/08/2022 19:30:21 - INFO - __main__ - ['Whole S M']
03/08/2022 19:30:21 - INFO - __main__ -  [crawl_domain] pushindaisies
03/08/2022 19:30:21 - INFO - __main__ - ['pushin Daisies']
03/08/2022 19:30:21 - INFO - __main__ -  [crawl_domain] bradfordloomis
03/08/2022 19:30:21 - INFO - __main__ - ['Bradford Loomis']
03/08/2022 19:30:21 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:30:22 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:30:24 - INFO - __main__ - Loaded 1953 examples from test data
03/08/2022 19:32:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-crawl_domain/crawl_domain_32_87_0.0001_8_predictions.txt
03/08/2022 19:32:07 - INFO - __main__ - EM on test data: 0.0502
03/08/2022 19:32:08 - INFO - __main__ - prefix=crawl_domain_32_87, lr=0.0001, bsz=8, dev_performance=0.03125, test_performance=0.05017921146953405
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003669261932373047 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13276", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12673, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "13277", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12673, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12673, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (13410): No such process
Task: samsum, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_yr1zw8xf/none_nz79iq3w
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_yr1zw8xf/none_nz79iq3w/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_yr1zw8xf/none_nz79iq3w/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/08/2022 19:32:16 - INFO - __main__ - Namespace(task_dir='data/samsum/', task_name='samsum', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-samsum', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 19:32:16 - INFO - __main__ - models/T5-large-ft-random/singletask-samsum
03/08/2022 19:32:16 - INFO - __main__ - Namespace(task_dir='data/samsum/', task_name='samsum', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-samsum', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 19:32:16 - INFO - __main__ - models/T5-large-ft-random/singletask-samsum
03/08/2022 19:32:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/08/2022 19:32:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/08/2022 19:32:16 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/08/2022 19:32:16 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/08/2022 19:32:16 - INFO - __main__ - args.device: cuda:0
03/08/2022 19:32:16 - INFO - __main__ - Using 2 gpus
03/08/2022 19:32:16 - INFO - __main__ - args.device: cuda:1
03/08/2022 19:32:16 - INFO - __main__ - Fine-tuning the following samples: ['samsum_32_100', 'samsum_32_13', 'samsum_32_21', 'samsum_32_42', 'samsum_32_87']
03/08/2022 19:32:16 - INFO - __main__ - Using 2 gpus
03/08/2022 19:32:16 - INFO - __main__ - Fine-tuning the following samples: ['samsum_32_100', 'samsum_32_13', 'samsum_32_21', 'samsum_32_42', 'samsum_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/08/2022 19:32:24 - INFO - __main__ - Running ... prefix=samsum_32_100, lr=0.0005, bsz=8 ...
03/08/2022 19:32:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:32:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:32:25 - INFO - __main__ - Printing 3 examples
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:32:25 - INFO - __main__ - Printing 3 examples
03/08/2022 19:32:25 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:32:25 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:32:25 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:32:25 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:32:25 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 19:32:25 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:32:25 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:32:25 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:32:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:32:25 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:32:25 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:32:26 - INFO - __main__ - Loaded 32 examples from train data
03/08/2022 19:32:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
use DistributedSampler
03/08/2022 19:32:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:32:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:32:26 - INFO - __main__ - Printing 3 examples
03/08/2022 19:32:26 - INFO - __main__ - Printing 3 examples
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:32:26 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:32:26 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:32:26 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:32:26 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:32:26 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:32:26 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:32:26 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:32:26 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:32:26 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:32:26 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:32:26 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:32:26 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:32:26 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:32:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:32:37 - INFO - __main__ - Starting training!
03/08/2022 19:32:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:32:37 - INFO - __main__ - Starting training!
03/08/2022 19:32:42 - INFO - __main__ - Step 10 Global step 10 Train loss 18.070795 on epoch=4
03/08/2022 19:32:47 - INFO - __main__ - Step 20 Global step 20 Train loss 15.972514 on epoch=9
03/08/2022 19:32:53 - INFO - __main__ - Step 30 Global step 30 Train loss 9.390389 on epoch=14
03/08/2022 19:32:58 - INFO - __main__ - Step 40 Global step 40 Train loss 6.688848 on epoch=19
03/08/2022 19:33:04 - INFO - __main__ - Step 50 Global step 50 Train loss 5.695567 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/08/2022 19:33:17 - INFO - __main__ - Global step 50 Train loss 11.163623 Rouge-L 0.24656587179155243 on epoch=24
03/08/2022 19:33:31 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.24656587179155243 on epoch=24, global_step=50
03/08/2022 19:33:36 - INFO - __main__ - Step 60 Global step 60 Train loss 4.914681 on epoch=29
03/08/2022 19:33:42 - INFO - __main__ - Step 70 Global step 70 Train loss 4.376892 on epoch=34
03/08/2022 19:33:47 - INFO - __main__ - Step 80 Global step 80 Train loss 3.257903 on epoch=39
03/08/2022 19:33:53 - INFO - __main__ - Step 90 Global step 90 Train loss 1.922482 on epoch=44
03/08/2022 19:33:58 - INFO - __main__ - Step 100 Global step 100 Train loss 1.062575 on epoch=49
03/08/2022 19:34:03 - INFO - __main__ - Global step 100 Train loss 3.106907 Rouge-L 0.286941786358452 on epoch=49
03/08/2022 19:34:37 - INFO - __main__ - Saving model with best Rouge-L: 0.24656587179155243 -> 0.286941786358452 on epoch=49, global_step=100
03/08/2022 19:34:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.793754 on epoch=54
03/08/2022 19:34:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.623438 on epoch=59
03/08/2022 19:34:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.496473 on epoch=64
03/08/2022 19:34:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.515145 on epoch=69
03/08/2022 19:35:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.425259 on epoch=74
03/08/2022 19:35:08 - INFO - __main__ - Global step 150 Train loss 0.570814 Rouge-L 0.2275808485988844 on epoch=74
03/08/2022 19:35:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.360452 on epoch=79
03/08/2022 19:35:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.397412 on epoch=84
03/08/2022 19:35:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.360004 on epoch=89
03/08/2022 19:35:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.345239 on epoch=94
03/08/2022 19:35:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.633372 on epoch=99
03/08/2022 19:35:41 - INFO - __main__ - Global step 200 Train loss 0.419296 Rouge-L 0.06408630845352752 on epoch=99
03/08/2022 19:35:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.658947 on epoch=104
03/08/2022 19:35:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.404798 on epoch=109
03/08/2022 19:35:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.340343 on epoch=114
03/08/2022 19:36:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.322095 on epoch=119
03/08/2022 19:36:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.287599 on epoch=124
03/08/2022 19:36:12 - INFO - __main__ - Global step 250 Train loss 0.402756 Rouge-L 0.11047805544096063 on epoch=124
03/08/2022 19:36:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.300483 on epoch=129
03/08/2022 19:36:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.275844 on epoch=134
03/08/2022 19:36:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.269103 on epoch=139
03/08/2022 19:36:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.290446 on epoch=144
03/08/2022 19:36:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.266218 on epoch=149
03/08/2022 19:36:43 - INFO - __main__ - Global step 300 Train loss 0.280419 Rouge-L 0.10087829700153075 on epoch=149
03/08/2022 19:36:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.268773 on epoch=154
03/08/2022 19:36:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.252777 on epoch=159
03/08/2022 19:36:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.239997 on epoch=164
03/08/2022 19:37:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.222984 on epoch=169
03/08/2022 19:37:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.238275 on epoch=174
03/08/2022 19:37:14 - INFO - __main__ - Global step 350 Train loss 0.244561 Rouge-L 0.09013521839262802 on epoch=174
03/08/2022 19:37:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.245950 on epoch=179
03/08/2022 19:37:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.222095 on epoch=184
03/08/2022 19:37:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.219487 on epoch=189
03/08/2022 19:37:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.222309 on epoch=194
03/08/2022 19:37:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.197537 on epoch=199
03/08/2022 19:37:45 - INFO - __main__ - Global step 400 Train loss 0.221476 Rouge-L 0.10302138999439683 on epoch=199
03/08/2022 19:37:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.219083 on epoch=204
03/08/2022 19:37:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.196960 on epoch=209
03/08/2022 19:38:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.203688 on epoch=214
03/08/2022 19:38:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.208835 on epoch=219
03/08/2022 19:38:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.186274 on epoch=224
03/08/2022 19:38:15 - INFO - __main__ - Global step 450 Train loss 0.202968 Rouge-L 0.058827700889042406 on epoch=224
03/08/2022 19:38:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.216388 on epoch=229
03/08/2022 19:38:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.189673 on epoch=234
03/08/2022 19:38:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.194160 on epoch=239
03/08/2022 19:38:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.186775 on epoch=244
03/08/2022 19:38:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.190845 on epoch=249
03/08/2022 19:38:46 - INFO - __main__ - Global step 500 Train loss 0.195568 Rouge-L 0.06961078807825583 on epoch=249
03/08/2022 19:38:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.175895 on epoch=254
03/08/2022 19:38:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.185139 on epoch=259
03/08/2022 19:39:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.181513 on epoch=264
03/08/2022 19:39:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.174528 on epoch=269
03/08/2022 19:39:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.174622 on epoch=274
03/08/2022 19:39:17 - INFO - __main__ - Global step 550 Train loss 0.178339 Rouge-L 0.07712207463412346 on epoch=274
03/08/2022 19:39:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.165914 on epoch=279
03/08/2022 19:39:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.176052 on epoch=284
03/08/2022 19:39:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.166803 on epoch=289
03/08/2022 19:39:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.170240 on epoch=294
03/08/2022 19:39:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.171824 on epoch=299
03/08/2022 19:39:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:39:46 - INFO - __main__ - Printing 3 examples
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:39:46 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:39:46 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:39:46 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 19:39:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:39:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:39:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:39:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:39:46 - INFO - __main__ - Printing 3 examples
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:39:46 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:39:46 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:39:46 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:39:46 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:39:46 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:39:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:39:46 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:39:47 - INFO - __main__ - Global step 600 Train loss 0.170167 Rouge-L 0.03934935832282006 on epoch=299
03/08/2022 19:39:47 - INFO - __main__ - save last model!
03/08/2022 19:39:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:39:55 - INFO - __main__ - Starting training!
03/08/2022 19:40:07 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:40:08 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 19:40:08 - INFO - __main__ - Printing 3 examples
03/08/2022 19:40:08 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 19:40:08 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 19:40:08 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 19:40:08 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 19:40:08 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 19:40:08 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 19:40:08 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:40:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:40:09 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 19:42:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_100_0.0005_8_predictions.txt
03/08/2022 19:42:45 - INFO - __main__ - Rouge-L on test data: 0.2583
03/08/2022 19:42:46 - INFO - __main__ - prefix=samsum_32_100, lr=0.0005, bsz=8, dev_performance=0.286941786358452, test_performance=0.2582639824898693
03/08/2022 19:42:46 - INFO - __main__ - Running ... prefix=samsum_32_100, lr=0.0003, bsz=8 ...
03/08/2022 19:42:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:42:47 - INFO - __main__ - Printing 3 examples
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:42:47 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:42:47 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:42:47 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 19:42:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:42:47 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:42:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:42:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:42:47 - INFO - __main__ - Printing 3 examples
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:42:47 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:42:47 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:42:47 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:42:47 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:42:47 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:42:47 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:42:47 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:42:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:42:57 - INFO - __main__ - Starting training!
03/08/2022 19:43:02 - INFO - __main__ - Step 10 Global step 10 Train loss 18.308819 on epoch=4
03/08/2022 19:43:07 - INFO - __main__ - Step 20 Global step 20 Train loss 13.967542 on epoch=9
03/08/2022 19:43:13 - INFO - __main__ - Step 30 Global step 30 Train loss 8.383229 on epoch=14
03/08/2022 19:43:18 - INFO - __main__ - Step 40 Global step 40 Train loss 8.163087 on epoch=19
03/08/2022 19:43:23 - INFO - __main__ - Step 50 Global step 50 Train loss 7.280226 on epoch=24
03/08/2022 19:43:36 - INFO - __main__ - Global step 50 Train loss 11.220580 Rouge-L 0.15842918979932563 on epoch=24
03/08/2022 19:44:08 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.15842918979932563 on epoch=24, global_step=50
03/08/2022 19:44:13 - INFO - __main__ - Step 60 Global step 60 Train loss 6.658958 on epoch=29
03/08/2022 19:44:19 - INFO - __main__ - Step 70 Global step 70 Train loss 6.272360 on epoch=34
03/08/2022 19:44:24 - INFO - __main__ - Step 80 Global step 80 Train loss 5.667968 on epoch=39
03/08/2022 19:44:30 - INFO - __main__ - Step 90 Global step 90 Train loss 5.130378 on epoch=44
03/08/2022 19:44:35 - INFO - __main__ - Step 100 Global step 100 Train loss 4.540452 on epoch=49
03/08/2022 19:44:48 - INFO - __main__ - Global step 100 Train loss 5.654024 Rouge-L 0.2260372647124147 on epoch=49
03/08/2022 19:45:21 - INFO - __main__ - Saving model with best Rouge-L: 0.15842918979932563 -> 0.2260372647124147 on epoch=49, global_step=100
03/08/2022 19:45:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.181666 on epoch=54
03/08/2022 19:45:32 - INFO - __main__ - Step 120 Global step 120 Train loss 2.298857 on epoch=59
03/08/2022 19:45:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.519601 on epoch=64
03/08/2022 19:45:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.175313 on epoch=69
03/08/2022 19:45:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.911760 on epoch=74
03/08/2022 19:45:53 - INFO - __main__ - Global step 150 Train loss 1.817439 Rouge-L 0.12518688339403022 on epoch=74
03/08/2022 19:45:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.759232 on epoch=79
03/08/2022 19:46:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.727768 on epoch=84
03/08/2022 19:46:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.598629 on epoch=89
03/08/2022 19:46:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.514504 on epoch=94
03/08/2022 19:46:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.493524 on epoch=99
03/08/2022 19:46:23 - INFO - __main__ - Global step 200 Train loss 0.618731 Rouge-L 0.12143667512677203 on epoch=99
03/08/2022 19:46:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.461954 on epoch=104
03/08/2022 19:46:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.484044 on epoch=109
03/08/2022 19:46:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.419687 on epoch=114
03/08/2022 19:46:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.401465 on epoch=119
03/08/2022 19:46:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.356478 on epoch=124
03/08/2022 19:46:54 - INFO - __main__ - Global step 250 Train loss 0.424726 Rouge-L 0.09149669085524866 on epoch=124
03/08/2022 19:46:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.418146 on epoch=129
03/08/2022 19:47:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.352818 on epoch=134
03/08/2022 19:47:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.323460 on epoch=139
03/08/2022 19:47:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.396887 on epoch=144
03/08/2022 19:47:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.371828 on epoch=149
03/08/2022 19:47:26 - INFO - __main__ - Global step 300 Train loss 0.372628 Rouge-L 0.10442211694956738 on epoch=149
03/08/2022 19:47:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.340547 on epoch=154
03/08/2022 19:47:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.286867 on epoch=159
03/08/2022 19:47:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.334139 on epoch=164
03/08/2022 19:47:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.290482 on epoch=169
03/08/2022 19:47:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.288506 on epoch=174
03/08/2022 19:47:57 - INFO - __main__ - Global step 350 Train loss 0.308108 Rouge-L 0.07638009402364718 on epoch=174
03/08/2022 19:48:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.332780 on epoch=179
03/08/2022 19:48:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.243756 on epoch=184
03/08/2022 19:48:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.314806 on epoch=189
03/08/2022 19:48:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.268932 on epoch=194
03/08/2022 19:48:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.271470 on epoch=199
03/08/2022 19:48:28 - INFO - __main__ - Global step 400 Train loss 0.286349 Rouge-L 0.06670987265727188 on epoch=199
03/08/2022 19:48:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.260904 on epoch=204
03/08/2022 19:48:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.263263 on epoch=209
03/08/2022 19:48:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.296024 on epoch=214
03/08/2022 19:48:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.262850 on epoch=219
03/08/2022 19:48:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.247662 on epoch=224
03/08/2022 19:48:59 - INFO - __main__ - Global step 450 Train loss 0.266140 Rouge-L 0.09563927144620293 on epoch=224
03/08/2022 19:49:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.258210 on epoch=229
03/08/2022 19:49:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.286900 on epoch=234
03/08/2022 19:49:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.253341 on epoch=239
03/08/2022 19:49:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.234844 on epoch=244
03/08/2022 19:49:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.238982 on epoch=249
03/08/2022 19:49:30 - INFO - __main__ - Global step 500 Train loss 0.254455 Rouge-L 0.07878697538160924 on epoch=249
03/08/2022 19:49:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.242003 on epoch=254
03/08/2022 19:49:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.245458 on epoch=259
03/08/2022 19:49:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.228110 on epoch=264
03/08/2022 19:49:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.221239 on epoch=269
03/08/2022 19:49:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.221709 on epoch=274
03/08/2022 19:50:02 - INFO - __main__ - Global step 550 Train loss 0.231704 Rouge-L 0.08618299938660438 on epoch=274
03/08/2022 19:50:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.227936 on epoch=279
03/08/2022 19:50:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.205821 on epoch=284
03/08/2022 19:50:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.225299 on epoch=289
03/08/2022 19:50:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.209332 on epoch=294
03/08/2022 19:50:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.225291 on epoch=299
03/08/2022 19:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:50:31 - INFO - __main__ - Printing 3 examples
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:50:31 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:50:31 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:50:31 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 19:50:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 19:50:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:50:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:50:31 - INFO - __main__ - Printing 3 examples
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:50:31 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:50:31 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:50:31 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:50:31 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:50:31 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:50:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:50:31 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:50:33 - INFO - __main__ - Global step 600 Train loss 0.218736 Rouge-L 0.06900212182383612 on epoch=299
03/08/2022 19:50:33 - INFO - __main__ - save last model!
03/08/2022 19:50:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:50:41 - INFO - __main__ - Starting training!
03/08/2022 19:51:14 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 19:51:14 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 19:51:14 - INFO - __main__ - Printing 3 examples
03/08/2022 19:51:14 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 19:51:14 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 19:51:14 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 19:51:14 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 19:51:14 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 19:51:14 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 19:51:14 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:51:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:51:16 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 19:56:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_100_0.0003_8_predictions.txt
03/08/2022 19:56:52 - INFO - __main__ - Rouge-L on test data: 0.2377
03/08/2022 19:56:52 - INFO - __main__ - prefix=samsum_32_100, lr=0.0003, bsz=8, dev_performance=0.2260372647124147, test_performance=0.23771674164197723
03/08/2022 19:56:52 - INFO - __main__ - Running ... prefix=samsum_32_100, lr=0.0002, bsz=8 ...
03/08/2022 19:56:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:56:53 - INFO - __main__ - Printing 3 examples
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 19:56:53 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 19:56:53 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 19:56:53 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 19:56:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 19:56:53 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:56:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 19:56:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 19:56:53 - INFO - __main__ - Printing 3 examples
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 19:56:53 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 19:56:53 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 19:56:53 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 19:56:53 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 19:56:53 - INFO - __main__ - Tokenizing Input ...
03/08/2022 19:56:53 - INFO - __main__ - Tokenizing Output ...
03/08/2022 19:56:54 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 19:57:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 19:57:04 - INFO - __main__ - Starting training!
03/08/2022 19:57:09 - INFO - __main__ - Step 10 Global step 10 Train loss 17.548895 on epoch=4
03/08/2022 19:57:14 - INFO - __main__ - Step 20 Global step 20 Train loss 9.971258 on epoch=9
03/08/2022 19:57:19 - INFO - __main__ - Step 30 Global step 30 Train loss 4.524474 on epoch=14
03/08/2022 19:57:25 - INFO - __main__ - Step 40 Global step 40 Train loss 4.456689 on epoch=19
03/08/2022 19:57:30 - INFO - __main__ - Step 50 Global step 50 Train loss 3.558951 on epoch=24
03/08/2022 19:57:40 - INFO - __main__ - Global step 50 Train loss 8.012053 Rouge-L 0.21294267333651842 on epoch=24
03/08/2022 19:58:13 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.21294267333651842 on epoch=24, global_step=50
03/08/2022 19:58:18 - INFO - __main__ - Step 60 Global step 60 Train loss 2.867162 on epoch=29
03/08/2022 19:58:24 - INFO - __main__ - Step 70 Global step 70 Train loss 2.527293 on epoch=34
03/08/2022 19:58:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.343461 on epoch=39
03/08/2022 19:58:35 - INFO - __main__ - Step 90 Global step 90 Train loss 2.318264 on epoch=44
03/08/2022 19:58:40 - INFO - __main__ - Step 100 Global step 100 Train loss 2.166267 on epoch=49
03/08/2022 19:58:46 - INFO - __main__ - Global step 100 Train loss 2.444489 Rouge-L 0.2349473725994896 on epoch=49
03/08/2022 19:59:17 - INFO - __main__ - Saving model with best Rouge-L: 0.21294267333651842 -> 0.2349473725994896 on epoch=49, global_step=100
03/08/2022 19:59:23 - INFO - __main__ - Step 110 Global step 110 Train loss 1.829112 on epoch=54
03/08/2022 19:59:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.675304 on epoch=59
03/08/2022 19:59:34 - INFO - __main__ - Step 130 Global step 130 Train loss 1.356206 on epoch=64
03/08/2022 19:59:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.289033 on epoch=69
03/08/2022 19:59:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.272631 on epoch=74
03/08/2022 19:59:49 - INFO - __main__ - Global step 150 Train loss 1.484457 Rouge-L 0.16376685144679715 on epoch=74
03/08/2022 19:59:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.082630 on epoch=79
03/08/2022 20:00:00 - INFO - __main__ - Step 170 Global step 170 Train loss 1.069911 on epoch=84
03/08/2022 20:00:06 - INFO - __main__ - Step 180 Global step 180 Train loss 1.014668 on epoch=89
03/08/2022 20:00:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.860373 on epoch=94
03/08/2022 20:00:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.850358 on epoch=99
03/08/2022 20:00:20 - INFO - __main__ - Global step 200 Train loss 0.975588 Rouge-L 0.13564941338720893 on epoch=99
03/08/2022 20:00:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.836299 on epoch=104
03/08/2022 20:00:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.681211 on epoch=109
03/08/2022 20:00:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.674350 on epoch=114
03/08/2022 20:00:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.652070 on epoch=119
03/08/2022 20:00:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.541930 on epoch=124
03/08/2022 20:00:51 - INFO - __main__ - Global step 250 Train loss 0.677172 Rouge-L 0.13049341182213273 on epoch=124
03/08/2022 20:00:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.563542 on epoch=129
03/08/2022 20:01:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.496564 on epoch=134
03/08/2022 20:01:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.546432 on epoch=139
03/08/2022 20:01:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.439182 on epoch=144
03/08/2022 20:01:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.503372 on epoch=149
03/08/2022 20:01:22 - INFO - __main__ - Global step 300 Train loss 0.509819 Rouge-L 0.10000470318428262 on epoch=149
03/08/2022 20:01:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.465519 on epoch=154
03/08/2022 20:01:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.444611 on epoch=159
03/08/2022 20:01:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.526005 on epoch=164
03/08/2022 20:01:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.433566 on epoch=169
03/08/2022 20:01:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.377543 on epoch=174
03/08/2022 20:01:53 - INFO - __main__ - Global step 350 Train loss 0.449449 Rouge-L 0.09144442107623069 on epoch=174
03/08/2022 20:01:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.395221 on epoch=179
03/08/2022 20:02:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.353485 on epoch=184
03/08/2022 20:02:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.355631 on epoch=189
03/08/2022 20:02:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.374994 on epoch=194
03/08/2022 20:02:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.349397 on epoch=199
03/08/2022 20:02:24 - INFO - __main__ - Global step 400 Train loss 0.365746 Rouge-L 0.08018787944249572 on epoch=199
03/08/2022 20:02:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.391567 on epoch=204
03/08/2022 20:02:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.361287 on epoch=209
03/08/2022 20:02:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.384270 on epoch=214
03/08/2022 20:02:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.323090 on epoch=219
03/08/2022 20:02:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.366670 on epoch=224
03/08/2022 20:03:04 - INFO - __main__ - Global step 450 Train loss 0.365377 Rouge-L 0.10176096417650222 on epoch=224
03/08/2022 20:03:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.314107 on epoch=229
03/08/2022 20:03:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.361690 on epoch=234
03/08/2022 20:03:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.310921 on epoch=239
03/08/2022 20:03:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.335717 on epoch=244
03/08/2022 20:03:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.316511 on epoch=249
03/08/2022 20:03:43 - INFO - __main__ - Global step 500 Train loss 0.327789 Rouge-L 0.08687677877228439 on epoch=249
03/08/2022 20:03:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.314961 on epoch=254
03/08/2022 20:03:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.333489 on epoch=259
03/08/2022 20:03:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.288913 on epoch=264
03/08/2022 20:04:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.312678 on epoch=269
03/08/2022 20:04:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.307097 on epoch=274
03/08/2022 20:04:22 - INFO - __main__ - Global step 550 Train loss 0.311428 Rouge-L 0.08986347252623517 on epoch=274
03/08/2022 20:04:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.305035 on epoch=279
03/08/2022 20:04:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.320430 on epoch=284
03/08/2022 20:04:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.320215 on epoch=289
03/08/2022 20:04:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.296279 on epoch=294
03/08/2022 20:04:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.276317 on epoch=299
03/08/2022 20:04:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:04:52 - INFO - __main__ - Printing 3 examples
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 20:04:52 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 20:04:52 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 20:04:52 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 20:04:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 20:04:52 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:04:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:04:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:04:52 - INFO - __main__ - Printing 3 examples
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 20:04:52 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 20:04:52 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 20:04:52 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 20:04:52 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 20:04:52 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:04:52 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:04:52 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:05:01 - INFO - __main__ - Global step 600 Train loss 0.303655 Rouge-L 0.09571531190793406 on epoch=299
03/08/2022 20:05:01 - INFO - __main__ - save last model!
03/08/2022 20:05:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:05:03 - INFO - __main__ - Starting training!
03/08/2022 20:05:49 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 20:05:50 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 20:05:50 - INFO - __main__ - Printing 3 examples
03/08/2022 20:05:50 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 20:05:50 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 20:05:50 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 20:05:50 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 20:05:50 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 20:05:50 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 20:05:50 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:05:50 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:05:51 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 20:08:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_100_0.0002_8_predictions.txt
03/08/2022 20:08:54 - INFO - __main__ - Rouge-L on test data: 0.2245
03/08/2022 20:08:55 - INFO - __main__ - prefix=samsum_32_100, lr=0.0002, bsz=8, dev_performance=0.2349473725994896, test_performance=0.2244779856710168
03/08/2022 20:08:55 - INFO - __main__ - Running ... prefix=samsum_32_100, lr=0.0001, bsz=8 ...
03/08/2022 20:08:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:08:55 - INFO - __main__ - Printing 3 examples
03/08/2022 20:08:55 - INFO - __main__ -  [samsum] summarize: Mike: I browsed through the photos Mike: nothing good there Scott: okay Mike: can you check yours and let me know? Scott: sure, will do later Mike: I'm pissed off with dan Scott: why Mike: he was supposed to move his ass over here  Mike: hasn't been here since I moved in Scott: I heard, he's at work all the time Mike: come on, it's a hop or two away Mike: he'd come if he really wanted to Scott: that's right Mike: need to go, mate Scott: will see you 2morrow? Mike: nope, I'm off Scott: ok then Mike: will be there on wed Scott: see u then
03/08/2022 20:08:55 - INFO - __main__ - ["Mike didn't find any good photos. Scott will check if he has any. Mike is angry with Dan. Dan hasn't visited Mike since Mike moved in. Mike will see Scott on Wednesday."]
03/08/2022 20:08:55 - INFO - __main__ -  [samsum] summarize: Benjamin: i'll be late, i've just had a car accident Ethan: Are you injured? Did you call the police? Benjamin: i'm ok, nobody's hurt, but the car is completely crashed Benjamin: the police is here, i can't talk right now. Ethan: Ok. Hope to see you soon!
03/08/2022 20:08:55 - INFO - __main__ - ['Benjamin had a car accident. Nobody got hurt but the car is crashed. He is talking to the police.']
03/08/2022 20:08:55 - INFO - __main__ -  [samsum] summarize: Liam: Bob, we have an emergency here, could you come over? Bob: the pipe again? Liam: yes, the children played with it apparently. Bob: I'm coming 
03/08/2022 20:08:55 - INFO - __main__ - ['Bob will come over to fix the pipe that children had played with.']
03/08/2022 20:08:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 20:08:55 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:08:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:08:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:08:55 - INFO - __main__ - Printing 3 examples
03/08/2022 20:08:55 - INFO - __main__ -  [samsum] summarize: Tatiana: i will be late Ruby: ok Ruby: me too
03/08/2022 20:08:55 - INFO - __main__ - ['Both Tatiana and Ruby will be late.']
03/08/2022 20:08:55 - INFO - __main__ -  [samsum] summarize: Imogen: hey, you up? Nathan: Weirdly, yes Nathan: What's up? Imogen: I woke up early... cant get back to sleep Nathan: I guess your parents talked to you about joining their business? Imogen: yeah Nathan: How are you feeling? Imogen: kjdhskjfhkjd Nathan: lol Imogen: tbh im kinda nervous... but also excited... and like fifty other things as well Imogen: how abt you? Nathan: waiting for the other shoe to drop Imogen: what do you mean? Nathan: just this past week's been kinda weird Imogen: :( Imogen: btw, you're coming tonight, right? Nathan: I wouldn't miss it for the world Imogen: <3 thank uuu!!! Nathan: Do you want to maybe get dinner or something beforehand? Imogen: yes!!! I've been craving lasagna Imogen: and chocolate... preferably in the form of ice cream
03/08/2022 20:08:56 - INFO - __main__ - ['Imogen woke up early and cannot get back to sleep because she is nervous and excited that her parents want her to join their business. Nathan is coming over tonight. They will eat lasagna and chocolate ice cream.']
03/08/2022 20:08:56 - INFO - __main__ -  [samsum] summarize: Martin: short break now? Fred: ok Martin: so in the kitchen in 5 mins? Fred: give me 10 Martin: ok
03/08/2022 20:08:56 - INFO - __main__ - ['Martin and Fred will meet in the kitchen for a short break in 10 minutes.']
03/08/2022 20:08:56 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:08:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:08:56 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:09:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:09:05 - INFO - __main__ - Starting training!
03/08/2022 20:09:10 - INFO - __main__ - Step 10 Global step 10 Train loss 17.732746 on epoch=4
03/08/2022 20:09:15 - INFO - __main__ - Step 20 Global step 20 Train loss 14.902150 on epoch=9
03/08/2022 20:09:20 - INFO - __main__ - Step 30 Global step 30 Train loss 11.231087 on epoch=14
03/08/2022 20:09:25 - INFO - __main__ - Step 40 Global step 40 Train loss 9.888912 on epoch=19
03/08/2022 20:09:31 - INFO - __main__ - Step 50 Global step 50 Train loss 9.294005 on epoch=24
03/08/2022 20:09:44 - INFO - __main__ - Global step 50 Train loss 12.609781 Rouge-L 0.22652139595921306 on epoch=24
03/08/2022 20:10:16 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.22652139595921306 on epoch=24, global_step=50
03/08/2022 20:10:21 - INFO - __main__ - Step 60 Global step 60 Train loss 8.351045 on epoch=29
03/08/2022 20:10:27 - INFO - __main__ - Step 70 Global step 70 Train loss 8.164864 on epoch=34
03/08/2022 20:10:32 - INFO - __main__ - Step 80 Global step 80 Train loss 8.880603 on epoch=39
03/08/2022 20:10:38 - INFO - __main__ - Step 90 Global step 90 Train loss 8.307548 on epoch=44
03/08/2022 20:10:43 - INFO - __main__ - Step 100 Global step 100 Train loss 7.646382 on epoch=49
03/08/2022 20:10:57 - INFO - __main__ - Global step 100 Train loss 8.270088 Rouge-L 0.2933118728267315 on epoch=49
03/08/2022 20:11:30 - INFO - __main__ - Saving model with best Rouge-L: 0.22652139595921306 -> 0.2933118728267315 on epoch=49, global_step=100
03/08/2022 20:11:36 - INFO - __main__ - Step 110 Global step 110 Train loss 7.510847 on epoch=54
03/08/2022 20:11:41 - INFO - __main__ - Step 120 Global step 120 Train loss 6.970448 on epoch=59
03/08/2022 20:11:47 - INFO - __main__ - Step 130 Global step 130 Train loss 6.578075 on epoch=64
03/08/2022 20:11:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.319463 on epoch=69
03/08/2022 20:11:58 - INFO - __main__ - Step 150 Global step 150 Train loss 5.300757 on epoch=74
03/08/2022 20:12:11 - INFO - __main__ - Global step 150 Train loss 6.535918 Rouge-L 0.33137307608518846 on epoch=74
03/08/2022 20:12:45 - INFO - __main__ - Saving model with best Rouge-L: 0.2933118728267315 -> 0.33137307608518846 on epoch=74, global_step=150
03/08/2022 20:12:50 - INFO - __main__ - Step 160 Global step 160 Train loss 4.663435 on epoch=79
03/08/2022 20:12:56 - INFO - __main__ - Step 170 Global step 170 Train loss 4.230657 on epoch=84
03/08/2022 20:13:01 - INFO - __main__ - Step 180 Global step 180 Train loss 3.649858 on epoch=89
03/08/2022 20:13:07 - INFO - __main__ - Step 190 Global step 190 Train loss 3.067849 on epoch=94
03/08/2022 20:13:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.635897 on epoch=99
03/08/2022 20:13:17 - INFO - __main__ - Global step 200 Train loss 3.649539 Rouge-L 0.23800662078242887 on epoch=99
03/08/2022 20:13:23 - INFO - __main__ - Step 210 Global step 210 Train loss 2.351881 on epoch=104
03/08/2022 20:13:28 - INFO - __main__ - Step 220 Global step 220 Train loss 2.145268 on epoch=109
03/08/2022 20:13:34 - INFO - __main__ - Step 230 Global step 230 Train loss 2.049197 on epoch=114
03/08/2022 20:13:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.891623 on epoch=119
03/08/2022 20:13:45 - INFO - __main__ - Step 250 Global step 250 Train loss 1.661214 on epoch=124
03/08/2022 20:13:48 - INFO - __main__ - Global step 250 Train loss 2.019836 Rouge-L 0.2565116465981138 on epoch=124
03/08/2022 20:13:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.555219 on epoch=129
03/08/2022 20:13:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.453726 on epoch=134
03/08/2022 20:14:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.551569 on epoch=139
03/08/2022 20:14:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.526322 on epoch=144
03/08/2022 20:14:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.501647 on epoch=149
03/08/2022 20:14:19 - INFO - __main__ - Global step 300 Train loss 1.517697 Rouge-L 0.18339438479927667 on epoch=149
03/08/2022 20:14:25 - INFO - __main__ - Step 310 Global step 310 Train loss 1.433425 on epoch=154
03/08/2022 20:14:30 - INFO - __main__ - Step 320 Global step 320 Train loss 1.338344 on epoch=159
03/08/2022 20:14:36 - INFO - __main__ - Step 330 Global step 330 Train loss 1.185712 on epoch=164
03/08/2022 20:14:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.035022 on epoch=169
03/08/2022 20:14:47 - INFO - __main__ - Step 350 Global step 350 Train loss 1.099811 on epoch=174
03/08/2022 20:14:50 - INFO - __main__ - Global step 350 Train loss 1.218463 Rouge-L 0.17048102263598672 on epoch=174
03/08/2022 20:14:56 - INFO - __main__ - Step 360 Global step 360 Train loss 1.058601 on epoch=179
03/08/2022 20:15:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.933189 on epoch=184
03/08/2022 20:15:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.971160 on epoch=189
03/08/2022 20:15:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.902700 on epoch=194
03/08/2022 20:15:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.940125 on epoch=199
03/08/2022 20:15:21 - INFO - __main__ - Global step 400 Train loss 0.961155 Rouge-L 0.17865646673701718 on epoch=199
03/08/2022 20:15:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.799823 on epoch=204
03/08/2022 20:15:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.779372 on epoch=209
03/08/2022 20:15:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.808145 on epoch=214
03/08/2022 20:15:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.737941 on epoch=219
03/08/2022 20:15:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.735473 on epoch=224
03/08/2022 20:15:51 - INFO - __main__ - Global step 450 Train loss 0.772151 Rouge-L 0.18175240529393982 on epoch=224
03/08/2022 20:15:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.645156 on epoch=229
03/08/2022 20:16:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.640786 on epoch=234
03/08/2022 20:16:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.648221 on epoch=239
03/08/2022 20:16:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.681268 on epoch=244
03/08/2022 20:16:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.520884 on epoch=249
03/08/2022 20:16:22 - INFO - __main__ - Global step 500 Train loss 0.627263 Rouge-L 0.13946725648922492 on epoch=249
03/08/2022 20:16:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.570322 on epoch=254
03/08/2022 20:16:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.552666 on epoch=259
03/08/2022 20:16:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.529323 on epoch=264
03/08/2022 20:16:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.582182 on epoch=269
03/08/2022 20:16:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.513562 on epoch=274
03/08/2022 20:16:54 - INFO - __main__ - Global step 550 Train loss 0.549611 Rouge-L 0.1595986065436897 on epoch=274
03/08/2022 20:16:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.470531 on epoch=279
03/08/2022 20:17:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.477227 on epoch=284
03/08/2022 20:17:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.452294 on epoch=289
03/08/2022 20:17:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.458852 on epoch=294
03/08/2022 20:17:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.468472 on epoch=299
03/08/2022 20:17:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:17:23 - INFO - __main__ - Printing 3 examples
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:17:23 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:17:23 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:17:23 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:17:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 20:17:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:17:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:17:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:17:23 - INFO - __main__ - Printing 3 examples
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:17:23 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:17:23 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:17:23 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:17:23 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:17:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:17:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:17:23 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:17:26 - INFO - __main__ - Global step 600 Train loss 0.465475 Rouge-L 0.1853673368614266 on epoch=299
03/08/2022 20:17:26 - INFO - __main__ - save last model!
03/08/2022 20:17:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:17:33 - INFO - __main__ - Starting training!
03/08/2022 20:18:09 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 20:18:10 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 20:18:10 - INFO - __main__ - Printing 3 examples
03/08/2022 20:18:10 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 20:18:10 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 20:18:10 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 20:18:10 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 20:18:10 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 20:18:10 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 20:18:10 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:18:10 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:18:11 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 20:23:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_100_0.0001_8_predictions.txt
03/08/2022 20:23:35 - INFO - __main__ - Rouge-L on test data: 0.3296
03/08/2022 20:23:35 - INFO - __main__ - prefix=samsum_32_100, lr=0.0001, bsz=8, dev_performance=0.33137307608518846, test_performance=0.32960297428266755
03/08/2022 20:23:35 - INFO - __main__ - Running ... prefix=samsum_32_13, lr=0.0005, bsz=8 ...
03/08/2022 20:23:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:23:36 - INFO - __main__ - Printing 3 examples
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:23:36 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:23:36 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:23:36 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:23:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 20:23:36 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:23:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:23:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:23:36 - INFO - __main__ - Printing 3 examples
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:23:36 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:23:36 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:23:36 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:23:36 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:23:36 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:23:36 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:23:36 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:23:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:23:46 - INFO - __main__ - Starting training!
03/08/2022 20:23:51 - INFO - __main__ - Step 10 Global step 10 Train loss 18.175261 on epoch=4
03/08/2022 20:23:56 - INFO - __main__ - Step 20 Global step 20 Train loss 9.624421 on epoch=9
03/08/2022 20:24:01 - INFO - __main__ - Step 30 Global step 30 Train loss 5.003859 on epoch=14
03/08/2022 20:24:07 - INFO - __main__ - Step 40 Global step 40 Train loss 2.459139 on epoch=19
03/08/2022 20:24:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.712725 on epoch=24
03/08/2022 20:24:17 - INFO - __main__ - Global step 50 Train loss 7.395081 Rouge-L 0.25270052521617914 on epoch=24
03/08/2022 20:24:50 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.25270052521617914 on epoch=24, global_step=50
03/08/2022 20:24:56 - INFO - __main__ - Step 60 Global step 60 Train loss 1.248082 on epoch=29
03/08/2022 20:25:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.041571 on epoch=34
03/08/2022 20:25:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.860636 on epoch=39
03/08/2022 20:25:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.788143 on epoch=44
03/08/2022 20:25:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.631140 on epoch=49
03/08/2022 20:25:22 - INFO - __main__ - Global step 100 Train loss 0.913914 Rouge-L 0.1821440282632265 on epoch=49
03/08/2022 20:25:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.465979 on epoch=54
03/08/2022 20:25:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.529119 on epoch=59
03/08/2022 20:25:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.969193 on epoch=64
03/08/2022 20:25:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.556031 on epoch=69
03/08/2022 20:25:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.451872 on epoch=74
03/08/2022 20:26:02 - INFO - __main__ - Global step 150 Train loss 0.594439 Rouge-L 0.06644497722824388 on epoch=74
03/08/2022 20:26:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.373730 on epoch=79
03/08/2022 20:26:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.306737 on epoch=84
03/08/2022 20:26:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.354967 on epoch=89
03/08/2022 20:26:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.307012 on epoch=94
03/08/2022 20:26:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.285702 on epoch=99
03/08/2022 20:26:43 - INFO - __main__ - Global step 200 Train loss 0.325630 Rouge-L 0.10034282059926558 on epoch=99
03/08/2022 20:26:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.330945 on epoch=104
03/08/2022 20:26:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.278202 on epoch=109
03/08/2022 20:27:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.280023 on epoch=114
03/08/2022 20:27:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.248964 on epoch=119
03/08/2022 20:27:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.263187 on epoch=124
03/08/2022 20:27:23 - INFO - __main__ - Global step 250 Train loss 0.280264 Rouge-L 0.08659601323040433 on epoch=124
03/08/2022 20:27:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.223033 on epoch=129
03/08/2022 20:27:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.254074 on epoch=134
03/08/2022 20:27:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.225144 on epoch=139
03/08/2022 20:27:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.195562 on epoch=144
03/08/2022 20:27:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.211605 on epoch=149
03/08/2022 20:28:04 - INFO - __main__ - Global step 300 Train loss 0.221884 Rouge-L 0.06099123544235469 on epoch=149
03/08/2022 20:28:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.223022 on epoch=154
03/08/2022 20:28:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.221096 on epoch=159
03/08/2022 20:28:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.202553 on epoch=164
03/08/2022 20:28:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.188285 on epoch=169
03/08/2022 20:28:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.196284 on epoch=174
03/08/2022 20:28:46 - INFO - __main__ - Global step 350 Train loss 0.206248 Rouge-L 0.0750463927907222 on epoch=174
03/08/2022 20:28:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.191571 on epoch=179
03/08/2022 20:28:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.183188 on epoch=184
03/08/2022 20:29:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.187659 on epoch=189
03/08/2022 20:29:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.183804 on epoch=194
03/08/2022 20:29:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.185913 on epoch=199
03/08/2022 20:29:27 - INFO - __main__ - Global step 400 Train loss 0.186427 Rouge-L 0.0718525032564371 on epoch=199
03/08/2022 20:29:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.169259 on epoch=204
03/08/2022 20:29:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.177339 on epoch=209
03/08/2022 20:29:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.232172 on epoch=214
03/08/2022 20:29:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.167530 on epoch=219
03/08/2022 20:29:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.164814 on epoch=224
03/08/2022 20:30:08 - INFO - __main__ - Global step 450 Train loss 0.182223 Rouge-L 0.029703688763743977 on epoch=224
03/08/2022 20:30:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.159858 on epoch=229
03/08/2022 20:30:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.162435 on epoch=234
03/08/2022 20:30:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.166132 on epoch=239
03/08/2022 20:30:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.163937 on epoch=244
03/08/2022 20:30:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.158704 on epoch=249
03/08/2022 20:30:49 - INFO - __main__ - Global step 500 Train loss 0.162213 Rouge-L 0.06974671309312121 on epoch=249
03/08/2022 20:30:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.196328 on epoch=254
03/08/2022 20:31:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.150134 on epoch=259
03/08/2022 20:31:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.151578 on epoch=264
03/08/2022 20:31:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.147905 on epoch=269
03/08/2022 20:31:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.142007 on epoch=274
03/08/2022 20:31:30 - INFO - __main__ - Global step 550 Train loss 0.157590 Rouge-L 0.062037732495394474 on epoch=274
03/08/2022 20:31:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.153283 on epoch=279
03/08/2022 20:31:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.161134 on epoch=284
03/08/2022 20:31:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.151603 on epoch=289
03/08/2022 20:31:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.142343 on epoch=294
03/08/2022 20:31:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.143982 on epoch=299
03/08/2022 20:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:32:00 - INFO - __main__ - Printing 3 examples
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:32:00 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:32:00 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:32:00 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:32:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 20:32:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:32:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:32:00 - INFO - __main__ - Printing 3 examples
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:32:00 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:32:00 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:32:00 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:32:00 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:32:00 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:32:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:32:00 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:32:03 - INFO - __main__ - Global step 600 Train loss 0.150469 Rouge-L 0.05680925493332016 on epoch=299
03/08/2022 20:32:03 - INFO - __main__ - save last model!
03/08/2022 20:32:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:32:10 - INFO - __main__ - Starting training!
03/08/2022 20:32:46 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 20:32:47 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 20:32:47 - INFO - __main__ - Printing 3 examples
03/08/2022 20:32:47 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 20:32:47 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 20:32:47 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 20:32:47 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 20:32:47 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 20:32:47 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 20:32:47 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:32:48 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:32:48 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 20:34:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_13_0.0005_8_predictions.txt
03/08/2022 20:34:19 - INFO - __main__ - Rouge-L on test data: 0.2213
03/08/2022 20:34:20 - INFO - __main__ - prefix=samsum_32_13, lr=0.0005, bsz=8, dev_performance=0.25270052521617914, test_performance=0.2213180524919809
03/08/2022 20:34:20 - INFO - __main__ - Running ... prefix=samsum_32_13, lr=0.0003, bsz=8 ...
03/08/2022 20:34:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:34:21 - INFO - __main__ - Printing 3 examples
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:34:21 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:34:21 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:34:21 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:34:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 20:34:21 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:34:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:34:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:34:21 - INFO - __main__ - Printing 3 examples
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:34:21 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:34:21 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:34:21 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:34:21 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:34:21 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:34:21 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:34:22 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:34:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:34:32 - INFO - __main__ - Starting training!
03/08/2022 20:34:37 - INFO - __main__ - Step 10 Global step 10 Train loss 18.226307 on epoch=4
03/08/2022 20:34:42 - INFO - __main__ - Step 20 Global step 20 Train loss 13.609793 on epoch=9
03/08/2022 20:34:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.594957 on epoch=14
03/08/2022 20:34:53 - INFO - __main__ - Step 40 Global step 40 Train loss 7.675200 on epoch=19
03/08/2022 20:34:59 - INFO - __main__ - Step 50 Global step 50 Train loss 7.170308 on epoch=24
03/08/2022 20:35:11 - INFO - __main__ - Global step 50 Train loss 11.055312 Rouge-L 0.12461646854594918 on epoch=24
03/08/2022 20:35:42 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.12461646854594918 on epoch=24, global_step=50
03/08/2022 20:35:48 - INFO - __main__ - Step 60 Global step 60 Train loss 6.742768 on epoch=29
03/08/2022 20:35:54 - INFO - __main__ - Step 70 Global step 70 Train loss 6.392506 on epoch=34
03/08/2022 20:35:59 - INFO - __main__ - Step 80 Global step 80 Train loss 5.142895 on epoch=39
03/08/2022 20:36:05 - INFO - __main__ - Step 90 Global step 90 Train loss 3.139727 on epoch=44
03/08/2022 20:36:11 - INFO - __main__ - Step 100 Global step 100 Train loss 2.048440 on epoch=49
03/08/2022 20:36:23 - INFO - __main__ - Global step 100 Train loss 4.693267 Rouge-L 0.14708274984817601 on epoch=49
03/08/2022 20:36:54 - INFO - __main__ - Saving model with best Rouge-L: 0.12461646854594918 -> 0.14708274984817601 on epoch=49, global_step=100
03/08/2022 20:37:00 - INFO - __main__ - Step 110 Global step 110 Train loss 1.462082 on epoch=54
03/08/2022 20:37:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.128611 on epoch=59
03/08/2022 20:37:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.943528 on epoch=64
03/08/2022 20:37:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.865918 on epoch=69
03/08/2022 20:37:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.859236 on epoch=74
03/08/2022 20:37:35 - INFO - __main__ - Global step 150 Train loss 1.051875 Rouge-L 0.0976531888332246 on epoch=74
03/08/2022 20:37:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.675806 on epoch=79
03/08/2022 20:37:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.594025 on epoch=84
03/08/2022 20:37:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.557729 on epoch=89
03/08/2022 20:37:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.503775 on epoch=94
03/08/2022 20:38:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.483982 on epoch=99
03/08/2022 20:38:08 - INFO - __main__ - Global step 200 Train loss 0.563063 Rouge-L 0.06801617984558275 on epoch=99
03/08/2022 20:38:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.453568 on epoch=104
03/08/2022 20:38:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.409899 on epoch=109
03/08/2022 20:38:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.379327 on epoch=114
03/08/2022 20:38:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.349407 on epoch=119
03/08/2022 20:38:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.344202 on epoch=124
03/08/2022 20:38:41 - INFO - __main__ - Global step 250 Train loss 0.387281 Rouge-L 0.0786775095704334 on epoch=124
03/08/2022 20:38:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.382076 on epoch=129
03/08/2022 20:38:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.313523 on epoch=134
03/08/2022 20:38:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.275038 on epoch=139
03/08/2022 20:39:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.314134 on epoch=144
03/08/2022 20:39:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.274908 on epoch=149
03/08/2022 20:39:14 - INFO - __main__ - Global step 300 Train loss 0.311936 Rouge-L 0.08767574434690614 on epoch=149
03/08/2022 20:39:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.268302 on epoch=154
03/08/2022 20:39:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.275341 on epoch=159
03/08/2022 20:39:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.336091 on epoch=164
03/08/2022 20:39:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.280709 on epoch=169
03/08/2022 20:39:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.298819 on epoch=174
03/08/2022 20:39:47 - INFO - __main__ - Global step 350 Train loss 0.291852 Rouge-L 0.09050493641696737 on epoch=174
03/08/2022 20:39:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.262122 on epoch=179
03/08/2022 20:39:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.264094 on epoch=184
03/08/2022 20:40:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.237924 on epoch=189
03/08/2022 20:40:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.234900 on epoch=194
03/08/2022 20:40:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.263708 on epoch=199
03/08/2022 20:40:21 - INFO - __main__ - Global step 400 Train loss 0.252550 Rouge-L 0.09479156665514352 on epoch=199
03/08/2022 20:40:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.245578 on epoch=204
03/08/2022 20:40:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.226229 on epoch=209
03/08/2022 20:40:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.249703 on epoch=214
03/08/2022 20:40:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.224855 on epoch=219
03/08/2022 20:40:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.230581 on epoch=224
03/08/2022 20:40:54 - INFO - __main__ - Global step 450 Train loss 0.235389 Rouge-L 0.05301753960316573 on epoch=224
03/08/2022 20:41:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.223053 on epoch=229
03/08/2022 20:41:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.206738 on epoch=234
03/08/2022 20:41:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.228645 on epoch=239
03/08/2022 20:41:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.219279 on epoch=244
03/08/2022 20:41:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.190666 on epoch=249
03/08/2022 20:41:28 - INFO - __main__ - Global step 500 Train loss 0.213676 Rouge-L 0.08586759716969651 on epoch=249
03/08/2022 20:41:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.202252 on epoch=254
03/08/2022 20:41:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.220380 on epoch=259
03/08/2022 20:41:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.180454 on epoch=264
03/08/2022 20:41:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.177525 on epoch=269
03/08/2022 20:41:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.169783 on epoch=274
03/08/2022 20:42:01 - INFO - __main__ - Global step 550 Train loss 0.190079 Rouge-L 0.09134839235205044 on epoch=274
03/08/2022 20:42:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.196868 on epoch=279
03/08/2022 20:42:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.196637 on epoch=284
03/08/2022 20:42:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.185779 on epoch=289
03/08/2022 20:42:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.184170 on epoch=294
03/08/2022 20:42:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.180043 on epoch=299
03/08/2022 20:42:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:42:31 - INFO - __main__ - Printing 3 examples
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:42:31 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:42:31 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:42:31 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:42:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 20:42:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:42:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:42:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:42:31 - INFO - __main__ - Printing 3 examples
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:42:31 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:42:31 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:42:31 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:42:31 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:42:31 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:42:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:42:31 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:42:35 - INFO - __main__ - Global step 600 Train loss 0.188699 Rouge-L 0.08007382002148242 on epoch=299
03/08/2022 20:42:35 - INFO - __main__ - save last model!
03/08/2022 20:42:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:42:41 - INFO - __main__ - Starting training!
03/08/2022 20:43:16 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 20:43:16 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 20:43:16 - INFO - __main__ - Printing 3 examples
03/08/2022 20:43:16 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 20:43:16 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 20:43:16 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 20:43:16 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 20:43:17 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 20:43:17 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 20:43:17 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:43:17 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:43:18 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 20:47:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_13_0.0003_8_predictions.txt
03/08/2022 20:47:44 - INFO - __main__ - Rouge-L on test data: 0.1576
03/08/2022 20:47:44 - INFO - __main__ - prefix=samsum_32_13, lr=0.0003, bsz=8, dev_performance=0.14708274984817601, test_performance=0.15755026339500533
03/08/2022 20:47:44 - INFO - __main__ - Running ... prefix=samsum_32_13, lr=0.0002, bsz=8 ...
03/08/2022 20:47:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:47:45 - INFO - __main__ - Printing 3 examples
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:47:45 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:47:45 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:47:45 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:47:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 20:47:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:47:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:47:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:47:45 - INFO - __main__ - Printing 3 examples
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:47:45 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:47:45 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:47:45 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:47:45 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:47:45 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:47:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:47:45 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:47:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:47:55 - INFO - __main__ - Starting training!
03/08/2022 20:48:01 - INFO - __main__ - Step 10 Global step 10 Train loss 18.246227 on epoch=4
03/08/2022 20:48:06 - INFO - __main__ - Step 20 Global step 20 Train loss 13.324060 on epoch=9
03/08/2022 20:48:11 - INFO - __main__ - Step 30 Global step 30 Train loss 10.532041 on epoch=14
03/08/2022 20:48:16 - INFO - __main__ - Step 40 Global step 40 Train loss 9.930374 on epoch=19
03/08/2022 20:48:22 - INFO - __main__ - Step 50 Global step 50 Train loss 9.838652 on epoch=24
03/08/2022 20:48:36 - INFO - __main__ - Global step 50 Train loss 12.374270 Rouge-L 0.08463015797338243 on epoch=24
03/08/2022 20:49:10 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.08463015797338243 on epoch=24, global_step=50
03/08/2022 20:49:16 - INFO - __main__ - Step 60 Global step 60 Train loss 8.896093 on epoch=29
03/08/2022 20:49:22 - INFO - __main__ - Step 70 Global step 70 Train loss 8.057070 on epoch=34
03/08/2022 20:49:28 - INFO - __main__ - Step 80 Global step 80 Train loss 7.076276 on epoch=39
03/08/2022 20:49:33 - INFO - __main__ - Step 90 Global step 90 Train loss 6.152627 on epoch=44
03/08/2022 20:49:39 - INFO - __main__ - Step 100 Global step 100 Train loss 5.045646 on epoch=49
03/08/2022 20:49:50 - INFO - __main__ - Global step 100 Train loss 7.045543 Rouge-L 0.11823911727503991 on epoch=49
03/08/2022 20:50:23 - INFO - __main__ - Saving model with best Rouge-L: 0.08463015797338243 -> 0.11823911727503991 on epoch=49, global_step=100
03/08/2022 20:50:29 - INFO - __main__ - Step 110 Global step 110 Train loss 4.487841 on epoch=54
03/08/2022 20:50:35 - INFO - __main__ - Step 120 Global step 120 Train loss 4.017898 on epoch=59
03/08/2022 20:50:41 - INFO - __main__ - Step 130 Global step 130 Train loss 3.457119 on epoch=64
03/08/2022 20:50:46 - INFO - __main__ - Step 140 Global step 140 Train loss 2.858461 on epoch=69
03/08/2022 20:50:52 - INFO - __main__ - Step 150 Global step 150 Train loss 2.171196 on epoch=74
03/08/2022 20:50:55 - INFO - __main__ - Global step 150 Train loss 3.398503 Rouge-L 0.12129093290410922 on epoch=74
03/08/2022 20:51:36 - INFO - __main__ - Saving model with best Rouge-L: 0.11823911727503991 -> 0.12129093290410922 on epoch=74, global_step=150
03/08/2022 20:51:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.988896 on epoch=79
03/08/2022 20:51:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.709006 on epoch=84
03/08/2022 20:51:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.407519 on epoch=89
03/08/2022 20:51:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.191439 on epoch=94
03/08/2022 20:52:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.078200 on epoch=99
03/08/2022 20:52:10 - INFO - __main__ - Global step 200 Train loss 1.475012 Rouge-L 0.11777692924548558 on epoch=99
03/08/2022 20:52:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.923151 on epoch=104
03/08/2022 20:52:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.845133 on epoch=109
03/08/2022 20:52:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.834368 on epoch=114
03/08/2022 20:52:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.705480 on epoch=119
03/08/2022 20:52:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.688673 on epoch=124
03/08/2022 20:52:44 - INFO - __main__ - Global step 250 Train loss 0.799361 Rouge-L 0.14420611475664669 on epoch=124
03/08/2022 20:53:17 - INFO - __main__ - Saving model with best Rouge-L: 0.12129093290410922 -> 0.14420611475664669 on epoch=124, global_step=250
03/08/2022 20:53:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.661575 on epoch=129
03/08/2022 20:53:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.546169 on epoch=134
03/08/2022 20:53:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.496449 on epoch=139
03/08/2022 20:53:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.480918 on epoch=144
03/08/2022 20:53:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.455220 on epoch=149
03/08/2022 20:53:50 - INFO - __main__ - Global step 300 Train loss 0.528066 Rouge-L 0.12347809576744193 on epoch=149
03/08/2022 20:53:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.424968 on epoch=154
03/08/2022 20:54:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.425563 on epoch=159
03/08/2022 20:54:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.449308 on epoch=164
03/08/2022 20:54:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.364529 on epoch=169
03/08/2022 20:54:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.346953 on epoch=174
03/08/2022 20:54:22 - INFO - __main__ - Global step 350 Train loss 0.402264 Rouge-L 0.15527246464858105 on epoch=174
03/08/2022 20:54:56 - INFO - __main__ - Saving model with best Rouge-L: 0.14420611475664669 -> 0.15527246464858105 on epoch=174, global_step=350
03/08/2022 20:55:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.375160 on epoch=179
03/08/2022 20:55:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.323539 on epoch=184
03/08/2022 20:55:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342036 on epoch=189
03/08/2022 20:55:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.339565 on epoch=194
03/08/2022 20:55:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.326074 on epoch=199
03/08/2022 20:55:29 - INFO - __main__ - Global step 400 Train loss 0.341275 Rouge-L 0.09769635093418702 on epoch=199
03/08/2022 20:55:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.295840 on epoch=204
03/08/2022 20:55:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.317767 on epoch=209
03/08/2022 20:55:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.309962 on epoch=214
03/08/2022 20:55:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.327107 on epoch=219
03/08/2022 20:55:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.311255 on epoch=224
03/08/2022 20:56:02 - INFO - __main__ - Global step 450 Train loss 0.312386 Rouge-L 0.10009047471710497 on epoch=224
03/08/2022 20:56:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.279029 on epoch=229
03/08/2022 20:56:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.320130 on epoch=234
03/08/2022 20:56:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.280547 on epoch=239
03/08/2022 20:56:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.272315 on epoch=244
03/08/2022 20:56:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.263426 on epoch=249
03/08/2022 20:56:35 - INFO - __main__ - Global step 500 Train loss 0.283090 Rouge-L 0.08775141972694224 on epoch=249
03/08/2022 20:56:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.265728 on epoch=254
03/08/2022 20:56:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.267698 on epoch=259
03/08/2022 20:56:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.283739 on epoch=264
03/08/2022 20:56:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.253881 on epoch=269
03/08/2022 20:57:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.296912 on epoch=274
03/08/2022 20:57:08 - INFO - __main__ - Global step 550 Train loss 0.273591 Rouge-L 0.09527627927404039 on epoch=274
03/08/2022 20:57:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.259016 on epoch=279
03/08/2022 20:57:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.259234 on epoch=284
03/08/2022 20:57:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.268544 on epoch=289
03/08/2022 20:57:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.261507 on epoch=294
03/08/2022 20:57:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.227538 on epoch=299
03/08/2022 20:57:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:57:39 - INFO - __main__ - Printing 3 examples
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 20:57:39 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 20:57:39 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 20:57:39 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 20:57:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 20:57:39 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:57:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 20:57:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 20:57:39 - INFO - __main__ - Printing 3 examples
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 20:57:39 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 20:57:39 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 20:57:39 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 20:57:39 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 20:57:39 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:57:39 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:57:39 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 20:57:42 - INFO - __main__ - Global step 600 Train loss 0.255168 Rouge-L 0.07106343999132297 on epoch=299
03/08/2022 20:57:42 - INFO - __main__ - save last model!
03/08/2022 20:57:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 20:57:48 - INFO - __main__ - Starting training!
03/08/2022 20:58:21 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 20:58:21 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 20:58:21 - INFO - __main__ - Printing 3 examples
03/08/2022 20:58:21 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 20:58:21 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 20:58:21 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 20:58:21 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 20:58:21 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 20:58:21 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 20:58:21 - INFO - __main__ - Tokenizing Input ...
03/08/2022 20:58:22 - INFO - __main__ - Tokenizing Output ...
03/08/2022 20:58:23 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 21:00:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_13_0.0002_8_predictions.txt
03/08/2022 21:00:26 - INFO - __main__ - Rouge-L on test data: 0.1517
03/08/2022 21:00:26 - INFO - __main__ - prefix=samsum_32_13, lr=0.0002, bsz=8, dev_performance=0.15527246464858105, test_performance=0.15173217943211076
03/08/2022 21:00:26 - INFO - __main__ - Running ... prefix=samsum_32_13, lr=0.0001, bsz=8 ...
03/08/2022 21:00:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:00:27 - INFO - __main__ - Printing 3 examples
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Amanda: i’m going shopping after work i want u to come with me  Ruth: i can’t. i’m pretty busy. What do u need to buy?  Amanda: a dress Ruth: you bought one last month remember?! Amanda: i know but need another one Ruth: still don’t understand why? Amanda: i think Rob is gonna pop the question! Ruth: you know he is gonna pop it anyway right?! Amanda: i know but just wanna look stunning..  Ruth: Fine! i’ll help u find the right dress.
03/08/2022 21:00:27 - INFO - __main__ - ['Ruth will help Amanda find a new dress.']
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Adrian: Can you talk? Simon: Not really, anything important? Adrian: Not that much. Simon: I'll be free at 5 Adrian: i'll you then
03/08/2022 21:00:27 - INFO - __main__ - ["Simon can't talk now. Simon will be free at 5."]
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Darcey: We're going to see my favorite band next weekend! Ellis: Who? Darcey: Cheap Trick! I love them! Ellis: Eh, don't know em really. Darcey: They're from my home town so I have to be a fan! LOL! Ellis: Oh, I see! Darcey: They're with Def Leppard. The show is in Nottingham. Ellis: Cool! Darcey: I need to find a place to eat first. Maybe BrewDogs. Ellis: Nah, that ones only pizza. Darcey: Oh, damn. What about the Malaysian place below it? Ellis: That was good. Try that. Darcey: Maybe the greek!
03/08/2022 21:00:27 - INFO - __main__ - ['Darcey is going to Cheap Trick concert next weekend. The concert will be held in Nottingham. Darcey needs to find a place to eat.']
03/08/2022 21:00:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 21:00:27 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:00:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:00:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:00:27 - INFO - __main__ - Printing 3 examples
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Owen: What beer do you want? 😉 Jenna: Coors. But what's available? Owen: Stella, coors, Peroni, Budweiser, carlsberg Noah: Pe-Ro-Ni for the Italian 😜. Owen: <file_photo> Enough? Noah: For this week… 😂😂 Owen: Can someone come down to help 😉 Jenna: Wait. Coming!
03/08/2022 21:00:27 - INFO - __main__ - ['Owen is buying beer. Jenna wants Coors, Noah Peroni.']
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Ollie: Can you get some milk at the store? Lisa: Sure skimmed? Ollie: yes thanks Lisa: no prob
03/08/2022 21:00:27 - INFO - __main__ - ["Lisa will buy skimmed milk on Ollie's request."]
03/08/2022 21:00:27 - INFO - __main__ -  [samsum] summarize: Troy: Ive been at home for half an hour Troy: and I am still cold Ashley: Ye its very cold outside Damon: Its insanely cold Damon: Im feeling sick  Troy: Urgh I think im making some tea  Troy: now  Ashley: good idea
03/08/2022 21:00:27 - INFO - __main__ - ["Troy's making tea."]
03/08/2022 21:00:27 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:00:27 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:00:28 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:00:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:00:38 - INFO - __main__ - Starting training!
03/08/2022 21:00:43 - INFO - __main__ - Step 10 Global step 10 Train loss 17.751324 on epoch=4
03/08/2022 21:00:49 - INFO - __main__ - Step 20 Global step 20 Train loss 14.641275 on epoch=9
03/08/2022 21:00:54 - INFO - __main__ - Step 30 Global step 30 Train loss 10.288412 on epoch=14
03/08/2022 21:01:00 - INFO - __main__ - Step 40 Global step 40 Train loss 7.373319 on epoch=19
03/08/2022 21:01:06 - INFO - __main__ - Step 50 Global step 50 Train loss 6.004380 on epoch=24
03/08/2022 21:01:20 - INFO - __main__ - Global step 50 Train loss 11.211743 Rouge-L 0.30368998757021104 on epoch=24
03/08/2022 21:01:51 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.30368998757021104 on epoch=24, global_step=50
03/08/2022 21:01:57 - INFO - __main__ - Step 60 Global step 60 Train loss 4.142968 on epoch=29
03/08/2022 21:02:02 - INFO - __main__ - Step 70 Global step 70 Train loss 3.928450 on epoch=34
03/08/2022 21:02:08 - INFO - __main__ - Step 80 Global step 80 Train loss 3.412332 on epoch=39
03/08/2022 21:02:14 - INFO - __main__ - Step 90 Global step 90 Train loss 2.866520 on epoch=44
03/08/2022 21:02:20 - INFO - __main__ - Step 100 Global step 100 Train loss 2.456265 on epoch=49
03/08/2022 21:02:28 - INFO - __main__ - Global step 100 Train loss 3.361307 Rouge-L 0.2979753924475009 on epoch=49
03/08/2022 21:02:33 - INFO - __main__ - Step 110 Global step 110 Train loss 2.090615 on epoch=54
03/08/2022 21:02:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.858409 on epoch=59
03/08/2022 21:02:45 - INFO - __main__ - Step 130 Global step 130 Train loss 1.665825 on epoch=64
03/08/2022 21:02:51 - INFO - __main__ - Step 140 Global step 140 Train loss 1.521387 on epoch=69
03/08/2022 21:02:56 - INFO - __main__ - Step 150 Global step 150 Train loss 1.479847 on epoch=74
03/08/2022 21:03:00 - INFO - __main__ - Global step 150 Train loss 1.723216 Rouge-L 0.23594465665892186 on epoch=74
03/08/2022 21:03:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.400441 on epoch=79
03/08/2022 21:03:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.292525 on epoch=84
03/08/2022 21:03:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.230768 on epoch=89
03/08/2022 21:03:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.131514 on epoch=94
03/08/2022 21:03:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.189586 on epoch=99
03/08/2022 21:03:33 - INFO - __main__ - Global step 200 Train loss 1.248967 Rouge-L 0.2414724470921582 on epoch=99
03/08/2022 21:03:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.144640 on epoch=104
03/08/2022 21:03:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.043229 on epoch=109
03/08/2022 21:03:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.949086 on epoch=114
03/08/2022 21:03:56 - INFO - __main__ - Step 240 Global step 240 Train loss 1.070006 on epoch=119
03/08/2022 21:04:01 - INFO - __main__ - Step 250 Global step 250 Train loss 1.000916 on epoch=124
03/08/2022 21:04:05 - INFO - __main__ - Global step 250 Train loss 1.041575 Rouge-L 0.21370845649009362 on epoch=124
03/08/2022 21:04:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.930329 on epoch=129
03/08/2022 21:04:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.862966 on epoch=134
03/08/2022 21:04:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.868344 on epoch=139
03/08/2022 21:04:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.828854 on epoch=144
03/08/2022 21:04:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.843170 on epoch=149
03/08/2022 21:04:37 - INFO - __main__ - Global step 300 Train loss 0.866733 Rouge-L 0.2064321729312143 on epoch=149
03/08/2022 21:04:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.815391 on epoch=154
03/08/2022 21:04:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.770448 on epoch=159
03/08/2022 21:04:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.716623 on epoch=164
03/08/2022 21:05:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.758050 on epoch=169
03/08/2022 21:05:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.710415 on epoch=174
03/08/2022 21:05:09 - INFO - __main__ - Global step 350 Train loss 0.754185 Rouge-L 0.22152393713711388 on epoch=174
03/08/2022 21:05:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.723247 on epoch=179
03/08/2022 21:05:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.644997 on epoch=184
03/08/2022 21:05:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.586454 on epoch=189
03/08/2022 21:05:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.586086 on epoch=194
03/08/2022 21:05:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.593647 on epoch=199
03/08/2022 21:05:42 - INFO - __main__ - Global step 400 Train loss 0.626886 Rouge-L 0.2213245273718449 on epoch=199
03/08/2022 21:05:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.563667 on epoch=204
03/08/2022 21:05:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.532843 on epoch=209
03/08/2022 21:05:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.552371 on epoch=214
03/08/2022 21:06:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.470741 on epoch=219
03/08/2022 21:06:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.511776 on epoch=224
03/08/2022 21:06:15 - INFO - __main__ - Global step 450 Train loss 0.526280 Rouge-L 0.20876949456912236 on epoch=224
03/08/2022 21:06:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.511886 on epoch=229
03/08/2022 21:06:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.412299 on epoch=234
03/08/2022 21:06:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.409946 on epoch=239
03/08/2022 21:06:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.419757 on epoch=244
03/08/2022 21:06:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.419596 on epoch=249
03/08/2022 21:06:48 - INFO - __main__ - Global step 500 Train loss 0.434697 Rouge-L 0.21725537484385216 on epoch=249
03/08/2022 21:06:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.378881 on epoch=254
03/08/2022 21:07:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.403121 on epoch=259
03/08/2022 21:07:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.387798 on epoch=264
03/08/2022 21:07:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.403312 on epoch=269
03/08/2022 21:07:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.374342 on epoch=274
03/08/2022 21:07:20 - INFO - __main__ - Global step 550 Train loss 0.389491 Rouge-L 0.24569263547375383 on epoch=274
03/08/2022 21:07:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.386148 on epoch=279
03/08/2022 21:07:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.329645 on epoch=284
03/08/2022 21:07:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.399119 on epoch=289
03/08/2022 21:07:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.348515 on epoch=294
03/08/2022 21:07:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.356856 on epoch=299
03/08/2022 21:07:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:07:49 - INFO - __main__ - Printing 3 examples
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:07:49 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:07:49 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:07:49 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:07:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 21:07:49 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:07:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:07:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:07:49 - INFO - __main__ - Printing 3 examples
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:07:49 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:07:49 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:07:49 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:07:49 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:07:49 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:07:49 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:07:50 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:07:50 - INFO - __main__ - Global step 600 Train loss 0.364057 Rouge-L 0.21253524095124096 on epoch=299
03/08/2022 21:07:50 - INFO - __main__ - save last model!
03/08/2022 21:08:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:08:00 - INFO - __main__ - Starting training!
03/08/2022 21:08:35 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 21:08:36 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 21:08:36 - INFO - __main__ - Printing 3 examples
03/08/2022 21:08:36 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 21:08:36 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 21:08:36 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 21:08:36 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 21:08:36 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 21:08:36 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 21:08:36 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:08:36 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:08:37 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 21:14:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_13_0.0001_8_predictions.txt
03/08/2022 21:14:05 - INFO - __main__ - Rouge-L on test data: 0.3378
03/08/2022 21:14:05 - INFO - __main__ - prefix=samsum_32_13, lr=0.0001, bsz=8, dev_performance=0.30368998757021104, test_performance=0.33781826626586214
03/08/2022 21:14:05 - INFO - __main__ - Running ... prefix=samsum_32_21, lr=0.0005, bsz=8 ...
03/08/2022 21:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:14:06 - INFO - __main__ - Printing 3 examples
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:14:06 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:14:06 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:14:06 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:14:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 21:14:06 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:14:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:14:06 - INFO - __main__ - Printing 3 examples
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:14:06 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:14:06 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:14:06 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:14:06 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:14:06 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:14:06 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:14:06 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:14:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:14:16 - INFO - __main__ - Starting training!
03/08/2022 21:14:24 - INFO - __main__ - Step 10 Global step 10 Train loss 18.712185 on epoch=4
03/08/2022 21:14:30 - INFO - __main__ - Step 20 Global step 20 Train loss 9.877957 on epoch=9
03/08/2022 21:14:35 - INFO - __main__ - Step 30 Global step 30 Train loss 4.029420 on epoch=14
03/08/2022 21:14:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.185184 on epoch=19
03/08/2022 21:14:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.436589 on epoch=24
03/08/2022 21:14:58 - INFO - __main__ - Global step 50 Train loss 7.248267 Rouge-L 0.25645172648512504 on epoch=24
03/08/2022 21:15:31 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.25645172648512504 on epoch=24, global_step=50
03/08/2022 21:15:36 - INFO - __main__ - Step 60 Global step 60 Train loss 1.091423 on epoch=29
03/08/2022 21:15:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.866751 on epoch=34
03/08/2022 21:15:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.792991 on epoch=39
03/08/2022 21:15:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.635976 on epoch=44
03/08/2022 21:15:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.554039 on epoch=49
03/08/2022 21:16:02 - INFO - __main__ - Global step 100 Train loss 0.788236 Rouge-L 0.20787021242558645 on epoch=49
03/08/2022 21:16:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.403789 on epoch=54
03/08/2022 21:16:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.412390 on epoch=59
03/08/2022 21:16:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.377497 on epoch=64
03/08/2022 21:16:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.278298 on epoch=69
03/08/2022 21:16:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.302418 on epoch=74
03/08/2022 21:16:33 - INFO - __main__ - Global step 150 Train loss 0.354878 Rouge-L 0.22215580621156594 on epoch=74
03/08/2022 21:16:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.338345 on epoch=79
03/08/2022 21:16:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.273046 on epoch=84
03/08/2022 21:16:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.265667 on epoch=89
03/08/2022 21:16:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.313998 on epoch=94
03/08/2022 21:17:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.251570 on epoch=99
03/08/2022 21:17:03 - INFO - __main__ - Global step 200 Train loss 0.288525 Rouge-L 0.20537322736911867 on epoch=99
03/08/2022 21:17:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.238705 on epoch=104
03/08/2022 21:17:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.240080 on epoch=109
03/08/2022 21:17:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.230934 on epoch=114
03/08/2022 21:17:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.240802 on epoch=119
03/08/2022 21:17:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.220993 on epoch=124
03/08/2022 21:17:35 - INFO - __main__ - Global step 250 Train loss 0.234303 Rouge-L 0.1961611581307174 on epoch=124
03/08/2022 21:17:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.235668 on epoch=129
03/08/2022 21:17:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.221701 on epoch=134
03/08/2022 21:17:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.236887 on epoch=139
03/08/2022 21:17:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.246721 on epoch=144
03/08/2022 21:18:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.209500 on epoch=149
03/08/2022 21:18:06 - INFO - __main__ - Global step 300 Train loss 0.230096 Rouge-L 0.17866362152214965 on epoch=149
03/08/2022 21:18:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.199926 on epoch=154
03/08/2022 21:18:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.179318 on epoch=159
03/08/2022 21:18:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.183785 on epoch=164
03/08/2022 21:18:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.187347 on epoch=169
03/08/2022 21:18:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.176859 on epoch=174
03/08/2022 21:18:37 - INFO - __main__ - Global step 350 Train loss 0.185447 Rouge-L 0.1661470809540483 on epoch=174
03/08/2022 21:18:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.166177 on epoch=179
03/08/2022 21:18:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.201400 on epoch=184
03/08/2022 21:18:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.168982 on epoch=189
03/08/2022 21:18:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.187272 on epoch=194
03/08/2022 21:19:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.154514 on epoch=199
03/08/2022 21:19:09 - INFO - __main__ - Global step 400 Train loss 0.175669 Rouge-L 0.1818907020393335 on epoch=199
03/08/2022 21:19:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.194936 on epoch=204
03/08/2022 21:19:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.155982 on epoch=209
03/08/2022 21:19:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.165463 on epoch=214
03/08/2022 21:19:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.140175 on epoch=219
03/08/2022 21:19:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.147693 on epoch=224
03/08/2022 21:19:40 - INFO - __main__ - Global step 450 Train loss 0.160850 Rouge-L 0.18723942821417847 on epoch=224
03/08/2022 21:19:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.140353 on epoch=229
03/08/2022 21:19:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.140186 on epoch=234
03/08/2022 21:19:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.145740 on epoch=239
03/08/2022 21:20:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.139157 on epoch=244
03/08/2022 21:20:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.136211 on epoch=249
03/08/2022 21:20:11 - INFO - __main__ - Global step 500 Train loss 0.140329 Rouge-L 0.18726167431692065 on epoch=249
03/08/2022 21:20:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.145603 on epoch=254
03/08/2022 21:20:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.147367 on epoch=259
03/08/2022 21:20:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.146828 on epoch=264
03/08/2022 21:20:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.135449 on epoch=269
03/08/2022 21:20:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.149919 on epoch=274
03/08/2022 21:20:42 - INFO - __main__ - Global step 550 Train loss 0.145033 Rouge-L 0.1807946429983183 on epoch=274
03/08/2022 21:20:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.135343 on epoch=279
03/08/2022 21:20:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.126372 on epoch=284
03/08/2022 21:20:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.132867 on epoch=289
03/08/2022 21:21:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.131893 on epoch=294
03/08/2022 21:21:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.132054 on epoch=299
03/08/2022 21:21:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:21:11 - INFO - __main__ - Printing 3 examples
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:21:11 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:21:11 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:21:11 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:21:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 21:21:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:21:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:21:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:21:11 - INFO - __main__ - Printing 3 examples
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:21:11 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:21:11 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:21:11 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:21:11 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:21:11 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:21:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:21:11 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:21:13 - INFO - __main__ - Global step 600 Train loss 0.131706 Rouge-L 0.1869574900424249 on epoch=299
03/08/2022 21:21:13 - INFO - __main__ - save last model!
03/08/2022 21:21:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:21:21 - INFO - __main__ - Starting training!
03/08/2022 21:21:59 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 21:21:59 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 21:21:59 - INFO - __main__ - Printing 3 examples
03/08/2022 21:21:59 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 21:21:59 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 21:21:59 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 21:21:59 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 21:21:59 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 21:21:59 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 21:21:59 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:22:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:22:01 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 21:26:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_21_0.0005_8_predictions.txt
03/08/2022 21:26:23 - INFO - __main__ - Rouge-L on test data: 0.2967
03/08/2022 21:26:23 - INFO - __main__ - prefix=samsum_32_21, lr=0.0005, bsz=8, dev_performance=0.25645172648512504, test_performance=0.29668494389067257
03/08/2022 21:26:23 - INFO - __main__ - Running ... prefix=samsum_32_21, lr=0.0003, bsz=8 ...
03/08/2022 21:26:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:26:24 - INFO - __main__ - Printing 3 examples
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:26:24 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:26:24 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:26:24 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:26:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 21:26:24 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:26:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:26:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:26:24 - INFO - __main__ - Printing 3 examples
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:26:24 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:26:24 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:26:24 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:26:24 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:26:24 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:26:24 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:26:25 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:26:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:26:35 - INFO - __main__ - Starting training!
03/08/2022 21:26:40 - INFO - __main__ - Step 10 Global step 10 Train loss 19.129108 on epoch=4
03/08/2022 21:26:45 - INFO - __main__ - Step 20 Global step 20 Train loss 12.329781 on epoch=9
03/08/2022 21:26:50 - INFO - __main__ - Step 30 Global step 30 Train loss 7.828538 on epoch=14
03/08/2022 21:26:55 - INFO - __main__ - Step 40 Global step 40 Train loss 5.582516 on epoch=19
03/08/2022 21:27:01 - INFO - __main__ - Step 50 Global step 50 Train loss 3.363287 on epoch=24
03/08/2022 21:27:10 - INFO - __main__ - Global step 50 Train loss 9.646646 Rouge-L 0.22726402714870797 on epoch=24
03/08/2022 21:27:42 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.22726402714870797 on epoch=24, global_step=50
03/08/2022 21:27:47 - INFO - __main__ - Step 60 Global step 60 Train loss 2.174813 on epoch=29
03/08/2022 21:27:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.689713 on epoch=34
03/08/2022 21:27:58 - INFO - __main__ - Step 80 Global step 80 Train loss 1.294246 on epoch=39
03/08/2022 21:28:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.176128 on epoch=44
03/08/2022 21:28:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.069902 on epoch=49
03/08/2022 21:28:13 - INFO - __main__ - Global step 100 Train loss 1.480960 Rouge-L 0.2249590286319132 on epoch=49
03/08/2022 21:28:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.961477 on epoch=54
03/08/2022 21:28:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.875964 on epoch=59
03/08/2022 21:28:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.781835 on epoch=64
03/08/2022 21:28:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.704863 on epoch=69
03/08/2022 21:28:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.566522 on epoch=74
03/08/2022 21:28:44 - INFO - __main__ - Global step 150 Train loss 0.778132 Rouge-L 0.2100314336491762 on epoch=74
03/08/2022 21:28:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.560478 on epoch=79
03/08/2022 21:28:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.470726 on epoch=84
03/08/2022 21:29:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.398692 on epoch=89
03/08/2022 21:29:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.418782 on epoch=94
03/08/2022 21:29:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.392870 on epoch=99
03/08/2022 21:29:16 - INFO - __main__ - Global step 200 Train loss 0.448310 Rouge-L 0.21116133250733898 on epoch=99
03/08/2022 21:29:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.337720 on epoch=104
03/08/2022 21:29:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.381987 on epoch=109
03/08/2022 21:29:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.327826 on epoch=114
03/08/2022 21:29:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.357078 on epoch=119
03/08/2022 21:29:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.338633 on epoch=124
03/08/2022 21:29:48 - INFO - __main__ - Global step 250 Train loss 0.348649 Rouge-L 0.189670151794508 on epoch=124
03/08/2022 21:29:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.316178 on epoch=129
03/08/2022 21:29:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.351047 on epoch=134
03/08/2022 21:30:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.322501 on epoch=139
03/08/2022 21:30:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.292316 on epoch=144
03/08/2022 21:30:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.322828 on epoch=149
03/08/2022 21:30:21 - INFO - __main__ - Global step 300 Train loss 0.320974 Rouge-L 0.18878013396735308 on epoch=149
03/08/2022 21:30:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.328209 on epoch=154
03/08/2022 21:30:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.321366 on epoch=159
03/08/2022 21:30:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.310288 on epoch=164
03/08/2022 21:30:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.305212 on epoch=169
03/08/2022 21:30:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.257072 on epoch=174
03/08/2022 21:30:54 - INFO - __main__ - Global step 350 Train loss 0.304429 Rouge-L 0.1775361144666661 on epoch=174
03/08/2022 21:31:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.269128 on epoch=179
03/08/2022 21:31:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.289076 on epoch=184
03/08/2022 21:31:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.228144 on epoch=189
03/08/2022 21:31:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.262692 on epoch=194
03/08/2022 21:31:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.239565 on epoch=199
03/08/2022 21:31:25 - INFO - __main__ - Global step 400 Train loss 0.257721 Rouge-L 0.14325193132889355 on epoch=199
03/08/2022 21:31:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.245439 on epoch=204
03/08/2022 21:31:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.229186 on epoch=209
03/08/2022 21:31:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.236097 on epoch=214
03/08/2022 21:31:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.238104 on epoch=219
03/08/2022 21:31:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.205604 on epoch=224
03/08/2022 21:31:58 - INFO - __main__ - Global step 450 Train loss 0.230886 Rouge-L 0.15542228800355873 on epoch=224
03/08/2022 21:32:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.215117 on epoch=229
03/08/2022 21:32:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.245486 on epoch=234
03/08/2022 21:32:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.242566 on epoch=239
03/08/2022 21:32:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.207316 on epoch=244
03/08/2022 21:32:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.199731 on epoch=249
03/08/2022 21:32:29 - INFO - __main__ - Global step 500 Train loss 0.222043 Rouge-L 0.16856519622453703 on epoch=249
03/08/2022 21:32:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.178911 on epoch=254
03/08/2022 21:32:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.201853 on epoch=259
03/08/2022 21:32:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.201929 on epoch=264
03/08/2022 21:32:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.202418 on epoch=269
03/08/2022 21:32:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.195434 on epoch=274
03/08/2022 21:33:03 - INFO - __main__ - Global step 550 Train loss 0.196109 Rouge-L 0.1648560764189945 on epoch=274
03/08/2022 21:33:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.188364 on epoch=279
03/08/2022 21:33:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.182892 on epoch=284
03/08/2022 21:33:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.205580 on epoch=289
03/08/2022 21:33:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.195327 on epoch=294
03/08/2022 21:33:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.172705 on epoch=299
03/08/2022 21:33:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:33:33 - INFO - __main__ - Printing 3 examples
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:33:33 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:33:33 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:33:33 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:33:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 21:33:33 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:33:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:33:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:33:33 - INFO - __main__ - Printing 3 examples
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:33:33 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:33:33 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:33:33 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:33:33 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:33:33 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:33:33 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:33:33 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:33:36 - INFO - __main__ - Global step 600 Train loss 0.188974 Rouge-L 0.13721572116250927 on epoch=299
03/08/2022 21:33:36 - INFO - __main__ - save last model!
03/08/2022 21:33:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:33:42 - INFO - __main__ - Starting training!
03/08/2022 21:34:22 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 21:34:22 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 21:34:22 - INFO - __main__ - Printing 3 examples
03/08/2022 21:34:22 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 21:34:22 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 21:34:22 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 21:34:22 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 21:34:22 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 21:34:22 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 21:34:22 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:34:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:34:24 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 21:37:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_21_0.0003_8_predictions.txt
03/08/2022 21:37:03 - INFO - __main__ - Rouge-L on test data: 0.2456
03/08/2022 21:37:03 - INFO - __main__ - prefix=samsum_32_21, lr=0.0003, bsz=8, dev_performance=0.22726402714870797, test_performance=0.2456497096219108
03/08/2022 21:37:03 - INFO - __main__ - Running ... prefix=samsum_32_21, lr=0.0002, bsz=8 ...
03/08/2022 21:37:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:37:04 - INFO - __main__ - Printing 3 examples
03/08/2022 21:37:04 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:37:04 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:37:04 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:37:04 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:37:04 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:37:04 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:37:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 21:37:04 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:37:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:37:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:37:05 - INFO - __main__ - Printing 3 examples
03/08/2022 21:37:05 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:37:05 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:37:05 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:37:05 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:37:05 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:37:05 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:37:05 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:37:05 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:37:05 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:37:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:37:15 - INFO - __main__ - Starting training!
03/08/2022 21:37:20 - INFO - __main__ - Step 10 Global step 10 Train loss 18.772968 on epoch=4
03/08/2022 21:37:25 - INFO - __main__ - Step 20 Global step 20 Train loss 14.500198 on epoch=9
03/08/2022 21:37:30 - INFO - __main__ - Step 30 Global step 30 Train loss 10.314003 on epoch=14
03/08/2022 21:37:35 - INFO - __main__ - Step 40 Global step 40 Train loss 8.201986 on epoch=19
03/08/2022 21:37:41 - INFO - __main__ - Step 50 Global step 50 Train loss 6.549227 on epoch=24
03/08/2022 21:37:55 - INFO - __main__ - Global step 50 Train loss 11.667676 Rouge-L 0.2878914597325778 on epoch=24
03/08/2022 21:38:27 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.2878914597325778 on epoch=24, global_step=50
03/08/2022 21:38:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.609124 on epoch=29
03/08/2022 21:38:38 - INFO - __main__ - Step 70 Global step 70 Train loss 3.377384 on epoch=34
03/08/2022 21:38:43 - INFO - __main__ - Step 80 Global step 80 Train loss 2.606929 on epoch=39
03/08/2022 21:38:49 - INFO - __main__ - Step 90 Global step 90 Train loss 2.211917 on epoch=44
03/08/2022 21:38:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.945118 on epoch=49
03/08/2022 21:38:58 - INFO - __main__ - Global step 100 Train loss 2.950094 Rouge-L 0.2664605535954111 on epoch=49
03/08/2022 21:39:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.928596 on epoch=54
03/08/2022 21:39:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.579073 on epoch=59
03/08/2022 21:39:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.212270 on epoch=64
03/08/2022 21:39:21 - INFO - __main__ - Step 140 Global step 140 Train loss 1.082627 on epoch=69
03/08/2022 21:39:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.070878 on epoch=74
03/08/2022 21:39:30 - INFO - __main__ - Global step 150 Train loss 1.374689 Rouge-L 0.25034910062356713 on epoch=74
03/08/2022 21:39:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.934637 on epoch=79
03/08/2022 21:39:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.864218 on epoch=84
03/08/2022 21:39:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.817259 on epoch=89
03/08/2022 21:39:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.845639 on epoch=94
03/08/2022 21:39:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.728779 on epoch=99
03/08/2022 21:40:02 - INFO - __main__ - Global step 200 Train loss 0.838106 Rouge-L 0.2348137423341623 on epoch=99
03/08/2022 21:40:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.666026 on epoch=104
03/08/2022 21:40:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.616214 on epoch=109
03/08/2022 21:40:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.591665 on epoch=114
03/08/2022 21:40:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.522752 on epoch=119
03/08/2022 21:40:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.547817 on epoch=124
03/08/2022 21:40:34 - INFO - __main__ - Global step 250 Train loss 0.588895 Rouge-L 0.2085575906951187 on epoch=124
03/08/2022 21:40:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.477710 on epoch=129
03/08/2022 21:40:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.459397 on epoch=134
03/08/2022 21:40:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.479282 on epoch=139
03/08/2022 21:40:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.379241 on epoch=144
03/08/2022 21:41:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.452039 on epoch=149
03/08/2022 21:41:05 - INFO - __main__ - Global step 300 Train loss 0.449534 Rouge-L 0.24202917916952602 on epoch=149
03/08/2022 21:41:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.370803 on epoch=154
03/08/2022 21:41:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.336099 on epoch=159
03/08/2022 21:41:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.398957 on epoch=164
03/08/2022 21:41:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.347429 on epoch=169
03/08/2022 21:41:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.293191 on epoch=174
03/08/2022 21:41:36 - INFO - __main__ - Global step 350 Train loss 0.349296 Rouge-L 0.20923382578423066 on epoch=174
03/08/2022 21:41:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.313888 on epoch=179
03/08/2022 21:41:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.347605 on epoch=184
03/08/2022 21:41:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.345358 on epoch=189
03/08/2022 21:41:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.345601 on epoch=194
03/08/2022 21:42:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.313233 on epoch=199
03/08/2022 21:42:07 - INFO - __main__ - Global step 400 Train loss 0.333137 Rouge-L 0.19419657054055156 on epoch=199
03/08/2022 21:42:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.302798 on epoch=204
03/08/2022 21:42:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.243821 on epoch=209
03/08/2022 21:42:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.262734 on epoch=214
03/08/2022 21:42:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.310119 on epoch=219
03/08/2022 21:42:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.297895 on epoch=224
03/08/2022 21:42:38 - INFO - __main__ - Global step 450 Train loss 0.283473 Rouge-L 0.21344847921506466 on epoch=224
03/08/2022 21:42:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.313387 on epoch=229
03/08/2022 21:42:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.300331 on epoch=234
03/08/2022 21:42:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.308790 on epoch=239
03/08/2022 21:42:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.280545 on epoch=244
03/08/2022 21:43:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.314712 on epoch=249
03/08/2022 21:43:10 - INFO - __main__ - Global step 500 Train loss 0.303553 Rouge-L 0.20278472951706633 on epoch=249
03/08/2022 21:43:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.274401 on epoch=254
03/08/2022 21:43:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.276286 on epoch=259
03/08/2022 21:43:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.281360 on epoch=264
03/08/2022 21:43:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.279113 on epoch=269
03/08/2022 21:43:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.266624 on epoch=274
03/08/2022 21:43:40 - INFO - __main__ - Global step 550 Train loss 0.275557 Rouge-L 0.18307028642822393 on epoch=274
03/08/2022 21:43:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.248039 on epoch=279
03/08/2022 21:43:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.266511 on epoch=284
03/08/2022 21:43:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.271099 on epoch=289
03/08/2022 21:44:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.214735 on epoch=294
03/08/2022 21:44:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.236869 on epoch=299
03/08/2022 21:44:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:44:08 - INFO - __main__ - Printing 3 examples
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:44:08 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:44:08 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:44:08 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:44:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 21:44:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:44:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:44:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:44:08 - INFO - __main__ - Printing 3 examples
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:44:08 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:44:08 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:44:08 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:44:08 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:44:08 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:44:08 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:44:09 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:44:12 - INFO - __main__ - Global step 600 Train loss 0.247450 Rouge-L 0.13853171273489873 on epoch=299
03/08/2022 21:44:12 - INFO - __main__ - save last model!
03/08/2022 21:44:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:44:18 - INFO - __main__ - Starting training!
03/08/2022 21:45:02 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 21:45:02 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 21:45:02 - INFO - __main__ - Printing 3 examples
03/08/2022 21:45:02 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 21:45:02 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 21:45:02 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 21:45:02 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 21:45:02 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 21:45:02 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 21:45:02 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:45:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:45:04 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 21:50:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_21_0.0002_8_predictions.txt
03/08/2022 21:50:57 - INFO - __main__ - Rouge-L on test data: 0.3200
03/08/2022 21:50:58 - INFO - __main__ - prefix=samsum_32_21, lr=0.0002, bsz=8, dev_performance=0.2878914597325778, test_performance=0.31998704867007405
03/08/2022 21:50:58 - INFO - __main__ - Running ... prefix=samsum_32_21, lr=0.0001, bsz=8 ...
03/08/2022 21:50:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:50:59 - INFO - __main__ - Printing 3 examples
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Ross: Hey Isabella ! you have lost weight. you must be on a diet  Ayehsa: No Ross, I have started jogging. Ross: Well that's great. Ayehsa: Thank you
03/08/2022 21:50:59 - INFO - __main__ - ['Ayehsa has started jogging.']
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Dean: Hi Abbi, you ok? Abbi: Yeah, just watching Dr Who, I'll pause it. Dean: Yeah, keep meaning to catch it on IPlayer. Any good? Abbi: Course, better to have a woman in it! She's so quirky and funny, love her companions too, even the old guy! Dean: Hmmm, heard mixed reviews. Bit too preachy and history lessonish, they say. Abbi: Suppose you're right, still I enjoy it. Dean: Been doing any coursework lately? You know, the one due in weds? Abbi: What coursework? Only kidding, I've done a bit, love learning about civil rights. Dean: Agree, it's interesting, but awful too. Abbi: Actually, Dr Who just did an ep about Rosa Parks and the bus boycott. Dean: Really! I'll have to check it out. Done much this weekend? Abbi: Yeh, swimming practice Sat., then work. Uncles party last night, good laugh! Dean: I had work too, working at Greggs part-time. Abbi: So jealous! You'll have to sort me out with a couple of free yum yums! Dean: See what I can do! Anyway, see you tomorrow pm, sociology! Zzzzzz. Abbi: Oh God, yeah. Can't stand it, dropping it at the end of this year, can't wait. Dean: Me too! See you! Abbi: Bye, Dean!
03/08/2022 21:50:59 - INFO - __main__ - ["Abbi and Dean watch Dr Who. They have a work due on Wednesday. Last weekend Abbi went to her swimming practice and her uncle's party. Dean was working."]
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Hana: Leave me alobe! Derek: please talk to me Hana: I have nothing 2 say 2 u Derek: I'm sorry, please let's meet Hana: no Derek: Let me explain! Hana: NO!
03/08/2022 21:50:59 - INFO - __main__ - ['Hana does not want to meet Derek or hear his explanations.']
03/08/2022 21:50:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 21:50:59 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:50:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:50:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:50:59 - INFO - __main__ - Printing 3 examples
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Monica: darling do you want me to buy some ice-cream after all? :) Charles: if you want Charles: I mean I'd have some, sure, but you don't have to buy it :) Monica: the thing is I'm stuffed to the brim Monica: and instead of ice-cream, I should have a cup of tea Monica: and maybe I should do some exercises haha Monica: but if you fancy an ice cream I'll go and buy it Charles: no, I think I'd rather eat something else, thank you :* Monica: okey dokey Monica: then I'll be at your place in 10 minutes Charles: OK Charles: just let me know when you're there Monica: will do :)
03/08/2022 21:50:59 - INFO - __main__ - ["Monica will be at Charles' place in 10 minutes and she is stuffed to the brim. Charles doesn't want any ice-cream, he'll eat something else."]
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Mike: i'm gonna throw that game outside the window Martin: playing FIFA again? Mike: how do you know? Martin: easy, each time you rage about a game it's always about FIFA Martin: you lost again? Mike: was playing online and it kept disconnecting me Martin: check your Internet Martin: I never had any issues with connection when playing FIFA Mike: I just have the worst of luck when it comes to it Mike: I end up getting destroyed or getting disconnected when I'm winning Martin: sounds more like excuses :P Martin: practice more and you'll win more Martin: or just buy better players Mike: you know I won't Mike: I don't want to pay more for this game Mike: the whole pack system is just another cash grab by EA Martin: like in most of sports games recently Martin: it's more about cash and packs than about the game Mike: hate it Martin: stop buying it then Mike: but it's FIFA Mike: I always buy FIFA
03/08/2022 21:50:59 - INFO - __main__ - ['Mike is raving about the FIFA game, because it disconnected him, when he was playing online.']
03/08/2022 21:50:59 - INFO - __main__ -  [samsum] summarize: Misha: Momma, does god see us all the time? Mother: Yes,dear. He watches us all the time exactly the way I do. Misha: Does he every get tired?  Mother: No, he never gets tired of watching over us. He is the one who takes care of us. He is the most powerful of all. Misha: Does God ever sleep? Mother: He does, but with his eyes open. Misha: Why is it so? Mother: Because if he sleeps the world will come to a halt. Misha: Can he perform miracles?  Mother: Yes he can. We are his miracles. Every creature he has created and every being on this earth is God's miracle. Misha: Wow, I wish I can see God. Mother: Sure we do see God all the time. We see him in different forms.Just that we don't recognize him most of the time.
03/08/2022 21:50:59 - INFO - __main__ - ['Misha asks her mother a number of questions about God: if he watches us all the time, if he gets tired, if he ever sleeps and if he can perform miracles. Mother answers positively and tells Misha that we can also see God all the time in different forms.']
03/08/2022 21:50:59 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:50:59 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:50:59 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:51:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:51:08 - INFO - __main__ - Starting training!
03/08/2022 21:51:13 - INFO - __main__ - Step 10 Global step 10 Train loss 19.176428 on epoch=4
03/08/2022 21:51:18 - INFO - __main__ - Step 20 Global step 20 Train loss 17.038906 on epoch=9
03/08/2022 21:51:24 - INFO - __main__ - Step 30 Global step 30 Train loss 11.527008 on epoch=14
03/08/2022 21:51:29 - INFO - __main__ - Step 40 Global step 40 Train loss 7.930232 on epoch=19
03/08/2022 21:51:35 - INFO - __main__ - Step 50 Global step 50 Train loss 6.554618 on epoch=24
03/08/2022 21:51:47 - INFO - __main__ - Global step 50 Train loss 12.445438 Rouge-L 0.2931919636999839 on epoch=24
03/08/2022 21:52:19 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.2931919636999839 on epoch=24, global_step=50
03/08/2022 21:52:25 - INFO - __main__ - Step 60 Global step 60 Train loss 4.995404 on epoch=29
03/08/2022 21:52:30 - INFO - __main__ - Step 70 Global step 70 Train loss 4.173440 on epoch=34
03/08/2022 21:52:36 - INFO - __main__ - Step 80 Global step 80 Train loss 3.473540 on epoch=39
03/08/2022 21:52:41 - INFO - __main__ - Step 90 Global step 90 Train loss 3.537603 on epoch=44
03/08/2022 21:52:47 - INFO - __main__ - Step 100 Global step 100 Train loss 2.698073 on epoch=49
03/08/2022 21:52:58 - INFO - __main__ - Global step 100 Train loss 3.775612 Rouge-L 0.22318140695541305 on epoch=49
03/08/2022 21:53:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.450773 on epoch=54
03/08/2022 21:53:09 - INFO - __main__ - Step 120 Global step 120 Train loss 2.132006 on epoch=59
03/08/2022 21:53:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.775254 on epoch=64
03/08/2022 21:53:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.671719 on epoch=69
03/08/2022 21:53:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.613028 on epoch=74
03/08/2022 21:53:29 - INFO - __main__ - Global step 150 Train loss 1.928556 Rouge-L 0.21547856732492665 on epoch=74
03/08/2022 21:53:35 - INFO - __main__ - Step 160 Global step 160 Train loss 1.581443 on epoch=79
03/08/2022 21:53:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.412681 on epoch=84
03/08/2022 21:53:46 - INFO - __main__ - Step 180 Global step 180 Train loss 1.228015 on epoch=89
03/08/2022 21:53:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.215377 on epoch=94
03/08/2022 21:53:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.133525 on epoch=99
03/08/2022 21:54:02 - INFO - __main__ - Global step 200 Train loss 1.314208 Rouge-L 0.1952301461165316 on epoch=99
03/08/2022 21:54:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.151901 on epoch=104
03/08/2022 21:54:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.997218 on epoch=109
03/08/2022 21:54:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.031835 on epoch=114
03/08/2022 21:54:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.010103 on epoch=119
03/08/2022 21:54:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.862212 on epoch=124
03/08/2022 21:54:33 - INFO - __main__ - Global step 250 Train loss 1.010654 Rouge-L 0.21172766567869783 on epoch=124
03/08/2022 21:54:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.868534 on epoch=129
03/08/2022 21:54:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.908840 on epoch=134
03/08/2022 21:54:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.847806 on epoch=139
03/08/2022 21:54:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.846949 on epoch=144
03/08/2022 21:55:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.769377 on epoch=149
03/08/2022 21:55:05 - INFO - __main__ - Global step 300 Train loss 0.848301 Rouge-L 0.2449265087276119 on epoch=149
03/08/2022 21:55:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.797563 on epoch=154
03/08/2022 21:55:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.714336 on epoch=159
03/08/2022 21:55:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.716966 on epoch=164
03/08/2022 21:55:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.802850 on epoch=169
03/08/2022 21:55:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.798608 on epoch=174
03/08/2022 21:55:37 - INFO - __main__ - Global step 350 Train loss 0.766065 Rouge-L 0.21432633527887313 on epoch=174
03/08/2022 21:55:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.731897 on epoch=179
03/08/2022 21:55:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.680257 on epoch=184
03/08/2022 21:55:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.604973 on epoch=189
03/08/2022 21:55:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.603438 on epoch=194
03/08/2022 21:56:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.521199 on epoch=199
03/08/2022 21:56:08 - INFO - __main__ - Global step 400 Train loss 0.628353 Rouge-L 0.2205783997193872 on epoch=199
03/08/2022 21:56:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.568152 on epoch=204
03/08/2022 21:56:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.544955 on epoch=209
03/08/2022 21:56:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.483193 on epoch=214
03/08/2022 21:56:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.500248 on epoch=219
03/08/2022 21:56:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.506624 on epoch=224
03/08/2022 21:56:41 - INFO - __main__ - Global step 450 Train loss 0.520635 Rouge-L 0.24534933704598216 on epoch=224
03/08/2022 21:56:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.470728 on epoch=229
03/08/2022 21:56:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.474181 on epoch=234
03/08/2022 21:56:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.417143 on epoch=239
03/08/2022 21:57:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.386463 on epoch=244
03/08/2022 21:57:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.361790 on epoch=249
03/08/2022 21:57:13 - INFO - __main__ - Global step 500 Train loss 0.422061 Rouge-L 0.22888635407792224 on epoch=249
03/08/2022 21:57:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.402585 on epoch=254
03/08/2022 21:57:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.360657 on epoch=259
03/08/2022 21:57:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.433132 on epoch=264
03/08/2022 21:57:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.365220 on epoch=269
03/08/2022 21:57:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.332204 on epoch=274
03/08/2022 21:57:45 - INFO - __main__ - Global step 550 Train loss 0.378760 Rouge-L 0.26255477298708707 on epoch=274
03/08/2022 21:57:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.408070 on epoch=279
03/08/2022 21:57:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.407733 on epoch=284
03/08/2022 21:58:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.409164 on epoch=289
03/08/2022 21:58:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.328466 on epoch=294
03/08/2022 21:58:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.364694 on epoch=299
03/08/2022 21:58:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:58:14 - INFO - __main__ - Printing 3 examples
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 21:58:14 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 21:58:14 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 21:58:14 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 21:58:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 21:58:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:58:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 21:58:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 21:58:14 - INFO - __main__ - Printing 3 examples
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 21:58:14 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 21:58:14 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 21:58:14 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 21:58:14 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 21:58:14 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:58:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:58:14 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 21:58:16 - INFO - __main__ - Global step 600 Train loss 0.383625 Rouge-L 0.236689754109193 on epoch=299
03/08/2022 21:58:16 - INFO - __main__ - save last model!
03/08/2022 21:58:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 21:58:24 - INFO - __main__ - Starting training!
03/08/2022 21:59:02 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 21:59:02 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 21:59:02 - INFO - __main__ - Printing 3 examples
03/08/2022 21:59:02 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 21:59:02 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 21:59:02 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 21:59:02 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 21:59:02 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 21:59:02 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 21:59:02 - INFO - __main__ - Tokenizing Input ...
03/08/2022 21:59:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 21:59:04 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 22:04:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_21_0.0001_8_predictions.txt
03/08/2022 22:04:42 - INFO - __main__ - Rouge-L on test data: 0.3161
03/08/2022 22:04:42 - INFO - __main__ - prefix=samsum_32_21, lr=0.0001, bsz=8, dev_performance=0.2931919636999839, test_performance=0.3160879287681751
03/08/2022 22:04:42 - INFO - __main__ - Running ... prefix=samsum_32_42, lr=0.0005, bsz=8 ...
03/08/2022 22:04:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:04:43 - INFO - __main__ - Printing 3 examples
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:04:43 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:04:43 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:04:43 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:04:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 22:04:43 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:04:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:04:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:04:43 - INFO - __main__ - Printing 3 examples
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:04:43 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:04:43 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:04:43 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:04:43 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:04:43 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:04:43 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:04:43 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:04:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:04:53 - INFO - __main__ - Starting training!
03/08/2022 22:04:58 - INFO - __main__ - Step 10 Global step 10 Train loss 18.984154 on epoch=4
03/08/2022 22:05:04 - INFO - __main__ - Step 20 Global step 20 Train loss 13.025450 on epoch=9
03/08/2022 22:05:09 - INFO - __main__ - Step 30 Global step 30 Train loss 7.428246 on epoch=14
03/08/2022 22:05:15 - INFO - __main__ - Step 40 Global step 40 Train loss 5.915729 on epoch=19
03/08/2022 22:05:21 - INFO - __main__ - Step 50 Global step 50 Train loss 5.160329 on epoch=24
03/08/2022 22:05:33 - INFO - __main__ - Global step 50 Train loss 10.102781 Rouge-L 0.211484802306618 on epoch=24
03/08/2022 22:06:04 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.211484802306618 on epoch=24, global_step=50
03/08/2022 22:06:10 - INFO - __main__ - Step 60 Global step 60 Train loss 4.562186 on epoch=29
03/08/2022 22:06:16 - INFO - __main__ - Step 70 Global step 70 Train loss 4.252599 on epoch=34
03/08/2022 22:06:21 - INFO - __main__ - Step 80 Global step 80 Train loss 3.597855 on epoch=39
03/08/2022 22:06:27 - INFO - __main__ - Step 90 Global step 90 Train loss 2.940732 on epoch=44
03/08/2022 22:06:32 - INFO - __main__ - Step 100 Global step 100 Train loss 1.852738 on epoch=49
03/08/2022 22:06:38 - INFO - __main__ - Global step 100 Train loss 3.441222 Rouge-L 0.23558187745139644 on epoch=49
03/08/2022 22:07:11 - INFO - __main__ - Saving model with best Rouge-L: 0.211484802306618 -> 0.23558187745139644 on epoch=49, global_step=100
03/08/2022 22:07:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.964487 on epoch=54
03/08/2022 22:07:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.566187 on epoch=59
03/08/2022 22:07:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.472777 on epoch=64
03/08/2022 22:07:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.338661 on epoch=69
03/08/2022 22:07:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.351165 on epoch=74
03/08/2022 22:07:43 - INFO - __main__ - Global step 150 Train loss 0.538655 Rouge-L 0.19216147118463034 on epoch=74
03/08/2022 22:07:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.294055 on epoch=79
03/08/2022 22:07:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.280521 on epoch=84
03/08/2022 22:07:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.247838 on epoch=89
03/08/2022 22:08:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.243594 on epoch=94
03/08/2022 22:08:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.256009 on epoch=99
03/08/2022 22:08:15 - INFO - __main__ - Global step 200 Train loss 0.264403 Rouge-L 0.21459852354116898 on epoch=99
03/08/2022 22:08:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.238478 on epoch=104
03/08/2022 22:08:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.229948 on epoch=109
03/08/2022 22:08:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.244234 on epoch=114
03/08/2022 22:08:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.229910 on epoch=119
03/08/2022 22:08:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.198778 on epoch=124
03/08/2022 22:08:48 - INFO - __main__ - Global step 250 Train loss 0.228270 Rouge-L 0.23348748237753308 on epoch=124
03/08/2022 22:08:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.210947 on epoch=129
03/08/2022 22:08:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.185723 on epoch=134
03/08/2022 22:09:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.163407 on epoch=139
03/08/2022 22:09:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.164601 on epoch=144
03/08/2022 22:09:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.166797 on epoch=149
03/08/2022 22:09:19 - INFO - __main__ - Global step 300 Train loss 0.178295 Rouge-L 0.18379679200216337 on epoch=149
03/08/2022 22:09:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.167536 on epoch=154
03/08/2022 22:09:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.169026 on epoch=159
03/08/2022 22:09:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.167467 on epoch=164
03/08/2022 22:09:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.168914 on epoch=169
03/08/2022 22:09:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.161604 on epoch=174
03/08/2022 22:09:53 - INFO - __main__ - Global step 350 Train loss 0.166909 Rouge-L 0.20133918484450308 on epoch=174
03/08/2022 22:09:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.155831 on epoch=179
03/08/2022 22:10:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.158534 on epoch=184
03/08/2022 22:10:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.147546 on epoch=189
03/08/2022 22:10:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.141969 on epoch=194
03/08/2022 22:10:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.138889 on epoch=199
03/08/2022 22:10:25 - INFO - __main__ - Global step 400 Train loss 0.148554 Rouge-L 0.20778203279227087 on epoch=199
03/08/2022 22:10:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.140858 on epoch=204
03/08/2022 22:10:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.136441 on epoch=209
03/08/2022 22:10:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.143178 on epoch=214
03/08/2022 22:10:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.137054 on epoch=219
03/08/2022 22:10:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.140356 on epoch=224
03/08/2022 22:10:59 - INFO - __main__ - Global step 450 Train loss 0.139577 Rouge-L 0.1869676853590055 on epoch=224
03/08/2022 22:11:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.136598 on epoch=229
03/08/2022 22:11:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.141074 on epoch=234
03/08/2022 22:11:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.130746 on epoch=239
03/08/2022 22:11:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.128678 on epoch=244
03/08/2022 22:11:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.126529 on epoch=249
03/08/2022 22:11:32 - INFO - __main__ - Global step 500 Train loss 0.132725 Rouge-L 0.19257539200784674 on epoch=249
03/08/2022 22:11:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.128892 on epoch=254
03/08/2022 22:11:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.134606 on epoch=259
03/08/2022 22:11:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.124955 on epoch=264
03/08/2022 22:11:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.134364 on epoch=269
03/08/2022 22:12:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.125081 on epoch=274
03/08/2022 22:12:04 - INFO - __main__ - Global step 550 Train loss 0.129580 Rouge-L 0.18416018469513692 on epoch=274
03/08/2022 22:12:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.123256 on epoch=279
03/08/2022 22:12:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.122249 on epoch=284
03/08/2022 22:12:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.122801 on epoch=289
03/08/2022 22:12:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.125297 on epoch=294
03/08/2022 22:12:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.132677 on epoch=299
03/08/2022 22:12:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:12:34 - INFO - __main__ - Printing 3 examples
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:12:34 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:12:34 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:12:34 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:12:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 22:12:34 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:12:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:12:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:12:34 - INFO - __main__ - Printing 3 examples
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:12:34 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:12:34 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:12:34 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:12:34 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:12:34 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:12:34 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:12:34 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:12:38 - INFO - __main__ - Global step 600 Train loss 0.125256 Rouge-L 0.18583909633936635 on epoch=299
03/08/2022 22:12:38 - INFO - __main__ - save last model!
03/08/2022 22:12:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:12:44 - INFO - __main__ - Starting training!
03/08/2022 22:13:22 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 22:13:23 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 22:13:23 - INFO - __main__ - Printing 3 examples
03/08/2022 22:13:23 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 22:13:23 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 22:13:23 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 22:13:23 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 22:13:23 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 22:13:23 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 22:13:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:13:24 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:13:25 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 22:16:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_42_0.0005_8_predictions.txt
03/08/2022 22:16:03 - INFO - __main__ - Rouge-L on test data: 0.2570
03/08/2022 22:16:03 - INFO - __main__ - prefix=samsum_32_42, lr=0.0005, bsz=8, dev_performance=0.23558187745139644, test_performance=0.2569913545982009
03/08/2022 22:16:03 - INFO - __main__ - Running ... prefix=samsum_32_42, lr=0.0003, bsz=8 ...
03/08/2022 22:16:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:16:04 - INFO - __main__ - Printing 3 examples
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:16:04 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:16:04 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:16:04 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:16:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 22:16:04 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:16:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:16:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:16:04 - INFO - __main__ - Printing 3 examples
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:16:04 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:16:04 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:16:04 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:16:04 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:16:04 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:16:04 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:16:04 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:16:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:16:14 - INFO - __main__ - Starting training!
03/08/2022 22:16:19 - INFO - __main__ - Step 10 Global step 10 Train loss 18.513037 on epoch=4
03/08/2022 22:16:25 - INFO - __main__ - Step 20 Global step 20 Train loss 10.590449 on epoch=9
03/08/2022 22:16:30 - INFO - __main__ - Step 30 Global step 30 Train loss 5.716475 on epoch=14
03/08/2022 22:16:36 - INFO - __main__ - Step 40 Global step 40 Train loss 5.360553 on epoch=19
03/08/2022 22:16:41 - INFO - __main__ - Step 50 Global step 50 Train loss 4.487180 on epoch=24
03/08/2022 22:16:53 - INFO - __main__ - Global step 50 Train loss 8.933539 Rouge-L 0.2201386802495926 on epoch=24
03/08/2022 22:17:25 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.2201386802495926 on epoch=24, global_step=50
03/08/2022 22:17:30 - INFO - __main__ - Step 60 Global step 60 Train loss 3.919605 on epoch=29
03/08/2022 22:17:36 - INFO - __main__ - Step 70 Global step 70 Train loss 2.137569 on epoch=34
03/08/2022 22:17:41 - INFO - __main__ - Step 80 Global step 80 Train loss 1.631743 on epoch=39
03/08/2022 22:17:47 - INFO - __main__ - Step 90 Global step 90 Train loss 1.285019 on epoch=44
03/08/2022 22:17:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.099911 on epoch=49
03/08/2022 22:17:57 - INFO - __main__ - Global step 100 Train loss 2.014769 Rouge-L 0.24670683064726734 on epoch=49
03/08/2022 22:18:29 - INFO - __main__ - Saving model with best Rouge-L: 0.2201386802495926 -> 0.24670683064726734 on epoch=49, global_step=100
03/08/2022 22:18:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.909684 on epoch=54
03/08/2022 22:18:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.783041 on epoch=59
03/08/2022 22:18:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.684965 on epoch=64
03/08/2022 22:18:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.569209 on epoch=69
03/08/2022 22:18:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.538170 on epoch=74
03/08/2022 22:19:01 - INFO - __main__ - Global step 150 Train loss 0.697014 Rouge-L 0.2054561825432103 on epoch=74
03/08/2022 22:19:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.475192 on epoch=79
03/08/2022 22:19:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.450517 on epoch=84
03/08/2022 22:19:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.402360 on epoch=89
03/08/2022 22:19:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.333567 on epoch=94
03/08/2022 22:19:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.315447 on epoch=99
03/08/2022 22:19:34 - INFO - __main__ - Global step 200 Train loss 0.395416 Rouge-L 0.22081607616702054 on epoch=99
03/08/2022 22:19:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.332623 on epoch=104
03/08/2022 22:19:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.319997 on epoch=109
03/08/2022 22:19:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.302470 on epoch=114
03/08/2022 22:19:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.291401 on epoch=119
03/08/2022 22:20:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.261812 on epoch=124
03/08/2022 22:20:08 - INFO - __main__ - Global step 250 Train loss 0.301661 Rouge-L 0.24082230829508056 on epoch=124
03/08/2022 22:20:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.275837 on epoch=129
03/08/2022 22:20:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.251013 on epoch=134
03/08/2022 22:20:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.252247 on epoch=139
03/08/2022 22:20:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.233217 on epoch=144
03/08/2022 22:20:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.234813 on epoch=149
03/08/2022 22:20:40 - INFO - __main__ - Global step 300 Train loss 0.249425 Rouge-L 0.2206427281367608 on epoch=149
03/08/2022 22:20:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.254489 on epoch=154
03/08/2022 22:20:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.220332 on epoch=159
03/08/2022 22:20:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.208707 on epoch=164
03/08/2022 22:21:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.261223 on epoch=169
03/08/2022 22:21:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.224230 on epoch=174
03/08/2022 22:21:12 - INFO - __main__ - Global step 350 Train loss 0.233796 Rouge-L 0.24594121666948512 on epoch=174
03/08/2022 22:21:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.220805 on epoch=179
03/08/2022 22:21:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.231032 on epoch=184
03/08/2022 22:21:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.213552 on epoch=189
03/08/2022 22:21:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.201091 on epoch=194
03/08/2022 22:21:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.199961 on epoch=199
03/08/2022 22:21:46 - INFO - __main__ - Global step 400 Train loss 0.213288 Rouge-L 0.1869355209438951 on epoch=199
03/08/2022 22:21:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.186241 on epoch=204
03/08/2022 22:21:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.185459 on epoch=209
03/08/2022 22:22:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.214682 on epoch=214
03/08/2022 22:22:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.212162 on epoch=219
03/08/2022 22:22:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.188131 on epoch=224
03/08/2022 22:22:18 - INFO - __main__ - Global step 450 Train loss 0.197335 Rouge-L 0.20742105914076295 on epoch=224
03/08/2022 22:22:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.171653 on epoch=229
03/08/2022 22:22:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.170961 on epoch=234
03/08/2022 22:22:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.182196 on epoch=239
03/08/2022 22:22:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.175406 on epoch=244
03/08/2022 22:22:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.221572 on epoch=249
03/08/2022 22:23:00 - INFO - __main__ - Global step 500 Train loss 0.184358 Rouge-L 0.18195337318146076 on epoch=249
03/08/2022 22:23:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.175956 on epoch=254
03/08/2022 22:23:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.187448 on epoch=259
03/08/2022 22:23:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.168444 on epoch=264
03/08/2022 22:23:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.157770 on epoch=269
03/08/2022 22:23:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.169368 on epoch=274
03/08/2022 22:23:34 - INFO - __main__ - Global step 550 Train loss 0.171797 Rouge-L 0.18706813433769753 on epoch=274
03/08/2022 22:23:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.165071 on epoch=279
03/08/2022 22:23:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.156099 on epoch=284
03/08/2022 22:23:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.151912 on epoch=289
03/08/2022 22:23:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.165243 on epoch=294
03/08/2022 22:24:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.174375 on epoch=299
03/08/2022 22:24:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:24:03 - INFO - __main__ - Printing 3 examples
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:24:03 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:24:03 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:24:03 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:24:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 22:24:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:24:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:24:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:24:03 - INFO - __main__ - Printing 3 examples
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:24:03 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:24:03 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:24:03 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:24:03 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:24:03 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:24:03 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:24:03 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:24:07 - INFO - __main__ - Global step 600 Train loss 0.162540 Rouge-L 0.20083714411187853 on epoch=299
03/08/2022 22:24:07 - INFO - __main__ - save last model!
03/08/2022 22:24:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:24:13 - INFO - __main__ - Starting training!
03/08/2022 22:24:52 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 22:24:53 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 22:24:53 - INFO - __main__ - Printing 3 examples
03/08/2022 22:24:53 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 22:24:53 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 22:24:53 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 22:24:53 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 22:24:53 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 22:24:53 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 22:24:53 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:24:53 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:24:54 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 22:26:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_42_0.0003_8_predictions.txt
03/08/2022 22:26:44 - INFO - __main__ - Rouge-L on test data: 0.2549
03/08/2022 22:26:44 - INFO - __main__ - prefix=samsum_32_42, lr=0.0003, bsz=8, dev_performance=0.24670683064726734, test_performance=0.2549443461864549
03/08/2022 22:26:44 - INFO - __main__ - Running ... prefix=samsum_32_42, lr=0.0002, bsz=8 ...
03/08/2022 22:26:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:26:45 - INFO - __main__ - Printing 3 examples
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:26:45 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:26:45 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:26:45 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:26:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 22:26:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:26:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:26:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:26:45 - INFO - __main__ - Printing 3 examples
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:26:45 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:26:45 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:26:45 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:26:45 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:26:45 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:26:45 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:26:45 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:26:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:26:55 - INFO - __main__ - Starting training!
03/08/2022 22:27:00 - INFO - __main__ - Step 10 Global step 10 Train loss 19.145941 on epoch=4
03/08/2022 22:27:05 - INFO - __main__ - Step 20 Global step 20 Train loss 13.353795 on epoch=9
03/08/2022 22:27:11 - INFO - __main__ - Step 30 Global step 30 Train loss 9.004045 on epoch=14
03/08/2022 22:27:17 - INFO - __main__ - Step 40 Global step 40 Train loss 6.028507 on epoch=19
03/08/2022 22:27:23 - INFO - __main__ - Step 50 Global step 50 Train loss 4.288518 on epoch=24
03/08/2022 22:27:35 - INFO - __main__ - Global step 50 Train loss 10.364161 Rouge-L 0.29169270641601486 on epoch=24
03/08/2022 22:28:08 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.29169270641601486 on epoch=24, global_step=50
03/08/2022 22:28:14 - INFO - __main__ - Step 60 Global step 60 Train loss 3.279220 on epoch=29
03/08/2022 22:28:20 - INFO - __main__ - Step 70 Global step 70 Train loss 2.454247 on epoch=34
03/08/2022 22:28:26 - INFO - __main__ - Step 80 Global step 80 Train loss 1.948505 on epoch=39
03/08/2022 22:28:31 - INFO - __main__ - Step 90 Global step 90 Train loss 1.582065 on epoch=44
03/08/2022 22:28:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.566298 on epoch=49
03/08/2022 22:28:49 - INFO - __main__ - Global step 100 Train loss 2.166067 Rouge-L 0.20874624481609005 on epoch=49
03/08/2022 22:28:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.700671 on epoch=54
03/08/2022 22:29:00 - INFO - __main__ - Step 120 Global step 120 Train loss 1.786022 on epoch=59
03/08/2022 22:29:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.066886 on epoch=64
03/08/2022 22:29:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.925611 on epoch=69
03/08/2022 22:29:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.868553 on epoch=74
03/08/2022 22:29:21 - INFO - __main__ - Global step 150 Train loss 1.269549 Rouge-L 0.2627226824694595 on epoch=74
03/08/2022 22:29:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.761338 on epoch=79
03/08/2022 22:29:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.725259 on epoch=84
03/08/2022 22:29:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.694468 on epoch=89
03/08/2022 22:29:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.608550 on epoch=94
03/08/2022 22:29:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.608894 on epoch=99
03/08/2022 22:29:55 - INFO - __main__ - Global step 200 Train loss 0.679702 Rouge-L 0.23596849011732113 on epoch=99
03/08/2022 22:30:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.582689 on epoch=104
03/08/2022 22:30:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.527176 on epoch=109
03/08/2022 22:30:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.441763 on epoch=114
03/08/2022 22:30:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.405007 on epoch=119
03/08/2022 22:30:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.384920 on epoch=124
03/08/2022 22:30:32 - INFO - __main__ - Global step 250 Train loss 0.468311 Rouge-L 0.18338164507896676 on epoch=124
03/08/2022 22:30:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.346187 on epoch=129
03/08/2022 22:30:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.348758 on epoch=134
03/08/2022 22:30:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.311440 on epoch=139
03/08/2022 22:30:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.355059 on epoch=144
03/08/2022 22:31:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.346376 on epoch=149
03/08/2022 22:31:04 - INFO - __main__ - Global step 300 Train loss 0.341564 Rouge-L 0.19628971040195126 on epoch=149
03/08/2022 22:31:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.318054 on epoch=154
03/08/2022 22:31:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.311151 on epoch=159
03/08/2022 22:31:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.297854 on epoch=164
03/08/2022 22:31:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.303938 on epoch=169
03/08/2022 22:31:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.317525 on epoch=174
03/08/2022 22:31:38 - INFO - __main__ - Global step 350 Train loss 0.309705 Rouge-L 0.1829247702667155 on epoch=174
03/08/2022 22:31:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.278999 on epoch=179
03/08/2022 22:31:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.295144 on epoch=184
03/08/2022 22:31:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.305801 on epoch=189
03/08/2022 22:32:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.271314 on epoch=194
03/08/2022 22:32:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.277236 on epoch=199
03/08/2022 22:32:13 - INFO - __main__ - Global step 400 Train loss 0.285699 Rouge-L 0.1868362990533069 on epoch=199
03/08/2022 22:32:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.245344 on epoch=204
03/08/2022 22:32:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.257649 on epoch=209
03/08/2022 22:32:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.260853 on epoch=214
03/08/2022 22:32:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.237654 on epoch=219
03/08/2022 22:32:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.240597 on epoch=224
03/08/2022 22:32:46 - INFO - __main__ - Global step 450 Train loss 0.248419 Rouge-L 0.19631712998892567 on epoch=224
03/08/2022 22:32:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.234743 on epoch=229
03/08/2022 22:32:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.244756 on epoch=234
03/08/2022 22:33:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.197895 on epoch=239
03/08/2022 22:33:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.284338 on epoch=244
03/08/2022 22:33:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.231637 on epoch=249
03/08/2022 22:33:22 - INFO - __main__ - Global step 500 Train loss 0.238674 Rouge-L 0.21739247277233364 on epoch=249
03/08/2022 22:33:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.235065 on epoch=254
03/08/2022 22:33:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.223998 on epoch=259
03/08/2022 22:33:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.209118 on epoch=264
03/08/2022 22:33:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.203790 on epoch=269
03/08/2022 22:33:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.215041 on epoch=274
03/08/2022 22:34:03 - INFO - __main__ - Global step 550 Train loss 0.217403 Rouge-L 0.19632417479153452 on epoch=274
03/08/2022 22:34:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.223161 on epoch=279
03/08/2022 22:34:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.198188 on epoch=284
03/08/2022 22:34:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.186355 on epoch=289
03/08/2022 22:34:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.211226 on epoch=294
03/08/2022 22:34:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.197964 on epoch=299
03/08/2022 22:34:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:34:33 - INFO - __main__ - Printing 3 examples
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:34:33 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:34:33 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:34:33 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:34:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 22:34:33 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:34:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:34:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:34:33 - INFO - __main__ - Printing 3 examples
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:34:33 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:34:33 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:34:33 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:34:33 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:34:33 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:34:33 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:34:33 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:34:35 - INFO - __main__ - Global step 600 Train loss 0.203379 Rouge-L 0.2054917173752314 on epoch=299
03/08/2022 22:34:35 - INFO - __main__ - save last model!
03/08/2022 22:34:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:34:44 - INFO - __main__ - Starting training!
03/08/2022 22:35:14 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 22:35:14 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 22:35:14 - INFO - __main__ - Printing 3 examples
03/08/2022 22:35:14 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 22:35:14 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 22:35:14 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 22:35:14 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 22:35:14 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 22:35:14 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 22:35:14 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:35:15 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:35:16 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 22:40:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_42_0.0002_8_predictions.txt
03/08/2022 22:40:21 - INFO - __main__ - Rouge-L on test data: 0.3237
03/08/2022 22:40:22 - INFO - __main__ - prefix=samsum_32_42, lr=0.0002, bsz=8, dev_performance=0.29169270641601486, test_performance=0.3236616186048135
03/08/2022 22:40:22 - INFO - __main__ - Running ... prefix=samsum_32_42, lr=0.0001, bsz=8 ...
03/08/2022 22:40:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:40:22 - INFO - __main__ - Printing 3 examples
03/08/2022 22:40:22 - INFO - __main__ -  [samsum] summarize: Violet: hi! i came across this Austin's article and i thought that you might find it interesting Violet: <file_other> Claire: Hi! :) Thanks, but I've already read it. :) Claire: But thanks for thinking about me :)
03/08/2022 22:40:22 - INFO - __main__ - ["Violet sent Claire Austin's article."]
03/08/2022 22:40:22 - INFO - __main__ -  [samsum] summarize: Pat: So does anyone know when the stream is going to happen? Lou: Unfortunately, no, but would really like to. Kevin: I don't think I'd be interested in this. Pat: Y? Kevin: Seeing all the blood and internal organs makes me dizzy. Lou: So you're so gentle? Pat: C'mon! Srsly? Kevin: Yup. Had the same thing since I was a child. Lou: Maybe it's time to change it? Pat: Yeah! Give it a try!
03/08/2022 22:40:22 - INFO - __main__ - ['Pat and Lou are waiting for The stream but Kevin is not interested as it makes him dizzy.']
03/08/2022 22:40:22 - INFO - __main__ -  [samsum] summarize: Jane: <gif_file> Jane: Whaddya think?  Shona: This ur tinder profile thing? Jane: Yeah, I'm updating my profile tonite. Kinda nervoous though... :(  Jane: What if i get another guy like John? o.O Shona: John was a dickhead Jane: preach sistah! Shona: anyhoo - this time I've got u :D No slimeballs for you  Jane: Not again *shudders* Jane: You know he forgot my birthday??!! Shona: wanker
03/08/2022 22:40:22 - INFO - __main__ - ["Jane is updating her Tinder profile tonight and together with Shona they don't want to find another guy like John, who forgot Jane's birthday."]
03/08/2022 22:40:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 22:40:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:40:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:40:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:40:23 - INFO - __main__ - Printing 3 examples
03/08/2022 22:40:23 - INFO - __main__ -  [samsum] summarize: Sandra: may i ask why have you taken my hat?! Ursula: i needed this one for my outfit Sandra: your is exactly the same Ursula: no, my has got pink flowers and your has purple Sandra: so no difference. Ursula: sometimes i just cant believe u r my sis Sandra: me too
03/08/2022 22:40:23 - INFO - __main__ - ["Ursula took Sandra's hat because she needs it for her outfit. Ursula's hat is not exactly the same, it has pink flowers, whereas Sandra's got purple ones."]
03/08/2022 22:40:23 - INFO - __main__ -  [samsum] summarize: Matija: No snow, you are spreading fake news. Sandra: The weather app said so :( Matija: There is some frost on the cars, not much more.
03/08/2022 22:40:23 - INFO - __main__ - ["It isn't snowing, despite the weather forecast."]
03/08/2022 22:40:23 - INFO - __main__ -  [samsum] summarize: Gail: :P Paul: :O Gail: Hi :) Paul: Oh... so you're no longer mad at me? Gail: I never was, you know I like to pretend I'm angry ;) Paul: You were fucking with me as usual :P Gail: Don't be vulgar Paul: Or...? Gail: Or? Paul: Or you'll get mad at me again? Gail: Oh come on now Paul: :P Gail: You know I like you Paul: Yeah yeah yeah Gail: You're worse than my spinster aunt :P Paul: Oh, now I understand who you got your evil woman genes from :P Gail: Hah! Paul: Ok, just kidding Gail: I would hope so! Paul: ;) Gail: Heh Paul: Wanna go out tonight, eat a pizza or something? Gail: Are you hitting on me? Paul: Yep Gail: Now that was unexpected Paul: Nah, just kidding Gail: :'( You just broke my heart Paul: Srsly?!? Gail: Nah :P :D
03/08/2022 22:40:23 - INFO - __main__ - ['Paul jokingly invites Gail to a date tonight.']
03/08/2022 22:40:23 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:40:23 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:40:23 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:40:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:40:33 - INFO - __main__ - Starting training!
03/08/2022 22:40:38 - INFO - __main__ - Step 10 Global step 10 Train loss 19.033934 on epoch=4
03/08/2022 22:40:43 - INFO - __main__ - Step 20 Global step 20 Train loss 16.681732 on epoch=9
03/08/2022 22:40:49 - INFO - __main__ - Step 30 Global step 30 Train loss 11.410085 on epoch=14
03/08/2022 22:40:54 - INFO - __main__ - Step 40 Global step 40 Train loss 9.786087 on epoch=19
03/08/2022 22:41:00 - INFO - __main__ - Step 50 Global step 50 Train loss 7.976054 on epoch=24
03/08/2022 22:41:14 - INFO - __main__ - Global step 50 Train loss 12.977578 Rouge-L 0.23371607758633578 on epoch=24
03/08/2022 22:41:49 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.23371607758633578 on epoch=24, global_step=50
03/08/2022 22:41:55 - INFO - __main__ - Step 60 Global step 60 Train loss 7.277969 on epoch=29
03/08/2022 22:42:01 - INFO - __main__ - Step 70 Global step 70 Train loss 6.670507 on epoch=34
03/08/2022 22:42:06 - INFO - __main__ - Step 80 Global step 80 Train loss 6.235838 on epoch=39
03/08/2022 22:42:12 - INFO - __main__ - Step 90 Global step 90 Train loss 5.973033 on epoch=44
03/08/2022 22:42:18 - INFO - __main__ - Step 100 Global step 100 Train loss 5.446382 on epoch=49
03/08/2022 22:42:32 - INFO - __main__ - Global step 100 Train loss 6.320746 Rouge-L 0.3141095989063035 on epoch=49
03/08/2022 22:43:04 - INFO - __main__ - Saving model with best Rouge-L: 0.23371607758633578 -> 0.3141095989063035 on epoch=49, global_step=100
03/08/2022 22:43:10 - INFO - __main__ - Step 110 Global step 110 Train loss 4.673100 on epoch=54
03/08/2022 22:43:16 - INFO - __main__ - Step 120 Global step 120 Train loss 3.666279 on epoch=59
03/08/2022 22:43:22 - INFO - __main__ - Step 130 Global step 130 Train loss 3.213915 on epoch=64
03/08/2022 22:43:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.815986 on epoch=69
03/08/2022 22:43:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.666408 on epoch=74
03/08/2022 22:43:44 - INFO - __main__ - Global step 150 Train loss 3.407138 Rouge-L 0.27433950298983056 on epoch=74
03/08/2022 22:43:50 - INFO - __main__ - Step 160 Global step 160 Train loss 2.220075 on epoch=79
03/08/2022 22:43:55 - INFO - __main__ - Step 170 Global step 170 Train loss 2.055554 on epoch=84
03/08/2022 22:44:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.700299 on epoch=89
03/08/2022 22:44:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.539274 on epoch=94
03/08/2022 22:44:13 - INFO - __main__ - Step 200 Global step 200 Train loss 1.406631 on epoch=99
03/08/2022 22:44:23 - INFO - __main__ - Global step 200 Train loss 1.784367 Rouge-L 0.30065936597725107 on epoch=99
03/08/2022 22:44:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.366567 on epoch=104
03/08/2022 22:44:34 - INFO - __main__ - Step 220 Global step 220 Train loss 1.110462 on epoch=109
03/08/2022 22:44:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.118907 on epoch=114
03/08/2022 22:44:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.064350 on epoch=119
03/08/2022 22:44:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.965574 on epoch=124
03/08/2022 22:44:56 - INFO - __main__ - Global step 250 Train loss 1.125172 Rouge-L 0.32347617999294903 on epoch=124
03/08/2022 22:45:29 - INFO - __main__ - Saving model with best Rouge-L: 0.3141095989063035 -> 0.32347617999294903 on epoch=124, global_step=250
03/08/2022 22:45:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.908478 on epoch=129
03/08/2022 22:45:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.865737 on epoch=134
03/08/2022 22:45:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.836620 on epoch=139
03/08/2022 22:45:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.788811 on epoch=144
03/08/2022 22:45:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.737734 on epoch=149
03/08/2022 22:46:03 - INFO - __main__ - Global step 300 Train loss 0.827476 Rouge-L 0.3122373941665776 on epoch=149
03/08/2022 22:46:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.711049 on epoch=154
03/08/2022 22:46:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.735430 on epoch=159
03/08/2022 22:46:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.691255 on epoch=164
03/08/2022 22:46:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.647670 on epoch=169
03/08/2022 22:46:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.597606 on epoch=174
03/08/2022 22:46:36 - INFO - __main__ - Global step 350 Train loss 0.676602 Rouge-L 0.29196928276837875 on epoch=174
03/08/2022 22:46:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.583814 on epoch=179
03/08/2022 22:46:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.584302 on epoch=184
03/08/2022 22:46:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.546952 on epoch=189
03/08/2022 22:46:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.578135 on epoch=194
03/08/2022 22:47:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.500619 on epoch=199
03/08/2022 22:47:10 - INFO - __main__ - Global step 400 Train loss 0.558764 Rouge-L 0.2736328194956157 on epoch=199
03/08/2022 22:47:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.430522 on epoch=204
03/08/2022 22:47:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.479465 on epoch=209
03/08/2022 22:47:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.458669 on epoch=214
03/08/2022 22:47:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.398505 on epoch=219
03/08/2022 22:47:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.449589 on epoch=224
03/08/2022 22:47:42 - INFO - __main__ - Global step 450 Train loss 0.443350 Rouge-L 0.2877412120701651 on epoch=224
03/08/2022 22:47:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.410717 on epoch=229
03/08/2022 22:47:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.391062 on epoch=234
03/08/2022 22:47:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.367411 on epoch=239
03/08/2022 22:48:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.358849 on epoch=244
03/08/2022 22:48:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.344913 on epoch=249
03/08/2022 22:48:15 - INFO - __main__ - Global step 500 Train loss 0.374590 Rouge-L 0.24755390441160374 on epoch=249
03/08/2022 22:48:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.362314 on epoch=254
03/08/2022 22:48:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.317553 on epoch=259
03/08/2022 22:48:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.343819 on epoch=264
03/08/2022 22:48:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.355478 on epoch=269
03/08/2022 22:48:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.313830 on epoch=274
03/08/2022 22:48:48 - INFO - __main__ - Global step 550 Train loss 0.338599 Rouge-L 0.23750897925295933 on epoch=274
03/08/2022 22:48:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.290496 on epoch=279
03/08/2022 22:48:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.262993 on epoch=284
03/08/2022 22:49:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.274653 on epoch=289
03/08/2022 22:49:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.313369 on epoch=294
03/08/2022 22:49:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.279833 on epoch=299
03/08/2022 22:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:49:18 - INFO - __main__ - Printing 3 examples
03/08/2022 22:49:18 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 22:49:18 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 22:49:18 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 22:49:18 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 22:49:18 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 22:49:18 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 22:49:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 22:49:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:49:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:49:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:49:19 - INFO - __main__ - Printing 3 examples
03/08/2022 22:49:19 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 22:49:19 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 22:49:19 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 22:49:19 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 22:49:19 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 22:49:19 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 22:49:19 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:49:19 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:49:19 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:49:21 - INFO - __main__ - Global step 600 Train loss 0.284269 Rouge-L 0.23684427421681728 on epoch=299
03/08/2022 22:49:21 - INFO - __main__ - save last model!
03/08/2022 22:49:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:49:28 - INFO - __main__ - Starting training!
03/08/2022 22:50:03 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 22:50:04 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 22:50:04 - INFO - __main__ - Printing 3 examples
03/08/2022 22:50:04 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 22:50:04 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 22:50:04 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 22:50:04 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 22:50:04 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 22:50:04 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 22:50:04 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:50:05 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:50:06 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 22:52:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_42_0.0001_8_predictions.txt
03/08/2022 22:52:45 - INFO - __main__ - Rouge-L on test data: 0.2949
03/08/2022 22:52:45 - INFO - __main__ - prefix=samsum_32_42, lr=0.0001, bsz=8, dev_performance=0.32347617999294903, test_performance=0.29494343100919945
03/08/2022 22:52:45 - INFO - __main__ - Running ... prefix=samsum_32_87, lr=0.0005, bsz=8 ...
03/08/2022 22:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:52:46 - INFO - __main__ - Printing 3 examples
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 22:52:46 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 22:52:46 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 22:52:46 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 22:52:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 22:52:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:52:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 22:52:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 22:52:46 - INFO - __main__ - Printing 3 examples
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 22:52:46 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 22:52:46 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 22:52:46 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 22:52:46 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 22:52:46 - INFO - __main__ - Tokenizing Input ...
03/08/2022 22:52:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 22:52:46 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 22:52:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 22:52:56 - INFO - __main__ - Starting training!
03/08/2022 22:53:01 - INFO - __main__ - Step 10 Global step 10 Train loss 18.332720 on epoch=4
03/08/2022 22:53:06 - INFO - __main__ - Step 20 Global step 20 Train loss 14.790235 on epoch=9
03/08/2022 22:53:11 - INFO - __main__ - Step 30 Global step 30 Train loss 10.293147 on epoch=14
03/08/2022 22:53:17 - INFO - __main__ - Step 40 Global step 40 Train loss 12.069015 on epoch=19
03/08/2022 22:53:23 - INFO - __main__ - Step 50 Global step 50 Train loss 10.370660 on epoch=24
03/08/2022 22:53:37 - INFO - __main__ - Global step 50 Train loss 13.171157 Rouge-L 0.15273502766159852 on epoch=24
03/08/2022 22:54:09 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.15273502766159852 on epoch=24, global_step=50
03/08/2022 22:54:14 - INFO - __main__ - Step 60 Global step 60 Train loss 9.536906 on epoch=29
03/08/2022 22:54:20 - INFO - __main__ - Step 70 Global step 70 Train loss 9.042494 on epoch=34
03/08/2022 22:54:26 - INFO - __main__ - Step 80 Global step 80 Train loss 8.651006 on epoch=39
03/08/2022 22:54:32 - INFO - __main__ - Step 90 Global step 90 Train loss 8.003860 on epoch=44
03/08/2022 22:54:37 - INFO - __main__ - Step 100 Global step 100 Train loss 6.759855 on epoch=49
03/08/2022 22:54:44 - INFO - __main__ - Global step 100 Train loss 8.398824 Rouge-L 0.0661112350970695 on epoch=49
03/08/2022 22:54:50 - INFO - __main__ - Step 110 Global step 110 Train loss 5.320119 on epoch=54
03/08/2022 22:54:55 - INFO - __main__ - Step 120 Global step 120 Train loss 4.427820 on epoch=59
03/08/2022 22:55:01 - INFO - __main__ - Step 130 Global step 130 Train loss 3.402004 on epoch=64
03/08/2022 22:55:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.716536 on epoch=69
03/08/2022 22:55:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.314524 on epoch=74
03/08/2022 22:55:15 - INFO - __main__ - Global step 150 Train loss 3.636201 Rouge-L 0.09688496441480746 on epoch=74
03/08/2022 22:55:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.926644 on epoch=79
03/08/2022 22:55:27 - INFO - __main__ - Step 170 Global step 170 Train loss 1.589389 on epoch=84
03/08/2022 22:55:33 - INFO - __main__ - Step 180 Global step 180 Train loss 1.354757 on epoch=89
03/08/2022 22:55:39 - INFO - __main__ - Step 190 Global step 190 Train loss 1.162750 on epoch=94
03/08/2022 22:55:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.135470 on epoch=99
03/08/2022 22:55:47 - INFO - __main__ - Global step 200 Train loss 1.433802 Rouge-L 0.08954292994845323 on epoch=99
03/08/2022 22:55:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.852352 on epoch=104
03/08/2022 22:55:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.761025 on epoch=109
03/08/2022 22:56:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.628225 on epoch=114
03/08/2022 22:56:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.572335 on epoch=119
03/08/2022 22:56:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.571806 on epoch=124
03/08/2022 22:56:21 - INFO - __main__ - Global step 250 Train loss 0.677149 Rouge-L 0.09434275241505394 on epoch=124
03/08/2022 22:56:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.505138 on epoch=129
03/08/2022 22:56:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.455345 on epoch=134
03/08/2022 22:56:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.430263 on epoch=139
03/08/2022 22:56:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.381119 on epoch=144
03/08/2022 22:56:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.356778 on epoch=149
03/08/2022 22:56:55 - INFO - __main__ - Global step 300 Train loss 0.425729 Rouge-L 0.0855856696183988 on epoch=149
03/08/2022 22:57:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.372148 on epoch=154
03/08/2022 22:57:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.319780 on epoch=159
03/08/2022 22:57:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.301383 on epoch=164
03/08/2022 22:57:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.295132 on epoch=169
03/08/2022 22:57:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.306595 on epoch=174
03/08/2022 22:57:27 - INFO - __main__ - Global step 350 Train loss 0.319007 Rouge-L 0.09742876515531176 on epoch=174
03/08/2022 22:57:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.276248 on epoch=179
03/08/2022 22:57:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.251042 on epoch=184
03/08/2022 22:57:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.261775 on epoch=189
03/08/2022 22:57:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.262014 on epoch=194
03/08/2022 22:57:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.221294 on epoch=199
03/08/2022 22:58:02 - INFO - __main__ - Global step 400 Train loss 0.254474 Rouge-L 0.08568735377288772 on epoch=199
03/08/2022 22:58:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.210367 on epoch=204
03/08/2022 22:58:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.229080 on epoch=209
03/08/2022 22:58:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.220950 on epoch=214
03/08/2022 22:58:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.240811 on epoch=219
03/08/2022 22:58:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.207109 on epoch=224
03/08/2022 22:58:36 - INFO - __main__ - Global step 450 Train loss 0.221663 Rouge-L 0.08325315335647163 on epoch=224
03/08/2022 22:58:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.227839 on epoch=229
03/08/2022 22:58:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.198244 on epoch=234
03/08/2022 22:58:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.195521 on epoch=239
03/08/2022 22:58:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.215231 on epoch=244
03/08/2022 22:59:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.186185 on epoch=249
03/08/2022 22:59:10 - INFO - __main__ - Global step 500 Train loss 0.204604 Rouge-L 0.09229892288266556 on epoch=249
03/08/2022 22:59:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.207873 on epoch=254
03/08/2022 22:59:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.180872 on epoch=259
03/08/2022 22:59:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.180069 on epoch=264
03/08/2022 22:59:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.186172 on epoch=269
03/08/2022 22:59:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.196368 on epoch=274
03/08/2022 22:59:44 - INFO - __main__ - Global step 550 Train loss 0.190271 Rouge-L 0.10189621793916234 on epoch=274
03/08/2022 22:59:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.179553 on epoch=279
03/08/2022 22:59:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.192743 on epoch=284
03/08/2022 23:00:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.169308 on epoch=289
03/08/2022 23:00:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.161727 on epoch=294
03/08/2022 23:00:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.173000 on epoch=299
03/08/2022 23:00:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:00:14 - INFO - __main__ - Printing 3 examples
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:00:14 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:00:14 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:00:14 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:00:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:00:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:00:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:00:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:00:14 - INFO - __main__ - Printing 3 examples
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:00:14 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:00:14 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:00:14 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:00:14 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:00:14 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:00:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:00:14 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:00:18 - INFO - __main__ - Global step 600 Train loss 0.175266 Rouge-L 0.08668460626669597 on epoch=299
03/08/2022 23:00:18 - INFO - __main__ - save last model!
03/08/2022 23:00:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:00:24 - INFO - __main__ - Starting training!
03/08/2022 23:00:58 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:00:59 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 23:00:59 - INFO - __main__ - Printing 3 examples
03/08/2022 23:00:59 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 23:00:59 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 23:00:59 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 23:00:59 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 23:00:59 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 23:00:59 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 23:00:59 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:00:59 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:01:00 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 23:06:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_87_0.0005_8_predictions.txt
03/08/2022 23:06:54 - INFO - __main__ - Rouge-L on test data: 0.1479
03/08/2022 23:06:54 - INFO - __main__ - prefix=samsum_32_87, lr=0.0005, bsz=8, dev_performance=0.15273502766159852, test_performance=0.14792461523548792
03/08/2022 23:06:54 - INFO - __main__ - Running ... prefix=samsum_32_87, lr=0.0003, bsz=8 ...
03/08/2022 23:06:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:06:55 - INFO - __main__ - Printing 3 examples
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:06:55 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:06:55 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:06:55 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:06:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 23:06:55 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:06:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:06:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:06:55 - INFO - __main__ - Printing 3 examples
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:06:55 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:06:55 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:06:55 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:06:55 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:06:55 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:06:55 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:06:55 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:07:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:07:04 - INFO - __main__ - Starting training!
03/08/2022 23:07:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.150455 on epoch=4
03/08/2022 23:07:15 - INFO - __main__ - Step 20 Global step 20 Train loss 13.980291 on epoch=9
03/08/2022 23:07:21 - INFO - __main__ - Step 30 Global step 30 Train loss 8.290331 on epoch=14
03/08/2022 23:07:27 - INFO - __main__ - Step 40 Global step 40 Train loss 6.615131 on epoch=19
03/08/2022 23:07:32 - INFO - __main__ - Step 50 Global step 50 Train loss 5.994531 on epoch=24
03/08/2022 23:07:46 - INFO - __main__ - Global step 50 Train loss 10.606148 Rouge-L 0.12493421608485766 on epoch=24
03/08/2022 23:08:17 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.12493421608485766 on epoch=24, global_step=50
03/08/2022 23:08:22 - INFO - __main__ - Step 60 Global step 60 Train loss 6.661399 on epoch=29
03/08/2022 23:08:28 - INFO - __main__ - Step 70 Global step 70 Train loss 7.857048 on epoch=34
03/08/2022 23:08:34 - INFO - __main__ - Step 80 Global step 80 Train loss 6.495596 on epoch=39
03/08/2022 23:08:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.747818 on epoch=44
03/08/2022 23:08:45 - INFO - __main__ - Step 100 Global step 100 Train loss 4.392617 on epoch=49
03/08/2022 23:08:49 - INFO - __main__ - Global step 100 Train loss 6.230896 Rouge-L 0.0592429785505743 on epoch=49
03/08/2022 23:08:54 - INFO - __main__ - Step 110 Global step 110 Train loss 2.883074 on epoch=54
03/08/2022 23:09:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.061779 on epoch=59
03/08/2022 23:09:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.496364 on epoch=64
03/08/2022 23:09:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.046874 on epoch=69
03/08/2022 23:09:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.876137 on epoch=74
03/08/2022 23:09:21 - INFO - __main__ - Global step 150 Train loss 1.672846 Rouge-L 0.08253753282583413 on epoch=74
03/08/2022 23:09:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.739124 on epoch=79
03/08/2022 23:09:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.648893 on epoch=84
03/08/2022 23:09:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.530308 on epoch=89
03/08/2022 23:09:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.540133 on epoch=94
03/08/2022 23:09:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.489309 on epoch=99
03/08/2022 23:09:54 - INFO - __main__ - Global step 200 Train loss 0.589553 Rouge-L 0.09585280990966369 on epoch=99
03/08/2022 23:10:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.404512 on epoch=104
03/08/2022 23:10:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.417565 on epoch=109
03/08/2022 23:10:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.398386 on epoch=114
03/08/2022 23:10:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.339835 on epoch=119
03/08/2022 23:10:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.358695 on epoch=124
03/08/2022 23:10:28 - INFO - __main__ - Global step 250 Train loss 0.383798 Rouge-L 0.0878189019184569 on epoch=124
03/08/2022 23:10:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.356935 on epoch=129
03/08/2022 23:10:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.302588 on epoch=134
03/08/2022 23:10:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.295325 on epoch=139
03/08/2022 23:10:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.319748 on epoch=144
03/08/2022 23:10:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.304171 on epoch=149
03/08/2022 23:11:02 - INFO - __main__ - Global step 300 Train loss 0.315753 Rouge-L 0.08136937507053407 on epoch=149
03/08/2022 23:11:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.239025 on epoch=154
03/08/2022 23:11:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.269081 on epoch=159
03/08/2022 23:11:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.260382 on epoch=164
03/08/2022 23:11:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.261662 on epoch=169
03/08/2022 23:11:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.260496 on epoch=174
03/08/2022 23:11:36 - INFO - __main__ - Global step 350 Train loss 0.258129 Rouge-L 0.07721966450227624 on epoch=174
03/08/2022 23:11:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.252633 on epoch=179
03/08/2022 23:11:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.247113 on epoch=184
03/08/2022 23:11:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.276752 on epoch=189
03/08/2022 23:11:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.250471 on epoch=194
03/08/2022 23:12:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.248100 on epoch=199
03/08/2022 23:12:10 - INFO - __main__ - Global step 400 Train loss 0.255014 Rouge-L 0.09218172462914737 on epoch=199
03/08/2022 23:12:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.235655 on epoch=204
03/08/2022 23:12:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.231790 on epoch=209
03/08/2022 23:12:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.220478 on epoch=214
03/08/2022 23:12:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.186951 on epoch=219
03/08/2022 23:12:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.211798 on epoch=224
03/08/2022 23:12:44 - INFO - __main__ - Global step 450 Train loss 0.217334 Rouge-L 0.0910656532013067 on epoch=224
03/08/2022 23:12:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.225286 on epoch=229
03/08/2022 23:12:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.231971 on epoch=234
03/08/2022 23:13:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.199213 on epoch=239
03/08/2022 23:13:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.200108 on epoch=244
03/08/2022 23:13:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.212093 on epoch=249
03/08/2022 23:13:16 - INFO - __main__ - Global step 500 Train loss 0.213734 Rouge-L 0.10214983594270272 on epoch=249
03/08/2022 23:13:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.179346 on epoch=254
03/08/2022 23:13:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181164 on epoch=259
03/08/2022 23:13:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.178268 on epoch=264
03/08/2022 23:13:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.172783 on epoch=269
03/08/2022 23:13:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.175346 on epoch=274
03/08/2022 23:13:50 - INFO - __main__ - Global step 550 Train loss 0.177382 Rouge-L 0.11304991433668664 on epoch=274
03/08/2022 23:13:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.177659 on epoch=279
03/08/2022 23:14:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.177098 on epoch=284
03/08/2022 23:14:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.181052 on epoch=289
03/08/2022 23:14:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.192513 on epoch=294
03/08/2022 23:14:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.158994 on epoch=299
03/08/2022 23:14:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:14:21 - INFO - __main__ - Printing 3 examples
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:14:21 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:14:21 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:14:21 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:14:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:14:21 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:14:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:14:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:14:21 - INFO - __main__ - Printing 3 examples
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:14:21 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:14:21 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:14:21 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:14:21 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:14:21 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:14:21 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:14:21 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:14:23 - INFO - __main__ - Global step 600 Train loss 0.177463 Rouge-L 0.08500764498195007 on epoch=299
03/08/2022 23:14:23 - INFO - __main__ - save last model!
03/08/2022 23:14:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:14:30 - INFO - __main__ - Starting training!
03/08/2022 23:15:04 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:15:04 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 23:15:04 - INFO - __main__ - Printing 3 examples
03/08/2022 23:15:04 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 23:15:04 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 23:15:04 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 23:15:04 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 23:15:04 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 23:15:04 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 23:15:04 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:15:05 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:15:06 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 23:20:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_87_0.0003_8_predictions.txt
03/08/2022 23:20:33 - INFO - __main__ - Rouge-L on test data: 0.1337
03/08/2022 23:20:34 - INFO - __main__ - prefix=samsum_32_87, lr=0.0003, bsz=8, dev_performance=0.12493421608485766, test_performance=0.13366083798965994
03/08/2022 23:20:34 - INFO - __main__ - Running ... prefix=samsum_32_87, lr=0.0002, bsz=8 ...
03/08/2022 23:20:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:20:35 - INFO - __main__ - Printing 3 examples
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:20:35 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:20:35 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:20:35 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:20:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 23:20:35 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:20:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:20:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:20:35 - INFO - __main__ - Printing 3 examples
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:20:35 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:20:35 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:20:35 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:20:35 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:20:35 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:20:35 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:20:35 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:20:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:20:44 - INFO - __main__ - Starting training!
03/08/2022 23:20:50 - INFO - __main__ - Step 10 Global step 10 Train loss 18.491095 on epoch=4
03/08/2022 23:20:55 - INFO - __main__ - Step 20 Global step 20 Train loss 12.328222 on epoch=9
03/08/2022 23:21:01 - INFO - __main__ - Step 30 Global step 30 Train loss 9.616648 on epoch=14
03/08/2022 23:21:06 - INFO - __main__ - Step 40 Global step 40 Train loss 8.560991 on epoch=19
03/08/2022 23:21:12 - INFO - __main__ - Step 50 Global step 50 Train loss 7.418456 on epoch=24
03/08/2022 23:21:29 - INFO - __main__ - Global step 50 Train loss 11.283082 Rouge-L 0.16063059183851164 on epoch=24
03/08/2022 23:22:00 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.16063059183851164 on epoch=24, global_step=50
03/08/2022 23:22:06 - INFO - __main__ - Step 60 Global step 60 Train loss 6.907716 on epoch=29
03/08/2022 23:22:12 - INFO - __main__ - Step 70 Global step 70 Train loss 6.271865 on epoch=34
03/08/2022 23:22:17 - INFO - __main__ - Step 80 Global step 80 Train loss 5.126250 on epoch=39
03/08/2022 23:22:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.039439 on epoch=44
03/08/2022 23:22:29 - INFO - __main__ - Step 100 Global step 100 Train loss 3.262292 on epoch=49
03/08/2022 23:22:34 - INFO - __main__ - Global step 100 Train loss 5.121512 Rouge-L 0.17273028519927391 on epoch=49
03/08/2022 23:23:07 - INFO - __main__ - Saving model with best Rouge-L: 0.16063059183851164 -> 0.17273028519927391 on epoch=49, global_step=100
03/08/2022 23:23:13 - INFO - __main__ - Step 110 Global step 110 Train loss 2.860224 on epoch=54
03/08/2022 23:23:18 - INFO - __main__ - Step 120 Global step 120 Train loss 2.524269 on epoch=59
03/08/2022 23:23:24 - INFO - __main__ - Step 130 Global step 130 Train loss 3.167704 on epoch=64
03/08/2022 23:23:30 - INFO - __main__ - Step 140 Global step 140 Train loss 2.641682 on epoch=69
03/08/2022 23:23:36 - INFO - __main__ - Step 150 Global step 150 Train loss 2.436597 on epoch=74
03/08/2022 23:23:39 - INFO - __main__ - Global step 150 Train loss 2.726095 Rouge-L 0.19436936809248512 on epoch=74
03/08/2022 23:24:10 - INFO - __main__ - Saving model with best Rouge-L: 0.17273028519927391 -> 0.19436936809248512 on epoch=74, global_step=150
03/08/2022 23:24:16 - INFO - __main__ - Step 160 Global step 160 Train loss 2.063753 on epoch=79
03/08/2022 23:24:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.697178 on epoch=84
03/08/2022 23:24:28 - INFO - __main__ - Step 180 Global step 180 Train loss 1.501862 on epoch=89
03/08/2022 23:24:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.407033 on epoch=94
03/08/2022 23:24:39 - INFO - __main__ - Step 200 Global step 200 Train loss 1.207518 on epoch=99
03/08/2022 23:24:44 - INFO - __main__ - Global step 200 Train loss 1.575469 Rouge-L 0.12837046080160316 on epoch=99
03/08/2022 23:24:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.074705 on epoch=104
03/08/2022 23:24:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.960272 on epoch=109
03/08/2022 23:25:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.741727 on epoch=114
03/08/2022 23:25:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.747303 on epoch=119
03/08/2022 23:25:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.654125 on epoch=124
03/08/2022 23:25:18 - INFO - __main__ - Global step 250 Train loss 0.835626 Rouge-L 0.10699199118331174 on epoch=124
03/08/2022 23:25:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.601709 on epoch=129
03/08/2022 23:25:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.522733 on epoch=134
03/08/2022 23:25:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.483690 on epoch=139
03/08/2022 23:25:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.470130 on epoch=144
03/08/2022 23:25:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.480542 on epoch=149
03/08/2022 23:25:52 - INFO - __main__ - Global step 300 Train loss 0.511761 Rouge-L 0.10607994394546401 on epoch=149
03/08/2022 23:25:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.484085 on epoch=154
03/08/2022 23:26:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.422951 on epoch=159
03/08/2022 23:26:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.414944 on epoch=164
03/08/2022 23:26:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.376161 on epoch=169
03/08/2022 23:26:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.362391 on epoch=174
03/08/2022 23:26:25 - INFO - __main__ - Global step 350 Train loss 0.412106 Rouge-L 0.1023656310141158 on epoch=174
03/08/2022 23:26:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.344791 on epoch=179
03/08/2022 23:26:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.364414 on epoch=184
03/08/2022 23:26:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.348408 on epoch=189
03/08/2022 23:26:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.343913 on epoch=194
03/08/2022 23:26:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.359669 on epoch=199
03/08/2022 23:26:59 - INFO - __main__ - Global step 400 Train loss 0.352239 Rouge-L 0.08338674492858617 on epoch=199
03/08/2022 23:27:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.348859 on epoch=204
03/08/2022 23:27:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.322697 on epoch=209
03/08/2022 23:27:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.293811 on epoch=214
03/08/2022 23:27:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.312674 on epoch=219
03/08/2022 23:27:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.312764 on epoch=224
03/08/2022 23:27:33 - INFO - __main__ - Global step 450 Train loss 0.318161 Rouge-L 0.07639479174249376 on epoch=224
03/08/2022 23:27:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.273081 on epoch=229
03/08/2022 23:27:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.289359 on epoch=234
03/08/2022 23:27:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.290044 on epoch=239
03/08/2022 23:27:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.257844 on epoch=244
03/08/2022 23:28:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.278657 on epoch=249
03/08/2022 23:28:07 - INFO - __main__ - Global step 500 Train loss 0.277797 Rouge-L 0.09288151708560594 on epoch=249
03/08/2022 23:28:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.291503 on epoch=254
03/08/2022 23:28:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.259843 on epoch=259
03/08/2022 23:28:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.248245 on epoch=264
03/08/2022 23:28:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.271198 on epoch=269
03/08/2022 23:28:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.229572 on epoch=274
03/08/2022 23:28:41 - INFO - __main__ - Global step 550 Train loss 0.260072 Rouge-L 0.07365115655399149 on epoch=274
03/08/2022 23:28:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.243977 on epoch=279
03/08/2022 23:28:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.238598 on epoch=284
03/08/2022 23:28:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.238903 on epoch=289
03/08/2022 23:29:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.238188 on epoch=294
03/08/2022 23:29:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.237875 on epoch=299
03/08/2022 23:29:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:29:11 - INFO - __main__ - Printing 3 examples
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:29:11 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:29:11 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:29:11 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:29:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:29:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:29:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:29:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:29:11 - INFO - __main__ - Printing 3 examples
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:29:11 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:29:11 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:29:11 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:29:11 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:29:11 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:29:11 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:29:11 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:29:14 - INFO - __main__ - Global step 600 Train loss 0.239508 Rouge-L 0.07793030388181424 on epoch=299
03/08/2022 23:29:14 - INFO - __main__ - save last model!
03/08/2022 23:29:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:29:20 - INFO - __main__ - Starting training!
03/08/2022 23:29:54 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:29:55 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 23:29:55 - INFO - __main__ - Printing 3 examples
03/08/2022 23:29:55 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 23:29:55 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 23:29:55 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 23:29:55 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 23:29:55 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 23:29:55 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 23:29:55 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:29:56 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:29:57 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 23:31:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_87_0.0002_8_predictions.txt
03/08/2022 23:31:45 - INFO - __main__ - Rouge-L on test data: 0.2012
03/08/2022 23:31:45 - INFO - __main__ - prefix=samsum_32_87, lr=0.0002, bsz=8, dev_performance=0.19436936809248512, test_performance=0.2011776414539688
03/08/2022 23:31:45 - INFO - __main__ - Running ... prefix=samsum_32_87, lr=0.0001, bsz=8 ...
03/08/2022 23:31:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:31:46 - INFO - __main__ - Printing 3 examples
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Kelly: I need you guys! i'm feeling down! :( Sam: what's wrong babe? Kelly: i went on a date with Tom yesterday evening and thought he was gonna pop the question :( Grace: but he didn't? Kelly: No :( Grace: why did you think he was gonna do it? Kelly: don't know just had that feeling and he called it a special date.. Sam: so you thought he meant the ring! Kelly: yeah.. silly me! :( Grace: why did he call it special then? Kelly: we went to a very popular restaurant which you have to book weeks in advance! Grace: did you tell him? Kelly: nope!  Grace: oh dear! :( Sam: should have told him!
03/08/2022 23:31:46 - INFO - __main__ - ['Kelly is upset, because her boyfriend took her to a fancy restaurant on a special date, so she suspected he was going to propose, which he did not.']
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Mia: We should collect some money for this family Mia: I can't stop thinking about their problems Lene: yes, and they were so hospitable Terry: I think we could organise a little street-funding in Berlin Terry: and then just send them the money Peter: But can one collect money just like this? Mia: I doubt. One has to register an organisation Mia: otherwise people would just embezzle money all the time
03/08/2022 23:31:46 - INFO - __main__ - ['Mia, Lene, Terry and Peter are planning to raise some money for this family. It is necessary to register an organization to be able to raise money, as Mia points out.']
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Kate: Hey, are you ok? Clark: Yeah, why? Kate: Just asking. I've heard some rumours in the cafeteria. Is it true you're changing jobs? Clark: It is. Seems like everyone's talking about it :-( Kate: Is it because of that clash with the big boss? Clark: Yes and no. It's not that simple. People will be talking different things. Kate: Hey! I don't give a fuck what they are saying. Let them talk. I'm more interested in your version of events. Clark: My version? I don't even know where to start. There was this old bugger, my health issues, some fits of depression. Maybe I'm just burnt-out. I just want a change. Kate: Listen. You're the one who's always backed me up. I'm really sorry about all that shit. Clark: Thanks, Kate.  Kate: :-) Clark: I'll be fine. I've already done some re-thinking.  Kate: Hope it'll be all for the good. But it doesn't change the fact that weI'll miss you.   Clark: We? Kate: Well, there's me, Gina, Tom...there are a few black sheep in this 'corporate family'. Clark: Yeah... the 'corporate family' with the big daddy. I'm really sick of his 'committment to duties'. Kate: Me too :-(  Clark: You cannot teach an old dog new tricks. Kate: Come to think of that now I don't know if I'm more sorry for you or for me because I must stay here.  Clark: Why? Kate: Primo, I'm not as outspoken as you. I can't stand my ground like you. So probably, I'll be stuck in this hell... like... forever? Clark: Don't make a hero of me. Kate: Secundo, with the mortgage that we took last year and the kids... No, I can't risk that much. Clark: I know. It's a little different in my case. Sorry, Kate. Kate: So, you know. Should you need some devoted team to lead your campaign, do not hesitate to contact us :-)  Clark: You're the best! Kate: I've been learning from the best :-) Clark: Good girl!  Kate: Keep my fingers crossed for you! Clark: Thx
03/08/2022 23:31:46 - INFO - __main__ - ["Clark is changing jobs. People gossip about possible reasons why he's leaving the corporation. In fact it was a combination of different factors. Kate has to stay in the job because she took a mortgage last year and has children to keep."]
03/08/2022 23:31:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 23:31:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:31:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:31:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:31:46 - INFO - __main__ - Printing 3 examples
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Gil: Have you read the new Frost book Corey: no I haven't Gil: you should get it Corey: ok thanks for the recommendation
03/08/2022 23:31:46 - INFO - __main__ - ['Corey has not yet read new Frost book that Gil recommends.']
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Rosie: Hello, Barbara. It's Rosie here from Boots, we have your bandages and prescription ready for collection. Barbara: Oh, hi Rosie. Can I collect them tomorrow? I'm a wee bit busy this afternoon. Rosie: Of course, that's fine. Barbara: Thanks, see you tomorrow.
03/08/2022 23:31:46 - INFO - __main__ - ['Barbara will collect stuff from Boots tomorrow.']
03/08/2022 23:31:46 - INFO - __main__ -  [samsum] summarize: Tina: Derek! It's your fuckin turn to do the dishes Derek: I did them yesterday!!! Tina: washing one spoon is not doing the dishes Derek: fuck off
03/08/2022 23:31:46 - INFO - __main__ - ["Tina is angry with Derek because he won't do the dishes."]
03/08/2022 23:31:46 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:31:46 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:31:46 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:31:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:31:55 - INFO - __main__ - Starting training!
03/08/2022 23:32:01 - INFO - __main__ - Step 10 Global step 10 Train loss 18.489824 on epoch=4
03/08/2022 23:32:06 - INFO - __main__ - Step 20 Global step 20 Train loss 15.575529 on epoch=9
03/08/2022 23:32:12 - INFO - __main__ - Step 30 Global step 30 Train loss 11.192900 on epoch=14
03/08/2022 23:32:18 - INFO - __main__ - Step 40 Global step 40 Train loss 8.927235 on epoch=19
03/08/2022 23:32:23 - INFO - __main__ - Step 50 Global step 50 Train loss 7.487233 on epoch=24
03/08/2022 23:32:37 - INFO - __main__ - Global step 50 Train loss 12.334545 Rouge-L 0.25874414431921544 on epoch=24
03/08/2022 23:33:09 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.25874414431921544 on epoch=24, global_step=50
03/08/2022 23:33:15 - INFO - __main__ - Step 60 Global step 60 Train loss 6.859597 on epoch=29
03/08/2022 23:33:21 - INFO - __main__ - Step 70 Global step 70 Train loss 5.665215 on epoch=34
03/08/2022 23:33:26 - INFO - __main__ - Step 80 Global step 80 Train loss 4.743280 on epoch=39
03/08/2022 23:33:32 - INFO - __main__ - Step 90 Global step 90 Train loss 4.015659 on epoch=44
03/08/2022 23:33:38 - INFO - __main__ - Step 100 Global step 100 Train loss 3.416323 on epoch=49
03/08/2022 23:33:48 - INFO - __main__ - Global step 100 Train loss 4.940015 Rouge-L 0.3058196545310406 on epoch=49
03/08/2022 23:34:21 - INFO - __main__ - Saving model with best Rouge-L: 0.25874414431921544 -> 0.3058196545310406 on epoch=49, global_step=100
03/08/2022 23:34:26 - INFO - __main__ - Step 110 Global step 110 Train loss 2.988733 on epoch=54
03/08/2022 23:34:32 - INFO - __main__ - Step 120 Global step 120 Train loss 2.809664 on epoch=59
03/08/2022 23:34:38 - INFO - __main__ - Step 130 Global step 130 Train loss 2.548714 on epoch=64
03/08/2022 23:34:44 - INFO - __main__ - Step 140 Global step 140 Train loss 2.244700 on epoch=69
03/08/2022 23:34:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.921771 on epoch=74
03/08/2022 23:34:54 - INFO - __main__ - Global step 150 Train loss 2.502717 Rouge-L 0.3103708628737755 on epoch=74
03/08/2022 23:35:25 - INFO - __main__ - Saving model with best Rouge-L: 0.3058196545310406 -> 0.3103708628737755 on epoch=74, global_step=150
03/08/2022 23:35:31 - INFO - __main__ - Step 160 Global step 160 Train loss 1.768221 on epoch=79
03/08/2022 23:35:36 - INFO - __main__ - Step 170 Global step 170 Train loss 1.703403 on epoch=84
03/08/2022 23:35:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.595356 on epoch=89
03/08/2022 23:35:48 - INFO - __main__ - Step 190 Global step 190 Train loss 1.407072 on epoch=94
03/08/2022 23:35:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.272076 on epoch=99
03/08/2022 23:35:58 - INFO - __main__ - Global step 200 Train loss 1.549226 Rouge-L 0.29962608221543996 on epoch=99
03/08/2022 23:36:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.248873 on epoch=104
03/08/2022 23:36:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.138629 on epoch=109
03/08/2022 23:36:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.082431 on epoch=114
03/08/2022 23:36:22 - INFO - __main__ - Step 240 Global step 240 Train loss 1.054473 on epoch=119
03/08/2022 23:36:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.981219 on epoch=124
03/08/2022 23:36:32 - INFO - __main__ - Global step 250 Train loss 1.101125 Rouge-L 0.2954258982656231 on epoch=124
03/08/2022 23:36:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.906604 on epoch=129
03/08/2022 23:36:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.920791 on epoch=134
03/08/2022 23:36:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.891215 on epoch=139
03/08/2022 23:36:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.842040 on epoch=144
03/08/2022 23:37:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.842100 on epoch=149
03/08/2022 23:37:05 - INFO - __main__ - Global step 300 Train loss 0.880550 Rouge-L 0.27246978078551964 on epoch=149
03/08/2022 23:37:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.809676 on epoch=154
03/08/2022 23:37:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.743947 on epoch=159
03/08/2022 23:37:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.703667 on epoch=164
03/08/2022 23:37:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.712380 on epoch=169
03/08/2022 23:37:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.661589 on epoch=174
03/08/2022 23:37:38 - INFO - __main__ - Global step 350 Train loss 0.726252 Rouge-L 0.26099921889850664 on epoch=174
03/08/2022 23:37:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.673962 on epoch=179
03/08/2022 23:37:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.634230 on epoch=184
03/08/2022 23:37:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.611676 on epoch=189
03/08/2022 23:38:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.547310 on epoch=194
03/08/2022 23:38:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.581944 on epoch=199
03/08/2022 23:38:11 - INFO - __main__ - Global step 400 Train loss 0.609824 Rouge-L 0.24874696014443032 on epoch=199
03/08/2022 23:38:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.518820 on epoch=204
03/08/2022 23:38:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.498047 on epoch=209
03/08/2022 23:38:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.540014 on epoch=214
03/08/2022 23:38:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.470647 on epoch=219
03/08/2022 23:38:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.474377 on epoch=224
03/08/2022 23:38:43 - INFO - __main__ - Global step 450 Train loss 0.500381 Rouge-L 0.24986122062797395 on epoch=224
03/08/2022 23:38:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.412609 on epoch=229
03/08/2022 23:38:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.423332 on epoch=234
03/08/2022 23:39:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.438138 on epoch=239
03/08/2022 23:39:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.360162 on epoch=244
03/08/2022 23:39:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.435096 on epoch=249
03/08/2022 23:39:16 - INFO - __main__ - Global step 500 Train loss 0.413867 Rouge-L 0.28943832168082584 on epoch=249
03/08/2022 23:39:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.377558 on epoch=254
03/08/2022 23:39:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.366815 on epoch=259
03/08/2022 23:39:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.383443 on epoch=264
03/08/2022 23:39:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.374018 on epoch=269
03/08/2022 23:39:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.385114 on epoch=274
03/08/2022 23:39:49 - INFO - __main__ - Global step 550 Train loss 0.377390 Rouge-L 0.2424565171474203 on epoch=274
03/08/2022 23:39:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.350613 on epoch=279
03/08/2022 23:40:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.356944 on epoch=284
03/08/2022 23:40:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.314246 on epoch=289
03/08/2022 23:40:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.296012 on epoch=294
03/08/2022 23:40:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.294376 on epoch=299
03/08/2022 23:40:20 - INFO - __main__ - Global step 600 Train loss 0.322438 Rouge-L 0.23772711184507805 on epoch=299
03/08/2022 23:40:21 - INFO - __main__ - save last model!
03/08/2022 23:40:59 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:40:59 - INFO - __main__ - Start tokenizing ... 818 instances
03/08/2022 23:40:59 - INFO - __main__ - Printing 3 examples
03/08/2022 23:40:59 - INFO - __main__ -  [samsum] summarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)  A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
03/08/2022 23:40:59 - INFO - __main__ - ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']
03/08/2022 23:40:59 - INFO - __main__ -  [samsum] summarize: Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob: I used to get one every year as a child! Loved them!  Emma: Yeah, i remember! they were filled with chocolates! Lauren: they are different these days! much more sophisticated! Haha! Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma: what do you fit inside? Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma: WOW! That’s brill! X Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren: i reckon it prepares them for Christmas  Emma: and makes it more about traditions and being kind to other people Lauren: my children get very excited every time they get one! Emma: i can see why! :)
03/08/2022 23:40:59 - INFO - __main__ - ['Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.']
03/08/2022 23:40:59 - INFO - __main__ -  [samsum] summarize: Jackie: Madison is pregnant Jackie: but she doesn't wanna talk about it Iggy: why Jackie: I don't know why because she doesn't wanna talk about it Iggy: ok Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions Jackie: and she looked way more anxious than excited Iggy: she's probably worrying about it Iggy: she's taking every commitment really seriously Jackie: it could be money problems or relationship problems Iggy: or maybe she wants an abortion Jackie: it could be all of the above Iggy: but you know what? Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it Jackie: why? Iggy: I felt they were immature and I couldn't picture this couple as parents Jackie: I felt similar way on Patricia's wedding Iggy: Patricia Stevens? Jackie: yes Iggy: so we're talking about the same person Jackie: what a coincidence Jackie: so she's pregnant? Iggy: she thought she was Jackie: damn...
03/08/2022 23:40:59 - INFO - __main__ - ["Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant."]
03/08/2022 23:40:59 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:41:00 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:41:01 - INFO - __main__ - Loaded 818 examples from test data
03/08/2022 23:43:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-samsum/samsum_32_87_0.0001_8_predictions.txt
03/08/2022 23:43:13 - INFO - __main__ - Rouge-L on test data: 0.2628
03/08/2022 23:43:13 - INFO - __main__ - prefix=samsum_32_87, lr=0.0001, bsz=8, dev_performance=0.3103708628737755, test_performance=0.2628200041191566
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00036406517028808594 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13417", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15065, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "13418", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15065, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15065, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (13505): No such process
Task: blimp-sentential_negation_npi_scope, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_g13tm_q4/none_2b9_tg1g
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_g13tm_q4/none_2b9_tg1g/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_g13tm_q4/none_2b9_tg1g/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/08/2022 23:43:22 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_scope/', task_name='blimp-sentential_negation_npi_scope', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 23:43:22 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope
03/08/2022 23:43:22 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_scope/', task_name='blimp-sentential_negation_npi_scope', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/08/2022 23:43:22 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope
03/08/2022 23:43:23 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/08/2022 23:43:23 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/08/2022 23:43:23 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/08/2022 23:43:23 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/08/2022 23:43:23 - INFO - __main__ - args.device: cuda:0
03/08/2022 23:43:23 - INFO - __main__ - Using 2 gpus
03/08/2022 23:43:23 - INFO - __main__ - args.device: cuda:1
03/08/2022 23:43:23 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_scope_16_100', 'blimp-sentential_negation_npi_scope_16_13', 'blimp-sentential_negation_npi_scope_16_21', 'blimp-sentential_negation_npi_scope_16_42', 'blimp-sentential_negation_npi_scope_16_87']
03/08/2022 23:43:23 - INFO - __main__ - Using 2 gpus
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/08/2022 23:43:23 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_scope_16_100', 'blimp-sentential_negation_npi_scope_16_13', 'blimp-sentential_negation_npi_scope_16_21', 'blimp-sentential_negation_npi_scope_16_42', 'blimp-sentential_negation_npi_scope_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/08/2022 23:43:28 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0005, bsz=8 ...
03/08/2022 23:43:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:43:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:43:29 - INFO - __main__ - Printing 3 examples
03/08/2022 23:43:29 - INFO - __main__ - Printing 3 examples
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:43:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:43:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:43:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:43:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:43:29 - INFO - __main__ - Printing 3 examples
03/08/2022 23:43:29 - INFO - __main__ - Printing 3 examples
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:43:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - ['sentence 1']
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:43:29 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:43:29 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:43:29 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:43:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:43:41 - INFO - __main__ - Starting training!
03/08/2022 23:43:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:43:41 - INFO - __main__ - Starting training!
03/08/2022 23:43:44 - INFO - __main__ - Step 10 Global step 10 Train loss 21.170696 on epoch=4
03/08/2022 23:43:49 - INFO - __main__ - Step 20 Global step 20 Train loss 17.072809 on epoch=9
03/08/2022 23:43:53 - INFO - __main__ - Step 30 Global step 30 Train loss 14.520389 on epoch=14
03/08/2022 23:43:57 - INFO - __main__ - Step 40 Global step 40 Train loss 13.253253 on epoch=19
03/08/2022 23:44:02 - INFO - __main__ - Step 50 Global step 50 Train loss 11.330160 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/08/2022 23:44:02 - INFO - __main__ - Global step 50 Train loss 15.469461 ACC 0.0 on epoch=24
03/08/2022 23:44:14 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 23:44:19 - INFO - __main__ - Step 60 Global step 60 Train loss 10.290257 on epoch=29
03/08/2022 23:44:24 - INFO - __main__ - Step 70 Global step 70 Train loss 9.288200 on epoch=34
03/08/2022 23:44:28 - INFO - __main__ - Step 80 Global step 80 Train loss 5.828633 on epoch=39
03/08/2022 23:44:33 - INFO - __main__ - Step 90 Global step 90 Train loss 2.235971 on epoch=44
03/08/2022 23:44:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.260826 on epoch=49
03/08/2022 23:44:38 - INFO - __main__ - Global step 100 Train loss 5.580778 ACC 0.5 on epoch=49
03/08/2022 23:45:12 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=49, global_step=100
03/08/2022 23:45:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.227181 on epoch=54
03/08/2022 23:45:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.255030 on epoch=59
03/08/2022 23:45:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.244654 on epoch=64
03/08/2022 23:45:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.248654 on epoch=69
03/08/2022 23:45:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.252639 on epoch=74
03/08/2022 23:45:35 - INFO - __main__ - Global step 150 Train loss 0.245631 ACC 0.5 on epoch=74
03/08/2022 23:45:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.248717 on epoch=79
03/08/2022 23:45:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.271032 on epoch=84
03/08/2022 23:45:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.235125 on epoch=89
03/08/2022 23:45:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.232079 on epoch=94
03/08/2022 23:45:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.243603 on epoch=99
03/08/2022 23:45:58 - INFO - __main__ - Global step 200 Train loss 0.246111 ACC 0.5 on epoch=99
03/08/2022 23:46:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.242910 on epoch=104
03/08/2022 23:46:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.238643 on epoch=109
03/08/2022 23:46:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.231473 on epoch=114
03/08/2022 23:46:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.238978 on epoch=119
03/08/2022 23:46:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.221696 on epoch=124
03/08/2022 23:46:22 - INFO - __main__ - Global step 250 Train loss 0.234740 ACC 0.4375 on epoch=124
03/08/2022 23:46:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.243004 on epoch=129
03/08/2022 23:46:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.233575 on epoch=134
03/08/2022 23:46:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.217312 on epoch=139
03/08/2022 23:46:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.228908 on epoch=144
03/08/2022 23:46:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.241204 on epoch=149
03/08/2022 23:46:45 - INFO - __main__ - Global step 300 Train loss 0.232801 ACC 0.5 on epoch=149
03/08/2022 23:46:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.240994 on epoch=154
03/08/2022 23:46:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.219179 on epoch=159
03/08/2022 23:46:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.243494 on epoch=164
03/08/2022 23:47:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.229733 on epoch=169
03/08/2022 23:47:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.234640 on epoch=174
03/08/2022 23:47:08 - INFO - __main__ - Global step 350 Train loss 0.233608 ACC 0.5 on epoch=174
03/08/2022 23:47:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.245858 on epoch=179
03/08/2022 23:47:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.231126 on epoch=184
03/08/2022 23:47:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.236647 on epoch=189
03/08/2022 23:47:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.239146 on epoch=194
03/08/2022 23:47:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.224571 on epoch=199
03/08/2022 23:47:31 - INFO - __main__ - Global step 400 Train loss 0.235470 ACC 0.5 on epoch=199
03/08/2022 23:47:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.231849 on epoch=204
03/08/2022 23:47:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.225489 on epoch=209
03/08/2022 23:47:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.240974 on epoch=214
03/08/2022 23:47:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.230340 on epoch=219
03/08/2022 23:47:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.231872 on epoch=224
03/08/2022 23:47:55 - INFO - __main__ - Global step 450 Train loss 0.232105 ACC 0.5 on epoch=224
03/08/2022 23:47:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.233945 on epoch=229
03/08/2022 23:48:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.222284 on epoch=234
03/08/2022 23:48:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.228267 on epoch=239
03/08/2022 23:48:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.209275 on epoch=244
03/08/2022 23:48:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.213320 on epoch=249
03/08/2022 23:48:18 - INFO - __main__ - Global step 500 Train loss 0.221418 ACC 0.4375 on epoch=249
03/08/2022 23:48:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.210255 on epoch=254
03/08/2022 23:48:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.217353 on epoch=259
03/08/2022 23:48:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.221004 on epoch=264
03/08/2022 23:48:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.219956 on epoch=269
03/08/2022 23:48:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.211120 on epoch=274
03/08/2022 23:48:41 - INFO - __main__ - Global step 550 Train loss 0.215938 ACC 0.5 on epoch=274
03/08/2022 23:48:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.224989 on epoch=279
03/08/2022 23:48:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.224826 on epoch=284
03/08/2022 23:48:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.192728 on epoch=289
03/08/2022 23:48:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.202723 on epoch=294
03/08/2022 23:49:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.255439 on epoch=299
03/08/2022 23:49:04 - INFO - __main__ - Global step 600 Train loss 0.220141 ACC 0.46875 on epoch=299
03/08/2022 23:49:04 - INFO - __main__ - save last model!
03/08/2022 23:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:49:05 - INFO - __main__ - Printing 3 examples
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:49:05 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:49:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:49:05 - INFO - __main__ - Printing 3 examples
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:49:05 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:05 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:49:05 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:49:05 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:49:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:49:15 - INFO - __main__ - Starting training!
03/08/2022 23:49:26 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:49:27 - INFO - __main__ - Start tokenizing ... 200 instances
03/08/2022 23:49:27 - INFO - __main__ - Printing 3 examples
03/08/2022 23:49:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/08/2022 23:49:27 - INFO - __main__ - ['sentence 2']
03/08/2022 23:49:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/08/2022 23:49:27 - INFO - __main__ - ['sentence 2']
03/08/2022 23:49:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/08/2022 23:49:27 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:27 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:49:27 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:49:27 - INFO - __main__ - Loaded 200 examples from test data
03/08/2022 23:49:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_100_0.0005_8_predictions.txt
03/08/2022 23:49:29 - INFO - __main__ - ACC on test data: 0.5050
03/08/2022 23:49:30 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.505
03/08/2022 23:49:30 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0003, bsz=8 ...
03/08/2022 23:49:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:49:30 - INFO - __main__ - Printing 3 examples
03/08/2022 23:49:30 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:49:30 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:30 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:49:30 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:30 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:49:30 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:49:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:49:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:49:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:49:31 - INFO - __main__ - Printing 3 examples
03/08/2022 23:49:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:49:31 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:49:31 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:49:31 - INFO - __main__ - ['sentence 1']
03/08/2022 23:49:31 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:49:31 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:49:31 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:49:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:49:40 - INFO - __main__ - Starting training!
03/08/2022 23:49:44 - INFO - __main__ - Step 10 Global step 10 Train loss 22.470634 on epoch=4
03/08/2022 23:49:48 - INFO - __main__ - Step 20 Global step 20 Train loss 20.391672 on epoch=9
03/08/2022 23:49:53 - INFO - __main__ - Step 30 Global step 30 Train loss 17.306587 on epoch=14
03/08/2022 23:49:57 - INFO - __main__ - Step 40 Global step 40 Train loss 13.724551 on epoch=19
03/08/2022 23:50:02 - INFO - __main__ - Step 50 Global step 50 Train loss 12.304883 on epoch=24
03/08/2022 23:50:02 - INFO - __main__ - Global step 50 Train loss 17.239664 ACC 0.0 on epoch=24
03/08/2022 23:50:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/08/2022 23:50:41 - INFO - __main__ - Step 60 Global step 60 Train loss 11.517911 on epoch=29
03/08/2022 23:50:46 - INFO - __main__ - Step 70 Global step 70 Train loss 10.440178 on epoch=34
03/08/2022 23:50:50 - INFO - __main__ - Step 80 Global step 80 Train loss 9.507037 on epoch=39
03/08/2022 23:50:55 - INFO - __main__ - Step 90 Global step 90 Train loss 8.757874 on epoch=44
03/08/2022 23:51:00 - INFO - __main__ - Step 100 Global step 100 Train loss 6.972276 on epoch=49
03/08/2022 23:51:00 - INFO - __main__ - Global step 100 Train loss 9.439055 ACC 0.0 on epoch=49
03/08/2022 23:51:05 - INFO - __main__ - Step 110 Global step 110 Train loss 4.152384 on epoch=54
03/08/2022 23:51:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.725208 on epoch=59
03/08/2022 23:51:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.226393 on epoch=64
03/08/2022 23:51:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.206205 on epoch=69
03/08/2022 23:51:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.167951 on epoch=74
03/08/2022 23:51:24 - INFO - __main__ - Global step 150 Train loss 1.095628 ACC 0.5625 on epoch=74
03/08/2022 23:51:57 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5625 on epoch=74, global_step=150
03/08/2022 23:52:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.141048 on epoch=79
03/08/2022 23:52:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.123452 on epoch=84
03/08/2022 23:52:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.105562 on epoch=89
03/08/2022 23:52:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.086889 on epoch=94
03/08/2022 23:52:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.071346 on epoch=99
03/08/2022 23:52:21 - INFO - __main__ - Global step 200 Train loss 0.105660 ACC 0.59375 on epoch=99
03/08/2022 23:52:54 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=99, global_step=200
03/08/2022 23:52:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.041050 on epoch=104
03/08/2022 23:53:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.086953 on epoch=109
03/08/2022 23:53:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.020426 on epoch=114
03/08/2022 23:53:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.013847 on epoch=119
03/08/2022 23:53:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.011858 on epoch=124
03/08/2022 23:53:18 - INFO - __main__ - Global step 250 Train loss 0.034827 ACC 0.625 on epoch=124
03/08/2022 23:53:52 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=124, global_step=250
03/08/2022 23:53:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.082355 on epoch=129
03/08/2022 23:54:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.021474 on epoch=134
03/08/2022 23:54:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.010060 on epoch=139
03/08/2022 23:54:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.179832 on epoch=144
03/08/2022 23:54:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005778 on epoch=149
03/08/2022 23:54:16 - INFO - __main__ - Global step 300 Train loss 0.059900 ACC 0.75 on epoch=149
03/08/2022 23:54:50 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=149, global_step=300
03/08/2022 23:54:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.011815 on epoch=154
03/08/2022 23:54:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003245 on epoch=159
03/08/2022 23:55:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.033320 on epoch=164
03/08/2022 23:55:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001609 on epoch=169
03/08/2022 23:55:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.040381 on epoch=174
03/08/2022 23:55:14 - INFO - __main__ - Global step 350 Train loss 0.018074 ACC 0.65625 on epoch=174
03/08/2022 23:55:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.040343 on epoch=179
03/08/2022 23:55:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001884 on epoch=184
03/08/2022 23:55:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002359 on epoch=189
03/08/2022 23:55:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.005942 on epoch=194
03/08/2022 23:55:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000639 on epoch=199
03/08/2022 23:55:38 - INFO - __main__ - Global step 400 Train loss 0.010233 ACC 0.78125 on epoch=199
03/08/2022 23:56:10 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=199, global_step=400
03/08/2022 23:56:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001023 on epoch=204
03/08/2022 23:56:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000576 on epoch=209
03/08/2022 23:56:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000440 on epoch=214
03/08/2022 23:56:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000809 on epoch=219
03/08/2022 23:56:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000424 on epoch=224
03/08/2022 23:56:34 - INFO - __main__ - Global step 450 Train loss 0.000654 ACC 0.78125 on epoch=224
03/08/2022 23:56:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000435 on epoch=229
03/08/2022 23:56:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.007514 on epoch=234
03/08/2022 23:56:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001741 on epoch=239
03/08/2022 23:56:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000690 on epoch=244
03/08/2022 23:56:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001303 on epoch=249
03/08/2022 23:56:58 - INFO - __main__ - Global step 500 Train loss 0.002337 ACC 0.84375 on epoch=249
03/08/2022 23:57:30 - INFO - __main__ - Saving model with best ACC: 0.78125 -> 0.84375 on epoch=249, global_step=500
03/08/2022 23:57:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000271 on epoch=254
03/08/2022 23:57:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000411 on epoch=259
03/08/2022 23:57:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000193 on epoch=264
03/08/2022 23:57:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000356 on epoch=269
03/08/2022 23:57:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000078 on epoch=274
03/08/2022 23:57:54 - INFO - __main__ - Global step 550 Train loss 0.000262 ACC 0.8125 on epoch=274
03/08/2022 23:57:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000171 on epoch=279
03/08/2022 23:58:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000139 on epoch=284
03/08/2022 23:58:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000091 on epoch=289
03/08/2022 23:58:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000061 on epoch=294
03/08/2022 23:58:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000080 on epoch=299
03/08/2022 23:58:17 - INFO - __main__ - Global step 600 Train loss 0.000108 ACC 0.8125 on epoch=299
03/08/2022 23:58:17 - INFO - __main__ - save last model!
03/08/2022 23:58:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:58:18 - INFO - __main__ - Printing 3 examples
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/08/2022 23:58:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:58:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:58:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:58:18 - INFO - __main__ - Printing 3 examples
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:58:18 - INFO - __main__ - ['sentence 1']
03/08/2022 23:58:18 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:58:18 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:58:19 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:58:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:58:29 - INFO - __main__ - Starting training!
03/08/2022 23:59:01 - INFO - __main__ - Loading checkpoint on the fly
03/08/2022 23:59:02 - INFO - __main__ - Start tokenizing ... 200 instances
03/08/2022 23:59:02 - INFO - __main__ - Printing 3 examples
03/08/2022 23:59:02 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/08/2022 23:59:02 - INFO - __main__ - ['sentence 2']
03/08/2022 23:59:02 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/08/2022 23:59:02 - INFO - __main__ - ['sentence 2']
03/08/2022 23:59:02 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/08/2022 23:59:02 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:02 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:59:02 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:59:02 - INFO - __main__ - Loaded 200 examples from test data
03/08/2022 23:59:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_100_0.0003_8_predictions.txt
03/08/2022 23:59:12 - INFO - __main__ - ACC on test data: 0.7400
03/08/2022 23:59:13 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0003, bsz=8, dev_performance=0.84375, test_performance=0.74
03/08/2022 23:59:13 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0002, bsz=8 ...
03/08/2022 23:59:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:59:14 - INFO - __main__ - Printing 3 examples
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/08/2022 23:59:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:59:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/08/2022 23:59:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/08/2022 23:59:14 - INFO - __main__ - Printing 3 examples
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/08/2022 23:59:14 - INFO - __main__ - ['sentence 1']
03/08/2022 23:59:14 - INFO - __main__ - Tokenizing Input ...
03/08/2022 23:59:14 - INFO - __main__ - Tokenizing Output ...
03/08/2022 23:59:14 - INFO - __main__ - Loaded 32 examples from dev data
03/08/2022 23:59:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/08/2022 23:59:23 - INFO - __main__ - Starting training!
03/08/2022 23:59:27 - INFO - __main__ - Step 10 Global step 10 Train loss 22.255474 on epoch=4
03/08/2022 23:59:31 - INFO - __main__ - Step 20 Global step 20 Train loss 21.078854 on epoch=9
03/08/2022 23:59:36 - INFO - __main__ - Step 30 Global step 30 Train loss 17.992582 on epoch=14
03/08/2022 23:59:40 - INFO - __main__ - Step 40 Global step 40 Train loss 15.353266 on epoch=19
03/08/2022 23:59:45 - INFO - __main__ - Step 50 Global step 50 Train loss 13.619780 on epoch=24
03/08/2022 23:59:46 - INFO - __main__ - Global step 50 Train loss 18.059992 ACC 0.0 on epoch=24
03/09/2022 00:00:20 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 00:00:25 - INFO - __main__ - Step 60 Global step 60 Train loss 12.977766 on epoch=29
03/09/2022 00:00:30 - INFO - __main__ - Step 70 Global step 70 Train loss 12.438708 on epoch=34
03/09/2022 00:00:34 - INFO - __main__ - Step 80 Global step 80 Train loss 11.908820 on epoch=39
03/09/2022 00:00:39 - INFO - __main__ - Step 90 Global step 90 Train loss 11.686325 on epoch=44
03/09/2022 00:00:44 - INFO - __main__ - Step 100 Global step 100 Train loss 11.183533 on epoch=49
03/09/2022 00:00:44 - INFO - __main__ - Global step 100 Train loss 12.039032 ACC 0.0 on epoch=49
03/09/2022 00:00:49 - INFO - __main__ - Step 110 Global step 110 Train loss 10.401354 on epoch=54
03/09/2022 00:00:53 - INFO - __main__ - Step 120 Global step 120 Train loss 9.404596 on epoch=59
03/09/2022 00:00:58 - INFO - __main__ - Step 130 Global step 130 Train loss 8.834810 on epoch=64
03/09/2022 00:01:03 - INFO - __main__ - Step 140 Global step 140 Train loss 8.322350 on epoch=69
03/09/2022 00:01:07 - INFO - __main__ - Step 150 Global step 150 Train loss 7.473743 on epoch=74
03/09/2022 00:01:08 - INFO - __main__ - Global step 150 Train loss 8.887371 ACC 0.0 on epoch=74
03/09/2022 00:01:12 - INFO - __main__ - Step 160 Global step 160 Train loss 5.639969 on epoch=79
03/09/2022 00:01:17 - INFO - __main__ - Step 170 Global step 170 Train loss 4.157128 on epoch=84
03/09/2022 00:01:22 - INFO - __main__ - Step 180 Global step 180 Train loss 2.538310 on epoch=89
03/09/2022 00:01:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.163447 on epoch=94
03/09/2022 00:01:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.405157 on epoch=99
03/09/2022 00:01:31 - INFO - __main__ - Global step 200 Train loss 2.980802 ACC 0.5 on epoch=99
03/09/2022 00:02:04 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=99, global_step=200
03/09/2022 00:02:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.409462 on epoch=104
03/09/2022 00:02:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.350926 on epoch=109
03/09/2022 00:02:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.374432 on epoch=114
03/09/2022 00:02:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.256609 on epoch=119
03/09/2022 00:02:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.458515 on epoch=124
03/09/2022 00:02:27 - INFO - __main__ - Global step 250 Train loss 0.369989 ACC 0.375 on epoch=124
03/09/2022 00:02:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.236061 on epoch=129
03/09/2022 00:02:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.258259 on epoch=134
03/09/2022 00:02:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.226360 on epoch=139
03/09/2022 00:02:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.253736 on epoch=144
03/09/2022 00:02:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.258128 on epoch=149
03/09/2022 00:02:51 - INFO - __main__ - Global step 300 Train loss 0.246509 ACC 0.46875 on epoch=149
03/09/2022 00:02:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.267284 on epoch=154
03/09/2022 00:03:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.242398 on epoch=159
03/09/2022 00:03:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.239319 on epoch=164
03/09/2022 00:03:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.236680 on epoch=169
03/09/2022 00:03:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.263016 on epoch=174
03/09/2022 00:03:15 - INFO - __main__ - Global step 350 Train loss 0.249739 ACC 0.375 on epoch=174
03/09/2022 00:03:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.246713 on epoch=179
03/09/2022 00:03:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.241112 on epoch=184
03/09/2022 00:03:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.238522 on epoch=189
03/09/2022 00:03:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.229314 on epoch=194
03/09/2022 00:03:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.233101 on epoch=199
03/09/2022 00:03:38 - INFO - __main__ - Global step 400 Train loss 0.237753 ACC 0.28125 on epoch=199
03/09/2022 00:03:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.240292 on epoch=204
03/09/2022 00:03:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.256407 on epoch=209
03/09/2022 00:03:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.227364 on epoch=214
03/09/2022 00:03:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.238490 on epoch=219
03/09/2022 00:04:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.217716 on epoch=224
03/09/2022 00:04:02 - INFO - __main__ - Global step 450 Train loss 0.236054 ACC 0.4375 on epoch=224
03/09/2022 00:04:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.226340 on epoch=229
03/09/2022 00:04:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.216592 on epoch=234
03/09/2022 00:04:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.232986 on epoch=239
03/09/2022 00:04:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.224890 on epoch=244
03/09/2022 00:04:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.204217 on epoch=249
03/09/2022 00:04:26 - INFO - __main__ - Global step 500 Train loss 0.221005 ACC 0.5 on epoch=249
03/09/2022 00:04:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.225756 on epoch=254
03/09/2022 00:04:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.222463 on epoch=259
03/09/2022 00:04:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.227957 on epoch=264
03/09/2022 00:04:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.230997 on epoch=269
03/09/2022 00:04:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.212405 on epoch=274
03/09/2022 00:04:49 - INFO - __main__ - Global step 550 Train loss 0.223916 ACC 0.5625 on epoch=274
03/09/2022 00:05:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=274, global_step=550
03/09/2022 00:05:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.184341 on epoch=279
03/09/2022 00:05:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.196834 on epoch=284
03/09/2022 00:05:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.209669 on epoch=289
03/09/2022 00:05:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.164291 on epoch=294
03/09/2022 00:05:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.249955 on epoch=299
03/09/2022 00:05:47 - INFO - __main__ - Global step 600 Train loss 0.201018 ACC 0.46875 on epoch=299
03/09/2022 00:05:47 - INFO - __main__ - save last model!
03/09/2022 00:05:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:05:48 - INFO - __main__ - Printing 3 examples
03/09/2022 00:05:48 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/09/2022 00:05:48 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:48 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/09/2022 00:05:48 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:48 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:05:48 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:05:48 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:05:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:05:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:05:48 - INFO - __main__ - Printing 3 examples
03/09/2022 00:05:48 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/09/2022 00:05:48 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:49 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/09/2022 00:05:49 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:49 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/09/2022 00:05:49 - INFO - __main__ - ['sentence 1']
03/09/2022 00:05:49 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:05:49 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:05:49 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:05:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:05:58 - INFO - __main__ - Starting training!
03/09/2022 00:06:29 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:06:29 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:06:29 - INFO - __main__ - Printing 3 examples
03/09/2022 00:06:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:06:29 - INFO - __main__ - ['sentence 2']
03/09/2022 00:06:30 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:06:30 - INFO - __main__ - ['sentence 2']
03/09/2022 00:06:30 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:06:30 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:30 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:06:30 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:06:30 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:06:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_100_0.0002_8_predictions.txt
03/09/2022 00:06:32 - INFO - __main__ - ACC on test data: 0.5150
03/09/2022 00:06:32 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.515
03/09/2022 00:06:32 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0001, bsz=8 ...
03/09/2022 00:06:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:06:33 - INFO - __main__ - Printing 3 examples
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: An organization that had hated Florence had not ever fought. [SEP] sentence 2: An organization that had not hated Florence had ever fought.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that had sold these movie theaters can not ever force Scott to leave. [SEP] sentence 2: A museum that had not sold these movie theaters can ever force Scott to leave.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:06:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:06:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:06:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:06:33 - INFO - __main__ - Printing 3 examples
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Sheila's senators that have attacked Emily had not ever taken that high school. [SEP] sentence 2: Sheila's senators that have not attacked Emily had ever taken that high school.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some cucumber that is alarming Steve had not ever grown. [SEP] sentence 2: Some cucumber that is not alarming Steve had ever grown.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All customers who are escaping from Tamara might not ever marry. [SEP] sentence 2: All customers who are not escaping from Tamara might ever marry.
03/09/2022 00:06:33 - INFO - __main__ - ['sentence 1']
03/09/2022 00:06:33 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:06:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:06:33 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:06:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:06:44 - INFO - __main__ - Starting training!
03/09/2022 00:06:47 - INFO - __main__ - Step 10 Global step 10 Train loss 22.070293 on epoch=4
03/09/2022 00:06:52 - INFO - __main__ - Step 20 Global step 20 Train loss 18.710659 on epoch=9
03/09/2022 00:06:56 - INFO - __main__ - Step 30 Global step 30 Train loss 17.620148 on epoch=14
03/09/2022 00:07:01 - INFO - __main__ - Step 40 Global step 40 Train loss 15.551305 on epoch=19
03/09/2022 00:07:05 - INFO - __main__ - Step 50 Global step 50 Train loss 14.439311 on epoch=24
03/09/2022 00:07:07 - INFO - __main__ - Global step 50 Train loss 17.678343 ACC 0.0625 on epoch=24
03/09/2022 00:07:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=24, global_step=50
03/09/2022 00:07:45 - INFO - __main__ - Step 60 Global step 60 Train loss 13.980942 on epoch=29
03/09/2022 00:07:50 - INFO - __main__ - Step 70 Global step 70 Train loss 13.799736 on epoch=34
03/09/2022 00:07:55 - INFO - __main__ - Step 80 Global step 80 Train loss 13.271459 on epoch=39
03/09/2022 00:07:59 - INFO - __main__ - Step 90 Global step 90 Train loss 13.296875 on epoch=44
03/09/2022 00:08:04 - INFO - __main__ - Step 100 Global step 100 Train loss 12.518396 on epoch=49
03/09/2022 00:08:05 - INFO - __main__ - Global step 100 Train loss 13.373482 ACC 0.03125 on epoch=49
03/09/2022 00:08:09 - INFO - __main__ - Step 110 Global step 110 Train loss 12.165838 on epoch=54
03/09/2022 00:08:14 - INFO - __main__ - Step 120 Global step 120 Train loss 11.756097 on epoch=59
03/09/2022 00:08:19 - INFO - __main__ - Step 130 Global step 130 Train loss 11.980494 on epoch=64
03/09/2022 00:08:24 - INFO - __main__ - Step 140 Global step 140 Train loss 11.267795 on epoch=69
03/09/2022 00:08:28 - INFO - __main__ - Step 150 Global step 150 Train loss 11.338982 on epoch=74
03/09/2022 00:08:29 - INFO - __main__ - Global step 150 Train loss 11.701839 ACC 0.0625 on epoch=74
03/09/2022 00:08:34 - INFO - __main__ - Step 160 Global step 160 Train loss 11.035461 on epoch=79
03/09/2022 00:08:38 - INFO - __main__ - Step 170 Global step 170 Train loss 10.376984 on epoch=84
03/09/2022 00:08:43 - INFO - __main__ - Step 180 Global step 180 Train loss 10.792168 on epoch=89
03/09/2022 00:08:48 - INFO - __main__ - Step 190 Global step 190 Train loss 10.141086 on epoch=94
03/09/2022 00:08:52 - INFO - __main__ - Step 200 Global step 200 Train loss 10.046959 on epoch=99
03/09/2022 00:08:53 - INFO - __main__ - Global step 200 Train loss 10.478532 ACC 0.1875 on epoch=99
03/09/2022 00:09:26 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=99, global_step=200
03/09/2022 00:09:30 - INFO - __main__ - Step 210 Global step 210 Train loss 9.470918 on epoch=104
03/09/2022 00:09:35 - INFO - __main__ - Step 220 Global step 220 Train loss 9.519222 on epoch=109
03/09/2022 00:09:40 - INFO - __main__ - Step 230 Global step 230 Train loss 8.480177 on epoch=114
03/09/2022 00:09:44 - INFO - __main__ - Step 240 Global step 240 Train loss 8.345716 on epoch=119
03/09/2022 00:09:49 - INFO - __main__ - Step 250 Global step 250 Train loss 7.799258 on epoch=124
03/09/2022 00:09:50 - INFO - __main__ - Global step 250 Train loss 8.723059 ACC 0.1875 on epoch=124
03/09/2022 00:09:54 - INFO - __main__ - Step 260 Global step 260 Train loss 6.814283 on epoch=129
03/09/2022 00:09:59 - INFO - __main__ - Step 270 Global step 270 Train loss 5.020731 on epoch=134
03/09/2022 00:10:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.985847 on epoch=139
03/09/2022 00:10:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.475727 on epoch=144
03/09/2022 00:10:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.383577 on epoch=149
03/09/2022 00:10:13 - INFO - __main__ - Global step 300 Train loss 2.936033 ACC 0.46875 on epoch=149
03/09/2022 00:10:47 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.46875 on epoch=149, global_step=300
03/09/2022 00:10:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.173461 on epoch=154
03/09/2022 00:10:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.247713 on epoch=159
03/09/2022 00:11:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.971068 on epoch=164
03/09/2022 00:11:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.995844 on epoch=169
03/09/2022 00:11:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.309772 on epoch=174
03/09/2022 00:11:11 - INFO - __main__ - Global step 350 Train loss 0.539572 ACC 0.5625 on epoch=174
03/09/2022 00:11:45 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5625 on epoch=174, global_step=350
03/09/2022 00:11:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.509246 on epoch=179
03/09/2022 00:11:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.359377 on epoch=184
03/09/2022 00:11:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.433765 on epoch=189
03/09/2022 00:12:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.343061 on epoch=194
03/09/2022 00:12:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.212216 on epoch=199
03/09/2022 00:12:08 - INFO - __main__ - Global step 400 Train loss 0.371533 ACC 0.46875 on epoch=199
03/09/2022 00:12:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.229211 on epoch=204
03/09/2022 00:12:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.231897 on epoch=209
03/09/2022 00:12:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.170357 on epoch=214
03/09/2022 00:12:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.105790 on epoch=219
03/09/2022 00:12:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.204443 on epoch=224
03/09/2022 00:12:32 - INFO - __main__ - Global step 450 Train loss 0.188340 ACC 0.625 on epoch=224
03/09/2022 00:13:06 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=224, global_step=450
03/09/2022 00:13:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.072991 on epoch=229
03/09/2022 00:13:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.056115 on epoch=234
03/09/2022 00:13:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.079319 on epoch=239
03/09/2022 00:13:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.038038 on epoch=244
03/09/2022 00:13:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.062319 on epoch=249
03/09/2022 00:13:29 - INFO - __main__ - Global step 500 Train loss 0.061756 ACC 0.59375 on epoch=249
03/09/2022 00:13:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.040214 on epoch=254
03/09/2022 00:13:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.064316 on epoch=259
03/09/2022 00:13:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.029767 on epoch=264
03/09/2022 00:13:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.035454 on epoch=269
03/09/2022 00:13:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.027213 on epoch=274
03/09/2022 00:13:53 - INFO - __main__ - Global step 550 Train loss 0.039393 ACC 0.53125 on epoch=274
03/09/2022 00:13:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018978 on epoch=279
03/09/2022 00:14:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017641 on epoch=284
03/09/2022 00:14:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036753 on epoch=289
03/09/2022 00:14:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021712 on epoch=294
03/09/2022 00:14:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006829 on epoch=299
03/09/2022 00:14:17 - INFO - __main__ - Global step 600 Train loss 0.020383 ACC 0.53125 on epoch=299
03/09/2022 00:14:17 - INFO - __main__ - save last model!
03/09/2022 00:14:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:14:17 - INFO - __main__ - Printing 3 examples
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:14:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:14:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:14:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:14:17 - INFO - __main__ - Printing 3 examples
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:14:17 - INFO - __main__ - ['sentence 1']
03/09/2022 00:14:17 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:14:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:14:17 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:14:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:14:27 - INFO - __main__ - Starting training!
03/09/2022 00:15:00 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:15:01 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:15:01 - INFO - __main__ - Printing 3 examples
03/09/2022 00:15:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:15:01 - INFO - __main__ - ['sentence 2']
03/09/2022 00:15:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:15:01 - INFO - __main__ - ['sentence 2']
03/09/2022 00:15:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:15:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:01 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:15:01 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:15:01 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:15:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_100_0.0001_8_predictions.txt
03/09/2022 00:15:03 - INFO - __main__ - ACC on test data: 0.5500
03/09/2022 00:15:03 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_100, lr=0.0001, bsz=8, dev_performance=0.625, test_performance=0.55
03/09/2022 00:15:03 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0005, bsz=8 ...
03/09/2022 00:15:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:15:04 - INFO - __main__ - Printing 3 examples
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:15:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:15:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:15:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:15:04 - INFO - __main__ - Printing 3 examples
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:15:04 - INFO - __main__ - ['sentence 1']
03/09/2022 00:15:04 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:15:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:15:04 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:15:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:15:15 - INFO - __main__ - Starting training!
03/09/2022 00:15:18 - INFO - __main__ - Step 10 Global step 10 Train loss 21.646057 on epoch=4
03/09/2022 00:15:24 - INFO - __main__ - Step 20 Global step 20 Train loss 17.169115 on epoch=9
03/09/2022 00:15:28 - INFO - __main__ - Step 30 Global step 30 Train loss 13.499249 on epoch=14
03/09/2022 00:15:33 - INFO - __main__ - Step 40 Global step 40 Train loss 12.000551 on epoch=19
03/09/2022 00:15:38 - INFO - __main__ - Step 50 Global step 50 Train loss 10.807493 on epoch=24
03/09/2022 00:15:38 - INFO - __main__ - Global step 50 Train loss 15.024493 ACC 0.0 on epoch=24
03/09/2022 00:16:12 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 00:16:17 - INFO - __main__ - Step 60 Global step 60 Train loss 9.000087 on epoch=29
03/09/2022 00:16:22 - INFO - __main__ - Step 70 Global step 70 Train loss 6.748053 on epoch=34
03/09/2022 00:16:26 - INFO - __main__ - Step 80 Global step 80 Train loss 1.934547 on epoch=39
03/09/2022 00:16:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.262505 on epoch=44
03/09/2022 00:16:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.247346 on epoch=49
03/09/2022 00:16:36 - INFO - __main__ - Global step 100 Train loss 3.638508 ACC 0.5 on epoch=49
03/09/2022 00:17:09 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=49, global_step=100
03/09/2022 00:17:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.332358 on epoch=54
03/09/2022 00:17:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.279225 on epoch=59
03/09/2022 00:17:23 - INFO - __main__ - Step 130 Global step 130 Train loss 1.578745 on epoch=64
03/09/2022 00:17:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.430818 on epoch=69
03/09/2022 00:17:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.232056 on epoch=74
03/09/2022 00:17:33 - INFO - __main__ - Global step 150 Train loss 0.570640 ACC 0.5 on epoch=74
03/09/2022 00:17:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.242876 on epoch=79
03/09/2022 00:17:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.225198 on epoch=84
03/09/2022 00:17:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.219406 on epoch=89
03/09/2022 00:17:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.217576 on epoch=94
03/09/2022 00:17:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.235910 on epoch=99
03/09/2022 00:17:56 - INFO - __main__ - Global step 200 Train loss 0.228193 ACC 0.5 on epoch=99
03/09/2022 00:18:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.206641 on epoch=104
03/09/2022 00:18:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.200023 on epoch=109
03/09/2022 00:18:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.184643 on epoch=114
03/09/2022 00:18:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.176136 on epoch=119
03/09/2022 00:18:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.191120 on epoch=124
03/09/2022 00:18:20 - INFO - __main__ - Global step 250 Train loss 0.191713 ACC 0.5 on epoch=124
03/09/2022 00:18:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.157186 on epoch=129
03/09/2022 00:18:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.172794 on epoch=134
03/09/2022 00:18:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.108245 on epoch=139
03/09/2022 00:18:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.081084 on epoch=144
03/09/2022 00:18:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.077868 on epoch=149
03/09/2022 00:18:44 - INFO - __main__ - Global step 300 Train loss 0.119436 ACC 0.5 on epoch=149
03/09/2022 00:18:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.229847 on epoch=154
03/09/2022 00:18:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.017080 on epoch=159
03/09/2022 00:18:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.006251 on epoch=164
03/09/2022 00:19:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002590 on epoch=169
03/09/2022 00:19:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002792 on epoch=174
03/09/2022 00:19:08 - INFO - __main__ - Global step 350 Train loss 0.051712 ACC 0.5625 on epoch=174
03/09/2022 00:19:41 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=174, global_step=350
03/09/2022 00:19:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.113825 on epoch=179
03/09/2022 00:19:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.003731 on epoch=184
03/09/2022 00:19:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.010799 on epoch=189
03/09/2022 00:20:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001214 on epoch=194
03/09/2022 00:20:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000903 on epoch=199
03/09/2022 00:20:05 - INFO - __main__ - Global step 400 Train loss 0.026094 ACC 0.5625 on epoch=199
03/09/2022 00:20:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000970 on epoch=204
03/09/2022 00:20:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000475 on epoch=209
03/09/2022 00:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000277 on epoch=214
03/09/2022 00:20:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000197 on epoch=219
03/09/2022 00:20:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000227 on epoch=224
03/09/2022 00:20:29 - INFO - __main__ - Global step 450 Train loss 0.000429 ACC 0.53125 on epoch=224
03/09/2022 00:20:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000480 on epoch=229
03/09/2022 00:20:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000160 on epoch=234
03/09/2022 00:20:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000462 on epoch=239
03/09/2022 00:20:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000242 on epoch=244
03/09/2022 00:20:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000042 on epoch=249
03/09/2022 00:20:53 - INFO - __main__ - Global step 500 Train loss 0.000277 ACC 0.5625 on epoch=249
03/09/2022 00:20:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000104 on epoch=254
03/09/2022 00:21:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000049 on epoch=259
03/09/2022 00:21:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.107975 on epoch=264
03/09/2022 00:21:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.476424 on epoch=269
03/09/2022 00:21:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.105441 on epoch=274
03/09/2022 00:21:18 - INFO - __main__ - Global step 550 Train loss 0.137999 ACC 0.34375 on epoch=274
03/09/2022 00:21:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.047616 on epoch=279
03/09/2022 00:21:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.013253 on epoch=284
03/09/2022 00:21:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006564 on epoch=289
03/09/2022 00:21:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001581 on epoch=294
03/09/2022 00:21:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001422 on epoch=299
03/09/2022 00:21:42 - INFO - __main__ - Global step 600 Train loss 0.014087 ACC 0.46875 on epoch=299
03/09/2022 00:21:42 - INFO - __main__ - save last model!
03/09/2022 00:21:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:21:42 - INFO - __main__ - Printing 3 examples
03/09/2022 00:21:42 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:21:42 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:42 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:21:42 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:42 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:21:43 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:21:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:21:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:21:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:21:43 - INFO - __main__ - Printing 3 examples
03/09/2022 00:21:43 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:21:43 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:43 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:21:43 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:43 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:21:43 - INFO - __main__ - ['sentence 1']
03/09/2022 00:21:43 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:21:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:21:43 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:21:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:21:52 - INFO - __main__ - Starting training!
03/09/2022 00:22:26 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:22:26 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:22:26 - INFO - __main__ - Printing 3 examples
03/09/2022 00:22:26 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:22:26 - INFO - __main__ - ['sentence 2']
03/09/2022 00:22:26 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:22:26 - INFO - __main__ - ['sentence 2']
03/09/2022 00:22:26 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:22:26 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:26 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:22:26 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:22:27 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:22:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_13_0.0005_8_predictions.txt
03/09/2022 00:22:30 - INFO - __main__ - ACC on test data: 0.5600
03/09/2022 00:22:30 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.56
03/09/2022 00:22:30 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0003, bsz=8 ...
03/09/2022 00:22:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:22:31 - INFO - __main__ - Printing 3 examples
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:22:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:22:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:22:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:22:31 - INFO - __main__ - Printing 3 examples
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:22:31 - INFO - __main__ - ['sentence 1']
03/09/2022 00:22:31 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:22:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:22:31 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:22:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:22:41 - INFO - __main__ - Starting training!
03/09/2022 00:22:45 - INFO - __main__ - Step 10 Global step 10 Train loss 22.137968 on epoch=4
03/09/2022 00:22:49 - INFO - __main__ - Step 20 Global step 20 Train loss 18.764530 on epoch=9
03/09/2022 00:22:54 - INFO - __main__ - Step 30 Global step 30 Train loss 16.026890 on epoch=14
03/09/2022 00:22:58 - INFO - __main__ - Step 40 Global step 40 Train loss 13.244388 on epoch=19
03/09/2022 00:23:03 - INFO - __main__ - Step 50 Global step 50 Train loss 12.785939 on epoch=24
03/09/2022 00:23:07 - INFO - __main__ - Global step 50 Train loss 16.591942 ACC 0.0 on epoch=24
03/09/2022 00:23:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 00:23:47 - INFO - __main__ - Step 60 Global step 60 Train loss 11.783890 on epoch=29
03/09/2022 00:23:52 - INFO - __main__ - Step 70 Global step 70 Train loss 11.327100 on epoch=34
03/09/2022 00:23:57 - INFO - __main__ - Step 80 Global step 80 Train loss 10.373871 on epoch=39
03/09/2022 00:24:01 - INFO - __main__ - Step 90 Global step 90 Train loss 9.453520 on epoch=44
03/09/2022 00:24:06 - INFO - __main__ - Step 100 Global step 100 Train loss 8.061377 on epoch=49
03/09/2022 00:24:08 - INFO - __main__ - Global step 100 Train loss 10.199951 ACC 0.0 on epoch=49
03/09/2022 00:24:12 - INFO - __main__ - Step 110 Global step 110 Train loss 6.255273 on epoch=54
03/09/2022 00:24:17 - INFO - __main__ - Step 120 Global step 120 Train loss 3.114497 on epoch=59
03/09/2022 00:24:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.544240 on epoch=64
03/09/2022 00:24:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.446685 on epoch=69
03/09/2022 00:24:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.808645 on epoch=74
03/09/2022 00:24:31 - INFO - __main__ - Global step 150 Train loss 2.433868 ACC 0.46875 on epoch=74
03/09/2022 00:25:07 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.46875 on epoch=74, global_step=150
03/09/2022 00:25:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.482043 on epoch=79
03/09/2022 00:25:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.477049 on epoch=84
03/09/2022 00:25:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.251812 on epoch=89
03/09/2022 00:25:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.300787 on epoch=94
03/09/2022 00:25:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.423516 on epoch=99
03/09/2022 00:25:31 - INFO - __main__ - Global step 200 Train loss 0.387041 ACC 0.5 on epoch=99
03/09/2022 00:26:09 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=99, global_step=200
03/09/2022 00:26:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.230944 on epoch=104
03/09/2022 00:26:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.242193 on epoch=109
03/09/2022 00:26:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.221310 on epoch=114
03/09/2022 00:26:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.233775 on epoch=119
03/09/2022 00:26:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.226905 on epoch=124
03/09/2022 00:26:32 - INFO - __main__ - Global step 250 Train loss 0.231025 ACC 0.5625 on epoch=124
03/09/2022 00:27:09 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=124, global_step=250
03/09/2022 00:27:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.231447 on epoch=129
03/09/2022 00:27:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.247597 on epoch=134
03/09/2022 00:27:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.230851 on epoch=139
03/09/2022 00:27:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.219192 on epoch=144
03/09/2022 00:27:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.222723 on epoch=149
03/09/2022 00:27:32 - INFO - __main__ - Global step 300 Train loss 0.230362 ACC 0.59375 on epoch=149
03/09/2022 00:28:11 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=149, global_step=300
03/09/2022 00:28:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.220109 on epoch=154
03/09/2022 00:28:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.229891 on epoch=159
03/09/2022 00:28:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.216515 on epoch=164
03/09/2022 00:28:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.220548 on epoch=169
03/09/2022 00:28:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.218100 on epoch=174
03/09/2022 00:28:34 - INFO - __main__ - Global step 350 Train loss 0.221033 ACC 0.5 on epoch=174
03/09/2022 00:28:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.357910 on epoch=179
03/09/2022 00:28:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.403573 on epoch=184
03/09/2022 00:28:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.216472 on epoch=189
03/09/2022 00:28:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.229170 on epoch=194
03/09/2022 00:28:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.227121 on epoch=199
03/09/2022 00:28:58 - INFO - __main__ - Global step 400 Train loss 0.286849 ACC 0.625 on epoch=199
03/09/2022 00:29:35 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=199, global_step=400
03/09/2022 00:29:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.228994 on epoch=204
03/09/2022 00:29:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.229937 on epoch=209
03/09/2022 00:29:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.211709 on epoch=214
03/09/2022 00:29:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.221421 on epoch=219
03/09/2022 00:29:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.230498 on epoch=224
03/09/2022 00:29:59 - INFO - __main__ - Global step 450 Train loss 0.224512 ACC 0.53125 on epoch=224
03/09/2022 00:30:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.213034 on epoch=229
03/09/2022 00:30:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.230982 on epoch=234
03/09/2022 00:30:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.219804 on epoch=239
03/09/2022 00:30:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.220803 on epoch=244
03/09/2022 00:30:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.238950 on epoch=249
03/09/2022 00:30:23 - INFO - __main__ - Global step 500 Train loss 0.224715 ACC 0.5 on epoch=249
03/09/2022 00:30:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.217848 on epoch=254
03/09/2022 00:30:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.545461 on epoch=259
03/09/2022 00:30:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.507566 on epoch=264
03/09/2022 00:30:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.329954 on epoch=269
03/09/2022 00:30:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.237588 on epoch=274
03/09/2022 00:30:46 - INFO - __main__ - Global step 550 Train loss 0.367683 ACC 0.53125 on epoch=274
03/09/2022 00:30:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.250442 on epoch=279
03/09/2022 00:30:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.237238 on epoch=284
03/09/2022 00:31:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.237783 on epoch=289
03/09/2022 00:31:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.222546 on epoch=294
03/09/2022 00:31:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.245810 on epoch=299
03/09/2022 00:31:10 - INFO - __main__ - Global step 600 Train loss 0.238764 ACC 0.46875 on epoch=299
03/09/2022 00:31:10 - INFO - __main__ - save last model!
03/09/2022 00:31:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:31:11 - INFO - __main__ - Printing 3 examples
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:31:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:31:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:31:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:31:11 - INFO - __main__ - Printing 3 examples
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:31:11 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:31:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:31:11 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:31:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:31:20 - INFO - __main__ - Starting training!
03/09/2022 00:31:52 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:31:53 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:31:53 - INFO - __main__ - Printing 3 examples
03/09/2022 00:31:53 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:31:53 - INFO - __main__ - ['sentence 2']
03/09/2022 00:31:53 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:31:53 - INFO - __main__ - ['sentence 2']
03/09/2022 00:31:53 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:31:53 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:53 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:31:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:31:53 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:31:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_13_0.0003_8_predictions.txt
03/09/2022 00:31:55 - INFO - __main__ - ACC on test data: 0.5400
03/09/2022 00:31:55 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0003, bsz=8, dev_performance=0.625, test_performance=0.54
03/09/2022 00:31:55 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0002, bsz=8 ...
03/09/2022 00:31:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:31:56 - INFO - __main__ - Printing 3 examples
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:31:56 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:31:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:31:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:31:56 - INFO - __main__ - Printing 3 examples
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:31:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:31:56 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:31:56 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:31:56 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:32:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:32:07 - INFO - __main__ - Starting training!
03/09/2022 00:32:11 - INFO - __main__ - Step 10 Global step 10 Train loss 22.188234 on epoch=4
03/09/2022 00:32:15 - INFO - __main__ - Step 20 Global step 20 Train loss 20.314121 on epoch=9
03/09/2022 00:32:20 - INFO - __main__ - Step 30 Global step 30 Train loss 17.502575 on epoch=14
03/09/2022 00:32:25 - INFO - __main__ - Step 40 Global step 40 Train loss 14.523799 on epoch=19
03/09/2022 00:32:29 - INFO - __main__ - Step 50 Global step 50 Train loss 13.468773 on epoch=24
03/09/2022 00:32:31 - INFO - __main__ - Global step 50 Train loss 17.599501 ACC 0.125 on epoch=24
03/09/2022 00:33:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
03/09/2022 00:33:13 - INFO - __main__ - Step 60 Global step 60 Train loss 13.103330 on epoch=29
03/09/2022 00:33:18 - INFO - __main__ - Step 70 Global step 70 Train loss 12.736879 on epoch=34
03/09/2022 00:33:22 - INFO - __main__ - Step 80 Global step 80 Train loss 11.684468 on epoch=39
03/09/2022 00:33:27 - INFO - __main__ - Step 90 Global step 90 Train loss 11.811262 on epoch=44
03/09/2022 00:33:32 - INFO - __main__ - Step 100 Global step 100 Train loss 10.485367 on epoch=49
03/09/2022 00:33:32 - INFO - __main__ - Global step 100 Train loss 11.964261 ACC 0.0 on epoch=49
03/09/2022 00:33:37 - INFO - __main__ - Step 110 Global step 110 Train loss 9.932990 on epoch=54
03/09/2022 00:33:42 - INFO - __main__ - Step 120 Global step 120 Train loss 9.572017 on epoch=59
03/09/2022 00:33:47 - INFO - __main__ - Step 130 Global step 130 Train loss 8.448877 on epoch=64
03/09/2022 00:33:52 - INFO - __main__ - Step 140 Global step 140 Train loss 7.264760 on epoch=69
03/09/2022 00:33:56 - INFO - __main__ - Step 150 Global step 150 Train loss 5.857911 on epoch=74
03/09/2022 00:34:06 - INFO - __main__ - Global step 150 Train loss 8.215312 ACC 0.03125 on epoch=74
03/09/2022 00:34:11 - INFO - __main__ - Step 160 Global step 160 Train loss 3.672450 on epoch=79
03/09/2022 00:34:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.025606 on epoch=84
03/09/2022 00:34:21 - INFO - __main__ - Step 180 Global step 180 Train loss 1.200827 on epoch=89
03/09/2022 00:34:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.041499 on epoch=94
03/09/2022 00:34:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.258265 on epoch=99
03/09/2022 00:34:31 - INFO - __main__ - Global step 200 Train loss 1.639729 ACC 0.46875 on epoch=99
03/09/2022 00:35:09 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.46875 on epoch=99, global_step=200
03/09/2022 00:35:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.187553 on epoch=104
03/09/2022 00:35:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.179969 on epoch=109
03/09/2022 00:35:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.158425 on epoch=114
03/09/2022 00:35:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.163342 on epoch=119
03/09/2022 00:35:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.119160 on epoch=124
03/09/2022 00:35:34 - INFO - __main__ - Global step 250 Train loss 0.161690 ACC 0.40625 on epoch=124
03/09/2022 00:35:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.105910 on epoch=129
03/09/2022 00:35:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.102926 on epoch=134
03/09/2022 00:35:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.106977 on epoch=139
03/09/2022 00:35:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.081241 on epoch=144
03/09/2022 00:35:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.091103 on epoch=149
03/09/2022 00:35:58 - INFO - __main__ - Global step 300 Train loss 0.097631 ACC 0.5625 on epoch=149
03/09/2022 00:36:35 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5625 on epoch=149, global_step=300
03/09/2022 00:36:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.053403 on epoch=154
03/09/2022 00:36:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.044000 on epoch=159
03/09/2022 00:36:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.033798 on epoch=164
03/09/2022 00:36:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.031213 on epoch=169
03/09/2022 00:36:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021412 on epoch=174
03/09/2022 00:37:00 - INFO - __main__ - Global step 350 Train loss 0.036765 ACC 0.375 on epoch=174
03/09/2022 00:37:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.025793 on epoch=179
03/09/2022 00:37:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.023118 on epoch=184
03/09/2022 00:37:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008150 on epoch=189
03/09/2022 00:37:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004633 on epoch=194
03/09/2022 00:37:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015394 on epoch=199
03/09/2022 00:37:24 - INFO - __main__ - Global step 400 Train loss 0.015418 ACC 0.40625 on epoch=199
03/09/2022 00:37:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010570 on epoch=204
03/09/2022 00:37:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.020700 on epoch=209
03/09/2022 00:37:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.041431 on epoch=214
03/09/2022 00:37:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008262 on epoch=219
03/09/2022 00:37:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.016225 on epoch=224
03/09/2022 00:37:48 - INFO - __main__ - Global step 450 Train loss 0.019437 ACC 0.5 on epoch=224
03/09/2022 00:37:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002006 on epoch=229
03/09/2022 00:37:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.008559 on epoch=234
03/09/2022 00:38:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001529 on epoch=239
03/09/2022 00:38:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001175 on epoch=244
03/09/2022 00:38:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001286 on epoch=249
03/09/2022 00:38:12 - INFO - __main__ - Global step 500 Train loss 0.002911 ACC 0.4375 on epoch=249
03/09/2022 00:38:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000676 on epoch=254
03/09/2022 00:38:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.015023 on epoch=259
03/09/2022 00:38:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001598 on epoch=264
03/09/2022 00:38:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004353 on epoch=269
03/09/2022 00:38:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.010441 on epoch=274
03/09/2022 00:38:37 - INFO - __main__ - Global step 550 Train loss 0.006418 ACC 0.4375 on epoch=274
03/09/2022 00:38:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003001 on epoch=279
03/09/2022 00:38:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.025676 on epoch=284
03/09/2022 00:38:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011994 on epoch=289
03/09/2022 00:38:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000715 on epoch=294
03/09/2022 00:39:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000370 on epoch=299
03/09/2022 00:39:01 - INFO - __main__ - Global step 600 Train loss 0.008351 ACC 0.40625 on epoch=299
03/09/2022 00:39:01 - INFO - __main__ - save last model!
03/09/2022 00:39:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:39:01 - INFO - __main__ - Printing 3 examples
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:39:01 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:39:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:39:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:39:01 - INFO - __main__ - Printing 3 examples
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:39:01 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:01 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:39:01 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:39:01 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:39:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:39:12 - INFO - __main__ - Starting training!
03/09/2022 00:39:44 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:39:45 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:39:45 - INFO - __main__ - Printing 3 examples
03/09/2022 00:39:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:39:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:39:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:39:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:39:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:39:45 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:39:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:39:45 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:39:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_13_0.0002_8_predictions.txt
03/09/2022 00:39:54 - INFO - __main__ - ACC on test data: 0.5450
03/09/2022 00:39:55 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.545
03/09/2022 00:39:55 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0001, bsz=8 ...
03/09/2022 00:39:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:39:56 - INFO - __main__ - Printing 3 examples
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Chad's colleagues that would examine stories can not ever need Vanessa to come here. [SEP] sentence 2: Chad's colleagues that would not examine stories can ever need Vanessa to come here.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That organization that had loved Naomi does not ever worry Benjamin. [SEP] sentence 2: That organization that had not loved Naomi does ever worry Benjamin.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many candles that might astound Naomi did not ever fade. [SEP] sentence 2: Many candles that might not astound Naomi did ever fade.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:39:56 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:39:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:39:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:39:56 - INFO - __main__ - Printing 3 examples
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: These cars that had alarmed Tina will not ever irritate Steve. [SEP] sentence 2: These cars that had not alarmed Tina will ever irritate Steve.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Derek's best friends who are admiring pedestrians might not ever advise Rebecca to paint. [SEP] sentence 2: Derek's best friends who are not admiring pedestrians might ever advise Rebecca to paint.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: All sisters of Jane who are annoying the dancer can not ever surrender. [SEP] sentence 2: All sisters of Jane who are not annoying the dancer can ever surrender.
03/09/2022 00:39:56 - INFO - __main__ - ['sentence 1']
03/09/2022 00:39:56 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:39:56 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:39:56 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:40:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:40:05 - INFO - __main__ - Starting training!
03/09/2022 00:40:09 - INFO - __main__ - Step 10 Global step 10 Train loss 22.604315 on epoch=4
03/09/2022 00:40:13 - INFO - __main__ - Step 20 Global step 20 Train loss 21.175951 on epoch=9
03/09/2022 00:40:18 - INFO - __main__ - Step 30 Global step 30 Train loss 18.597767 on epoch=14
03/09/2022 00:40:23 - INFO - __main__ - Step 40 Global step 40 Train loss 16.887226 on epoch=19
03/09/2022 00:40:27 - INFO - __main__ - Step 50 Global step 50 Train loss 15.774549 on epoch=24
03/09/2022 00:40:29 - INFO - __main__ - Global step 50 Train loss 19.007963 ACC 0.0625 on epoch=24
03/09/2022 00:41:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=24, global_step=50
03/09/2022 00:41:10 - INFO - __main__ - Step 60 Global step 60 Train loss 14.199656 on epoch=29
03/09/2022 00:41:14 - INFO - __main__ - Step 70 Global step 70 Train loss 14.079112 on epoch=34
03/09/2022 00:41:19 - INFO - __main__ - Step 80 Global step 80 Train loss 14.225469 on epoch=39
03/09/2022 00:41:24 - INFO - __main__ - Step 90 Global step 90 Train loss 13.573085 on epoch=44
03/09/2022 00:41:29 - INFO - __main__ - Step 100 Global step 100 Train loss 13.277484 on epoch=49
03/09/2022 00:41:29 - INFO - __main__ - Global step 100 Train loss 13.870962 ACC 0.0 on epoch=49
03/09/2022 00:41:34 - INFO - __main__ - Step 110 Global step 110 Train loss 12.775123 on epoch=54
03/09/2022 00:41:39 - INFO - __main__ - Step 120 Global step 120 Train loss 12.860445 on epoch=59
03/09/2022 00:41:43 - INFO - __main__ - Step 130 Global step 130 Train loss 11.902771 on epoch=64
03/09/2022 00:41:48 - INFO - __main__ - Step 140 Global step 140 Train loss 11.706377 on epoch=69
03/09/2022 00:41:53 - INFO - __main__ - Step 150 Global step 150 Train loss 11.737673 on epoch=74
03/09/2022 00:41:53 - INFO - __main__ - Global step 150 Train loss 12.196478 ACC 0.03125 on epoch=74
03/09/2022 00:41:58 - INFO - __main__ - Step 160 Global step 160 Train loss 12.017856 on epoch=79
03/09/2022 00:42:03 - INFO - __main__ - Step 170 Global step 170 Train loss 11.540324 on epoch=84
03/09/2022 00:42:07 - INFO - __main__ - Step 180 Global step 180 Train loss 10.966375 on epoch=89
03/09/2022 00:42:12 - INFO - __main__ - Step 190 Global step 190 Train loss 10.836672 on epoch=94
03/09/2022 00:42:17 - INFO - __main__ - Step 200 Global step 200 Train loss 10.226765 on epoch=99
03/09/2022 00:42:17 - INFO - __main__ - Global step 200 Train loss 11.117599 ACC 0.0 on epoch=99
03/09/2022 00:42:22 - INFO - __main__ - Step 210 Global step 210 Train loss 10.406594 on epoch=104
03/09/2022 00:42:27 - INFO - __main__ - Step 220 Global step 220 Train loss 9.647196 on epoch=109
03/09/2022 00:42:31 - INFO - __main__ - Step 230 Global step 230 Train loss 9.686319 on epoch=114
03/09/2022 00:42:36 - INFO - __main__ - Step 240 Global step 240 Train loss 9.056349 on epoch=119
03/09/2022 00:42:40 - INFO - __main__ - Step 250 Global step 250 Train loss 8.709715 on epoch=124
03/09/2022 00:42:41 - INFO - __main__ - Global step 250 Train loss 9.501234 ACC 0.09375 on epoch=124
03/09/2022 00:43:18 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.09375 on epoch=124, global_step=250
03/09/2022 00:43:23 - INFO - __main__ - Step 260 Global step 260 Train loss 8.561047 on epoch=129
03/09/2022 00:43:28 - INFO - __main__ - Step 270 Global step 270 Train loss 8.039997 on epoch=134
03/09/2022 00:43:32 - INFO - __main__ - Step 280 Global step 280 Train loss 7.089187 on epoch=139
03/09/2022 00:43:37 - INFO - __main__ - Step 290 Global step 290 Train loss 6.334805 on epoch=144
03/09/2022 00:43:42 - INFO - __main__ - Step 300 Global step 300 Train loss 5.678748 on epoch=149
03/09/2022 00:43:43 - INFO - __main__ - Global step 300 Train loss 7.140758 ACC 0.0 on epoch=149
03/09/2022 00:43:47 - INFO - __main__ - Step 310 Global step 310 Train loss 4.690518 on epoch=154
03/09/2022 00:43:52 - INFO - __main__ - Step 320 Global step 320 Train loss 3.734402 on epoch=159
03/09/2022 00:43:57 - INFO - __main__ - Step 330 Global step 330 Train loss 2.270551 on epoch=164
03/09/2022 00:44:01 - INFO - __main__ - Step 340 Global step 340 Train loss 2.103838 on epoch=169
03/09/2022 00:44:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.939864 on epoch=174
03/09/2022 00:44:07 - INFO - __main__ - Global step 350 Train loss 2.747835 ACC 0.53125 on epoch=174
03/09/2022 00:44:45 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.53125 on epoch=174, global_step=350
03/09/2022 00:44:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.328585 on epoch=179
03/09/2022 00:44:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.364939 on epoch=184
03/09/2022 00:44:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.219229 on epoch=189
03/09/2022 00:45:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.192759 on epoch=194
03/09/2022 00:45:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.213595 on epoch=199
03/09/2022 00:45:09 - INFO - __main__ - Global step 400 Train loss 0.263821 ACC 0.46875 on epoch=199
03/09/2022 00:45:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.184169 on epoch=204
03/09/2022 00:45:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.203837 on epoch=209
03/09/2022 00:45:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.179038 on epoch=214
03/09/2022 00:45:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.157769 on epoch=219
03/09/2022 00:45:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.139760 on epoch=224
03/09/2022 00:45:33 - INFO - __main__ - Global step 450 Train loss 0.172915 ACC 0.53125 on epoch=224
03/09/2022 00:45:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.156848 on epoch=229
03/09/2022 00:45:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.166518 on epoch=234
03/09/2022 00:45:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.143983 on epoch=239
03/09/2022 00:45:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.114569 on epoch=244
03/09/2022 00:45:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.119818 on epoch=249
03/09/2022 00:45:57 - INFO - __main__ - Global step 500 Train loss 0.140347 ACC 0.53125 on epoch=249
03/09/2022 00:46:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.110867 on epoch=254
03/09/2022 00:46:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.096863 on epoch=259
03/09/2022 00:46:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.119691 on epoch=264
03/09/2022 00:46:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.091192 on epoch=269
03/09/2022 00:46:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.086059 on epoch=274
03/09/2022 00:46:21 - INFO - __main__ - Global step 550 Train loss 0.100934 ACC 0.46875 on epoch=274
03/09/2022 00:46:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.078067 on epoch=279
03/09/2022 00:46:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.056039 on epoch=284
03/09/2022 00:46:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.084202 on epoch=289
03/09/2022 00:46:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.060229 on epoch=294
03/09/2022 00:46:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.054977 on epoch=299
03/09/2022 00:46:46 - INFO - __main__ - Global step 600 Train loss 0.066703 ACC 0.46875 on epoch=299
03/09/2022 00:46:46 - INFO - __main__ - save last model!
03/09/2022 00:46:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:46:46 - INFO - __main__ - Printing 3 examples
03/09/2022 00:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 00:46:46 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 00:46:46 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:46 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 00:46:46 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:46:46 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:46:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:46:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:46:47 - INFO - __main__ - Printing 3 examples
03/09/2022 00:46:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 00:46:47 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 00:46:47 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 00:46:47 - INFO - __main__ - ['sentence 2']
03/09/2022 00:46:47 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:46:47 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:46:47 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:46:57 - INFO - __main__ - Starting training!
03/09/2022 00:47:29 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:47:29 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:47:29 - INFO - __main__ - Printing 3 examples
03/09/2022 00:47:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:47:29 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:47:29 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:29 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:47:29 - INFO - __main__ - ['sentence 1']
03/09/2022 00:47:29 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:47:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:47:30 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:47:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_13_0.0001_8_predictions.txt
03/09/2022 00:47:32 - INFO - __main__ - ACC on test data: 0.5050
03/09/2022 00:47:32 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_13, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.505
03/09/2022 00:47:32 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0005, bsz=8 ...
03/09/2022 00:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:47:33 - INFO - __main__ - Printing 3 examples
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:47:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:47:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:47:33 - INFO - __main__ - Printing 3 examples
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 00:47:33 - INFO - __main__ - ['sentence 2']
03/09/2022 00:47:33 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:47:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:47:33 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:47:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:47:43 - INFO - __main__ - Starting training!
03/09/2022 00:47:47 - INFO - __main__ - Step 10 Global step 10 Train loss 21.558315 on epoch=4
03/09/2022 00:47:51 - INFO - __main__ - Step 20 Global step 20 Train loss 20.630133 on epoch=9
03/09/2022 00:47:56 - INFO - __main__ - Step 30 Global step 30 Train loss 16.343067 on epoch=14
03/09/2022 00:48:01 - INFO - __main__ - Step 40 Global step 40 Train loss 13.860944 on epoch=19
03/09/2022 00:48:05 - INFO - __main__ - Step 50 Global step 50 Train loss 11.761559 on epoch=24
03/09/2022 00:48:06 - INFO - __main__ - Global step 50 Train loss 16.830803 ACC 0.0 on epoch=24
03/09/2022 00:48:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 00:48:47 - INFO - __main__ - Step 60 Global step 60 Train loss 10.438032 on epoch=29
03/09/2022 00:48:52 - INFO - __main__ - Step 70 Global step 70 Train loss 8.779661 on epoch=34
03/09/2022 00:48:56 - INFO - __main__ - Step 80 Global step 80 Train loss 7.021853 on epoch=39
03/09/2022 00:49:01 - INFO - __main__ - Step 90 Global step 90 Train loss 3.637281 on epoch=44
03/09/2022 00:49:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.588841 on epoch=49
03/09/2022 00:49:06 - INFO - __main__ - Global step 100 Train loss 6.293134 ACC 0.5 on epoch=49
03/09/2022 00:49:43 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=49, global_step=100
03/09/2022 00:49:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.323432 on epoch=54
03/09/2022 00:49:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.870902 on epoch=59
03/09/2022 00:49:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.258997 on epoch=64
03/09/2022 00:50:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.330587 on epoch=69
03/09/2022 00:50:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.264539 on epoch=74
03/09/2022 00:50:07 - INFO - __main__ - Global step 150 Train loss 0.809692 ACC 0.5 on epoch=74
03/09/2022 00:50:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.257296 on epoch=79
03/09/2022 00:50:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.310675 on epoch=84
03/09/2022 00:50:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.245462 on epoch=89
03/09/2022 00:50:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.437092 on epoch=94
03/09/2022 00:50:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.238421 on epoch=99
03/09/2022 00:50:31 - INFO - __main__ - Global step 200 Train loss 0.297789 ACC 0.5 on epoch=99
03/09/2022 00:50:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.231116 on epoch=104
03/09/2022 00:50:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.346147 on epoch=109
03/09/2022 00:50:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.231106 on epoch=114
03/09/2022 00:50:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.226784 on epoch=119
03/09/2022 00:50:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.230072 on epoch=124
03/09/2022 00:50:55 - INFO - __main__ - Global step 250 Train loss 0.253045 ACC 0.5 on epoch=124
03/09/2022 00:51:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.220055 on epoch=129
03/09/2022 00:51:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.228989 on epoch=134
03/09/2022 00:51:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.222851 on epoch=139
03/09/2022 00:51:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.201487 on epoch=144
03/09/2022 00:51:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.236450 on epoch=149
03/09/2022 00:51:19 - INFO - __main__ - Global step 300 Train loss 0.221966 ACC 0.53125 on epoch=149
03/09/2022 00:51:57 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
03/09/2022 00:52:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.222657 on epoch=154
03/09/2022 00:52:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.222428 on epoch=159
03/09/2022 00:52:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.216034 on epoch=164
03/09/2022 00:52:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.232379 on epoch=169
03/09/2022 00:52:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.225447 on epoch=174
03/09/2022 00:52:20 - INFO - __main__ - Global step 350 Train loss 0.223789 ACC 0.5 on epoch=174
03/09/2022 00:52:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.233505 on epoch=179
03/09/2022 00:52:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.220543 on epoch=184
03/09/2022 00:52:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.218136 on epoch=189
03/09/2022 00:52:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.220039 on epoch=194
03/09/2022 00:52:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.214357 on epoch=199
03/09/2022 00:52:44 - INFO - __main__ - Global step 400 Train loss 0.221316 ACC 0.5625 on epoch=199
03/09/2022 00:53:22 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
03/09/2022 00:53:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.216912 on epoch=204
03/09/2022 00:53:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.217307 on epoch=209
03/09/2022 00:53:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.212234 on epoch=214
03/09/2022 00:53:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.217304 on epoch=219
03/09/2022 00:53:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.218683 on epoch=224
03/09/2022 00:53:46 - INFO - __main__ - Global step 450 Train loss 0.216488 ACC 0.5625 on epoch=224
03/09/2022 00:53:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.213622 on epoch=229
03/09/2022 00:53:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.196937 on epoch=234
03/09/2022 00:54:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.201204 on epoch=239
03/09/2022 00:54:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.190525 on epoch=244
03/09/2022 00:54:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.197742 on epoch=249
03/09/2022 00:54:09 - INFO - __main__ - Global step 500 Train loss 0.200006 ACC 0.53125 on epoch=249
03/09/2022 00:54:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.197939 on epoch=254
03/09/2022 00:54:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.191695 on epoch=259
03/09/2022 00:54:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.203819 on epoch=264
03/09/2022 00:54:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.154686 on epoch=269
03/09/2022 00:54:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.166059 on epoch=274
03/09/2022 00:54:33 - INFO - __main__ - Global step 550 Train loss 0.182840 ACC 0.5 on epoch=274
03/09/2022 00:54:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.155737 on epoch=279
03/09/2022 00:54:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.167521 on epoch=284
03/09/2022 00:54:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.158044 on epoch=289
03/09/2022 00:54:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.733081 on epoch=294
03/09/2022 00:54:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.186836 on epoch=299
03/09/2022 00:54:57 - INFO - __main__ - Global step 600 Train loss 0.280244 ACC 0.5 on epoch=299
03/09/2022 00:54:57 - INFO - __main__ - save last model!
03/09/2022 00:54:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:54:58 - INFO - __main__ - Printing 3 examples
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 00:54:58 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:54:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:54:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:54:58 - INFO - __main__ - Printing 3 examples
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 00:54:58 - INFO - __main__ - ['sentence 2']
03/09/2022 00:54:58 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:54:58 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:54:58 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:55:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:55:08 - INFO - __main__ - Starting training!
03/09/2022 00:55:41 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 00:55:41 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 00:55:41 - INFO - __main__ - Printing 3 examples
03/09/2022 00:55:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 00:55:41 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 00:55:41 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 00:55:41 - INFO - __main__ - ['sentence 1']
03/09/2022 00:55:41 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:55:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:55:42 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 00:55:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_21_0.0005_8_predictions.txt
03/09/2022 00:55:44 - INFO - __main__ - ACC on test data: 0.5100
03/09/2022 00:55:44 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.51
03/09/2022 00:55:44 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0003, bsz=8 ...
03/09/2022 00:55:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:55:45 - INFO - __main__ - Printing 3 examples
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 00:55:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:55:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 00:55:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 00:55:45 - INFO - __main__ - Printing 3 examples
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 00:55:45 - INFO - __main__ - ['sentence 2']
03/09/2022 00:55:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 00:55:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 00:55:45 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 00:55:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 00:55:55 - INFO - __main__ - Starting training!
03/09/2022 00:55:59 - INFO - __main__ - Step 10 Global step 10 Train loss 20.877855 on epoch=4
03/09/2022 00:56:03 - INFO - __main__ - Step 20 Global step 20 Train loss 19.359352 on epoch=9
03/09/2022 00:56:08 - INFO - __main__ - Step 30 Global step 30 Train loss 15.253757 on epoch=14
03/09/2022 00:56:13 - INFO - __main__ - Step 40 Global step 40 Train loss 13.380014 on epoch=19
03/09/2022 00:56:17 - INFO - __main__ - Step 50 Global step 50 Train loss 12.821136 on epoch=24
03/09/2022 00:56:18 - INFO - __main__ - Global step 50 Train loss 16.338421 ACC 0.0 on epoch=24
03/09/2022 00:56:54 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 00:56:59 - INFO - __main__ - Step 60 Global step 60 Train loss 12.227682 on epoch=29
03/09/2022 00:57:03 - INFO - __main__ - Step 70 Global step 70 Train loss 10.929150 on epoch=34
03/09/2022 00:57:08 - INFO - __main__ - Step 80 Global step 80 Train loss 9.369338 on epoch=39
03/09/2022 00:57:13 - INFO - __main__ - Step 90 Global step 90 Train loss 9.540754 on epoch=44
03/09/2022 00:57:17 - INFO - __main__ - Step 100 Global step 100 Train loss 8.002378 on epoch=49
03/09/2022 00:57:18 - INFO - __main__ - Global step 100 Train loss 10.013861 ACC 0.09375 on epoch=49
03/09/2022 00:57:53 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=49, global_step=100
03/09/2022 00:57:57 - INFO - __main__ - Step 110 Global step 110 Train loss 5.318181 on epoch=54
03/09/2022 00:58:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.032761 on epoch=59
03/09/2022 00:58:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.753311 on epoch=64
03/09/2022 00:58:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.194539 on epoch=69
03/09/2022 00:58:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.142540 on epoch=74
03/09/2022 00:58:17 - INFO - __main__ - Global step 150 Train loss 1.688266 ACC 0.5 on epoch=74
03/09/2022 00:58:56 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=74, global_step=150
03/09/2022 00:59:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.212498 on epoch=79
03/09/2022 00:59:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.117465 on epoch=84
03/09/2022 00:59:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.074902 on epoch=89
03/09/2022 00:59:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.077325 on epoch=94
03/09/2022 00:59:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.033873 on epoch=99
03/09/2022 00:59:19 - INFO - __main__ - Global step 200 Train loss 0.103213 ACC 0.4375 on epoch=99
03/09/2022 00:59:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.111244 on epoch=104
03/09/2022 00:59:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.024429 on epoch=109
03/09/2022 00:59:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.038603 on epoch=114
03/09/2022 00:59:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.023686 on epoch=119
03/09/2022 00:59:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.010739 on epoch=124
03/09/2022 00:59:44 - INFO - __main__ - Global step 250 Train loss 0.041740 ACC 0.71875 on epoch=124
03/09/2022 01:00:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.71875 on epoch=124, global_step=250
03/09/2022 01:00:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.016661 on epoch=129
03/09/2022 01:00:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.010632 on epoch=134
03/09/2022 01:00:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.008456 on epoch=139
03/09/2022 01:00:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.022495 on epoch=144
03/09/2022 01:00:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.004888 on epoch=149
03/09/2022 01:00:47 - INFO - __main__ - Global step 300 Train loss 0.012626 ACC 0.65625 on epoch=149
03/09/2022 01:00:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.026829 on epoch=154
03/09/2022 01:00:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.018545 on epoch=159
03/09/2022 01:01:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.028939 on epoch=164
03/09/2022 01:01:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.010700 on epoch=169
03/09/2022 01:01:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004497 on epoch=174
03/09/2022 01:01:11 - INFO - __main__ - Global step 350 Train loss 0.017902 ACC 0.59375 on epoch=174
03/09/2022 01:01:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.008676 on epoch=179
03/09/2022 01:01:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.024616 on epoch=184
03/09/2022 01:01:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.006991 on epoch=189
03/09/2022 01:01:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001877 on epoch=194
03/09/2022 01:01:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.014964 on epoch=199
03/09/2022 01:01:35 - INFO - __main__ - Global step 400 Train loss 0.011425 ACC 0.75 on epoch=199
03/09/2022 01:02:14 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=199, global_step=400
03/09/2022 01:02:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.004786 on epoch=204
03/09/2022 01:02:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001596 on epoch=209
03/09/2022 01:02:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000581 on epoch=214
03/09/2022 01:02:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000913 on epoch=219
03/09/2022 01:02:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000752 on epoch=224
03/09/2022 01:02:38 - INFO - __main__ - Global step 450 Train loss 0.001726 ACC 0.625 on epoch=224
03/09/2022 01:02:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000298 on epoch=229
03/09/2022 01:02:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001433 on epoch=234
03/09/2022 01:02:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000789 on epoch=239
03/09/2022 01:02:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.012985 on epoch=244
03/09/2022 01:03:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006519 on epoch=249
03/09/2022 01:03:01 - INFO - __main__ - Global step 500 Train loss 0.004405 ACC 0.75 on epoch=249
03/09/2022 01:03:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.104072 on epoch=254
03/09/2022 01:03:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.011085 on epoch=259
03/09/2022 01:03:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.014879 on epoch=264
03/09/2022 01:03:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000384 on epoch=269
03/09/2022 01:03:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.012284 on epoch=274
03/09/2022 01:03:25 - INFO - __main__ - Global step 550 Train loss 0.028541 ACC 0.5625 on epoch=274
03/09/2022 01:03:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.034217 on epoch=279
03/09/2022 01:03:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.454751 on epoch=284
03/09/2022 01:03:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.153359 on epoch=289
03/09/2022 01:03:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.136297 on epoch=294
03/09/2022 01:03:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.229477 on epoch=299
03/09/2022 01:03:49 - INFO - __main__ - Global step 600 Train loss 0.201620 ACC 0.5625 on epoch=299
03/09/2022 01:03:49 - INFO - __main__ - save last model!
03/09/2022 01:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:03:50 - INFO - __main__ - Printing 3 examples
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:03:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:03:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:03:50 - INFO - __main__ - Printing 3 examples
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 01:03:50 - INFO - __main__ - ['sentence 2']
03/09/2022 01:03:50 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:03:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:03:50 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:04:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:04:00 - INFO - __main__ - Starting training!
03/09/2022 01:04:32 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:04:33 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:04:33 - INFO - __main__ - Printing 3 examples
03/09/2022 01:04:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:04:33 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:04:33 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:33 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:04:33 - INFO - __main__ - ['sentence 1']
03/09/2022 01:04:33 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:04:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:04:33 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:04:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_21_0.0003_8_predictions.txt
03/09/2022 01:04:36 - INFO - __main__ - ACC on test data: 0.5100
03/09/2022 01:04:36 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0003, bsz=8, dev_performance=0.75, test_performance=0.51
03/09/2022 01:04:36 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0002, bsz=8 ...
03/09/2022 01:04:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:04:37 - INFO - __main__ - Printing 3 examples
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:04:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:04:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:04:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:04:37 - INFO - __main__ - Printing 3 examples
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 01:04:37 - INFO - __main__ - ['sentence 2']
03/09/2022 01:04:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:04:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:04:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:04:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:04:47 - INFO - __main__ - Starting training!
03/09/2022 01:04:51 - INFO - __main__ - Step 10 Global step 10 Train loss 22.068501 on epoch=4
03/09/2022 01:04:56 - INFO - __main__ - Step 20 Global step 20 Train loss 19.355289 on epoch=9
03/09/2022 01:05:01 - INFO - __main__ - Step 30 Global step 30 Train loss 16.261168 on epoch=14
03/09/2022 01:05:05 - INFO - __main__ - Step 40 Global step 40 Train loss 14.468640 on epoch=19
03/09/2022 01:05:10 - INFO - __main__ - Step 50 Global step 50 Train loss 14.018468 on epoch=24
03/09/2022 01:05:11 - INFO - __main__ - Global step 50 Train loss 17.234413 ACC 0.03125 on epoch=24
03/09/2022 01:05:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.03125 on epoch=24, global_step=50
03/09/2022 01:05:53 - INFO - __main__ - Step 60 Global step 60 Train loss 12.585641 on epoch=29
03/09/2022 01:05:58 - INFO - __main__ - Step 70 Global step 70 Train loss 12.555218 on epoch=34
03/09/2022 01:06:02 - INFO - __main__ - Step 80 Global step 80 Train loss 11.618415 on epoch=39
03/09/2022 01:06:07 - INFO - __main__ - Step 90 Global step 90 Train loss 11.062547 on epoch=44
03/09/2022 01:06:12 - INFO - __main__ - Step 100 Global step 100 Train loss 10.211889 on epoch=49
03/09/2022 01:06:13 - INFO - __main__ - Global step 100 Train loss 11.606741 ACC 0.0625 on epoch=49
03/09/2022 01:06:49 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.0625 on epoch=49, global_step=100
03/09/2022 01:06:53 - INFO - __main__ - Step 110 Global step 110 Train loss 9.761312 on epoch=54
03/09/2022 01:06:58 - INFO - __main__ - Step 120 Global step 120 Train loss 9.365737 on epoch=59
03/09/2022 01:07:03 - INFO - __main__ - Step 130 Global step 130 Train loss 8.506361 on epoch=64
03/09/2022 01:07:08 - INFO - __main__ - Step 140 Global step 140 Train loss 7.773417 on epoch=69
03/09/2022 01:07:12 - INFO - __main__ - Step 150 Global step 150 Train loss 5.106921 on epoch=74
03/09/2022 01:07:25 - INFO - __main__ - Global step 150 Train loss 8.102750 ACC 0.125 on epoch=74
03/09/2022 01:08:01 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.125 on epoch=74, global_step=150
03/09/2022 01:08:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.126360 on epoch=79
03/09/2022 01:08:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.467351 on epoch=84
03/09/2022 01:08:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.306620 on epoch=89
03/09/2022 01:08:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.167474 on epoch=94
03/09/2022 01:08:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.188307 on epoch=99
03/09/2022 01:08:25 - INFO - __main__ - Global step 200 Train loss 0.651223 ACC 0.53125 on epoch=99
03/09/2022 01:09:01 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.53125 on epoch=99, global_step=200
03/09/2022 01:09:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.183022 on epoch=104
03/09/2022 01:09:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.140545 on epoch=109
03/09/2022 01:09:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.130905 on epoch=114
03/09/2022 01:09:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.125550 on epoch=119
03/09/2022 01:09:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.117187 on epoch=124
03/09/2022 01:09:25 - INFO - __main__ - Global step 250 Train loss 0.139442 ACC 0.5625 on epoch=124
03/09/2022 01:10:00 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=124, global_step=250
03/09/2022 01:10:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.122865 on epoch=129
03/09/2022 01:10:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.103314 on epoch=134
03/09/2022 01:10:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.097556 on epoch=139
03/09/2022 01:10:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.089053 on epoch=144
03/09/2022 01:10:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.072975 on epoch=149
03/09/2022 01:10:25 - INFO - __main__ - Global step 300 Train loss 0.097153 ACC 0.5625 on epoch=149
03/09/2022 01:10:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.070618 on epoch=154
03/09/2022 01:10:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.067684 on epoch=159
03/09/2022 01:10:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.071338 on epoch=164
03/09/2022 01:10:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.079365 on epoch=169
03/09/2022 01:10:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.077307 on epoch=174
03/09/2022 01:10:49 - INFO - __main__ - Global step 350 Train loss 0.073262 ACC 0.5625 on epoch=174
03/09/2022 01:10:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.164526 on epoch=179
03/09/2022 01:10:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.044274 on epoch=184
03/09/2022 01:11:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.042711 on epoch=189
03/09/2022 01:11:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.073783 on epoch=194
03/09/2022 01:11:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.041335 on epoch=199
03/09/2022 01:11:13 - INFO - __main__ - Global step 400 Train loss 0.073326 ACC 0.5625 on epoch=199
03/09/2022 01:11:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.043670 on epoch=204
03/09/2022 01:11:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.041576 on epoch=209
03/09/2022 01:11:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046651 on epoch=214
03/09/2022 01:11:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.018059 on epoch=219
03/09/2022 01:11:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.021296 on epoch=224
03/09/2022 01:11:37 - INFO - __main__ - Global step 450 Train loss 0.034250 ACC 0.53125 on epoch=224
03/09/2022 01:11:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.041166 on epoch=229
03/09/2022 01:11:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034214 on epoch=234
03/09/2022 01:11:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.028262 on epoch=239
03/09/2022 01:11:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.008780 on epoch=244
03/09/2022 01:12:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.117698 on epoch=249
03/09/2022 01:12:01 - INFO - __main__ - Global step 500 Train loss 0.046024 ACC 0.65625 on epoch=249
03/09/2022 01:12:37 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=249, global_step=500
03/09/2022 01:12:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.052063 on epoch=254
03/09/2022 01:12:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016797 on epoch=259
03/09/2022 01:12:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.011799 on epoch=264
03/09/2022 01:12:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.007020 on epoch=269
03/09/2022 01:13:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005752 on epoch=274
03/09/2022 01:13:01 - INFO - __main__ - Global step 550 Train loss 0.018686 ACC 0.59375 on epoch=274
03/09/2022 01:13:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001392 on epoch=279
03/09/2022 01:13:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001449 on epoch=284
03/09/2022 01:13:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000535 on epoch=289
03/09/2022 01:13:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000970 on epoch=294
03/09/2022 01:13:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000266 on epoch=299
03/09/2022 01:13:25 - INFO - __main__ - Global step 600 Train loss 0.000922 ACC 0.59375 on epoch=299
03/09/2022 01:13:25 - INFO - __main__ - save last model!
03/09/2022 01:13:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:13:25 - INFO - __main__ - Printing 3 examples
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:13:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:13:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:13:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:13:25 - INFO - __main__ - Printing 3 examples
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 01:13:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:13:25 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:13:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:13:25 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:13:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:13:36 - INFO - __main__ - Starting training!
03/09/2022 01:14:07 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:14:08 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:14:08 - INFO - __main__ - Printing 3 examples
03/09/2022 01:14:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:14:08 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:14:08 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:14:08 - INFO - __main__ - ['sentence 1']
03/09/2022 01:14:08 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:14:08 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:14:08 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:14:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_21_0.0002_8_predictions.txt
03/09/2022 01:14:11 - INFO - __main__ - ACC on test data: 0.4600
03/09/2022 01:14:11 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0002, bsz=8, dev_performance=0.65625, test_performance=0.46
03/09/2022 01:14:11 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0001, bsz=8 ...
03/09/2022 01:14:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:14:12 - INFO - __main__ - Printing 3 examples
03/09/2022 01:14:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Samuel's grandfather who had not hidden that couch will ever wave. [SEP] sentence 2: Samuel's grandfather who had hidden that couch will not ever wave.
03/09/2022 01:14:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The guests that are not observing Theresa had ever boycotted some committees. [SEP] sentence 2: The guests that are observing Theresa had not ever boycotted some committees.
03/09/2022 01:14:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most art galleries that could not disturb some lady can ever vanish. [SEP] sentence 2: Most art galleries that could disturb some lady can not ever vanish.
03/09/2022 01:14:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:14:12 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:14:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:14:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:14:12 - INFO - __main__ - Printing 3 examples
03/09/2022 01:14:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A lot of shoes that have not impressed Meredith had ever warped. [SEP] sentence 2: A lot of shoes that have impressed Meredith had not ever warped.
03/09/2022 01:14:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Tanya's son who is not observing Lissa had ever vanished. [SEP] sentence 2: Tanya's son who is observing Lissa had not ever vanished.
03/09/2022 01:14:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:13 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Joel's daughters who did not boast about Tamara have ever toured some schools. [SEP] sentence 2: Joel's daughters who did boast about Tamara have not ever toured some schools.
03/09/2022 01:14:13 - INFO - __main__ - ['sentence 2']
03/09/2022 01:14:13 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:14:13 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:14:13 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:14:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:14:23 - INFO - __main__ - Starting training!
03/09/2022 01:14:27 - INFO - __main__ - Step 10 Global step 10 Train loss 21.648802 on epoch=4
03/09/2022 01:14:31 - INFO - __main__ - Step 20 Global step 20 Train loss 20.570333 on epoch=9
03/09/2022 01:14:36 - INFO - __main__ - Step 30 Global step 30 Train loss 15.870485 on epoch=14
03/09/2022 01:14:41 - INFO - __main__ - Step 40 Global step 40 Train loss 14.171293 on epoch=19
03/09/2022 01:14:46 - INFO - __main__ - Step 50 Global step 50 Train loss 13.858439 on epoch=24
03/09/2022 01:14:51 - INFO - __main__ - Global step 50 Train loss 17.223871 ACC 0.0 on epoch=24
03/09/2022 01:15:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:15:31 - INFO - __main__ - Step 60 Global step 60 Train loss 14.211220 on epoch=29
03/09/2022 01:15:36 - INFO - __main__ - Step 70 Global step 70 Train loss 13.650447 on epoch=34
03/09/2022 01:15:41 - INFO - __main__ - Step 80 Global step 80 Train loss 13.080931 on epoch=39
03/09/2022 01:15:46 - INFO - __main__ - Step 90 Global step 90 Train loss 13.380548 on epoch=44
03/09/2022 01:15:50 - INFO - __main__ - Step 100 Global step 100 Train loss 12.314369 on epoch=49
03/09/2022 01:15:54 - INFO - __main__ - Global step 100 Train loss 13.327503 ACC 0.0 on epoch=49
03/09/2022 01:15:58 - INFO - __main__ - Step 110 Global step 110 Train loss 11.931258 on epoch=54
03/09/2022 01:16:03 - INFO - __main__ - Step 120 Global step 120 Train loss 12.426623 on epoch=59
03/09/2022 01:16:08 - INFO - __main__ - Step 130 Global step 130 Train loss 11.865817 on epoch=64
03/09/2022 01:16:13 - INFO - __main__ - Step 140 Global step 140 Train loss 11.363538 on epoch=69
03/09/2022 01:16:17 - INFO - __main__ - Step 150 Global step 150 Train loss 11.279588 on epoch=74
03/09/2022 01:16:18 - INFO - __main__ - Global step 150 Train loss 11.773365 ACC 0.03125 on epoch=74
03/09/2022 01:16:52 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=74, global_step=150
03/09/2022 01:16:57 - INFO - __main__ - Step 160 Global step 160 Train loss 11.011259 on epoch=79
03/09/2022 01:17:02 - INFO - __main__ - Step 170 Global step 170 Train loss 10.861669 on epoch=84
03/09/2022 01:17:06 - INFO - __main__ - Step 180 Global step 180 Train loss 10.021495 on epoch=89
03/09/2022 01:17:11 - INFO - __main__ - Step 190 Global step 190 Train loss 9.867723 on epoch=94
03/09/2022 01:17:16 - INFO - __main__ - Step 200 Global step 200 Train loss 9.321026 on epoch=99
03/09/2022 01:17:17 - INFO - __main__ - Global step 200 Train loss 10.216634 ACC 0.03125 on epoch=99
03/09/2022 01:17:22 - INFO - __main__ - Step 210 Global step 210 Train loss 10.107553 on epoch=104
03/09/2022 01:17:26 - INFO - __main__ - Step 220 Global step 220 Train loss 9.148279 on epoch=109
03/09/2022 01:17:31 - INFO - __main__ - Step 230 Global step 230 Train loss 8.755167 on epoch=114
03/09/2022 01:17:36 - INFO - __main__ - Step 240 Global step 240 Train loss 8.255949 on epoch=119
03/09/2022 01:17:40 - INFO - __main__ - Step 250 Global step 250 Train loss 7.835603 on epoch=124
03/09/2022 01:17:42 - INFO - __main__ - Global step 250 Train loss 8.820510 ACC 0.1875 on epoch=124
03/09/2022 01:18:16 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.1875 on epoch=124, global_step=250
03/09/2022 01:18:21 - INFO - __main__ - Step 260 Global step 260 Train loss 7.065497 on epoch=129
03/09/2022 01:18:26 - INFO - __main__ - Step 270 Global step 270 Train loss 5.817749 on epoch=134
03/09/2022 01:18:30 - INFO - __main__ - Step 280 Global step 280 Train loss 3.395464 on epoch=139
03/09/2022 01:18:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.990871 on epoch=144
03/09/2022 01:18:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.294388 on epoch=149
03/09/2022 01:18:40 - INFO - __main__ - Global step 300 Train loss 3.512794 ACC 0.53125 on epoch=149
03/09/2022 01:19:15 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.53125 on epoch=149, global_step=300
03/09/2022 01:19:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.213206 on epoch=154
03/09/2022 01:19:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.186237 on epoch=159
03/09/2022 01:19:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.203729 on epoch=164
03/09/2022 01:19:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.191246 on epoch=169
03/09/2022 01:19:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.201303 on epoch=174
03/09/2022 01:19:38 - INFO - __main__ - Global step 350 Train loss 0.199144 ACC 0.59375 on epoch=174
03/09/2022 01:20:15 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=174, global_step=350
03/09/2022 01:20:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.203230 on epoch=179
03/09/2022 01:20:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.186599 on epoch=184
03/09/2022 01:20:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.170172 on epoch=189
03/09/2022 01:20:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.133381 on epoch=194
03/09/2022 01:20:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.137162 on epoch=199
03/09/2022 01:20:39 - INFO - __main__ - Global step 400 Train loss 0.166109 ACC 0.625 on epoch=199
03/09/2022 01:21:15 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=199, global_step=400
03/09/2022 01:21:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.157800 on epoch=204
03/09/2022 01:21:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.146613 on epoch=209
03/09/2022 01:21:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.140828 on epoch=214
03/09/2022 01:21:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.139893 on epoch=219
03/09/2022 01:21:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.131146 on epoch=224
03/09/2022 01:21:39 - INFO - __main__ - Global step 450 Train loss 0.143256 ACC 0.53125 on epoch=224
03/09/2022 01:21:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.129908 on epoch=229
03/09/2022 01:21:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.125150 on epoch=234
03/09/2022 01:21:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.121680 on epoch=239
03/09/2022 01:21:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.099821 on epoch=244
03/09/2022 01:22:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.090185 on epoch=249
03/09/2022 01:22:03 - INFO - __main__ - Global step 500 Train loss 0.113349 ACC 0.59375 on epoch=249
03/09/2022 01:22:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.095118 on epoch=254
03/09/2022 01:22:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.120095 on epoch=259
03/09/2022 01:22:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.113529 on epoch=264
03/09/2022 01:22:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.165842 on epoch=269
03/09/2022 01:22:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.095591 on epoch=274
03/09/2022 01:22:27 - INFO - __main__ - Global step 550 Train loss 0.118035 ACC 0.5625 on epoch=274
03/09/2022 01:22:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.079445 on epoch=279
03/09/2022 01:22:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.119626 on epoch=284
03/09/2022 01:22:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.083972 on epoch=289
03/09/2022 01:22:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.089372 on epoch=294
03/09/2022 01:22:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.077164 on epoch=299
03/09/2022 01:22:51 - INFO - __main__ - Global step 600 Train loss 0.089916 ACC 0.5625 on epoch=299
03/09/2022 01:22:51 - INFO - __main__ - save last model!
03/09/2022 01:22:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:22:52 - INFO - __main__ - Printing 3 examples
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:22:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:22:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:22:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:22:52 - INFO - __main__ - Printing 3 examples
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:22:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:22:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:22:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:22:52 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:23:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:23:02 - INFO - __main__ - Starting training!
03/09/2022 01:23:34 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:23:34 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:23:34 - INFO - __main__ - Printing 3 examples
03/09/2022 01:23:34 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:23:34 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:34 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:23:34 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:34 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:23:34 - INFO - __main__ - ['sentence 1']
03/09/2022 01:23:34 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:23:34 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:23:34 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:23:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_21_0.0001_8_predictions.txt
03/09/2022 01:23:37 - INFO - __main__ - ACC on test data: 0.4550
03/09/2022 01:23:37 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_21, lr=0.0001, bsz=8, dev_performance=0.625, test_performance=0.455
03/09/2022 01:23:37 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0005, bsz=8 ...
03/09/2022 01:23:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:23:38 - INFO - __main__ - Printing 3 examples
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:23:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:23:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:23:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:23:38 - INFO - __main__ - Printing 3 examples
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:23:38 - INFO - __main__ - ['sentence 2']
03/09/2022 01:23:38 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:23:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:23:38 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:23:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:23:47 - INFO - __main__ - Starting training!
03/09/2022 01:23:51 - INFO - __main__ - Step 10 Global step 10 Train loss 21.320293 on epoch=4
03/09/2022 01:23:56 - INFO - __main__ - Step 20 Global step 20 Train loss 18.769810 on epoch=9
03/09/2022 01:24:00 - INFO - __main__ - Step 30 Global step 30 Train loss 14.682544 on epoch=14
03/09/2022 01:24:05 - INFO - __main__ - Step 40 Global step 40 Train loss 12.859259 on epoch=19
03/09/2022 01:24:10 - INFO - __main__ - Step 50 Global step 50 Train loss 11.219716 on epoch=24
03/09/2022 01:24:10 - INFO - __main__ - Global step 50 Train loss 15.770325 ACC 0.0 on epoch=24
03/09/2022 01:24:46 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:24:51 - INFO - __main__ - Step 60 Global step 60 Train loss 9.586031 on epoch=29
03/09/2022 01:24:56 - INFO - __main__ - Step 70 Global step 70 Train loss 7.745831 on epoch=34
03/09/2022 01:25:00 - INFO - __main__ - Step 80 Global step 80 Train loss 4.617238 on epoch=39
03/09/2022 01:25:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.480303 on epoch=44
03/09/2022 01:25:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.216430 on epoch=49
03/09/2022 01:25:10 - INFO - __main__ - Global step 100 Train loss 4.529167 ACC 0.375 on epoch=49
03/09/2022 01:25:45 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=49, global_step=100
03/09/2022 01:25:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.213122 on epoch=54
03/09/2022 01:25:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.219549 on epoch=59
03/09/2022 01:25:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.180263 on epoch=64
03/09/2022 01:26:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.179309 on epoch=69
03/09/2022 01:26:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.134181 on epoch=74
03/09/2022 01:26:09 - INFO - __main__ - Global step 150 Train loss 0.185285 ACC 0.4375 on epoch=74
03/09/2022 01:26:46 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.4375 on epoch=74, global_step=150
03/09/2022 01:26:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.088210 on epoch=79
03/09/2022 01:26:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.051728 on epoch=84
03/09/2022 01:27:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.032003 on epoch=89
03/09/2022 01:27:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.034566 on epoch=94
03/09/2022 01:27:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.024635 on epoch=99
03/09/2022 01:27:10 - INFO - __main__ - Global step 200 Train loss 0.046228 ACC 0.4375 on epoch=99
03/09/2022 01:27:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.013243 on epoch=104
03/09/2022 01:27:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004017 on epoch=109
03/09/2022 01:27:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.048525 on epoch=114
03/09/2022 01:27:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.006187 on epoch=119
03/09/2022 01:27:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001431 on epoch=124
03/09/2022 01:27:34 - INFO - __main__ - Global step 250 Train loss 0.014680 ACC 0.4375 on epoch=124
03/09/2022 01:27:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000815 on epoch=129
03/09/2022 01:27:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000353 on epoch=134
03/09/2022 01:27:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000591 on epoch=139
03/09/2022 01:27:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000426 on epoch=144
03/09/2022 01:27:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002699 on epoch=149
03/09/2022 01:27:57 - INFO - __main__ - Global step 300 Train loss 0.000977 ACC 0.5 on epoch=149
03/09/2022 01:28:34 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=149, global_step=300
03/09/2022 01:28:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001101 on epoch=154
03/09/2022 01:28:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000599 on epoch=159
03/09/2022 01:28:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000166 on epoch=164
03/09/2022 01:28:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.028083 on epoch=169
03/09/2022 01:28:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.085375 on epoch=174
03/09/2022 01:28:58 - INFO - __main__ - Global step 350 Train loss 0.023065 ACC 0.53125 on epoch=174
03/09/2022 01:29:35 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=174, global_step=350
03/09/2022 01:29:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.019533 on epoch=179
03/09/2022 01:29:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000363 on epoch=184
03/09/2022 01:29:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000313 on epoch=189
03/09/2022 01:29:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000312 on epoch=194
03/09/2022 01:29:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000262 on epoch=199
03/09/2022 01:29:59 - INFO - __main__ - Global step 400 Train loss 0.004156 ACC 0.5 on epoch=199
03/09/2022 01:30:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000267 on epoch=204
03/09/2022 01:30:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000115 on epoch=209
03/09/2022 01:30:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000099 on epoch=214
03/09/2022 01:30:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000235 on epoch=219
03/09/2022 01:30:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000081 on epoch=224
03/09/2022 01:30:23 - INFO - __main__ - Global step 450 Train loss 0.000159 ACC 0.46875 on epoch=224
03/09/2022 01:30:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000072 on epoch=229
03/09/2022 01:30:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000077 on epoch=234
03/09/2022 01:30:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000038 on epoch=239
03/09/2022 01:30:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000044 on epoch=244
03/09/2022 01:30:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000051 on epoch=249
03/09/2022 01:30:47 - INFO - __main__ - Global step 500 Train loss 0.000057 ACC 0.46875 on epoch=249
03/09/2022 01:30:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000071 on epoch=254
03/09/2022 01:30:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000033 on epoch=259
03/09/2022 01:31:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007843 on epoch=264
03/09/2022 01:31:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.023954 on epoch=269
03/09/2022 01:31:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.214214 on epoch=274
03/09/2022 01:31:10 - INFO - __main__ - Global step 550 Train loss 0.049223 ACC 0.5 on epoch=274
03/09/2022 01:31:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.418404 on epoch=279
03/09/2022 01:31:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.242634 on epoch=284
03/09/2022 01:31:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.259417 on epoch=289
03/09/2022 01:31:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.062792 on epoch=294
03/09/2022 01:31:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.093552 on epoch=299
03/09/2022 01:31:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:31:35 - INFO - __main__ - Printing 3 examples
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:31:35 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:31:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:31:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:31:35 - INFO - __main__ - Printing 3 examples
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:31:35 - INFO - __main__ - ['sentence 2']
03/09/2022 01:31:35 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:31:35 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:31:35 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:31:36 - INFO - __main__ - Global step 600 Train loss 0.215360 ACC 0.40625 on epoch=299
03/09/2022 01:31:36 - INFO - __main__ - save last model!
03/09/2022 01:31:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:31:45 - INFO - __main__ - Starting training!
03/09/2022 01:32:19 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:32:20 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:32:20 - INFO - __main__ - Printing 3 examples
03/09/2022 01:32:20 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:32:20 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:20 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:32:20 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:20 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:32:20 - INFO - __main__ - ['sentence 1']
03/09/2022 01:32:20 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:32:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:32:21 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:32:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_42_0.0005_8_predictions.txt
03/09/2022 01:32:23 - INFO - __main__ - ACC on test data: 0.5450
03/09/2022 01:32:23 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.545
03/09/2022 01:32:23 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0003, bsz=8 ...
03/09/2022 01:32:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:32:24 - INFO - __main__ - Printing 3 examples
03/09/2022 01:32:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:32:24 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:32:24 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:32:24 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:32:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:32:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:32:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:32:24 - INFO - __main__ - Printing 3 examples
03/09/2022 01:32:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:32:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:32:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:32:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:32:25 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:32:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:32:25 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:32:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:32:35 - INFO - __main__ - Starting training!
03/09/2022 01:32:39 - INFO - __main__ - Step 10 Global step 10 Train loss 20.651600 on epoch=4
03/09/2022 01:32:43 - INFO - __main__ - Step 20 Global step 20 Train loss 16.481426 on epoch=9
03/09/2022 01:32:48 - INFO - __main__ - Step 30 Global step 30 Train loss 13.225906 on epoch=14
03/09/2022 01:32:53 - INFO - __main__ - Step 40 Global step 40 Train loss 12.460950 on epoch=19
03/09/2022 01:32:57 - INFO - __main__ - Step 50 Global step 50 Train loss 11.243212 on epoch=24
03/09/2022 01:32:58 - INFO - __main__ - Global step 50 Train loss 14.812619 ACC 0.0 on epoch=24
03/09/2022 01:33:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:33:38 - INFO - __main__ - Step 60 Global step 60 Train loss 11.557126 on epoch=29
03/09/2022 01:33:43 - INFO - __main__ - Step 70 Global step 70 Train loss 10.417071 on epoch=34
03/09/2022 01:33:48 - INFO - __main__ - Step 80 Global step 80 Train loss 9.832905 on epoch=39
03/09/2022 01:33:53 - INFO - __main__ - Step 90 Global step 90 Train loss 7.560802 on epoch=44
03/09/2022 01:33:58 - INFO - __main__ - Step 100 Global step 100 Train loss 2.423518 on epoch=49
03/09/2022 01:33:58 - INFO - __main__ - Global step 100 Train loss 8.358284 ACC 0.5 on epoch=49
03/09/2022 01:34:34 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=49, global_step=100
03/09/2022 01:34:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.350277 on epoch=54
03/09/2022 01:34:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.209279 on epoch=59
03/09/2022 01:34:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.181462 on epoch=64
03/09/2022 01:34:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.153360 on epoch=69
03/09/2022 01:34:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.165645 on epoch=74
03/09/2022 01:34:59 - INFO - __main__ - Global step 150 Train loss 0.212005 ACC 0.5 on epoch=74
03/09/2022 01:35:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.132891 on epoch=79
03/09/2022 01:35:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.124039 on epoch=84
03/09/2022 01:35:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.075573 on epoch=89
03/09/2022 01:35:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.077507 on epoch=94
03/09/2022 01:35:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.044153 on epoch=99
03/09/2022 01:35:23 - INFO - __main__ - Global step 200 Train loss 0.090832 ACC 0.375 on epoch=99
03/09/2022 01:35:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.027966 on epoch=104
03/09/2022 01:35:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.045178 on epoch=109
03/09/2022 01:35:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.015007 on epoch=114
03/09/2022 01:35:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.030947 on epoch=119
03/09/2022 01:35:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.013224 on epoch=124
03/09/2022 01:35:48 - INFO - __main__ - Global step 250 Train loss 0.026465 ACC 0.46875 on epoch=124
03/09/2022 01:35:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.010434 on epoch=129
03/09/2022 01:35:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.009589 on epoch=134
03/09/2022 01:36:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.005159 on epoch=139
03/09/2022 01:36:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002658 on epoch=144
03/09/2022 01:36:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001976 on epoch=149
03/09/2022 01:36:13 - INFO - __main__ - Global step 300 Train loss 0.005963 ACC 0.5 on epoch=149
03/09/2022 01:36:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000950 on epoch=154
03/09/2022 01:36:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000641 on epoch=159
03/09/2022 01:36:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002704 on epoch=164
03/09/2022 01:36:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000382 on epoch=169
03/09/2022 01:36:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000488 on epoch=174
03/09/2022 01:36:37 - INFO - __main__ - Global step 350 Train loss 0.001033 ACC 0.5 on epoch=174
03/09/2022 01:36:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000147 on epoch=179
03/09/2022 01:36:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000344 on epoch=184
03/09/2022 01:36:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000193 on epoch=189
03/09/2022 01:36:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000179 on epoch=194
03/09/2022 01:37:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000146 on epoch=199
03/09/2022 01:37:02 - INFO - __main__ - Global step 400 Train loss 0.000202 ACC 0.4375 on epoch=199
03/09/2022 01:37:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.017214 on epoch=204
03/09/2022 01:37:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.008136 on epoch=209
03/09/2022 01:37:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.005798 on epoch=214
03/09/2022 01:37:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001805 on epoch=219
03/09/2022 01:37:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005128 on epoch=224
03/09/2022 01:37:26 - INFO - __main__ - Global step 450 Train loss 0.007616 ACC 0.40625 on epoch=224
03/09/2022 01:37:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000789 on epoch=229
03/09/2022 01:37:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000322 on epoch=234
03/09/2022 01:37:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008636 on epoch=239
03/09/2022 01:37:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001209 on epoch=244
03/09/2022 01:37:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000263 on epoch=249
03/09/2022 01:37:50 - INFO - __main__ - Global step 500 Train loss 0.002244 ACC 0.4375 on epoch=249
03/09/2022 01:37:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000137 on epoch=254
03/09/2022 01:38:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000502 on epoch=259
03/09/2022 01:38:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000611 on epoch=264
03/09/2022 01:38:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001342 on epoch=269
03/09/2022 01:38:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000128 on epoch=274
03/09/2022 01:38:15 - INFO - __main__ - Global step 550 Train loss 0.000544 ACC 0.46875 on epoch=274
03/09/2022 01:38:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000427 on epoch=279
03/09/2022 01:38:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000320 on epoch=284
03/09/2022 01:38:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000123 on epoch=289
03/09/2022 01:38:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000046 on epoch=294
03/09/2022 01:38:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000160 on epoch=299
03/09/2022 01:38:39 - INFO - __main__ - Global step 600 Train loss 0.000215 ACC 0.34375 on epoch=299
03/09/2022 01:38:39 - INFO - __main__ - save last model!
03/09/2022 01:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:38:40 - INFO - __main__ - Printing 3 examples
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:38:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:38:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:38:40 - INFO - __main__ - Printing 3 examples
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:38:40 - INFO - __main__ - ['sentence 2']
03/09/2022 01:38:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:38:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:38:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:38:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:38:51 - INFO - __main__ - Starting training!
03/09/2022 01:39:23 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:39:24 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:39:24 - INFO - __main__ - Printing 3 examples
03/09/2022 01:39:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:39:24 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:39:24 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:24 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:39:24 - INFO - __main__ - ['sentence 1']
03/09/2022 01:39:24 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:39:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:39:24 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:39:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_42_0.0003_8_predictions.txt
03/09/2022 01:39:26 - INFO - __main__ - ACC on test data: 0.4950
03/09/2022 01:39:26 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0003, bsz=8, dev_performance=0.5, test_performance=0.495
03/09/2022 01:39:26 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0002, bsz=8 ...
03/09/2022 01:39:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:39:27 - INFO - __main__ - Printing 3 examples
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:39:27 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:39:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:39:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:39:27 - INFO - __main__ - Printing 3 examples
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:39:27 - INFO - __main__ - ['sentence 2']
03/09/2022 01:39:27 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:39:27 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:39:27 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:39:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:39:37 - INFO - __main__ - Starting training!
03/09/2022 01:39:40 - INFO - __main__ - Step 10 Global step 10 Train loss 21.127970 on epoch=4
03/09/2022 01:39:45 - INFO - __main__ - Step 20 Global step 20 Train loss 16.722923 on epoch=9
03/09/2022 01:39:50 - INFO - __main__ - Step 30 Global step 30 Train loss 14.848300 on epoch=14
03/09/2022 01:39:54 - INFO - __main__ - Step 40 Global step 40 Train loss 13.863978 on epoch=19
03/09/2022 01:39:59 - INFO - __main__ - Step 50 Global step 50 Train loss 13.466768 on epoch=24
03/09/2022 01:40:00 - INFO - __main__ - Global step 50 Train loss 16.005989 ACC 0.0 on epoch=24
03/09/2022 01:40:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:40:40 - INFO - __main__ - Step 60 Global step 60 Train loss 12.888906 on epoch=29
03/09/2022 01:40:45 - INFO - __main__ - Step 70 Global step 70 Train loss 11.977595 on epoch=34
03/09/2022 01:40:49 - INFO - __main__ - Step 80 Global step 80 Train loss 11.667209 on epoch=39
03/09/2022 01:40:54 - INFO - __main__ - Step 90 Global step 90 Train loss 10.891045 on epoch=44
03/09/2022 01:40:59 - INFO - __main__ - Step 100 Global step 100 Train loss 10.674943 on epoch=49
03/09/2022 01:40:59 - INFO - __main__ - Global step 100 Train loss 11.619939 ACC 0.0 on epoch=49
03/09/2022 01:41:04 - INFO - __main__ - Step 110 Global step 110 Train loss 9.861111 on epoch=54
03/09/2022 01:41:08 - INFO - __main__ - Step 120 Global step 120 Train loss 9.221165 on epoch=59
03/09/2022 01:41:13 - INFO - __main__ - Step 130 Global step 130 Train loss 8.711599 on epoch=64
03/09/2022 01:41:18 - INFO - __main__ - Step 140 Global step 140 Train loss 7.529580 on epoch=69
03/09/2022 01:41:23 - INFO - __main__ - Step 150 Global step 150 Train loss 3.613019 on epoch=74
03/09/2022 01:41:33 - INFO - __main__ - Global step 150 Train loss 7.787294 ACC 0.125 on epoch=74
03/09/2022 01:42:11 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.125 on epoch=74, global_step=150
03/09/2022 01:42:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.001148 on epoch=79
03/09/2022 01:42:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.348354 on epoch=84
03/09/2022 01:42:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.204130 on epoch=89
03/09/2022 01:42:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.165026 on epoch=94
03/09/2022 01:42:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.126571 on epoch=99
03/09/2022 01:42:34 - INFO - __main__ - Global step 200 Train loss 0.369046 ACC 0.5 on epoch=99
03/09/2022 01:43:12 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.5 on epoch=99, global_step=200
03/09/2022 01:43:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.133564 on epoch=104
03/09/2022 01:43:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.080907 on epoch=109
03/09/2022 01:43:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.118891 on epoch=114
03/09/2022 01:43:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.079456 on epoch=119
03/09/2022 01:43:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.068162 on epoch=124
03/09/2022 01:43:36 - INFO - __main__ - Global step 250 Train loss 0.096196 ACC 0.4375 on epoch=124
03/09/2022 01:43:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.084304 on epoch=129
03/09/2022 01:43:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.041481 on epoch=134
03/09/2022 01:43:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.039435 on epoch=139
03/09/2022 01:43:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.045189 on epoch=144
03/09/2022 01:44:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.029427 on epoch=149
03/09/2022 01:44:00 - INFO - __main__ - Global step 300 Train loss 0.047967 ACC 0.40625 on epoch=149
03/09/2022 01:44:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035981 on epoch=154
03/09/2022 01:44:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.030982 on epoch=159
03/09/2022 01:44:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.016114 on epoch=164
03/09/2022 01:44:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.013835 on epoch=169
03/09/2022 01:44:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020080 on epoch=174
03/09/2022 01:44:24 - INFO - __main__ - Global step 350 Train loss 0.023398 ACC 0.4375 on epoch=174
03/09/2022 01:44:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006356 on epoch=179
03/09/2022 01:44:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011415 on epoch=184
03/09/2022 01:44:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.014110 on epoch=189
03/09/2022 01:44:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.364931 on epoch=194
03/09/2022 01:44:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.269542 on epoch=199
03/09/2022 01:44:48 - INFO - __main__ - Global step 400 Train loss 0.133271 ACC 0.5 on epoch=199
03/09/2022 01:44:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.114697 on epoch=204
03/09/2022 01:44:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.161299 on epoch=209
03/09/2022 01:45:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.170426 on epoch=214
03/09/2022 01:45:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.112178 on epoch=219
03/09/2022 01:45:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.090990 on epoch=224
03/09/2022 01:45:12 - INFO - __main__ - Global step 450 Train loss 0.129918 ACC 0.40625 on epoch=224
03/09/2022 01:45:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.096992 on epoch=229
03/09/2022 01:45:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.161960 on epoch=234
03/09/2022 01:45:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.043395 on epoch=239
03/09/2022 01:45:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.046616 on epoch=244
03/09/2022 01:45:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.073471 on epoch=249
03/09/2022 01:45:36 - INFO - __main__ - Global step 500 Train loss 0.084487 ACC 0.34375 on epoch=249
03/09/2022 01:45:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.079184 on epoch=254
03/09/2022 01:45:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.102886 on epoch=259
03/09/2022 01:45:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.033293 on epoch=264
03/09/2022 01:45:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.010697 on epoch=269
03/09/2022 01:45:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022048 on epoch=274
03/09/2022 01:46:00 - INFO - __main__ - Global step 550 Train loss 0.049621 ACC 0.40625 on epoch=274
03/09/2022 01:46:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.012682 on epoch=279
03/09/2022 01:46:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015637 on epoch=284
03/09/2022 01:46:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007377 on epoch=289
03/09/2022 01:46:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004760 on epoch=294
03/09/2022 01:46:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009645 on epoch=299
03/09/2022 01:46:24 - INFO - __main__ - Global step 600 Train loss 0.010020 ACC 0.46875 on epoch=299
03/09/2022 01:46:24 - INFO - __main__ - save last model!
03/09/2022 01:46:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:46:25 - INFO - __main__ - Printing 3 examples
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:46:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:46:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:46:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:46:25 - INFO - __main__ - Printing 3 examples
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:46:25 - INFO - __main__ - ['sentence 2']
03/09/2022 01:46:25 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:46:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:46:25 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:46:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:46:35 - INFO - __main__ - Starting training!
03/09/2022 01:47:07 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:47:08 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:47:08 - INFO - __main__ - Printing 3 examples
03/09/2022 01:47:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:47:08 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:47:08 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:08 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:47:08 - INFO - __main__ - ['sentence 1']
03/09/2022 01:47:08 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:47:08 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:47:08 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:47:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_42_0.0002_8_predictions.txt
03/09/2022 01:47:10 - INFO - __main__ - ACC on test data: 0.5050
03/09/2022 01:47:11 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.505
03/09/2022 01:47:11 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0001, bsz=8 ...
03/09/2022 01:47:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:47:12 - INFO - __main__ - Printing 3 examples
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A museum that did not work with an actor had ever discussed William. [SEP] sentence 2: A museum that did work with an actor had not ever discussed William.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most customers who are not exiting every high school had ever hugged. [SEP] sentence 2: Most customers who are exiting every high school had not ever hugged.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: That lake that was not looking like every photograph should ever freeze. [SEP] sentence 2: That lake that was looking like every photograph should not ever freeze.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:47:12 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:47:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:47:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:47:12 - INFO - __main__ - Printing 3 examples
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A cousin of Caroline that can not like that boy did ever cry. [SEP] sentence 2: A cousin of Caroline that can like that boy did not ever cry.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most children that have not escaped from some closet can ever succeed. [SEP] sentence 2: Most children that have escaped from some closet can not ever succeed.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Those girls' daughters who would not pass that high school have ever conferred. [SEP] sentence 2: Those girls' daughters who would pass that high school have not ever conferred.
03/09/2022 01:47:12 - INFO - __main__ - ['sentence 2']
03/09/2022 01:47:12 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:47:12 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:47:12 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:47:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:47:22 - INFO - __main__ - Starting training!
03/09/2022 01:47:26 - INFO - __main__ - Step 10 Global step 10 Train loss 22.339983 on epoch=4
03/09/2022 01:47:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.859396 on epoch=9
03/09/2022 01:47:35 - INFO - __main__ - Step 30 Global step 30 Train loss 17.902328 on epoch=14
03/09/2022 01:47:39 - INFO - __main__ - Step 40 Global step 40 Train loss 15.814240 on epoch=19
03/09/2022 01:47:44 - INFO - __main__ - Step 50 Global step 50 Train loss 15.069814 on epoch=24
03/09/2022 01:47:56 - INFO - __main__ - Global step 50 Train loss 18.197151 ACC 0.0 on epoch=24
03/09/2022 01:48:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:48:38 - INFO - __main__ - Step 60 Global step 60 Train loss 14.708402 on epoch=29
03/09/2022 01:48:42 - INFO - __main__ - Step 70 Global step 70 Train loss 14.739449 on epoch=34
03/09/2022 01:48:47 - INFO - __main__ - Step 80 Global step 80 Train loss 13.911871 on epoch=39
03/09/2022 01:48:52 - INFO - __main__ - Step 90 Global step 90 Train loss 13.962857 on epoch=44
03/09/2022 01:48:57 - INFO - __main__ - Step 100 Global step 100 Train loss 13.495152 on epoch=49
03/09/2022 01:49:03 - INFO - __main__ - Global step 100 Train loss 14.163547 ACC 0.0 on epoch=49
03/09/2022 01:49:08 - INFO - __main__ - Step 110 Global step 110 Train loss 12.451940 on epoch=54
03/09/2022 01:49:13 - INFO - __main__ - Step 120 Global step 120 Train loss 12.897901 on epoch=59
03/09/2022 01:49:18 - INFO - __main__ - Step 130 Global step 130 Train loss 11.928988 on epoch=64
03/09/2022 01:49:22 - INFO - __main__ - Step 140 Global step 140 Train loss 12.450241 on epoch=69
03/09/2022 01:49:27 - INFO - __main__ - Step 150 Global step 150 Train loss 12.133955 on epoch=74
03/09/2022 01:49:35 - INFO - __main__ - Global step 150 Train loss 12.372605 ACC 0.0 on epoch=74
03/09/2022 01:49:40 - INFO - __main__ - Step 160 Global step 160 Train loss 11.565691 on epoch=79
03/09/2022 01:49:45 - INFO - __main__ - Step 170 Global step 170 Train loss 11.452096 on epoch=84
03/09/2022 01:49:49 - INFO - __main__ - Step 180 Global step 180 Train loss 11.102186 on epoch=89
03/09/2022 01:49:54 - INFO - __main__ - Step 190 Global step 190 Train loss 10.979413 on epoch=94
03/09/2022 01:49:59 - INFO - __main__ - Step 200 Global step 200 Train loss 10.824267 on epoch=99
03/09/2022 01:50:00 - INFO - __main__ - Global step 200 Train loss 11.184731 ACC 0.0 on epoch=99
03/09/2022 01:50:05 - INFO - __main__ - Step 210 Global step 210 Train loss 10.357237 on epoch=104
03/09/2022 01:50:10 - INFO - __main__ - Step 220 Global step 220 Train loss 9.758595 on epoch=109
03/09/2022 01:50:15 - INFO - __main__ - Step 230 Global step 230 Train loss 9.944359 on epoch=114
03/09/2022 01:50:19 - INFO - __main__ - Step 240 Global step 240 Train loss 9.165565 on epoch=119
03/09/2022 01:50:24 - INFO - __main__ - Step 250 Global step 250 Train loss 8.961858 on epoch=124
03/09/2022 01:50:26 - INFO - __main__ - Global step 250 Train loss 9.637524 ACC 0.125 on epoch=124
03/09/2022 01:51:02 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.125 on epoch=124, global_step=250
03/09/2022 01:51:06 - INFO - __main__ - Step 260 Global step 260 Train loss 8.330757 on epoch=129
03/09/2022 01:51:11 - INFO - __main__ - Step 270 Global step 270 Train loss 7.945649 on epoch=134
03/09/2022 01:51:16 - INFO - __main__ - Step 280 Global step 280 Train loss 7.340285 on epoch=139
03/09/2022 01:51:21 - INFO - __main__ - Step 290 Global step 290 Train loss 7.217337 on epoch=144
03/09/2022 01:51:25 - INFO - __main__ - Step 300 Global step 300 Train loss 5.641692 on epoch=149
03/09/2022 01:51:27 - INFO - __main__ - Global step 300 Train loss 7.295144 ACC 0.125 on epoch=149
03/09/2022 01:51:31 - INFO - __main__ - Step 310 Global step 310 Train loss 4.401644 on epoch=154
03/09/2022 01:51:36 - INFO - __main__ - Step 320 Global step 320 Train loss 3.524151 on epoch=159
03/09/2022 01:51:41 - INFO - __main__ - Step 330 Global step 330 Train loss 2.393050 on epoch=164
03/09/2022 01:51:46 - INFO - __main__ - Step 340 Global step 340 Train loss 2.055092 on epoch=169
03/09/2022 01:51:50 - INFO - __main__ - Step 350 Global step 350 Train loss 2.476039 on epoch=174
03/09/2022 01:51:51 - INFO - __main__ - Global step 350 Train loss 2.969995 ACC 0.375 on epoch=174
03/09/2022 01:52:28 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.375 on epoch=174, global_step=350
03/09/2022 01:52:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.630371 on epoch=179
03/09/2022 01:52:37 - INFO - __main__ - Step 370 Global step 370 Train loss 1.394291 on epoch=184
03/09/2022 01:52:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.733512 on epoch=189
03/09/2022 01:52:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.434655 on epoch=194
03/09/2022 01:52:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.960997 on epoch=199
03/09/2022 01:52:51 - INFO - __main__ - Global step 400 Train loss 1.430765 ACC 0.5 on epoch=199
03/09/2022 01:53:28 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=199, global_step=400
03/09/2022 01:53:33 - INFO - __main__ - Step 410 Global step 410 Train loss 1.143361 on epoch=204
03/09/2022 01:53:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.708870 on epoch=209
03/09/2022 01:53:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.667740 on epoch=214
03/09/2022 01:53:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.385487 on epoch=219
03/09/2022 01:53:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.222462 on epoch=224
03/09/2022 01:53:52 - INFO - __main__ - Global step 450 Train loss 0.625584 ACC 0.5 on epoch=224
03/09/2022 01:53:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.193141 on epoch=229
03/09/2022 01:54:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.149985 on epoch=234
03/09/2022 01:54:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.157012 on epoch=239
03/09/2022 01:54:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.221643 on epoch=244
03/09/2022 01:54:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.142678 on epoch=249
03/09/2022 01:54:16 - INFO - __main__ - Global step 500 Train loss 0.172892 ACC 0.40625 on epoch=249
03/09/2022 01:54:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.141667 on epoch=254
03/09/2022 01:54:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.133142 on epoch=259
03/09/2022 01:54:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.092747 on epoch=264
03/09/2022 01:54:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.094358 on epoch=269
03/09/2022 01:54:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.173668 on epoch=274
03/09/2022 01:54:40 - INFO - __main__ - Global step 550 Train loss 0.127116 ACC 0.5 on epoch=274
03/09/2022 01:54:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.064390 on epoch=279
03/09/2022 01:54:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.062540 on epoch=284
03/09/2022 01:54:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.056149 on epoch=289
03/09/2022 01:54:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.074448 on epoch=294
03/09/2022 01:55:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.030400 on epoch=299
03/09/2022 01:55:05 - INFO - __main__ - Global step 600 Train loss 0.057585 ACC 0.46875 on epoch=299
03/09/2022 01:55:05 - INFO - __main__ - save last model!
03/09/2022 01:55:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:55:05 - INFO - __main__ - Printing 3 examples
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 01:55:05 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:55:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:55:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:55:05 - INFO - __main__ - Printing 3 examples
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 01:55:05 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:05 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:55:05 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:55:05 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:55:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:55:16 - INFO - __main__ - Starting training!
03/09/2022 01:55:46 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 01:55:47 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 01:55:47 - INFO - __main__ - Printing 3 examples
03/09/2022 01:55:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 01:55:47 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 01:55:47 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:47 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 01:55:47 - INFO - __main__ - ['sentence 1']
03/09/2022 01:55:47 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:55:47 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:55:47 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 01:55:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_42_0.0001_8_predictions.txt
03/09/2022 01:55:49 - INFO - __main__ - ACC on test data: 0.5050
03/09/2022 01:55:51 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_42, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.505
03/09/2022 01:55:51 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0005, bsz=8 ...
03/09/2022 01:55:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:55:52 - INFO - __main__ - Printing 3 examples
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 01:55:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:55:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 01:55:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 01:55:52 - INFO - __main__ - Printing 3 examples
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 01:55:52 - INFO - __main__ - ['sentence 2']
03/09/2022 01:55:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 01:55:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 01:55:52 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 01:56:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 01:56:03 - INFO - __main__ - Starting training!
03/09/2022 01:56:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.727728 on epoch=4
03/09/2022 01:56:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.136127 on epoch=9
03/09/2022 01:56:15 - INFO - __main__ - Step 30 Global step 30 Train loss 15.553299 on epoch=14
03/09/2022 01:56:20 - INFO - __main__ - Step 40 Global step 40 Train loss 13.304419 on epoch=19
03/09/2022 01:56:24 - INFO - __main__ - Step 50 Global step 50 Train loss 12.125626 on epoch=24
03/09/2022 01:56:25 - INFO - __main__ - Global step 50 Train loss 16.169439 ACC 0.0 on epoch=24
03/09/2022 01:57:01 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 01:57:06 - INFO - __main__ - Step 60 Global step 60 Train loss 10.165894 on epoch=29
03/09/2022 01:57:11 - INFO - __main__ - Step 70 Global step 70 Train loss 9.075122 on epoch=34
03/09/2022 01:57:15 - INFO - __main__ - Step 80 Global step 80 Train loss 5.675960 on epoch=39
03/09/2022 01:57:20 - INFO - __main__ - Step 90 Global step 90 Train loss 3.077800 on epoch=44
03/09/2022 01:57:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.510677 on epoch=49
03/09/2022 01:57:25 - INFO - __main__ - Global step 100 Train loss 5.701091 ACC 0.5 on epoch=49
03/09/2022 01:58:02 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=49, global_step=100
03/09/2022 01:58:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.267726 on epoch=54
03/09/2022 01:58:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.224052 on epoch=59
03/09/2022 01:58:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.246016 on epoch=64
03/09/2022 01:58:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.246474 on epoch=69
03/09/2022 01:58:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.232014 on epoch=74
03/09/2022 01:58:26 - INFO - __main__ - Global step 150 Train loss 0.243256 ACC 0.5 on epoch=74
03/09/2022 01:58:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.234130 on epoch=79
03/09/2022 01:58:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.236804 on epoch=84
03/09/2022 01:58:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.223460 on epoch=89
03/09/2022 01:58:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.229020 on epoch=94
03/09/2022 01:58:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.222877 on epoch=99
03/09/2022 01:58:50 - INFO - __main__ - Global step 200 Train loss 0.229258 ACC 0.4375 on epoch=99
03/09/2022 01:58:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.227786 on epoch=104
03/09/2022 01:58:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.225304 on epoch=109
03/09/2022 01:59:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.241398 on epoch=114
03/09/2022 01:59:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.238458 on epoch=119
03/09/2022 01:59:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.224290 on epoch=124
03/09/2022 01:59:14 - INFO - __main__ - Global step 250 Train loss 0.231447 ACC 0.5 on epoch=124
03/09/2022 01:59:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.215919 on epoch=129
03/09/2022 01:59:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.232777 on epoch=134
03/09/2022 01:59:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.223505 on epoch=139
03/09/2022 01:59:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.224958 on epoch=144
03/09/2022 01:59:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.228636 on epoch=149
03/09/2022 01:59:38 - INFO - __main__ - Global step 300 Train loss 0.225159 ACC 0.5 on epoch=149
03/09/2022 01:59:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.227820 on epoch=154
03/09/2022 01:59:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.207989 on epoch=159
03/09/2022 01:59:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.214723 on epoch=164
03/09/2022 01:59:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.223191 on epoch=169
03/09/2022 02:00:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.233678 on epoch=174
03/09/2022 02:00:02 - INFO - __main__ - Global step 350 Train loss 0.221480 ACC 0.625 on epoch=174
03/09/2022 02:00:37 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.625 on epoch=174, global_step=350
03/09/2022 02:00:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.229525 on epoch=179
03/09/2022 02:00:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.220360 on epoch=184
03/09/2022 02:00:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.225723 on epoch=189
03/09/2022 02:00:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.216203 on epoch=194
03/09/2022 02:01:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.235373 on epoch=199
03/09/2022 02:01:01 - INFO - __main__ - Global step 400 Train loss 0.225437 ACC 0.5 on epoch=199
03/09/2022 02:01:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.212420 on epoch=204
03/09/2022 02:01:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.221844 on epoch=209
03/09/2022 02:01:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.217182 on epoch=214
03/09/2022 02:01:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.213612 on epoch=219
03/09/2022 02:01:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.227155 on epoch=224
03/09/2022 02:01:25 - INFO - __main__ - Global step 450 Train loss 0.218443 ACC 0.5625 on epoch=224
03/09/2022 02:01:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.219625 on epoch=229
03/09/2022 02:01:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.220850 on epoch=234
03/09/2022 02:01:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.212146 on epoch=239
03/09/2022 02:01:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.217307 on epoch=244
03/09/2022 02:01:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.224648 on epoch=249
03/09/2022 02:01:49 - INFO - __main__ - Global step 500 Train loss 0.218915 ACC 0.5625 on epoch=249
03/09/2022 02:01:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.226594 on epoch=254
03/09/2022 02:01:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.232309 on epoch=259
03/09/2022 02:02:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.228046 on epoch=264
03/09/2022 02:02:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.228960 on epoch=269
03/09/2022 02:02:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.226969 on epoch=274
03/09/2022 02:02:13 - INFO - __main__ - Global step 550 Train loss 0.228575 ACC 0.5625 on epoch=274
03/09/2022 02:02:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.219226 on epoch=279
03/09/2022 02:02:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.240462 on epoch=284
03/09/2022 02:02:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.230692 on epoch=289
03/09/2022 02:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.229397 on epoch=294
03/09/2022 02:02:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.229349 on epoch=299
03/09/2022 02:02:37 - INFO - __main__ - Global step 600 Train loss 0.229825 ACC 0.59375 on epoch=299
03/09/2022 02:02:37 - INFO - __main__ - save last model!
03/09/2022 02:02:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:02:38 - INFO - __main__ - Printing 3 examples
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:02:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:02:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:02:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:02:38 - INFO - __main__ - Printing 3 examples
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:02:38 - INFO - __main__ - ['sentence 2']
03/09/2022 02:02:38 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:02:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:02:38 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:02:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:02:48 - INFO - __main__ - Starting training!
03/09/2022 02:03:17 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:03:17 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 02:03:17 - INFO - __main__ - Printing 3 examples
03/09/2022 02:03:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 02:03:17 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 02:03:17 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 02:03:17 - INFO - __main__ - ['sentence 1']
03/09/2022 02:03:17 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:03:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:03:18 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 02:03:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_87_0.0005_8_predictions.txt
03/09/2022 02:03:20 - INFO - __main__ - ACC on test data: 0.5300
03/09/2022 02:03:21 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0005, bsz=8, dev_performance=0.625, test_performance=0.53
03/09/2022 02:03:21 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0003, bsz=8 ...
03/09/2022 02:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:03:22 - INFO - __main__ - Printing 3 examples
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:03:22 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:03:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:03:22 - INFO - __main__ - Printing 3 examples
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:03:22 - INFO - __main__ - ['sentence 2']
03/09/2022 02:03:22 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:03:22 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:03:22 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:03:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:03:32 - INFO - __main__ - Starting training!
03/09/2022 02:03:36 - INFO - __main__ - Step 10 Global step 10 Train loss 21.167156 on epoch=4
03/09/2022 02:03:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.301535 on epoch=9
03/09/2022 02:03:45 - INFO - __main__ - Step 30 Global step 30 Train loss 14.576930 on epoch=14
03/09/2022 02:03:50 - INFO - __main__ - Step 40 Global step 40 Train loss 13.434988 on epoch=19
03/09/2022 02:03:55 - INFO - __main__ - Step 50 Global step 50 Train loss 12.643049 on epoch=24
03/09/2022 02:03:55 - INFO - __main__ - Global step 50 Train loss 15.824732 ACC 0.03125 on epoch=24
03/09/2022 02:04:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.03125 on epoch=24, global_step=50
03/09/2022 02:04:38 - INFO - __main__ - Step 60 Global step 60 Train loss 11.250024 on epoch=29
03/09/2022 02:04:43 - INFO - __main__ - Step 70 Global step 70 Train loss 10.950469 on epoch=34
03/09/2022 02:04:47 - INFO - __main__ - Step 80 Global step 80 Train loss 9.906713 on epoch=39
03/09/2022 02:04:52 - INFO - __main__ - Step 90 Global step 90 Train loss 9.540571 on epoch=44
03/09/2022 02:04:57 - INFO - __main__ - Step 100 Global step 100 Train loss 8.066552 on epoch=49
03/09/2022 02:04:57 - INFO - __main__ - Global step 100 Train loss 9.942865 ACC 0.0 on epoch=49
03/09/2022 02:05:02 - INFO - __main__ - Step 110 Global step 110 Train loss 4.619689 on epoch=54
03/09/2022 02:05:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.455805 on epoch=59
03/09/2022 02:05:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.366299 on epoch=64
03/09/2022 02:05:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.352894 on epoch=69
03/09/2022 02:05:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.306587 on epoch=74
03/09/2022 02:05:21 - INFO - __main__ - Global step 150 Train loss 1.220255 ACC 0.65625 on epoch=74
03/09/2022 02:06:00 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.65625 on epoch=74, global_step=150
03/09/2022 02:06:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.222483 on epoch=79
03/09/2022 02:06:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.434968 on epoch=84
03/09/2022 02:06:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.517860 on epoch=89
03/09/2022 02:06:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.443268 on epoch=94
03/09/2022 02:06:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.217383 on epoch=99
03/09/2022 02:06:24 - INFO - __main__ - Global step 200 Train loss 0.567192 ACC 0.53125 on epoch=99
03/09/2022 02:06:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.245273 on epoch=104
03/09/2022 02:06:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.231681 on epoch=109
03/09/2022 02:06:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.219777 on epoch=114
03/09/2022 02:06:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.237738 on epoch=119
03/09/2022 02:06:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.216417 on epoch=124
03/09/2022 02:06:48 - INFO - __main__ - Global step 250 Train loss 0.230177 ACC 0.5625 on epoch=124
03/09/2022 02:06:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.229190 on epoch=129
03/09/2022 02:06:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.226577 on epoch=134
03/09/2022 02:07:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.235685 on epoch=139
03/09/2022 02:07:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.338292 on epoch=144
03/09/2022 02:07:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.219029 on epoch=149
03/09/2022 02:07:12 - INFO - __main__ - Global step 300 Train loss 0.249755 ACC 0.5 on epoch=149
03/09/2022 02:07:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.207770 on epoch=154
03/09/2022 02:07:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.231312 on epoch=159
03/09/2022 02:07:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.214490 on epoch=164
03/09/2022 02:07:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.212357 on epoch=169
03/09/2022 02:07:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.206633 on epoch=174
03/09/2022 02:07:36 - INFO - __main__ - Global step 350 Train loss 0.214512 ACC 0.5 on epoch=174
03/09/2022 02:07:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.222844 on epoch=179
03/09/2022 02:07:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.189986 on epoch=184
03/09/2022 02:07:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.201366 on epoch=189
03/09/2022 02:07:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.220745 on epoch=194
03/09/2022 02:08:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.217941 on epoch=199
03/09/2022 02:08:00 - INFO - __main__ - Global step 400 Train loss 0.210576 ACC 0.59375 on epoch=199
03/09/2022 02:08:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.202951 on epoch=204
03/09/2022 02:08:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.202276 on epoch=209
03/09/2022 02:08:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.201963 on epoch=214
03/09/2022 02:08:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.188061 on epoch=219
03/09/2022 02:08:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.184654 on epoch=224
03/09/2022 02:08:24 - INFO - __main__ - Global step 450 Train loss 0.195981 ACC 0.53125 on epoch=224
03/09/2022 02:08:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.215791 on epoch=229
03/09/2022 02:08:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.184518 on epoch=234
03/09/2022 02:08:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.185862 on epoch=239
03/09/2022 02:08:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.159204 on epoch=244
03/09/2022 02:08:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.140674 on epoch=249
03/09/2022 02:08:48 - INFO - __main__ - Global step 500 Train loss 0.177210 ACC 0.46875 on epoch=249
03/09/2022 02:08:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.099612 on epoch=254
03/09/2022 02:08:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.106708 on epoch=259
03/09/2022 02:09:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.050776 on epoch=264
03/09/2022 02:09:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.072879 on epoch=269
03/09/2022 02:09:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.053886 on epoch=274
03/09/2022 02:09:12 - INFO - __main__ - Global step 550 Train loss 0.076772 ACC 0.46875 on epoch=274
03/09/2022 02:09:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.039317 on epoch=279
03/09/2022 02:09:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.018021 on epoch=284
03/09/2022 02:09:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.010815 on epoch=289
03/09/2022 02:09:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.007533 on epoch=294
03/09/2022 02:09:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002250 on epoch=299
03/09/2022 02:09:36 - INFO - __main__ - Global step 600 Train loss 0.015587 ACC 0.53125 on epoch=299
03/09/2022 02:09:36 - INFO - __main__ - save last model!
03/09/2022 02:09:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:09:37 - INFO - __main__ - Printing 3 examples
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:09:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:09:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:09:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:09:37 - INFO - __main__ - Printing 3 examples
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:09:37 - INFO - __main__ - ['sentence 2']
03/09/2022 02:09:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:09:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:09:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:09:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:09:47 - INFO - __main__ - Starting training!
03/09/2022 02:10:16 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:10:17 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 02:10:17 - INFO - __main__ - Printing 3 examples
03/09/2022 02:10:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 02:10:17 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 02:10:17 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:17 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 02:10:17 - INFO - __main__ - ['sentence 1']
03/09/2022 02:10:17 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:10:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:10:17 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 02:10:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_87_0.0003_8_predictions.txt
03/09/2022 02:10:20 - INFO - __main__ - ACC on test data: 0.5150
03/09/2022 02:10:20 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0003, bsz=8, dev_performance=0.65625, test_performance=0.515
03/09/2022 02:10:20 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0002, bsz=8 ...
03/09/2022 02:10:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:10:21 - INFO - __main__ - Printing 3 examples
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:10:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:10:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:10:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:10:21 - INFO - __main__ - Printing 3 examples
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:10:21 - INFO - __main__ - ['sentence 2']
03/09/2022 02:10:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:10:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:10:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:10:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:10:32 - INFO - __main__ - Starting training!
03/09/2022 02:10:36 - INFO - __main__ - Step 10 Global step 10 Train loss 21.450346 on epoch=4
03/09/2022 02:10:40 - INFO - __main__ - Step 20 Global step 20 Train loss 17.305906 on epoch=9
03/09/2022 02:10:44 - INFO - __main__ - Step 30 Global step 30 Train loss 15.811894 on epoch=14
03/09/2022 02:10:49 - INFO - __main__ - Step 40 Global step 40 Train loss 14.874530 on epoch=19
03/09/2022 02:10:53 - INFO - __main__ - Step 50 Global step 50 Train loss 13.498596 on epoch=24
03/09/2022 02:11:02 - INFO - __main__ - Global step 50 Train loss 16.588255 ACC 0.0 on epoch=24
03/09/2022 02:11:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 02:11:45 - INFO - __main__ - Step 60 Global step 60 Train loss 13.227626 on epoch=29
03/09/2022 02:11:49 - INFO - __main__ - Step 70 Global step 70 Train loss 12.983587 on epoch=34
03/09/2022 02:11:54 - INFO - __main__ - Step 80 Global step 80 Train loss 12.264529 on epoch=39
03/09/2022 02:11:59 - INFO - __main__ - Step 90 Global step 90 Train loss 10.952342 on epoch=44
03/09/2022 02:12:03 - INFO - __main__ - Step 100 Global step 100 Train loss 11.033342 on epoch=49
03/09/2022 02:12:04 - INFO - __main__ - Global step 100 Train loss 12.092285 ACC 0.03125 on epoch=49
03/09/2022 02:12:42 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=49, global_step=100
03/09/2022 02:12:47 - INFO - __main__ - Step 110 Global step 110 Train loss 10.464848 on epoch=54
03/09/2022 02:12:52 - INFO - __main__ - Step 120 Global step 120 Train loss 9.732637 on epoch=59
03/09/2022 02:12:57 - INFO - __main__ - Step 130 Global step 130 Train loss 9.524376 on epoch=64
03/09/2022 02:13:01 - INFO - __main__ - Step 140 Global step 140 Train loss 8.354854 on epoch=69
03/09/2022 02:13:06 - INFO - __main__ - Step 150 Global step 150 Train loss 7.594576 on epoch=74
03/09/2022 02:13:07 - INFO - __main__ - Global step 150 Train loss 9.134258 ACC 0.03125 on epoch=74
03/09/2022 02:13:12 - INFO - __main__ - Step 160 Global step 160 Train loss 6.215928 on epoch=79
03/09/2022 02:13:17 - INFO - __main__ - Step 170 Global step 170 Train loss 3.616025 on epoch=84
03/09/2022 02:13:21 - INFO - __main__ - Step 180 Global step 180 Train loss 1.450041 on epoch=89
03/09/2022 02:13:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.403320 on epoch=94
03/09/2022 02:13:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.185817 on epoch=99
03/09/2022 02:13:31 - INFO - __main__ - Global step 200 Train loss 2.374226 ACC 0.53125 on epoch=99
03/09/2022 02:14:10 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.53125 on epoch=99, global_step=200
03/09/2022 02:14:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.161720 on epoch=104
03/09/2022 02:14:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.160370 on epoch=109
03/09/2022 02:14:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.110306 on epoch=114
03/09/2022 02:14:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.105058 on epoch=119
03/09/2022 02:14:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.094445 on epoch=124
03/09/2022 02:14:34 - INFO - __main__ - Global step 250 Train loss 0.126380 ACC 0.53125 on epoch=124
03/09/2022 02:14:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.220422 on epoch=129
03/09/2022 02:14:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.084980 on epoch=134
03/09/2022 02:14:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.058884 on epoch=139
03/09/2022 02:14:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.071877 on epoch=144
03/09/2022 02:14:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.074431 on epoch=149
03/09/2022 02:14:58 - INFO - __main__ - Global step 300 Train loss 0.102119 ACC 0.4375 on epoch=149
03/09/2022 02:15:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.045065 on epoch=154
03/09/2022 02:15:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.130828 on epoch=159
03/09/2022 02:15:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.077008 on epoch=164
03/09/2022 02:15:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.103067 on epoch=169
03/09/2022 02:15:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.094564 on epoch=174
03/09/2022 02:15:22 - INFO - __main__ - Global step 350 Train loss 0.090106 ACC 0.5 on epoch=174
03/09/2022 02:15:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.037972 on epoch=179
03/09/2022 02:15:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.076900 on epoch=184
03/09/2022 02:15:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.072086 on epoch=189
03/09/2022 02:15:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.058776 on epoch=194
03/09/2022 02:15:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.074533 on epoch=199
03/09/2022 02:15:46 - INFO - __main__ - Global step 400 Train loss 0.064053 ACC 0.53125 on epoch=199
03/09/2022 02:15:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.110758 on epoch=204
03/09/2022 02:15:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.191569 on epoch=209
03/09/2022 02:16:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.127500 on epoch=214
03/09/2022 02:16:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.113419 on epoch=219
03/09/2022 02:16:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.112830 on epoch=224
03/09/2022 02:16:10 - INFO - __main__ - Global step 450 Train loss 0.131215 ACC 0.53125 on epoch=224
03/09/2022 02:16:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.114217 on epoch=229
03/09/2022 02:16:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.085218 on epoch=234
03/09/2022 02:16:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.116720 on epoch=239
03/09/2022 02:16:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.083928 on epoch=244
03/09/2022 02:16:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.097430 on epoch=249
03/09/2022 02:16:34 - INFO - __main__ - Global step 500 Train loss 0.099502 ACC 0.53125 on epoch=249
03/09/2022 02:16:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.079596 on epoch=254
03/09/2022 02:16:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.070084 on epoch=259
03/09/2022 02:16:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.104018 on epoch=264
03/09/2022 02:16:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.091856 on epoch=269
03/09/2022 02:16:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.068191 on epoch=274
03/09/2022 02:16:58 - INFO - __main__ - Global step 550 Train loss 0.082749 ACC 0.46875 on epoch=274
03/09/2022 02:17:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.114995 on epoch=279
03/09/2022 02:17:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.097377 on epoch=284
03/09/2022 02:17:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.124653 on epoch=289
03/09/2022 02:17:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.067942 on epoch=294
03/09/2022 02:17:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.182351 on epoch=299
03/09/2022 02:17:22 - INFO - __main__ - Global step 600 Train loss 0.117463 ACC 0.5625 on epoch=299
03/09/2022 02:17:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:17:23 - INFO - __main__ - Printing 3 examples
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:17:23 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:17:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:17:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:17:23 - INFO - __main__ - Printing 3 examples
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:17:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:17:23 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:17:23 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:17:23 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:17:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:17:33 - INFO - __main__ - Starting training!
03/09/2022 02:18:00 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=299, global_step=600
03/09/2022 02:18:00 - INFO - __main__ - save last model!
03/09/2022 02:18:41 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:18:41 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 02:18:41 - INFO - __main__ - Printing 3 examples
03/09/2022 02:18:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 02:18:41 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 02:18:41 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:41 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 02:18:41 - INFO - __main__ - ['sentence 1']
03/09/2022 02:18:41 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:18:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:18:41 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 02:18:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_87_0.0002_8_predictions.txt
03/09/2022 02:18:44 - INFO - __main__ - ACC on test data: 0.5050
03/09/2022 02:18:44 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.505
03/09/2022 02:18:44 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0001, bsz=8 ...
03/09/2022 02:18:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:18:45 - INFO - __main__ - Printing 3 examples
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: A student that is not criticizing Victoria will ever predict that guy to forfeit. [SEP] sentence 2: A student that is criticizing Victoria will not ever predict that guy to forfeit.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Windows that would not stun Eric had ever shattered. [SEP] sentence 2: Windows that would stun Eric had not ever shattered.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Jennifer's handyman that had not bothered Amelia would ever find out what do steer. [SEP] sentence 2: Jennifer's handyman that had bothered Amelia would not ever find out what do steer.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:18:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:18:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:18:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:18:45 - INFO - __main__ - Printing 3 examples
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Most companies that have not upset Mary have ever irritated Sonia. [SEP] sentence 2: Most companies that have upset Mary have not ever irritated Sonia.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Many nieces of Emily who are not climbing up a lot of slopes had ever forced Marla's sisters to walk through most hospitals. [SEP] sentence 2: Many nieces of Emily who are climbing up a lot of slopes had not ever forced Marla's sisters to walk through most hospitals.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Pamela's boss that can not reveal Christine can ever advise some man's employees to heal Todd. [SEP] sentence 2: Pamela's boss that can reveal Christine can not ever advise some man's employees to heal Todd.
03/09/2022 02:18:45 - INFO - __main__ - ['sentence 2']
03/09/2022 02:18:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:18:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:18:45 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:18:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:18:56 - INFO - __main__ - Starting training!
03/09/2022 02:19:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.664858 on epoch=4
03/09/2022 02:19:04 - INFO - __main__ - Step 20 Global step 20 Train loss 19.985481 on epoch=9
03/09/2022 02:19:09 - INFO - __main__ - Step 30 Global step 30 Train loss 16.997263 on epoch=14
03/09/2022 02:19:13 - INFO - __main__ - Step 40 Global step 40 Train loss 16.018837 on epoch=19
03/09/2022 02:19:18 - INFO - __main__ - Step 50 Global step 50 Train loss 17.436575 on epoch=24
03/09/2022 02:19:27 - INFO - __main__ - Global step 50 Train loss 18.420605 ACC 0.0 on epoch=24
03/09/2022 02:20:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 02:20:09 - INFO - __main__ - Step 60 Global step 60 Train loss 15.590881 on epoch=29
03/09/2022 02:20:14 - INFO - __main__ - Step 70 Global step 70 Train loss 15.314445 on epoch=34
03/09/2022 02:20:19 - INFO - __main__ - Step 80 Global step 80 Train loss 14.855634 on epoch=39
03/09/2022 02:20:24 - INFO - __main__ - Step 90 Global step 90 Train loss 14.151190 on epoch=44
03/09/2022 02:20:28 - INFO - __main__ - Step 100 Global step 100 Train loss 13.086889 on epoch=49
03/09/2022 02:20:29 - INFO - __main__ - Global step 100 Train loss 14.599809 ACC 0.0 on epoch=49
03/09/2022 02:20:34 - INFO - __main__ - Step 110 Global step 110 Train loss 13.554952 on epoch=54
03/09/2022 02:20:38 - INFO - __main__ - Step 120 Global step 120 Train loss 13.055384 on epoch=59
03/09/2022 02:20:43 - INFO - __main__ - Step 130 Global step 130 Train loss 13.015437 on epoch=64
03/09/2022 02:20:48 - INFO - __main__ - Step 140 Global step 140 Train loss 12.660791 on epoch=69
03/09/2022 02:20:53 - INFO - __main__ - Step 150 Global step 150 Train loss 12.191202 on epoch=74
03/09/2022 02:20:53 - INFO - __main__ - Global step 150 Train loss 12.895552 ACC 0.0 on epoch=74
03/09/2022 02:20:58 - INFO - __main__ - Step 160 Global step 160 Train loss 11.185459 on epoch=79
03/09/2022 02:21:03 - INFO - __main__ - Step 170 Global step 170 Train loss 11.303253 on epoch=84
03/09/2022 02:21:08 - INFO - __main__ - Step 180 Global step 180 Train loss 11.680151 on epoch=89
03/09/2022 02:21:13 - INFO - __main__ - Step 190 Global step 190 Train loss 10.788404 on epoch=94
03/09/2022 02:21:17 - INFO - __main__ - Step 200 Global step 200 Train loss 10.346628 on epoch=99
03/09/2022 02:21:18 - INFO - __main__ - Global step 200 Train loss 11.060779 ACC 0.0 on epoch=99
03/09/2022 02:21:23 - INFO - __main__ - Step 210 Global step 210 Train loss 10.579006 on epoch=104
03/09/2022 02:21:27 - INFO - __main__ - Step 220 Global step 220 Train loss 9.944574 on epoch=109
03/09/2022 02:21:32 - INFO - __main__ - Step 230 Global step 230 Train loss 9.682113 on epoch=114
03/09/2022 02:21:37 - INFO - __main__ - Step 240 Global step 240 Train loss 9.773138 on epoch=119
03/09/2022 02:21:42 - INFO - __main__ - Step 250 Global step 250 Train loss 8.821144 on epoch=124
03/09/2022 02:21:42 - INFO - __main__ - Global step 250 Train loss 9.759995 ACC 0.0 on epoch=124
03/09/2022 02:21:47 - INFO - __main__ - Step 260 Global step 260 Train loss 8.908262 on epoch=129
03/09/2022 02:21:51 - INFO - __main__ - Step 270 Global step 270 Train loss 8.700134 on epoch=134
03/09/2022 02:21:56 - INFO - __main__ - Step 280 Global step 280 Train loss 7.312380 on epoch=139
03/09/2022 02:22:01 - INFO - __main__ - Step 290 Global step 290 Train loss 7.664055 on epoch=144
03/09/2022 02:22:06 - INFO - __main__ - Step 300 Global step 300 Train loss 5.936607 on epoch=149
03/09/2022 02:22:06 - INFO - __main__ - Global step 300 Train loss 7.704288 ACC 0.0 on epoch=149
03/09/2022 02:22:11 - INFO - __main__ - Step 310 Global step 310 Train loss 5.258840 on epoch=154
03/09/2022 02:22:15 - INFO - __main__ - Step 320 Global step 320 Train loss 4.558797 on epoch=159
03/09/2022 02:22:20 - INFO - __main__ - Step 330 Global step 330 Train loss 3.896814 on epoch=164
03/09/2022 02:22:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.311373 on epoch=169
03/09/2022 02:22:30 - INFO - __main__ - Step 350 Global step 350 Train loss 1.684517 on epoch=174
03/09/2022 02:22:30 - INFO - __main__ - Global step 350 Train loss 3.542068 ACC 0.5 on epoch=174
03/09/2022 02:23:06 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=174, global_step=350
03/09/2022 02:23:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.661911 on epoch=179
03/09/2022 02:23:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.794985 on epoch=184
03/09/2022 02:23:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.466366 on epoch=189
03/09/2022 02:23:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.274734 on epoch=194
03/09/2022 02:23:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.257836 on epoch=199
03/09/2022 02:23:30 - INFO - __main__ - Global step 400 Train loss 0.691166 ACC 0.5 on epoch=199
03/09/2022 02:23:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.213752 on epoch=204
03/09/2022 02:23:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.192872 on epoch=209
03/09/2022 02:23:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.180735 on epoch=214
03/09/2022 02:23:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.198124 on epoch=219
03/09/2022 02:23:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.343432 on epoch=224
03/09/2022 02:23:54 - INFO - __main__ - Global step 450 Train loss 0.225783 ACC 0.4375 on epoch=224
03/09/2022 02:23:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.316596 on epoch=229
03/09/2022 02:24:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186361 on epoch=234
03/09/2022 02:24:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.307787 on epoch=239
03/09/2022 02:24:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.188550 on epoch=244
03/09/2022 02:24:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.163318 on epoch=249
03/09/2022 02:24:18 - INFO - __main__ - Global step 500 Train loss 0.232522 ACC 0.5625 on epoch=249
03/09/2022 02:24:55 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=249, global_step=500
03/09/2022 02:25:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.188830 on epoch=254
03/09/2022 02:25:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.148562 on epoch=259
03/09/2022 02:25:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.176342 on epoch=264
03/09/2022 02:25:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.154991 on epoch=269
03/09/2022 02:25:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.122386 on epoch=274
03/09/2022 02:25:19 - INFO - __main__ - Global step 550 Train loss 0.158222 ACC 0.53125 on epoch=274
03/09/2022 02:25:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.135467 on epoch=279
03/09/2022 02:25:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.168502 on epoch=284
03/09/2022 02:25:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.140529 on epoch=289
03/09/2022 02:25:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.142956 on epoch=294
03/09/2022 02:25:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.132948 on epoch=299
03/09/2022 02:25:42 - INFO - __main__ - Global step 600 Train loss 0.144081 ACC 0.5625 on epoch=299
03/09/2022 02:25:42 - INFO - __main__ - save last model!
03/09/2022 02:26:22 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:26:23 - INFO - __main__ - Start tokenizing ... 200 instances
03/09/2022 02:26:23 - INFO - __main__ - Printing 3 examples
03/09/2022 02:26:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Beth's senator that has not lifted that candle has ever threatened to smile. [SEP] sentence 2: Beth's senator that has lifted that candle has not ever threatened to smile.
03/09/2022 02:26:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:26:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: The lake that was not distracting Sonia has ever frozen. [SEP] sentence 2: The lake that was distracting Sonia has not ever frozen.
03/09/2022 02:26:23 - INFO - __main__ - ['sentence 2']
03/09/2022 02:26:23 - INFO - __main__ -  [blimp-sentential_negation_npi_scope] sentence 1: Some jacket that would distract Veronica had not ever soaked. [SEP] sentence 2: Some jacket that would not distract Veronica had ever soaked.
03/09/2022 02:26:23 - INFO - __main__ - ['sentence 1']
03/09/2022 02:26:23 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:26:23 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:26:23 - INFO - __main__ - Loaded 200 examples from test data
03/09/2022 02:26:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_scope/blimp-sentential_negation_npi_scope_16_87_0.0001_8_predictions.txt
03/09/2022 02:26:25 - INFO - __main__ - ACC on test data: 0.5250
03/09/2022 02:26:26 - INFO - __main__ - prefix=blimp-sentential_negation_npi_scope_16_87, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.525
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003628730773925781 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13512", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9790, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "13513", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9790, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9790, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (13536): No such process
Task: quoref, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_kkztap7k/none_eleo7fa0
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_kkztap7k/none_eleo7fa0/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_kkztap7k/none_eleo7fa0/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/09/2022 02:26:34 - INFO - __main__ - Namespace(task_dir='data/quoref/', task_name='quoref', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-quoref', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 02:26:34 - INFO - __main__ - models/T5-large-ft-random/singletask-quoref
03/09/2022 02:26:34 - INFO - __main__ - Namespace(task_dir='data/quoref/', task_name='quoref', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-quoref', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 02:26:34 - INFO - __main__ - models/T5-large-ft-random/singletask-quoref
03/09/2022 02:26:34 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/09/2022 02:26:34 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/09/2022 02:26:34 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/09/2022 02:26:34 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/09/2022 02:26:34 - INFO - __main__ - args.device: cuda:0
03/09/2022 02:26:34 - INFO - __main__ - args.device: cuda:1
03/09/2022 02:26:34 - INFO - __main__ - Using 2 gpus
03/09/2022 02:26:34 - INFO - __main__ - Using 2 gpus
03/09/2022 02:26:34 - INFO - __main__ - Fine-tuning the following samples: ['quoref_32_100', 'quoref_32_13', 'quoref_32_21', 'quoref_32_42', 'quoref_32_87']
03/09/2022 02:26:34 - INFO - __main__ - Fine-tuning the following samples: ['quoref_32_100', 'quoref_32_13', 'quoref_32_21', 'quoref_32_42', 'quoref_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/09/2022 02:26:39 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.0005, bsz=8 ...
03/09/2022 02:26:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:26:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:26:40 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:26:40 - INFO - __main__ - ['Jimmy']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:26:40 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:26:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:26:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:26:40 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:26:40 - INFO - __main__ - ['Jimmy']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:26:40 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:26:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:26:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:26:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:26:40 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:26:40 - INFO - __main__ - ['Jeff']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:26:40 - INFO - __main__ - ['November 1991']
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:26:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:26:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:26:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:26:40 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:26:40 - INFO - __main__ - ['Jeff']
03/09/2022 02:26:40 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:26:40 - INFO - __main__ - ['November 1991']
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:26:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:26:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:26:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:26:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:26:52 - INFO - __main__ - Starting training!
03/09/2022 02:26:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:26:52 - INFO - __main__ - Starting training!
03/09/2022 02:26:58 - INFO - __main__ - Step 10 Global step 10 Train loss 18.128262 on epoch=4
03/09/2022 02:27:03 - INFO - __main__ - Step 20 Global step 20 Train loss 11.763361 on epoch=9
03/09/2022 02:27:09 - INFO - __main__ - Step 30 Global step 30 Train loss 13.084760 on epoch=14
03/09/2022 02:27:15 - INFO - __main__ - Step 40 Global step 40 Train loss 9.551533 on epoch=19
03/09/2022 02:27:21 - INFO - __main__ - Step 50 Global step 50 Train loss 8.208218 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/09/2022 02:27:22 - INFO - __main__ - Global step 50 Train loss 12.147225 QA-F1 0.0 on epoch=24
03/09/2022 02:27:36 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 02:27:42 - INFO - __main__ - Step 60 Global step 60 Train loss 7.633636 on epoch=29
03/09/2022 02:27:48 - INFO - __main__ - Step 70 Global step 70 Train loss 7.064548 on epoch=34
03/09/2022 02:27:54 - INFO - __main__ - Step 80 Global step 80 Train loss 7.148694 on epoch=39
03/09/2022 02:28:00 - INFO - __main__ - Step 90 Global step 90 Train loss 5.927362 on epoch=44
03/09/2022 02:28:06 - INFO - __main__ - Step 100 Global step 100 Train loss 5.850536 on epoch=49
03/09/2022 02:28:06 - INFO - __main__ - Global step 100 Train loss 6.724955 QA-F1 0.0 on epoch=49
03/09/2022 02:28:12 - INFO - __main__ - Step 110 Global step 110 Train loss 4.835528 on epoch=54
03/09/2022 02:28:18 - INFO - __main__ - Step 120 Global step 120 Train loss 4.492223 on epoch=59
03/09/2022 02:28:24 - INFO - __main__ - Step 130 Global step 130 Train loss 3.231967 on epoch=64
03/09/2022 02:28:30 - INFO - __main__ - Step 140 Global step 140 Train loss 3.138393 on epoch=69
03/09/2022 02:28:36 - INFO - __main__ - Step 150 Global step 150 Train loss 2.764939 on epoch=74
03/09/2022 02:28:37 - INFO - __main__ - Global step 150 Train loss 3.692610 QA-F1 0.0 on epoch=74
03/09/2022 02:28:43 - INFO - __main__ - Step 160 Global step 160 Train loss 2.959178 on epoch=79
03/09/2022 02:28:49 - INFO - __main__ - Step 170 Global step 170 Train loss 2.588676 on epoch=84
03/09/2022 02:28:55 - INFO - __main__ - Step 180 Global step 180 Train loss 2.240983 on epoch=89
03/09/2022 02:29:01 - INFO - __main__ - Step 190 Global step 190 Train loss 2.442187 on epoch=94
03/09/2022 02:29:07 - INFO - __main__ - Step 200 Global step 200 Train loss 2.302952 on epoch=99
03/09/2022 02:29:08 - INFO - __main__ - Global step 200 Train loss 2.506796 QA-F1 0.0 on epoch=99
03/09/2022 02:29:14 - INFO - __main__ - Step 210 Global step 210 Train loss 1.905099 on epoch=104
03/09/2022 02:29:20 - INFO - __main__ - Step 220 Global step 220 Train loss 2.043021 on epoch=109
03/09/2022 02:29:26 - INFO - __main__ - Step 230 Global step 230 Train loss 1.554732 on epoch=114
03/09/2022 02:29:32 - INFO - __main__ - Step 240 Global step 240 Train loss 1.728402 on epoch=119
03/09/2022 02:29:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.494277 on epoch=124
03/09/2022 02:29:38 - INFO - __main__ - Global step 250 Train loss 1.745106 QA-F1 0.0 on epoch=124
03/09/2022 02:29:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.616550 on epoch=129
03/09/2022 02:29:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.434203 on epoch=134
03/09/2022 02:29:56 - INFO - __main__ - Step 280 Global step 280 Train loss 1.373949 on epoch=139
03/09/2022 02:30:02 - INFO - __main__ - Step 290 Global step 290 Train loss 1.430810 on epoch=144
03/09/2022 02:30:08 - INFO - __main__ - Step 300 Global step 300 Train loss 1.302779 on epoch=149
03/09/2022 02:30:09 - INFO - __main__ - Global step 300 Train loss 1.431659 QA-F1 0.0 on epoch=149
03/09/2022 02:30:15 - INFO - __main__ - Step 310 Global step 310 Train loss 1.377080 on epoch=154
03/09/2022 02:30:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.433132 on epoch=159
03/09/2022 02:30:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.366820 on epoch=164
03/09/2022 02:30:33 - INFO - __main__ - Step 340 Global step 340 Train loss 1.321352 on epoch=169
03/09/2022 02:30:39 - INFO - __main__ - Step 350 Global step 350 Train loss 1.181919 on epoch=174
03/09/2022 02:30:40 - INFO - __main__ - Global step 350 Train loss 1.336061 QA-F1 0.0 on epoch=174
03/09/2022 02:30:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.157404 on epoch=179
03/09/2022 02:30:52 - INFO - __main__ - Step 370 Global step 370 Train loss 1.199156 on epoch=184
03/09/2022 02:30:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.163739 on epoch=189
03/09/2022 02:31:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.128322 on epoch=194
03/09/2022 02:31:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.104273 on epoch=199
03/09/2022 02:31:10 - INFO - __main__ - Global step 400 Train loss 1.150579 QA-F1 0.0 on epoch=199
03/09/2022 02:31:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.088549 on epoch=204
03/09/2022 02:31:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.123963 on epoch=209
03/09/2022 02:31:28 - INFO - __main__ - Step 430 Global step 430 Train loss 1.170190 on epoch=214
03/09/2022 02:31:34 - INFO - __main__ - Step 440 Global step 440 Train loss 1.027720 on epoch=219
03/09/2022 02:31:40 - INFO - __main__ - Step 450 Global step 450 Train loss 1.122124 on epoch=224
03/09/2022 02:31:41 - INFO - __main__ - Global step 450 Train loss 1.106509 QA-F1 0.0 on epoch=224
03/09/2022 02:31:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.016272 on epoch=229
03/09/2022 02:31:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.045732 on epoch=234
03/09/2022 02:31:59 - INFO - __main__ - Step 480 Global step 480 Train loss 1.089690 on epoch=239
03/09/2022 02:32:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.106135 on epoch=244
03/09/2022 02:32:11 - INFO - __main__ - Step 500 Global step 500 Train loss 1.112200 on epoch=249
03/09/2022 02:32:12 - INFO - __main__ - Global step 500 Train loss 1.074006 QA-F1 0.0 on epoch=249
03/09/2022 02:32:17 - INFO - __main__ - Step 510 Global step 510 Train loss 1.063078 on epoch=254
03/09/2022 02:32:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.150816 on epoch=259
03/09/2022 02:32:29 - INFO - __main__ - Step 530 Global step 530 Train loss 1.074201 on epoch=264
03/09/2022 02:32:35 - INFO - __main__ - Step 540 Global step 540 Train loss 1.029245 on epoch=269
03/09/2022 02:32:41 - INFO - __main__ - Step 550 Global step 550 Train loss 1.029636 on epoch=274
03/09/2022 02:32:42 - INFO - __main__ - Global step 550 Train loss 1.069395 QA-F1 0.0 on epoch=274
03/09/2022 02:32:48 - INFO - __main__ - Step 560 Global step 560 Train loss 1.024242 on epoch=279
03/09/2022 02:32:54 - INFO - __main__ - Step 570 Global step 570 Train loss 1.059090 on epoch=284
03/09/2022 02:33:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.958930 on epoch=289
03/09/2022 02:33:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.967140 on epoch=294
03/09/2022 02:33:12 - INFO - __main__ - Step 600 Global step 600 Train loss 1.013861 on epoch=299
03/09/2022 02:33:13 - INFO - __main__ - Global step 600 Train loss 1.004653 QA-F1 0.03125 on epoch=299
03/09/2022 02:33:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:33:13 - INFO - __main__ - Printing 3 examples
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:33:13 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:33:13 - INFO - __main__ - ['Jimmy']
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:33:13 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:33:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:33:13 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:33:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:33:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:33:13 - INFO - __main__ - Printing 3 examples
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:33:13 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:33:13 - INFO - __main__ - ['Jeff']
03/09/2022 02:33:13 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:33:13 - INFO - __main__ - ['November 1991']
03/09/2022 02:33:13 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:33:13 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:33:13 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:33:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:33:23 - INFO - __main__ - Starting training!
03/09/2022 02:33:50 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.03125 on epoch=299, global_step=600
03/09/2022 02:33:50 - INFO - __main__ - save last model!
03/09/2022 02:34:11 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:34:11 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 02:34:11 - INFO - __main__ - Printing 3 examples
03/09/2022 02:34:11 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:34:11 - INFO - __main__ - ['Frankie']
03/09/2022 02:34:11 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:34:11 - INFO - __main__ - ['Frankie']
03/09/2022 02:34:11 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:34:11 - INFO - __main__ - ['Frankie']
03/09/2022 02:34:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:34:15 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:34:18 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 02:35:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_100_0.0005_8_predictions.txt
03/09/2022 02:35:38 - INFO - __main__ - QA-F1 on test data: 0.0059
03/09/2022 02:35:39 - INFO - __main__ - prefix=quoref_32_100, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.005927763992280121
03/09/2022 02:35:39 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.0003, bsz=8 ...
03/09/2022 02:35:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:35:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:35:40 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:35:40 - INFO - __main__ - ['Jimmy']
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:35:40 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:35:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:35:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:35:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:35:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:35:40 - INFO - __main__ - Printing 3 examples
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:35:40 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:35:40 - INFO - __main__ - ['Jeff']
03/09/2022 02:35:40 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:35:40 - INFO - __main__ - ['November 1991']
03/09/2022 02:35:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:35:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:35:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:35:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:35:50 - INFO - __main__ - Starting training!
03/09/2022 02:35:56 - INFO - __main__ - Step 10 Global step 10 Train loss 18.373323 on epoch=4
03/09/2022 02:36:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.012676 on epoch=9
03/09/2022 02:36:08 - INFO - __main__ - Step 30 Global step 30 Train loss 10.396387 on epoch=14
03/09/2022 02:36:14 - INFO - __main__ - Step 40 Global step 40 Train loss 8.714985 on epoch=19
03/09/2022 02:36:20 - INFO - __main__ - Step 50 Global step 50 Train loss 8.374521 on epoch=24
03/09/2022 02:36:22 - INFO - __main__ - Global step 50 Train loss 12.374377 QA-F1 0.015625 on epoch=24
03/09/2022 02:36:58 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.015625 on epoch=24, global_step=50
03/09/2022 02:37:04 - INFO - __main__ - Step 60 Global step 60 Train loss 7.523002 on epoch=29
03/09/2022 02:37:10 - INFO - __main__ - Step 70 Global step 70 Train loss 7.482057 on epoch=34
03/09/2022 02:37:16 - INFO - __main__ - Step 80 Global step 80 Train loss 6.814327 on epoch=39
03/09/2022 02:37:22 - INFO - __main__ - Step 90 Global step 90 Train loss 6.593438 on epoch=44
03/09/2022 02:37:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.613065 on epoch=49
03/09/2022 02:37:29 - INFO - __main__ - Global step 100 Train loss 7.005177 QA-F1 0.0 on epoch=49
03/09/2022 02:37:35 - INFO - __main__ - Step 110 Global step 110 Train loss 6.081946 on epoch=54
03/09/2022 02:37:41 - INFO - __main__ - Step 120 Global step 120 Train loss 5.758019 on epoch=59
03/09/2022 02:37:47 - INFO - __main__ - Step 130 Global step 130 Train loss 5.284575 on epoch=64
03/09/2022 02:37:53 - INFO - __main__ - Step 140 Global step 140 Train loss 5.038803 on epoch=69
03/09/2022 02:37:59 - INFO - __main__ - Step 150 Global step 150 Train loss 4.684557 on epoch=74
03/09/2022 02:38:00 - INFO - __main__ - Global step 150 Train loss 5.369580 QA-F1 0.0 on epoch=74
03/09/2022 02:38:06 - INFO - __main__ - Step 160 Global step 160 Train loss 4.167920 on epoch=79
03/09/2022 02:38:12 - INFO - __main__ - Step 170 Global step 170 Train loss 3.530186 on epoch=84
03/09/2022 02:38:18 - INFO - __main__ - Step 180 Global step 180 Train loss 3.448943 on epoch=89
03/09/2022 02:38:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.773616 on epoch=94
03/09/2022 02:38:30 - INFO - __main__ - Step 200 Global step 200 Train loss 3.037972 on epoch=99
03/09/2022 02:38:31 - INFO - __main__ - Global step 200 Train loss 3.391727 QA-F1 0.04583333333333334 on epoch=99
03/09/2022 02:39:07 - INFO - __main__ - Saving model with best QA-F1: 0.015625 -> 0.04583333333333334 on epoch=99, global_step=200
03/09/2022 02:39:13 - INFO - __main__ - Step 210 Global step 210 Train loss 2.589029 on epoch=104
03/09/2022 02:39:19 - INFO - __main__ - Step 220 Global step 220 Train loss 2.578324 on epoch=109
03/09/2022 02:39:25 - INFO - __main__ - Step 230 Global step 230 Train loss 2.439313 on epoch=114
03/09/2022 02:39:31 - INFO - __main__ - Step 240 Global step 240 Train loss 2.099429 on epoch=119
03/09/2022 02:39:37 - INFO - __main__ - Step 250 Global step 250 Train loss 2.230483 on epoch=124
03/09/2022 02:39:38 - INFO - __main__ - Global step 250 Train loss 2.387316 QA-F1 0.020833333333333332 on epoch=124
03/09/2022 02:39:44 - INFO - __main__ - Step 260 Global step 260 Train loss 2.180637 on epoch=129
03/09/2022 02:39:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.819471 on epoch=134
03/09/2022 02:39:56 - INFO - __main__ - Step 280 Global step 280 Train loss 1.912398 on epoch=139
03/09/2022 02:40:02 - INFO - __main__ - Step 290 Global step 290 Train loss 1.886669 on epoch=144
03/09/2022 02:40:08 - INFO - __main__ - Step 300 Global step 300 Train loss 1.658406 on epoch=149
03/09/2022 02:40:09 - INFO - __main__ - Global step 300 Train loss 1.891516 QA-F1 0.0 on epoch=149
03/09/2022 02:40:15 - INFO - __main__ - Step 310 Global step 310 Train loss 1.341601 on epoch=154
03/09/2022 02:40:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.643040 on epoch=159
03/09/2022 02:40:26 - INFO - __main__ - Step 330 Global step 330 Train loss 1.436148 on epoch=164
03/09/2022 02:40:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.415907 on epoch=169
03/09/2022 02:40:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.511655 on epoch=174
03/09/2022 02:40:39 - INFO - __main__ - Global step 350 Train loss 1.469670 QA-F1 0.03125 on epoch=174
03/09/2022 02:40:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.382666 on epoch=179
03/09/2022 02:40:51 - INFO - __main__ - Step 370 Global step 370 Train loss 1.132861 on epoch=184
03/09/2022 02:40:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.528895 on epoch=189
03/09/2022 02:41:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.308310 on epoch=194
03/09/2022 02:41:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.287021 on epoch=199
03/09/2022 02:41:10 - INFO - __main__ - Global step 400 Train loss 0.727951 QA-F1 0.0625 on epoch=199
03/09/2022 02:41:47 - INFO - __main__ - Saving model with best QA-F1: 0.04583333333333334 -> 0.0625 on epoch=199, global_step=400
03/09/2022 02:41:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.208632 on epoch=204
03/09/2022 02:41:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.181298 on epoch=209
03/09/2022 02:42:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.184881 on epoch=214
03/09/2022 02:42:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.150689 on epoch=219
03/09/2022 02:42:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.175092 on epoch=224
03/09/2022 02:42:18 - INFO - __main__ - Global step 450 Train loss 0.180119 QA-F1 0.08333333333333333 on epoch=224
03/09/2022 02:42:53 - INFO - __main__ - Saving model with best QA-F1: 0.0625 -> 0.08333333333333333 on epoch=224, global_step=450
03/09/2022 02:42:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.143613 on epoch=229
03/09/2022 02:43:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.107714 on epoch=234
03/09/2022 02:43:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.094953 on epoch=239
03/09/2022 02:43:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.134026 on epoch=244
03/09/2022 02:43:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.140547 on epoch=249
03/09/2022 02:43:24 - INFO - __main__ - Global step 500 Train loss 0.124171 QA-F1 0.09375 on epoch=249
03/09/2022 02:44:02 - INFO - __main__ - Saving model with best QA-F1: 0.08333333333333333 -> 0.09375 on epoch=249, global_step=500
03/09/2022 02:44:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.139824 on epoch=254
03/09/2022 02:44:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.156316 on epoch=259
03/09/2022 02:44:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.123620 on epoch=264
03/09/2022 02:44:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.132800 on epoch=269
03/09/2022 02:44:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.136643 on epoch=274
03/09/2022 02:44:33 - INFO - __main__ - Global step 550 Train loss 0.137841 QA-F1 0.0625 on epoch=274
03/09/2022 02:44:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.129217 on epoch=279
03/09/2022 02:44:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.124554 on epoch=284
03/09/2022 02:44:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.134414 on epoch=289
03/09/2022 02:44:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.088358 on epoch=294
03/09/2022 02:45:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.109932 on epoch=299
03/09/2022 02:45:04 - INFO - __main__ - Global step 600 Train loss 0.117295 QA-F1 0.0625 on epoch=299
03/09/2022 02:45:04 - INFO - __main__ - save last model!
03/09/2022 02:45:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:45:04 - INFO - __main__ - Printing 3 examples
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:45:04 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:45:04 - INFO - __main__ - ['Jimmy']
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:45:04 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:45:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:45:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:45:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:45:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:45:04 - INFO - __main__ - Printing 3 examples
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:45:04 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:45:04 - INFO - __main__ - ['Jeff']
03/09/2022 02:45:04 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:45:04 - INFO - __main__ - ['November 1991']
03/09/2022 02:45:04 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:45:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:45:04 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:45:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:45:14 - INFO - __main__ - Starting training!
03/09/2022 02:45:44 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:45:45 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 02:45:45 - INFO - __main__ - Printing 3 examples
03/09/2022 02:45:45 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:45:45 - INFO - __main__ - ['Frankie']
03/09/2022 02:45:45 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:45:45 - INFO - __main__ - ['Frankie']
03/09/2022 02:45:45 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:45:45 - INFO - __main__ - ['Frankie']
03/09/2022 02:45:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:45:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:45:52 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 02:47:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_100_0.0003_8_predictions.txt
03/09/2022 02:47:15 - INFO - __main__ - QA-F1 on test data: 0.0518
03/09/2022 02:47:15 - INFO - __main__ - prefix=quoref_32_100, lr=0.0003, bsz=8, dev_performance=0.09375, test_performance=0.05177179451372999
03/09/2022 02:47:15 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.0002, bsz=8 ...
03/09/2022 02:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:47:16 - INFO - __main__ - Printing 3 examples
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:47:16 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:47:16 - INFO - __main__ - ['Jimmy']
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:47:16 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:47:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:47:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:47:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:47:16 - INFO - __main__ - Printing 3 examples
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:47:16 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:47:16 - INFO - __main__ - ['Jeff']
03/09/2022 02:47:16 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:47:16 - INFO - __main__ - ['November 1991']
03/09/2022 02:47:16 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:47:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:47:16 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:47:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:47:25 - INFO - __main__ - Starting training!
03/09/2022 02:47:31 - INFO - __main__ - Step 10 Global step 10 Train loss 20.377846 on epoch=4
03/09/2022 02:47:36 - INFO - __main__ - Step 20 Global step 20 Train loss 16.488644 on epoch=9
03/09/2022 02:47:42 - INFO - __main__ - Step 30 Global step 30 Train loss 11.178746 on epoch=14
03/09/2022 02:47:48 - INFO - __main__ - Step 40 Global step 40 Train loss 9.870177 on epoch=19
03/09/2022 02:47:54 - INFO - __main__ - Step 50 Global step 50 Train loss 9.952636 on epoch=24
03/09/2022 02:47:55 - INFO - __main__ - Global step 50 Train loss 13.573609 QA-F1 0.0 on epoch=24
03/09/2022 02:48:31 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 02:48:37 - INFO - __main__ - Step 60 Global step 60 Train loss 9.923516 on epoch=29
03/09/2022 02:48:43 - INFO - __main__ - Step 70 Global step 70 Train loss 8.889834 on epoch=34
03/09/2022 02:48:49 - INFO - __main__ - Step 80 Global step 80 Train loss 8.221582 on epoch=39
03/09/2022 02:48:55 - INFO - __main__ - Step 90 Global step 90 Train loss 8.097498 on epoch=44
03/09/2022 02:49:01 - INFO - __main__ - Step 100 Global step 100 Train loss 7.366715 on epoch=49
03/09/2022 02:49:02 - INFO - __main__ - Global step 100 Train loss 8.499829 QA-F1 0.03645833333333333 on epoch=49
03/09/2022 02:49:37 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.03645833333333333 on epoch=49, global_step=100
03/09/2022 02:49:43 - INFO - __main__ - Step 110 Global step 110 Train loss 7.103465 on epoch=54
03/09/2022 02:49:49 - INFO - __main__ - Step 120 Global step 120 Train loss 6.730447 on epoch=59
03/09/2022 02:49:55 - INFO - __main__ - Step 130 Global step 130 Train loss 6.846898 on epoch=64
03/09/2022 02:50:01 - INFO - __main__ - Step 140 Global step 140 Train loss 6.262802 on epoch=69
03/09/2022 02:50:07 - INFO - __main__ - Step 150 Global step 150 Train loss 6.547383 on epoch=74
03/09/2022 02:50:08 - INFO - __main__ - Global step 150 Train loss 6.698199 QA-F1 0.0 on epoch=74
03/09/2022 02:50:14 - INFO - __main__ - Step 160 Global step 160 Train loss 6.164571 on epoch=79
03/09/2022 02:50:20 - INFO - __main__ - Step 170 Global step 170 Train loss 5.918471 on epoch=84
03/09/2022 02:50:26 - INFO - __main__ - Step 180 Global step 180 Train loss 5.289701 on epoch=89
03/09/2022 02:50:32 - INFO - __main__ - Step 190 Global step 190 Train loss 5.678429 on epoch=94
03/09/2022 02:50:38 - INFO - __main__ - Step 200 Global step 200 Train loss 5.185870 on epoch=99
03/09/2022 02:50:39 - INFO - __main__ - Global step 200 Train loss 5.647408 QA-F1 0.0 on epoch=99
03/09/2022 02:50:45 - INFO - __main__ - Step 210 Global step 210 Train loss 4.861062 on epoch=104
03/09/2022 02:50:51 - INFO - __main__ - Step 220 Global step 220 Train loss 4.611997 on epoch=109
03/09/2022 02:50:57 - INFO - __main__ - Step 230 Global step 230 Train loss 4.407841 on epoch=114
03/09/2022 02:51:03 - INFO - __main__ - Step 240 Global step 240 Train loss 4.259993 on epoch=119
03/09/2022 02:51:09 - INFO - __main__ - Step 250 Global step 250 Train loss 3.841309 on epoch=124
03/09/2022 02:51:10 - INFO - __main__ - Global step 250 Train loss 4.396441 QA-F1 0.0 on epoch=124
03/09/2022 02:51:16 - INFO - __main__ - Step 260 Global step 260 Train loss 3.369671 on epoch=129
03/09/2022 02:51:22 - INFO - __main__ - Step 270 Global step 270 Train loss 3.326672 on epoch=134
03/09/2022 02:51:28 - INFO - __main__ - Step 280 Global step 280 Train loss 2.893080 on epoch=139
03/09/2022 02:51:34 - INFO - __main__ - Step 290 Global step 290 Train loss 3.079052 on epoch=144
03/09/2022 02:51:40 - INFO - __main__ - Step 300 Global step 300 Train loss 2.718244 on epoch=149
03/09/2022 02:51:41 - INFO - __main__ - Global step 300 Train loss 3.077344 QA-F1 0.015625 on epoch=149
03/09/2022 02:51:47 - INFO - __main__ - Step 310 Global step 310 Train loss 2.640692 on epoch=154
03/09/2022 02:51:53 - INFO - __main__ - Step 320 Global step 320 Train loss 2.740271 on epoch=159
03/09/2022 02:51:59 - INFO - __main__ - Step 330 Global step 330 Train loss 2.586908 on epoch=164
03/09/2022 02:52:05 - INFO - __main__ - Step 340 Global step 340 Train loss 2.456441 on epoch=169
03/09/2022 02:52:11 - INFO - __main__ - Step 350 Global step 350 Train loss 2.492135 on epoch=174
03/09/2022 02:52:12 - INFO - __main__ - Global step 350 Train loss 2.583289 QA-F1 0.0 on epoch=174
03/09/2022 02:52:18 - INFO - __main__ - Step 360 Global step 360 Train loss 2.419977 on epoch=179
03/09/2022 02:52:24 - INFO - __main__ - Step 370 Global step 370 Train loss 2.707814 on epoch=184
03/09/2022 02:52:30 - INFO - __main__ - Step 380 Global step 380 Train loss 2.457169 on epoch=189
03/09/2022 02:52:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.945688 on epoch=194
03/09/2022 02:52:42 - INFO - __main__ - Step 400 Global step 400 Train loss 2.224865 on epoch=199
03/09/2022 02:52:43 - INFO - __main__ - Global step 400 Train loss 2.351103 QA-F1 0.0 on epoch=199
03/09/2022 02:52:49 - INFO - __main__ - Step 410 Global step 410 Train loss 2.182773 on epoch=204
03/09/2022 02:52:55 - INFO - __main__ - Step 420 Global step 420 Train loss 1.957235 on epoch=209
03/09/2022 02:53:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.977415 on epoch=214
03/09/2022 02:53:07 - INFO - __main__ - Step 440 Global step 440 Train loss 2.029675 on epoch=219
03/09/2022 02:53:13 - INFO - __main__ - Step 450 Global step 450 Train loss 1.602193 on epoch=224
03/09/2022 02:53:14 - INFO - __main__ - Global step 450 Train loss 1.949858 QA-F1 0.03125 on epoch=224
03/09/2022 02:53:20 - INFO - __main__ - Step 460 Global step 460 Train loss 1.673047 on epoch=229
03/09/2022 02:53:26 - INFO - __main__ - Step 470 Global step 470 Train loss 1.558266 on epoch=234
03/09/2022 02:53:32 - INFO - __main__ - Step 480 Global step 480 Train loss 1.671539 on epoch=239
03/09/2022 02:53:38 - INFO - __main__ - Step 490 Global step 490 Train loss 1.836470 on epoch=244
03/09/2022 02:53:44 - INFO - __main__ - Step 500 Global step 500 Train loss 1.692736 on epoch=249
03/09/2022 02:53:45 - INFO - __main__ - Global step 500 Train loss 1.686412 QA-F1 0.020833333333333332 on epoch=249
03/09/2022 02:53:51 - INFO - __main__ - Step 510 Global step 510 Train loss 1.740853 on epoch=254
03/09/2022 02:53:56 - INFO - __main__ - Step 520 Global step 520 Train loss 1.619689 on epoch=259
03/09/2022 02:54:02 - INFO - __main__ - Step 530 Global step 530 Train loss 1.456874 on epoch=264
03/09/2022 02:54:08 - INFO - __main__ - Step 540 Global step 540 Train loss 1.434800 on epoch=269
03/09/2022 02:54:14 - INFO - __main__ - Step 550 Global step 550 Train loss 1.464717 on epoch=274
03/09/2022 02:54:15 - INFO - __main__ - Global step 550 Train loss 1.543386 QA-F1 0.0 on epoch=274
03/09/2022 02:54:21 - INFO - __main__ - Step 560 Global step 560 Train loss 1.562075 on epoch=279
03/09/2022 02:54:27 - INFO - __main__ - Step 570 Global step 570 Train loss 1.456723 on epoch=284
03/09/2022 02:54:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.250872 on epoch=289
03/09/2022 02:54:39 - INFO - __main__ - Step 590 Global step 590 Train loss 1.329642 on epoch=294
03/09/2022 02:54:45 - INFO - __main__ - Step 600 Global step 600 Train loss 1.422759 on epoch=299
03/09/2022 02:54:46 - INFO - __main__ - Global step 600 Train loss 1.404414 QA-F1 0.03125 on epoch=299
03/09/2022 02:54:46 - INFO - __main__ - save last model!
03/09/2022 02:54:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:54:46 - INFO - __main__ - Printing 3 examples
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:54:46 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:54:46 - INFO - __main__ - ['Jimmy']
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:54:46 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:54:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 02:54:46 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:54:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:54:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:54:46 - INFO - __main__ - Printing 3 examples
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:54:46 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:54:46 - INFO - __main__ - ['Jeff']
03/09/2022 02:54:46 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:54:46 - INFO - __main__ - ['November 1991']
03/09/2022 02:54:46 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:54:47 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:54:47 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:54:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:54:56 - INFO - __main__ - Starting training!
03/09/2022 02:55:27 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 02:55:28 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 02:55:28 - INFO - __main__ - Printing 3 examples
03/09/2022 02:55:28 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:55:28 - INFO - __main__ - ['Frankie']
03/09/2022 02:55:28 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:55:28 - INFO - __main__ - ['Frankie']
03/09/2022 02:55:28 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 02:55:28 - INFO - __main__ - ['Frankie']
03/09/2022 02:55:28 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:55:32 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:55:35 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 02:57:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_100_0.0002_8_predictions.txt
03/09/2022 02:57:13 - INFO - __main__ - QA-F1 on test data: 0.0060
03/09/2022 02:57:14 - INFO - __main__ - prefix=quoref_32_100, lr=0.0002, bsz=8, dev_performance=0.03645833333333333, test_performance=0.006043687495300399
03/09/2022 02:57:14 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.0001, bsz=8 ...
03/09/2022 02:57:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:57:14 - INFO - __main__ - Printing 3 examples
03/09/2022 02:57:14 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/09/2022 02:57:14 - INFO - __main__ - ['Ringo Starr']
03/09/2022 02:57:14 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/09/2022 02:57:14 - INFO - __main__ - ['Jimmy']
03/09/2022 02:57:15 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/09/2022 02:57:15 - INFO - __main__ - ['The Alan Bush Music Trust']
03/09/2022 02:57:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 02:57:15 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:57:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 02:57:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 02:57:15 - INFO - __main__ - Printing 3 examples
03/09/2022 02:57:15 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/09/2022 02:57:15 - INFO - __main__ - ['Elk', 'pronghorn']
03/09/2022 02:57:15 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/09/2022 02:57:15 - INFO - __main__ - ['Jeff']
03/09/2022 02:57:15 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/09/2022 02:57:15 - INFO - __main__ - ['November 1991']
03/09/2022 02:57:15 - INFO - __main__ - Tokenizing Input ...
03/09/2022 02:57:15 - INFO - __main__ - Tokenizing Output ...
03/09/2022 02:57:15 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 02:57:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 02:57:24 - INFO - __main__ - Starting training!
03/09/2022 02:57:29 - INFO - __main__ - Step 10 Global step 10 Train loss 19.198956 on epoch=4
03/09/2022 02:57:34 - INFO - __main__ - Step 20 Global step 20 Train loss 19.344471 on epoch=9
03/09/2022 02:57:40 - INFO - __main__ - Step 30 Global step 30 Train loss 17.560530 on epoch=14
03/09/2022 02:57:46 - INFO - __main__ - Step 40 Global step 40 Train loss 14.458143 on epoch=19
03/09/2022 02:57:52 - INFO - __main__ - Step 50 Global step 50 Train loss 12.961720 on epoch=24
03/09/2022 02:57:54 - INFO - __main__ - Global step 50 Train loss 16.704763 QA-F1 0.07812499999999999 on epoch=24
03/09/2022 02:58:29 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.07812499999999999 on epoch=24, global_step=50
03/09/2022 02:58:35 - INFO - __main__ - Step 60 Global step 60 Train loss 11.947459 on epoch=29
03/09/2022 02:58:41 - INFO - __main__ - Step 70 Global step 70 Train loss 10.787691 on epoch=34
03/09/2022 02:58:47 - INFO - __main__ - Step 80 Global step 80 Train loss 9.974793 on epoch=39
03/09/2022 02:58:53 - INFO - __main__ - Step 90 Global step 90 Train loss 9.638136 on epoch=44
03/09/2022 02:58:59 - INFO - __main__ - Step 100 Global step 100 Train loss 9.181395 on epoch=49
03/09/2022 02:59:00 - INFO - __main__ - Global step 100 Train loss 10.305895 QA-F1 0.0 on epoch=49
03/09/2022 02:59:06 - INFO - __main__ - Step 110 Global step 110 Train loss 9.037969 on epoch=54
03/09/2022 02:59:12 - INFO - __main__ - Step 120 Global step 120 Train loss 8.912839 on epoch=59
03/09/2022 02:59:18 - INFO - __main__ - Step 130 Global step 130 Train loss 8.402810 on epoch=64
03/09/2022 02:59:24 - INFO - __main__ - Step 140 Global step 140 Train loss 8.210976 on epoch=69
03/09/2022 02:59:30 - INFO - __main__ - Step 150 Global step 150 Train loss 8.247242 on epoch=74
03/09/2022 02:59:32 - INFO - __main__ - Global step 150 Train loss 8.562366 QA-F1 0.0 on epoch=74
03/09/2022 02:59:38 - INFO - __main__ - Step 160 Global step 160 Train loss 8.055608 on epoch=79
03/09/2022 02:59:44 - INFO - __main__ - Step 170 Global step 170 Train loss 7.760317 on epoch=84
03/09/2022 02:59:50 - INFO - __main__ - Step 180 Global step 180 Train loss 7.319313 on epoch=89
03/09/2022 02:59:56 - INFO - __main__ - Step 190 Global step 190 Train loss 7.615851 on epoch=94
03/09/2022 03:00:01 - INFO - __main__ - Step 200 Global step 200 Train loss 7.065907 on epoch=99
03/09/2022 03:00:03 - INFO - __main__ - Global step 200 Train loss 7.563400 QA-F1 0.0 on epoch=99
03/09/2022 03:00:09 - INFO - __main__ - Step 210 Global step 210 Train loss 7.173219 on epoch=104
03/09/2022 03:00:15 - INFO - __main__ - Step 220 Global step 220 Train loss 7.171973 on epoch=109
03/09/2022 03:00:21 - INFO - __main__ - Step 230 Global step 230 Train loss 6.553613 on epoch=114
03/09/2022 03:00:27 - INFO - __main__ - Step 240 Global step 240 Train loss 7.024561 on epoch=119
03/09/2022 03:00:33 - INFO - __main__ - Step 250 Global step 250 Train loss 6.502428 on epoch=124
03/09/2022 03:00:34 - INFO - __main__ - Global step 250 Train loss 6.885158 QA-F1 0.0 on epoch=124
03/09/2022 03:00:40 - INFO - __main__ - Step 260 Global step 260 Train loss 6.444829 on epoch=129
03/09/2022 03:00:46 - INFO - __main__ - Step 270 Global step 270 Train loss 6.726026 on epoch=134
03/09/2022 03:00:52 - INFO - __main__ - Step 280 Global step 280 Train loss 6.301433 on epoch=139
03/09/2022 03:00:58 - INFO - __main__ - Step 290 Global step 290 Train loss 6.347190 on epoch=144
03/09/2022 03:01:04 - INFO - __main__ - Step 300 Global step 300 Train loss 6.059079 on epoch=149
03/09/2022 03:01:05 - INFO - __main__ - Global step 300 Train loss 6.375711 QA-F1 0.0 on epoch=149
03/09/2022 03:01:11 - INFO - __main__ - Step 310 Global step 310 Train loss 5.951547 on epoch=154
03/09/2022 03:01:17 - INFO - __main__ - Step 320 Global step 320 Train loss 5.987581 on epoch=159
03/09/2022 03:01:23 - INFO - __main__ - Step 330 Global step 330 Train loss 5.803484 on epoch=164
03/09/2022 03:01:29 - INFO - __main__ - Step 340 Global step 340 Train loss 5.586986 on epoch=169
03/09/2022 03:01:35 - INFO - __main__ - Step 350 Global step 350 Train loss 5.633157 on epoch=174
03/09/2022 03:01:36 - INFO - __main__ - Global step 350 Train loss 5.792551 QA-F1 0.0 on epoch=174
03/09/2022 03:01:42 - INFO - __main__ - Step 360 Global step 360 Train loss 5.493556 on epoch=179
03/09/2022 03:01:48 - INFO - __main__ - Step 370 Global step 370 Train loss 5.259440 on epoch=184
03/09/2022 03:01:54 - INFO - __main__ - Step 380 Global step 380 Train loss 5.555694 on epoch=189
03/09/2022 03:02:00 - INFO - __main__ - Step 390 Global step 390 Train loss 5.100528 on epoch=194
03/09/2022 03:02:06 - INFO - __main__ - Step 400 Global step 400 Train loss 4.912874 on epoch=199
03/09/2022 03:02:07 - INFO - __main__ - Global step 400 Train loss 5.264419 QA-F1 0.0 on epoch=199
03/09/2022 03:02:13 - INFO - __main__ - Step 410 Global step 410 Train loss 4.757699 on epoch=204
03/09/2022 03:02:19 - INFO - __main__ - Step 420 Global step 420 Train loss 4.745222 on epoch=209
03/09/2022 03:02:25 - INFO - __main__ - Step 430 Global step 430 Train loss 4.756899 on epoch=214
03/09/2022 03:02:31 - INFO - __main__ - Step 440 Global step 440 Train loss 4.378909 on epoch=219
03/09/2022 03:02:37 - INFO - __main__ - Step 450 Global step 450 Train loss 4.463498 on epoch=224
03/09/2022 03:02:38 - INFO - __main__ - Global step 450 Train loss 4.620445 QA-F1 0.0 on epoch=224
03/09/2022 03:02:44 - INFO - __main__ - Step 460 Global step 460 Train loss 4.154003 on epoch=229
03/09/2022 03:02:50 - INFO - __main__ - Step 470 Global step 470 Train loss 4.068869 on epoch=234
03/09/2022 03:02:56 - INFO - __main__ - Step 480 Global step 480 Train loss 4.072785 on epoch=239
03/09/2022 03:03:02 - INFO - __main__ - Step 490 Global step 490 Train loss 4.246181 on epoch=244
03/09/2022 03:03:08 - INFO - __main__ - Step 500 Global step 500 Train loss 2.874576 on epoch=249
03/09/2022 03:03:09 - INFO - __main__ - Global step 500 Train loss 3.883282 QA-F1 0.0 on epoch=249
03/09/2022 03:03:15 - INFO - __main__ - Step 510 Global step 510 Train loss 3.380238 on epoch=254
03/09/2022 03:03:21 - INFO - __main__ - Step 520 Global step 520 Train loss 3.441066 on epoch=259
03/09/2022 03:03:26 - INFO - __main__ - Step 530 Global step 530 Train loss 2.981053 on epoch=264
03/09/2022 03:03:32 - INFO - __main__ - Step 540 Global step 540 Train loss 2.993460 on epoch=269
03/09/2022 03:03:38 - INFO - __main__ - Step 550 Global step 550 Train loss 2.638848 on epoch=274
03/09/2022 03:03:40 - INFO - __main__ - Global step 550 Train loss 3.086933 QA-F1 0.0 on epoch=274
03/09/2022 03:03:46 - INFO - __main__ - Step 560 Global step 560 Train loss 2.909787 on epoch=279
03/09/2022 03:03:52 - INFO - __main__ - Step 570 Global step 570 Train loss 2.696279 on epoch=284
03/09/2022 03:03:57 - INFO - __main__ - Step 580 Global step 580 Train loss 2.569871 on epoch=289
03/09/2022 03:04:03 - INFO - __main__ - Step 590 Global step 590 Train loss 2.658381 on epoch=294
03/09/2022 03:04:09 - INFO - __main__ - Step 600 Global step 600 Train loss 2.764821 on epoch=299
03/09/2022 03:04:10 - INFO - __main__ - Global step 600 Train loss 2.719828 QA-F1 0.0 on epoch=299
03/09/2022 03:04:10 - INFO - __main__ - save last model!
03/09/2022 03:04:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:04:11 - INFO - __main__ - Printing 3 examples
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:04:11 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:04:11 - INFO - __main__ - ['Up']
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:04:11 - INFO - __main__ - ['Norway']
03/09/2022 03:04:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:04:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:04:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:04:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:04:11 - INFO - __main__ - Printing 3 examples
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:04:11 - INFO - __main__ - ['George Gipp']
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:04:11 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:04:11 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:04:11 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:04:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:04:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:04:11 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:04:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:04:20 - INFO - __main__ - Starting training!
03/09/2022 03:04:52 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:04:52 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:04:52 - INFO - __main__ - Printing 3 examples
03/09/2022 03:04:52 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:04:52 - INFO - __main__ - ['Frankie']
03/09/2022 03:04:52 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:04:52 - INFO - __main__ - ['Frankie']
03/09/2022 03:04:52 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:04:52 - INFO - __main__ - ['Frankie']
03/09/2022 03:04:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:04:56 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:04:59 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:07:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_100_0.0001_8_predictions.txt
03/09/2022 03:07:41 - INFO - __main__ - QA-F1 on test data: 0.0473
03/09/2022 03:07:41 - INFO - __main__ - prefix=quoref_32_100, lr=0.0001, bsz=8, dev_performance=0.07812499999999999, test_performance=0.04732161965412585
03/09/2022 03:07:41 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.0005, bsz=8 ...
03/09/2022 03:07:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:07:42 - INFO - __main__ - Printing 3 examples
03/09/2022 03:07:42 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:07:42 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:07:42 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:07:42 - INFO - __main__ - ['Up']
03/09/2022 03:07:42 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:07:42 - INFO - __main__ - ['Norway']
03/09/2022 03:07:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:07:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:07:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:07:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:07:42 - INFO - __main__ - Printing 3 examples
03/09/2022 03:07:42 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:07:42 - INFO - __main__ - ['George Gipp']
03/09/2022 03:07:42 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:07:43 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:07:43 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:07:43 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:07:43 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:07:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:07:43 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:07:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:07:51 - INFO - __main__ - Starting training!
03/09/2022 03:07:57 - INFO - __main__ - Step 10 Global step 10 Train loss 19.421398 on epoch=4
03/09/2022 03:08:03 - INFO - __main__ - Step 20 Global step 20 Train loss 13.978388 on epoch=9
03/09/2022 03:08:08 - INFO - __main__ - Step 30 Global step 30 Train loss 13.991045 on epoch=14
03/09/2022 03:08:14 - INFO - __main__ - Step 40 Global step 40 Train loss 10.500856 on epoch=19
03/09/2022 03:08:20 - INFO - __main__ - Step 50 Global step 50 Train loss 7.803737 on epoch=24
03/09/2022 03:08:21 - INFO - __main__ - Global step 50 Train loss 13.139085 QA-F1 0.0 on epoch=24
03/09/2022 03:08:56 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 03:09:02 - INFO - __main__ - Step 60 Global step 60 Train loss 7.020170 on epoch=29
03/09/2022 03:09:08 - INFO - __main__ - Step 70 Global step 70 Train loss 7.000237 on epoch=34
03/09/2022 03:09:14 - INFO - __main__ - Step 80 Global step 80 Train loss 6.743549 on epoch=39
03/09/2022 03:09:20 - INFO - __main__ - Step 90 Global step 90 Train loss 6.012971 on epoch=44
03/09/2022 03:09:26 - INFO - __main__ - Step 100 Global step 100 Train loss 5.737845 on epoch=49
03/09/2022 03:09:27 - INFO - __main__ - Global step 100 Train loss 6.502954 QA-F1 0.0 on epoch=49
03/09/2022 03:09:33 - INFO - __main__ - Step 110 Global step 110 Train loss 4.769762 on epoch=54
03/09/2022 03:09:39 - INFO - __main__ - Step 120 Global step 120 Train loss 4.322423 on epoch=59
03/09/2022 03:09:45 - INFO - __main__ - Step 130 Global step 130 Train loss 3.272681 on epoch=64
03/09/2022 03:09:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.838032 on epoch=69
03/09/2022 03:09:57 - INFO - __main__ - Step 150 Global step 150 Train loss 2.484219 on epoch=74
03/09/2022 03:09:58 - INFO - __main__ - Global step 150 Train loss 3.537423 QA-F1 0.0 on epoch=74
03/09/2022 03:10:04 - INFO - __main__ - Step 160 Global step 160 Train loss 2.617011 on epoch=79
03/09/2022 03:10:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.419181 on epoch=84
03/09/2022 03:10:16 - INFO - __main__ - Step 180 Global step 180 Train loss 2.202386 on epoch=89
03/09/2022 03:10:22 - INFO - __main__ - Step 190 Global step 190 Train loss 2.179654 on epoch=94
03/09/2022 03:10:28 - INFO - __main__ - Step 200 Global step 200 Train loss 1.833649 on epoch=99
03/09/2022 03:10:28 - INFO - __main__ - Global step 200 Train loss 2.250376 QA-F1 0.0 on epoch=99
03/09/2022 03:10:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.009561 on epoch=104
03/09/2022 03:10:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.725602 on epoch=109
03/09/2022 03:10:46 - INFO - __main__ - Step 230 Global step 230 Train loss 1.756192 on epoch=114
03/09/2022 03:10:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.790232 on epoch=119
03/09/2022 03:10:58 - INFO - __main__ - Step 250 Global step 250 Train loss 1.539360 on epoch=124
03/09/2022 03:10:59 - INFO - __main__ - Global step 250 Train loss 1.764189 QA-F1 0.0 on epoch=124
03/09/2022 03:11:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.581196 on epoch=129
03/09/2022 03:11:11 - INFO - __main__ - Step 270 Global step 270 Train loss 1.563650 on epoch=134
03/09/2022 03:11:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.611734 on epoch=139
03/09/2022 03:11:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.563458 on epoch=144
03/09/2022 03:11:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.376019 on epoch=149
03/09/2022 03:11:30 - INFO - __main__ - Global step 300 Train loss 1.539211 QA-F1 0.0 on epoch=149
03/09/2022 03:11:36 - INFO - __main__ - Step 310 Global step 310 Train loss 1.300601 on epoch=154
03/09/2022 03:11:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.303364 on epoch=159
03/09/2022 03:11:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.336795 on epoch=164
03/09/2022 03:11:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.254056 on epoch=169
03/09/2022 03:11:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.264361 on epoch=174
03/09/2022 03:12:00 - INFO - __main__ - Global step 350 Train loss 1.291835 QA-F1 0.0 on epoch=174
03/09/2022 03:12:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.152169 on epoch=179
03/09/2022 03:12:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.131449 on epoch=184
03/09/2022 03:12:18 - INFO - __main__ - Step 380 Global step 380 Train loss 1.131065 on epoch=189
03/09/2022 03:12:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.203038 on epoch=194
03/09/2022 03:12:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.018331 on epoch=199
03/09/2022 03:12:31 - INFO - __main__ - Global step 400 Train loss 1.127210 QA-F1 0.0 on epoch=199
03/09/2022 03:12:37 - INFO - __main__ - Step 410 Global step 410 Train loss 1.091225 on epoch=204
03/09/2022 03:12:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.981675 on epoch=209
03/09/2022 03:12:49 - INFO - __main__ - Step 430 Global step 430 Train loss 1.013324 on epoch=214
03/09/2022 03:12:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.945621 on epoch=219
03/09/2022 03:13:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.935857 on epoch=224
03/09/2022 03:13:02 - INFO - __main__ - Global step 450 Train loss 0.993541 QA-F1 0.046875 on epoch=224
03/09/2022 03:13:38 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.046875 on epoch=224, global_step=450
03/09/2022 03:13:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.895573 on epoch=229
03/09/2022 03:13:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.734618 on epoch=234
03/09/2022 03:13:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.659055 on epoch=239
03/09/2022 03:14:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.552083 on epoch=244
03/09/2022 03:14:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437980 on epoch=249
03/09/2022 03:14:09 - INFO - __main__ - Global step 500 Train loss 0.655862 QA-F1 0.06770833333333333 on epoch=249
03/09/2022 03:14:45 - INFO - __main__ - Saving model with best QA-F1: 0.046875 -> 0.06770833333333333 on epoch=249, global_step=500
03/09/2022 03:14:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.443660 on epoch=254
03/09/2022 03:14:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.270652 on epoch=259
03/09/2022 03:15:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.231614 on epoch=264
03/09/2022 03:15:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.195196 on epoch=269
03/09/2022 03:15:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.163041 on epoch=274
03/09/2022 03:15:16 - INFO - __main__ - Global step 550 Train loss 0.260833 QA-F1 0.06770833333333333 on epoch=274
03/09/2022 03:15:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.331432 on epoch=279
03/09/2022 03:15:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.088795 on epoch=284
03/09/2022 03:15:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.106546 on epoch=289
03/09/2022 03:15:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.113836 on epoch=294
03/09/2022 03:15:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.121157 on epoch=299
03/09/2022 03:15:48 - INFO - __main__ - Global step 600 Train loss 0.152353 QA-F1 0.06770833333333333 on epoch=299
03/09/2022 03:15:48 - INFO - __main__ - save last model!
03/09/2022 03:15:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:15:48 - INFO - __main__ - Printing 3 examples
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:15:48 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:15:48 - INFO - __main__ - ['Up']
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:15:48 - INFO - __main__ - ['Norway']
03/09/2022 03:15:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:15:48 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:15:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:15:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:15:48 - INFO - __main__ - Printing 3 examples
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:15:48 - INFO - __main__ - ['George Gipp']
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:15:48 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:15:48 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:15:48 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:15:48 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:15:48 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:15:48 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:15:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:15:58 - INFO - __main__ - Starting training!
03/09/2022 03:16:28 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:16:28 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:16:28 - INFO - __main__ - Printing 3 examples
03/09/2022 03:16:28 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:16:28 - INFO - __main__ - ['Frankie']
03/09/2022 03:16:28 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:16:28 - INFO - __main__ - ['Frankie']
03/09/2022 03:16:29 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:16:29 - INFO - __main__ - ['Frankie']
03/09/2022 03:16:29 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:16:33 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:16:35 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:17:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_13_0.0005_8_predictions.txt
03/09/2022 03:17:53 - INFO - __main__ - QA-F1 on test data: 0.0138
03/09/2022 03:17:53 - INFO - __main__ - prefix=quoref_32_13, lr=0.0005, bsz=8, dev_performance=0.06770833333333333, test_performance=0.013752018590728266
03/09/2022 03:17:53 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.0003, bsz=8 ...
03/09/2022 03:17:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:17:54 - INFO - __main__ - Printing 3 examples
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:17:54 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:17:54 - INFO - __main__ - ['Up']
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:17:54 - INFO - __main__ - ['Norway']
03/09/2022 03:17:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:17:54 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:17:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:17:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:17:54 - INFO - __main__ - Printing 3 examples
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:17:54 - INFO - __main__ - ['George Gipp']
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:17:54 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:17:54 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:17:54 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:17:54 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:17:55 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:17:55 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:18:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:18:03 - INFO - __main__ - Starting training!
03/09/2022 03:18:09 - INFO - __main__ - Step 10 Global step 10 Train loss 19.795681 on epoch=4
03/09/2022 03:18:15 - INFO - __main__ - Step 20 Global step 20 Train loss 16.029209 on epoch=9
03/09/2022 03:18:20 - INFO - __main__ - Step 30 Global step 30 Train loss 11.812292 on epoch=14
03/09/2022 03:18:26 - INFO - __main__ - Step 40 Global step 40 Train loss 10.903189 on epoch=19
03/09/2022 03:18:32 - INFO - __main__ - Step 50 Global step 50 Train loss 10.198138 on epoch=24
03/09/2022 03:18:33 - INFO - __main__ - Global step 50 Train loss 13.747702 QA-F1 0.020833333333333332 on epoch=24
03/09/2022 03:19:09 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.020833333333333332 on epoch=24, global_step=50
03/09/2022 03:19:15 - INFO - __main__ - Step 60 Global step 60 Train loss 9.099164 on epoch=29
03/09/2022 03:19:21 - INFO - __main__ - Step 70 Global step 70 Train loss 9.477560 on epoch=34
03/09/2022 03:19:27 - INFO - __main__ - Step 80 Global step 80 Train loss 7.890245 on epoch=39
03/09/2022 03:19:33 - INFO - __main__ - Step 90 Global step 90 Train loss 7.168783 on epoch=44
03/09/2022 03:19:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.965667 on epoch=49
03/09/2022 03:19:40 - INFO - __main__ - Global step 100 Train loss 8.120284 QA-F1 0.0 on epoch=49
03/09/2022 03:19:45 - INFO - __main__ - Step 110 Global step 110 Train loss 6.666273 on epoch=54
03/09/2022 03:19:51 - INFO - __main__ - Step 120 Global step 120 Train loss 6.070429 on epoch=59
03/09/2022 03:19:57 - INFO - __main__ - Step 130 Global step 130 Train loss 6.385420 on epoch=64
03/09/2022 03:20:03 - INFO - __main__ - Step 140 Global step 140 Train loss 6.074580 on epoch=69
03/09/2022 03:20:09 - INFO - __main__ - Step 150 Global step 150 Train loss 5.830970 on epoch=74
03/09/2022 03:20:10 - INFO - __main__ - Global step 150 Train loss 6.205534 QA-F1 0.0 on epoch=74
03/09/2022 03:20:16 - INFO - __main__ - Step 160 Global step 160 Train loss 5.257508 on epoch=79
03/09/2022 03:20:22 - INFO - __main__ - Step 170 Global step 170 Train loss 4.487283 on epoch=84
03/09/2022 03:20:28 - INFO - __main__ - Step 180 Global step 180 Train loss 4.446721 on epoch=89
03/09/2022 03:20:34 - INFO - __main__ - Step 190 Global step 190 Train loss 3.582813 on epoch=94
03/09/2022 03:20:40 - INFO - __main__ - Step 200 Global step 200 Train loss 3.215651 on epoch=99
03/09/2022 03:20:41 - INFO - __main__ - Global step 200 Train loss 4.197996 QA-F1 0.0 on epoch=99
03/09/2022 03:20:47 - INFO - __main__ - Step 210 Global step 210 Train loss 2.756827 on epoch=104
03/09/2022 03:20:53 - INFO - __main__ - Step 220 Global step 220 Train loss 2.800903 on epoch=109
03/09/2022 03:20:59 - INFO - __main__ - Step 230 Global step 230 Train loss 2.713489 on epoch=114
03/09/2022 03:21:05 - INFO - __main__ - Step 240 Global step 240 Train loss 2.604265 on epoch=119
03/09/2022 03:21:11 - INFO - __main__ - Step 250 Global step 250 Train loss 2.728333 on epoch=124
03/09/2022 03:21:12 - INFO - __main__ - Global step 250 Train loss 2.720763 QA-F1 0.0 on epoch=124
03/09/2022 03:21:18 - INFO - __main__ - Step 260 Global step 260 Train loss 2.160805 on epoch=129
03/09/2022 03:21:24 - INFO - __main__ - Step 270 Global step 270 Train loss 2.652896 on epoch=134
03/09/2022 03:21:30 - INFO - __main__ - Step 280 Global step 280 Train loss 2.064854 on epoch=139
03/09/2022 03:21:36 - INFO - __main__ - Step 290 Global step 290 Train loss 2.014434 on epoch=144
03/09/2022 03:21:42 - INFO - __main__ - Step 300 Global step 300 Train loss 2.028996 on epoch=149
03/09/2022 03:21:42 - INFO - __main__ - Global step 300 Train loss 2.184397 QA-F1 0.0 on epoch=149
03/09/2022 03:21:48 - INFO - __main__ - Step 310 Global step 310 Train loss 2.124938 on epoch=154
03/09/2022 03:21:54 - INFO - __main__ - Step 320 Global step 320 Train loss 1.898075 on epoch=159
03/09/2022 03:22:00 - INFO - __main__ - Step 330 Global step 330 Train loss 1.824327 on epoch=164
03/09/2022 03:22:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.909781 on epoch=169
03/09/2022 03:22:12 - INFO - __main__ - Step 350 Global step 350 Train loss 1.842569 on epoch=174
03/09/2022 03:22:13 - INFO - __main__ - Global step 350 Train loss 1.919938 QA-F1 0.046875 on epoch=174
03/09/2022 03:22:49 - INFO - __main__ - Saving model with best QA-F1: 0.020833333333333332 -> 0.046875 on epoch=174, global_step=350
03/09/2022 03:22:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.936315 on epoch=179
03/09/2022 03:23:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.666713 on epoch=184
03/09/2022 03:23:07 - INFO - __main__ - Step 380 Global step 380 Train loss 1.760980 on epoch=189
03/09/2022 03:23:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.647292 on epoch=194
03/09/2022 03:23:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.676271 on epoch=199
03/09/2022 03:23:20 - INFO - __main__ - Global step 400 Train loss 1.737514 QA-F1 0.015625 on epoch=199
03/09/2022 03:23:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.568919 on epoch=204
03/09/2022 03:23:32 - INFO - __main__ - Step 420 Global step 420 Train loss 1.347069 on epoch=209
03/09/2022 03:23:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.382189 on epoch=214
03/09/2022 03:23:44 - INFO - __main__ - Step 440 Global step 440 Train loss 1.499285 on epoch=219
03/09/2022 03:23:50 - INFO - __main__ - Step 450 Global step 450 Train loss 1.684881 on epoch=224
03/09/2022 03:23:51 - INFO - __main__ - Global step 450 Train loss 1.496468 QA-F1 0.05625 on epoch=224
03/09/2022 03:24:27 - INFO - __main__ - Saving model with best QA-F1: 0.046875 -> 0.05625 on epoch=224, global_step=450
03/09/2022 03:24:33 - INFO - __main__ - Step 460 Global step 460 Train loss 1.267201 on epoch=229
03/09/2022 03:24:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.656395 on epoch=234
03/09/2022 03:24:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.365192 on epoch=239
03/09/2022 03:24:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.304062 on epoch=244
03/09/2022 03:24:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.206441 on epoch=249
03/09/2022 03:24:58 - INFO - __main__ - Global step 500 Train loss 0.559858 QA-F1 0.21875 on epoch=249
03/09/2022 03:25:34 - INFO - __main__ - Saving model with best QA-F1: 0.05625 -> 0.21875 on epoch=249, global_step=500
03/09/2022 03:25:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.223482 on epoch=254
03/09/2022 03:25:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.129756 on epoch=259
03/09/2022 03:25:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.150618 on epoch=264
03/09/2022 03:25:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.167884 on epoch=269
03/09/2022 03:26:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.079399 on epoch=274
03/09/2022 03:26:06 - INFO - __main__ - Global step 550 Train loss 0.150228 QA-F1 0.296875 on epoch=274
03/09/2022 03:26:42 - INFO - __main__ - Saving model with best QA-F1: 0.21875 -> 0.296875 on epoch=274, global_step=550
03/09/2022 03:26:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.181582 on epoch=279
03/09/2022 03:26:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.121877 on epoch=284
03/09/2022 03:27:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.291931 on epoch=289
03/09/2022 03:27:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.351160 on epoch=294
03/09/2022 03:27:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.231828 on epoch=299
03/09/2022 03:27:13 - INFO - __main__ - Global step 600 Train loss 0.235676 QA-F1 0.15625 on epoch=299
03/09/2022 03:27:13 - INFO - __main__ - save last model!
03/09/2022 03:27:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:27:13 - INFO - __main__ - Printing 3 examples
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:27:13 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:27:13 - INFO - __main__ - ['Up']
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:27:13 - INFO - __main__ - ['Norway']
03/09/2022 03:27:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:27:13 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:27:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:27:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:27:13 - INFO - __main__ - Printing 3 examples
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:27:13 - INFO - __main__ - ['George Gipp']
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:27:13 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:27:13 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:27:13 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:27:13 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:27:14 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:27:14 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:27:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:27:24 - INFO - __main__ - Starting training!
03/09/2022 03:27:55 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:27:55 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:27:55 - INFO - __main__ - Printing 3 examples
03/09/2022 03:27:55 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:27:55 - INFO - __main__ - ['Frankie']
03/09/2022 03:27:55 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:27:55 - INFO - __main__ - ['Frankie']
03/09/2022 03:27:55 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:27:55 - INFO - __main__ - ['Frankie']
03/09/2022 03:27:55 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:28:00 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:28:02 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:29:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_13_0.0003_8_predictions.txt
03/09/2022 03:29:18 - INFO - __main__ - QA-F1 on test data: 0.2362
03/09/2022 03:29:19 - INFO - __main__ - prefix=quoref_32_13, lr=0.0003, bsz=8, dev_performance=0.296875, test_performance=0.23617708456418135
03/09/2022 03:29:19 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.0002, bsz=8 ...
03/09/2022 03:29:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:29:20 - INFO - __main__ - Printing 3 examples
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:29:20 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:29:20 - INFO - __main__ - ['Up']
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:29:20 - INFO - __main__ - ['Norway']
03/09/2022 03:29:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:29:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:29:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:29:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:29:20 - INFO - __main__ - Printing 3 examples
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:29:20 - INFO - __main__ - ['George Gipp']
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:29:20 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:29:20 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:29:20 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:29:20 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:29:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:29:20 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:29:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:29:29 - INFO - __main__ - Starting training!
03/09/2022 03:29:34 - INFO - __main__ - Step 10 Global step 10 Train loss 19.614346 on epoch=4
03/09/2022 03:29:40 - INFO - __main__ - Step 20 Global step 20 Train loss 17.408136 on epoch=9
03/09/2022 03:29:46 - INFO - __main__ - Step 30 Global step 30 Train loss 14.397738 on epoch=14
03/09/2022 03:29:52 - INFO - __main__ - Step 40 Global step 40 Train loss 11.488472 on epoch=19
03/09/2022 03:29:58 - INFO - __main__ - Step 50 Global step 50 Train loss 10.387980 on epoch=24
03/09/2022 03:29:59 - INFO - __main__ - Global step 50 Train loss 14.659335 QA-F1 0.05208333333333333 on epoch=24
03/09/2022 03:30:35 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.05208333333333333 on epoch=24, global_step=50
03/09/2022 03:30:41 - INFO - __main__ - Step 60 Global step 60 Train loss 9.893884 on epoch=29
03/09/2022 03:30:47 - INFO - __main__ - Step 70 Global step 70 Train loss 9.306463 on epoch=34
03/09/2022 03:30:53 - INFO - __main__ - Step 80 Global step 80 Train loss 8.682258 on epoch=39
03/09/2022 03:30:59 - INFO - __main__ - Step 90 Global step 90 Train loss 8.439112 on epoch=44
03/09/2022 03:31:05 - INFO - __main__ - Step 100 Global step 100 Train loss 7.904432 on epoch=49
03/09/2022 03:31:07 - INFO - __main__ - Global step 100 Train loss 8.845230 QA-F1 0.03125 on epoch=49
03/09/2022 03:31:13 - INFO - __main__ - Step 110 Global step 110 Train loss 7.671102 on epoch=54
03/09/2022 03:31:19 - INFO - __main__ - Step 120 Global step 120 Train loss 7.271155 on epoch=59
03/09/2022 03:31:25 - INFO - __main__ - Step 130 Global step 130 Train loss 6.883966 on epoch=64
03/09/2022 03:31:31 - INFO - __main__ - Step 140 Global step 140 Train loss 6.828214 on epoch=69
03/09/2022 03:31:37 - INFO - __main__ - Step 150 Global step 150 Train loss 6.792619 on epoch=74
03/09/2022 03:31:38 - INFO - __main__ - Global step 150 Train loss 7.089411 QA-F1 0.0 on epoch=74
03/09/2022 03:31:44 - INFO - __main__ - Step 160 Global step 160 Train loss 6.213355 on epoch=79
03/09/2022 03:31:50 - INFO - __main__ - Step 170 Global step 170 Train loss 5.931093 on epoch=84
03/09/2022 03:31:55 - INFO - __main__ - Step 180 Global step 180 Train loss 5.604136 on epoch=89
03/09/2022 03:32:01 - INFO - __main__ - Step 190 Global step 190 Train loss 5.460567 on epoch=94
03/09/2022 03:32:07 - INFO - __main__ - Step 200 Global step 200 Train loss 4.950524 on epoch=99
03/09/2022 03:32:09 - INFO - __main__ - Global step 200 Train loss 5.631936 QA-F1 0.0 on epoch=99
03/09/2022 03:32:14 - INFO - __main__ - Step 210 Global step 210 Train loss 5.011226 on epoch=104
03/09/2022 03:32:20 - INFO - __main__ - Step 220 Global step 220 Train loss 4.715357 on epoch=109
03/09/2022 03:32:26 - INFO - __main__ - Step 230 Global step 230 Train loss 4.356696 on epoch=114
03/09/2022 03:32:32 - INFO - __main__ - Step 240 Global step 240 Train loss 3.766259 on epoch=119
03/09/2022 03:32:38 - INFO - __main__ - Step 250 Global step 250 Train loss 3.632708 on epoch=124
03/09/2022 03:32:40 - INFO - __main__ - Global step 250 Train loss 4.296449 QA-F1 0.0 on epoch=124
03/09/2022 03:32:46 - INFO - __main__ - Step 260 Global step 260 Train loss 2.973919 on epoch=129
03/09/2022 03:32:51 - INFO - __main__ - Step 270 Global step 270 Train loss 2.964735 on epoch=134
03/09/2022 03:32:57 - INFO - __main__ - Step 280 Global step 280 Train loss 2.665164 on epoch=139
03/09/2022 03:33:03 - INFO - __main__ - Step 290 Global step 290 Train loss 2.771868 on epoch=144
03/09/2022 03:33:09 - INFO - __main__ - Step 300 Global step 300 Train loss 2.723422 on epoch=149
03/09/2022 03:33:10 - INFO - __main__ - Global step 300 Train loss 2.819821 QA-F1 0.03125 on epoch=149
03/09/2022 03:33:16 - INFO - __main__ - Step 310 Global step 310 Train loss 2.760536 on epoch=154
03/09/2022 03:33:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.390884 on epoch=159
03/09/2022 03:33:28 - INFO - __main__ - Step 330 Global step 330 Train loss 2.409367 on epoch=164
03/09/2022 03:33:34 - INFO - __main__ - Step 340 Global step 340 Train loss 2.425347 on epoch=169
03/09/2022 03:33:40 - INFO - __main__ - Step 350 Global step 350 Train loss 2.235721 on epoch=174
03/09/2022 03:33:41 - INFO - __main__ - Global step 350 Train loss 2.444371 QA-F1 0.03125 on epoch=174
03/09/2022 03:33:47 - INFO - __main__ - Step 360 Global step 360 Train loss 2.290188 on epoch=179
03/09/2022 03:33:53 - INFO - __main__ - Step 370 Global step 370 Train loss 2.145508 on epoch=184
03/09/2022 03:33:58 - INFO - __main__ - Step 380 Global step 380 Train loss 2.125207 on epoch=189
03/09/2022 03:34:04 - INFO - __main__ - Step 390 Global step 390 Train loss 2.057905 on epoch=194
03/09/2022 03:34:10 - INFO - __main__ - Step 400 Global step 400 Train loss 1.850920 on epoch=199
03/09/2022 03:34:11 - INFO - __main__ - Global step 400 Train loss 2.093946 QA-F1 0.03125 on epoch=199
03/09/2022 03:34:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.946743 on epoch=204
03/09/2022 03:34:23 - INFO - __main__ - Step 420 Global step 420 Train loss 1.927704 on epoch=209
03/09/2022 03:34:29 - INFO - __main__ - Step 430 Global step 430 Train loss 2.008113 on epoch=214
03/09/2022 03:34:35 - INFO - __main__ - Step 440 Global step 440 Train loss 1.971696 on epoch=219
03/09/2022 03:34:41 - INFO - __main__ - Step 450 Global step 450 Train loss 1.714455 on epoch=224
03/09/2022 03:34:42 - INFO - __main__ - Global step 450 Train loss 1.913743 QA-F1 0.078125 on epoch=224
03/09/2022 03:35:18 - INFO - __main__ - Saving model with best QA-F1: 0.05208333333333333 -> 0.078125 on epoch=224, global_step=450
03/09/2022 03:35:24 - INFO - __main__ - Step 460 Global step 460 Train loss 1.906593 on epoch=229
03/09/2022 03:35:30 - INFO - __main__ - Step 470 Global step 470 Train loss 1.597174 on epoch=234
03/09/2022 03:35:36 - INFO - __main__ - Step 480 Global step 480 Train loss 1.595264 on epoch=239
03/09/2022 03:35:42 - INFO - __main__ - Step 490 Global step 490 Train loss 1.588560 on epoch=244
03/09/2022 03:35:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.706552 on epoch=249
03/09/2022 03:35:49 - INFO - __main__ - Global step 500 Train loss 1.678829 QA-F1 0.078125 on epoch=249
03/09/2022 03:35:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.467501 on epoch=254
03/09/2022 03:36:01 - INFO - __main__ - Step 520 Global step 520 Train loss 1.508110 on epoch=259
03/09/2022 03:36:07 - INFO - __main__ - Step 530 Global step 530 Train loss 1.544157 on epoch=264
03/09/2022 03:36:13 - INFO - __main__ - Step 540 Global step 540 Train loss 1.947512 on epoch=269
03/09/2022 03:36:19 - INFO - __main__ - Step 550 Global step 550 Train loss 1.447306 on epoch=274
03/09/2022 03:36:20 - INFO - __main__ - Global step 550 Train loss 1.582917 QA-F1 0.046875 on epoch=274
03/09/2022 03:36:26 - INFO - __main__ - Step 560 Global step 560 Train loss 1.140550 on epoch=279
03/09/2022 03:36:32 - INFO - __main__ - Step 570 Global step 570 Train loss 1.303986 on epoch=284
03/09/2022 03:36:38 - INFO - __main__ - Step 580 Global step 580 Train loss 1.170774 on epoch=289
03/09/2022 03:36:44 - INFO - __main__ - Step 590 Global step 590 Train loss 1.167507 on epoch=294
03/09/2022 03:36:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.961772 on epoch=299
03/09/2022 03:36:51 - INFO - __main__ - Global step 600 Train loss 1.148918 QA-F1 0.078125 on epoch=299
03/09/2022 03:36:51 - INFO - __main__ - save last model!
03/09/2022 03:36:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:36:52 - INFO - __main__ - Printing 3 examples
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:36:52 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:36:52 - INFO - __main__ - ['Up']
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:36:52 - INFO - __main__ - ['Norway']
03/09/2022 03:36:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:36:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:36:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:36:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:36:52 - INFO - __main__ - Printing 3 examples
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:36:52 - INFO - __main__ - ['George Gipp']
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:36:52 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:36:52 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:36:52 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:36:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:36:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:36:52 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:37:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:37:02 - INFO - __main__ - Starting training!
03/09/2022 03:37:32 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:37:33 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:37:33 - INFO - __main__ - Printing 3 examples
03/09/2022 03:37:33 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:37:33 - INFO - __main__ - ['Frankie']
03/09/2022 03:37:33 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:37:33 - INFO - __main__ - ['Frankie']
03/09/2022 03:37:33 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:37:33 - INFO - __main__ - ['Frankie']
03/09/2022 03:37:33 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:37:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:37:40 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:38:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_13_0.0002_8_predictions.txt
03/09/2022 03:38:42 - INFO - __main__ - QA-F1 on test data: 0.0061
03/09/2022 03:38:42 - INFO - __main__ - prefix=quoref_32_13, lr=0.0002, bsz=8, dev_performance=0.078125, test_performance=0.006065618968844775
03/09/2022 03:38:42 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.0001, bsz=8 ...
03/09/2022 03:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:38:43 - INFO - __main__ - Printing 3 examples
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/09/2022 03:38:43 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/09/2022 03:38:43 - INFO - __main__ - ['Up']
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/09/2022 03:38:43 - INFO - __main__ - ['Norway']
03/09/2022 03:38:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:38:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:38:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:38:43 - INFO - __main__ - Printing 3 examples
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/09/2022 03:38:43 - INFO - __main__ - ['George Gipp']
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/09/2022 03:38:43 - INFO - __main__ - ['The Blooming Baroque']
03/09/2022 03:38:43 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/09/2022 03:38:43 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/09/2022 03:38:43 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:38:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:38:43 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:38:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:38:53 - INFO - __main__ - Starting training!
03/09/2022 03:38:59 - INFO - __main__ - Step 10 Global step 10 Train loss 19.786325 on epoch=4
03/09/2022 03:39:04 - INFO - __main__ - Step 20 Global step 20 Train loss 18.922392 on epoch=9
03/09/2022 03:39:10 - INFO - __main__ - Step 30 Global step 30 Train loss 13.891803 on epoch=14
03/09/2022 03:39:16 - INFO - __main__ - Step 40 Global step 40 Train loss 11.324595 on epoch=19
03/09/2022 03:39:22 - INFO - __main__ - Step 50 Global step 50 Train loss 10.315855 on epoch=24
03/09/2022 03:39:24 - INFO - __main__ - Global step 50 Train loss 14.848194 QA-F1 0.02455357142857143 on epoch=24
03/09/2022 03:40:01 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.02455357142857143 on epoch=24, global_step=50
03/09/2022 03:40:07 - INFO - __main__ - Step 60 Global step 60 Train loss 9.826002 on epoch=29
03/09/2022 03:40:13 - INFO - __main__ - Step 70 Global step 70 Train loss 9.144794 on epoch=34
03/09/2022 03:40:19 - INFO - __main__ - Step 80 Global step 80 Train loss 8.899905 on epoch=39
03/09/2022 03:40:25 - INFO - __main__ - Step 90 Global step 90 Train loss 8.923014 on epoch=44
03/09/2022 03:40:31 - INFO - __main__ - Step 100 Global step 100 Train loss 8.351332 on epoch=49
03/09/2022 03:40:32 - INFO - __main__ - Global step 100 Train loss 9.029010 QA-F1 0.0625 on epoch=49
03/09/2022 03:41:09 - INFO - __main__ - Saving model with best QA-F1: 0.02455357142857143 -> 0.0625 on epoch=49, global_step=100
03/09/2022 03:41:15 - INFO - __main__ - Step 110 Global step 110 Train loss 8.187224 on epoch=54
03/09/2022 03:41:21 - INFO - __main__ - Step 120 Global step 120 Train loss 7.979393 on epoch=59
03/09/2022 03:41:27 - INFO - __main__ - Step 130 Global step 130 Train loss 7.954412 on epoch=64
03/09/2022 03:41:33 - INFO - __main__ - Step 140 Global step 140 Train loss 8.499731 on epoch=69
03/09/2022 03:41:39 - INFO - __main__ - Step 150 Global step 150 Train loss 7.807955 on epoch=74
03/09/2022 03:41:40 - INFO - __main__ - Global step 150 Train loss 8.085743 QA-F1 0.020833333333333332 on epoch=74
03/09/2022 03:41:46 - INFO - __main__ - Step 160 Global step 160 Train loss 7.400184 on epoch=79
03/09/2022 03:41:52 - INFO - __main__ - Step 170 Global step 170 Train loss 7.534085 on epoch=84
03/09/2022 03:41:58 - INFO - __main__ - Step 180 Global step 180 Train loss 7.180700 on epoch=89
03/09/2022 03:42:04 - INFO - __main__ - Step 190 Global step 190 Train loss 7.313670 on epoch=94
03/09/2022 03:42:10 - INFO - __main__ - Step 200 Global step 200 Train loss 7.318110 on epoch=99
03/09/2022 03:42:11 - INFO - __main__ - Global step 200 Train loss 7.349350 QA-F1 0.0 on epoch=99
03/09/2022 03:42:17 - INFO - __main__ - Step 210 Global step 210 Train loss 6.869586 on epoch=104
03/09/2022 03:42:23 - INFO - __main__ - Step 220 Global step 220 Train loss 7.095587 on epoch=109
03/09/2022 03:42:29 - INFO - __main__ - Step 230 Global step 230 Train loss 6.981214 on epoch=114
03/09/2022 03:42:35 - INFO - __main__ - Step 240 Global step 240 Train loss 6.855250 on epoch=119
03/09/2022 03:42:41 - INFO - __main__ - Step 250 Global step 250 Train loss 6.469301 on epoch=124
03/09/2022 03:42:42 - INFO - __main__ - Global step 250 Train loss 6.854188 QA-F1 0.0 on epoch=124
03/09/2022 03:42:48 - INFO - __main__ - Step 260 Global step 260 Train loss 6.495808 on epoch=129
03/09/2022 03:42:54 - INFO - __main__ - Step 270 Global step 270 Train loss 6.487351 on epoch=134
03/09/2022 03:43:00 - INFO - __main__ - Step 280 Global step 280 Train loss 6.303208 on epoch=139
03/09/2022 03:43:06 - INFO - __main__ - Step 290 Global step 290 Train loss 6.345886 on epoch=144
03/09/2022 03:43:12 - INFO - __main__ - Step 300 Global step 300 Train loss 6.415033 on epoch=149
03/09/2022 03:43:14 - INFO - __main__ - Global step 300 Train loss 6.409457 QA-F1 0.0 on epoch=149
03/09/2022 03:43:20 - INFO - __main__ - Step 310 Global step 310 Train loss 5.765653 on epoch=154
03/09/2022 03:43:26 - INFO - __main__ - Step 320 Global step 320 Train loss 5.859910 on epoch=159
03/09/2022 03:43:32 - INFO - __main__ - Step 330 Global step 330 Train loss 5.495133 on epoch=164
03/09/2022 03:43:38 - INFO - __main__ - Step 340 Global step 340 Train loss 5.626143 on epoch=169
03/09/2022 03:43:44 - INFO - __main__ - Step 350 Global step 350 Train loss 5.477500 on epoch=174
03/09/2022 03:43:45 - INFO - __main__ - Global step 350 Train loss 5.644868 QA-F1 0.015625 on epoch=174
03/09/2022 03:43:51 - INFO - __main__ - Step 360 Global step 360 Train loss 5.243207 on epoch=179
03/09/2022 03:43:57 - INFO - __main__ - Step 370 Global step 370 Train loss 5.211543 on epoch=184
03/09/2022 03:44:03 - INFO - __main__ - Step 380 Global step 380 Train loss 4.615748 on epoch=189
03/09/2022 03:44:09 - INFO - __main__ - Step 390 Global step 390 Train loss 4.831286 on epoch=194
03/09/2022 03:44:15 - INFO - __main__ - Step 400 Global step 400 Train loss 3.673555 on epoch=199
03/09/2022 03:44:16 - INFO - __main__ - Global step 400 Train loss 4.715068 QA-F1 0.0 on epoch=199
03/09/2022 03:44:22 - INFO - __main__ - Step 410 Global step 410 Train loss 3.860622 on epoch=204
03/09/2022 03:44:28 - INFO - __main__ - Step 420 Global step 420 Train loss 3.687954 on epoch=209
03/09/2022 03:44:34 - INFO - __main__ - Step 430 Global step 430 Train loss 3.658953 on epoch=214
03/09/2022 03:44:40 - INFO - __main__ - Step 440 Global step 440 Train loss 3.733325 on epoch=219
03/09/2022 03:44:46 - INFO - __main__ - Step 450 Global step 450 Train loss 3.155664 on epoch=224
03/09/2022 03:44:47 - INFO - __main__ - Global step 450 Train loss 3.619304 QA-F1 0.0 on epoch=224
03/09/2022 03:44:53 - INFO - __main__ - Step 460 Global step 460 Train loss 3.520025 on epoch=229
03/09/2022 03:44:59 - INFO - __main__ - Step 470 Global step 470 Train loss 3.591246 on epoch=234
03/09/2022 03:45:05 - INFO - __main__ - Step 480 Global step 480 Train loss 3.103206 on epoch=239
03/09/2022 03:45:11 - INFO - __main__ - Step 490 Global step 490 Train loss 2.946859 on epoch=244
03/09/2022 03:45:17 - INFO - __main__ - Step 500 Global step 500 Train loss 2.658363 on epoch=249
03/09/2022 03:45:18 - INFO - __main__ - Global step 500 Train loss 3.163940 QA-F1 0.0 on epoch=249
03/09/2022 03:45:24 - INFO - __main__ - Step 510 Global step 510 Train loss 3.184087 on epoch=254
03/09/2022 03:45:30 - INFO - __main__ - Step 520 Global step 520 Train loss 2.882890 on epoch=259
03/09/2022 03:45:36 - INFO - __main__ - Step 530 Global step 530 Train loss 2.829278 on epoch=264
03/09/2022 03:45:42 - INFO - __main__ - Step 540 Global step 540 Train loss 2.935047 on epoch=269
03/09/2022 03:45:48 - INFO - __main__ - Step 550 Global step 550 Train loss 3.126545 on epoch=274
03/09/2022 03:45:49 - INFO - __main__ - Global step 550 Train loss 2.991570 QA-F1 0.020833333333333332 on epoch=274
03/09/2022 03:45:55 - INFO - __main__ - Step 560 Global step 560 Train loss 2.537611 on epoch=279
03/09/2022 03:46:01 - INFO - __main__ - Step 570 Global step 570 Train loss 2.690672 on epoch=284
03/09/2022 03:46:07 - INFO - __main__ - Step 580 Global step 580 Train loss 2.348068 on epoch=289
03/09/2022 03:46:13 - INFO - __main__ - Step 590 Global step 590 Train loss 2.611821 on epoch=294
03/09/2022 03:46:18 - INFO - __main__ - Step 600 Global step 600 Train loss 2.669378 on epoch=299
03/09/2022 03:46:19 - INFO - __main__ - Global step 600 Train loss 2.571510 QA-F1 0.05208333333333333 on epoch=299
03/09/2022 03:46:19 - INFO - __main__ - save last model!
03/09/2022 03:46:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:46:21 - INFO - __main__ - Printing 3 examples
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 03:46:21 - INFO - __main__ - ['Allan']
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 03:46:21 - INFO - __main__ - ['Nigro']
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 03:46:21 - INFO - __main__ - ['Minogue']
03/09/2022 03:46:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:46:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:46:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:46:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:46:21 - INFO - __main__ - Printing 3 examples
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 03:46:21 - INFO - __main__ - ['Alan']
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 03:46:21 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 03:46:21 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 03:46:21 - INFO - __main__ - ['Albert']
03/09/2022 03:46:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:46:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:46:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:46:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:46:32 - INFO - __main__ - Starting training!
03/09/2022 03:47:00 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:47:01 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:47:01 - INFO - __main__ - Printing 3 examples
03/09/2022 03:47:01 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:47:01 - INFO - __main__ - ['Frankie']
03/09/2022 03:47:01 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:47:01 - INFO - __main__ - ['Frankie']
03/09/2022 03:47:01 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:47:01 - INFO - __main__ - ['Frankie']
03/09/2022 03:47:01 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:47:05 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:47:08 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:48:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_13_0.0001_8_predictions.txt
03/09/2022 03:48:43 - INFO - __main__ - QA-F1 on test data: 0.0223
03/09/2022 03:48:43 - INFO - __main__ - prefix=quoref_32_13, lr=0.0001, bsz=8, dev_performance=0.0625, test_performance=0.02226590613687388
03/09/2022 03:48:43 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.0005, bsz=8 ...
03/09/2022 03:48:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:48:44 - INFO - __main__ - Printing 3 examples
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 03:48:44 - INFO - __main__ - ['Allan']
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 03:48:44 - INFO - __main__ - ['Nigro']
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 03:48:44 - INFO - __main__ - ['Minogue']
03/09/2022 03:48:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:48:44 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:48:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:48:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:48:44 - INFO - __main__ - Printing 3 examples
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 03:48:44 - INFO - __main__ - ['Alan']
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 03:48:44 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 03:48:44 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 03:48:44 - INFO - __main__ - ['Albert']
03/09/2022 03:48:44 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:48:44 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:48:44 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:48:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:48:53 - INFO - __main__ - Starting training!
03/09/2022 03:48:58 - INFO - __main__ - Step 10 Global step 10 Train loss 18.519920 on epoch=4
03/09/2022 03:49:03 - INFO - __main__ - Step 20 Global step 20 Train loss 19.532505 on epoch=9
03/09/2022 03:49:09 - INFO - __main__ - Step 30 Global step 30 Train loss 12.176453 on epoch=14
03/09/2022 03:49:15 - INFO - __main__ - Step 40 Global step 40 Train loss 7.218570 on epoch=19
03/09/2022 03:49:21 - INFO - __main__ - Step 50 Global step 50 Train loss 6.654115 on epoch=24
03/09/2022 03:49:23 - INFO - __main__ - Global step 50 Train loss 12.820312 QA-F1 0.04241071428571429 on epoch=24
03/09/2022 03:49:58 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.04241071428571429 on epoch=24, global_step=50
03/09/2022 03:50:04 - INFO - __main__ - Step 60 Global step 60 Train loss 6.435396 on epoch=29
03/09/2022 03:50:10 - INFO - __main__ - Step 70 Global step 70 Train loss 5.483201 on epoch=34
03/09/2022 03:50:16 - INFO - __main__ - Step 80 Global step 80 Train loss 5.013272 on epoch=39
03/09/2022 03:50:22 - INFO - __main__ - Step 90 Global step 90 Train loss 4.548862 on epoch=44
03/09/2022 03:50:28 - INFO - __main__ - Step 100 Global step 100 Train loss 3.862677 on epoch=49
03/09/2022 03:50:29 - INFO - __main__ - Global step 100 Train loss 5.068682 QA-F1 0.0 on epoch=49
03/09/2022 03:50:35 - INFO - __main__ - Step 110 Global step 110 Train loss 3.365657 on epoch=54
03/09/2022 03:50:41 - INFO - __main__ - Step 120 Global step 120 Train loss 2.769489 on epoch=59
03/09/2022 03:50:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.566556 on epoch=64
03/09/2022 03:50:53 - INFO - __main__ - Step 140 Global step 140 Train loss 2.690623 on epoch=69
03/09/2022 03:50:59 - INFO - __main__ - Step 150 Global step 150 Train loss 2.059792 on epoch=74
03/09/2022 03:51:00 - INFO - __main__ - Global step 150 Train loss 2.690423 QA-F1 0.0 on epoch=74
03/09/2022 03:51:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.104815 on epoch=79
03/09/2022 03:51:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.809721 on epoch=84
03/09/2022 03:51:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.989240 on epoch=89
03/09/2022 03:51:24 - INFO - __main__ - Step 190 Global step 190 Train loss 1.701443 on epoch=94
03/09/2022 03:51:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.543017 on epoch=99
03/09/2022 03:51:33 - INFO - __main__ - Global step 200 Train loss 1.829647 QA-F1 0.03854166666666667 on epoch=99
03/09/2022 03:51:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.790350 on epoch=104
03/09/2022 03:51:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.963596 on epoch=109
03/09/2022 03:51:51 - INFO - __main__ - Step 230 Global step 230 Train loss 1.362677 on epoch=114
03/09/2022 03:51:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.739094 on epoch=119
03/09/2022 03:52:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.561616 on epoch=124
03/09/2022 03:52:04 - INFO - __main__ - Global step 250 Train loss 1.683466 QA-F1 0.020833333333333332 on epoch=124
03/09/2022 03:52:10 - INFO - __main__ - Step 260 Global step 260 Train loss 1.422039 on epoch=129
03/09/2022 03:52:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.273478 on epoch=134
03/09/2022 03:52:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.284840 on epoch=139
03/09/2022 03:52:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.201422 on epoch=144
03/09/2022 03:52:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.175403 on epoch=149
03/09/2022 03:52:35 - INFO - __main__ - Global step 300 Train loss 1.271437 QA-F1 0.0 on epoch=149
03/09/2022 03:52:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.211013 on epoch=154
03/09/2022 03:52:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.226420 on epoch=159
03/09/2022 03:52:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.031868 on epoch=164
03/09/2022 03:52:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.030741 on epoch=169
03/09/2022 03:53:05 - INFO - __main__ - Step 350 Global step 350 Train loss 1.121261 on epoch=174
03/09/2022 03:53:06 - INFO - __main__ - Global step 350 Train loss 1.124261 QA-F1 0.0 on epoch=174
03/09/2022 03:53:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.943837 on epoch=179
03/09/2022 03:53:18 - INFO - __main__ - Step 370 Global step 370 Train loss 1.015041 on epoch=184
03/09/2022 03:53:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.895452 on epoch=189
03/09/2022 03:53:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.907506 on epoch=194
03/09/2022 03:53:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.915342 on epoch=199
03/09/2022 03:53:37 - INFO - __main__ - Global step 400 Train loss 0.935436 QA-F1 0.0 on epoch=199
03/09/2022 03:53:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.890796 on epoch=204
03/09/2022 03:53:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.934570 on epoch=209
03/09/2022 03:53:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.881773 on epoch=214
03/09/2022 03:54:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.848461 on epoch=219
03/09/2022 03:54:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.875802 on epoch=224
03/09/2022 03:54:08 - INFO - __main__ - Global step 450 Train loss 0.886280 QA-F1 0.0 on epoch=224
03/09/2022 03:54:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.917567 on epoch=229
03/09/2022 03:54:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.773678 on epoch=234
03/09/2022 03:54:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.780675 on epoch=239
03/09/2022 03:54:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.783235 on epoch=244
03/09/2022 03:54:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.762273 on epoch=249
03/09/2022 03:54:39 - INFO - __main__ - Global step 500 Train loss 0.803486 QA-F1 0.0 on epoch=249
03/09/2022 03:54:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.665854 on epoch=254
03/09/2022 03:54:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.787174 on epoch=259
03/09/2022 03:54:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.911212 on epoch=264
03/09/2022 03:55:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.572558 on epoch=269
03/09/2022 03:55:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.458191 on epoch=274
03/09/2022 03:55:10 - INFO - __main__ - Global step 550 Train loss 0.678998 QA-F1 0.0 on epoch=274
03/09/2022 03:55:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.347899 on epoch=279
03/09/2022 03:55:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.192621 on epoch=284
03/09/2022 03:55:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.146094 on epoch=289
03/09/2022 03:55:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.134663 on epoch=294
03/09/2022 03:55:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.134261 on epoch=299
03/09/2022 03:55:41 - INFO - __main__ - Global step 600 Train loss 0.191107 QA-F1 0.0 on epoch=299
03/09/2022 03:55:41 - INFO - __main__ - save last model!
03/09/2022 03:55:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:55:41 - INFO - __main__ - Printing 3 examples
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 03:55:41 - INFO - __main__ - ['Allan']
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 03:55:41 - INFO - __main__ - ['Nigro']
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 03:55:41 - INFO - __main__ - ['Minogue']
03/09/2022 03:55:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 03:55:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:55:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:55:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:55:41 - INFO - __main__ - Printing 3 examples
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 03:55:41 - INFO - __main__ - ['Alan']
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 03:55:41 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 03:55:41 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 03:55:41 - INFO - __main__ - ['Albert']
03/09/2022 03:55:41 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:55:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:55:41 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:55:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:55:52 - INFO - __main__ - Starting training!
03/09/2022 03:56:22 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 03:56:23 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 03:56:23 - INFO - __main__ - Printing 3 examples
03/09/2022 03:56:23 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:56:23 - INFO - __main__ - ['Frankie']
03/09/2022 03:56:23 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:56:23 - INFO - __main__ - ['Frankie']
03/09/2022 03:56:23 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 03:56:23 - INFO - __main__ - ['Frankie']
03/09/2022 03:56:23 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:56:27 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:56:30 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 03:58:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_21_0.0005_8_predictions.txt
03/09/2022 03:58:05 - INFO - __main__ - QA-F1 on test data: 0.0241
03/09/2022 03:58:06 - INFO - __main__ - prefix=quoref_32_21, lr=0.0005, bsz=8, dev_performance=0.04241071428571429, test_performance=0.024108716850652332
03/09/2022 03:58:06 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.0003, bsz=8 ...
03/09/2022 03:58:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:58:07 - INFO - __main__ - Printing 3 examples
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 03:58:07 - INFO - __main__ - ['Allan']
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 03:58:07 - INFO - __main__ - ['Nigro']
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 03:58:07 - INFO - __main__ - ['Minogue']
03/09/2022 03:58:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 03:58:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:58:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 03:58:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 03:58:07 - INFO - __main__ - Printing 3 examples
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 03:58:07 - INFO - __main__ - ['Alan']
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 03:58:07 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 03:58:07 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 03:58:07 - INFO - __main__ - ['Albert']
03/09/2022 03:58:07 - INFO - __main__ - Tokenizing Input ...
03/09/2022 03:58:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 03:58:07 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 03:58:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 03:58:17 - INFO - __main__ - Starting training!
03/09/2022 03:58:24 - INFO - __main__ - Step 10 Global step 10 Train loss 17.062582 on epoch=4
03/09/2022 03:58:30 - INFO - __main__ - Step 20 Global step 20 Train loss 12.783976 on epoch=9
03/09/2022 03:58:36 - INFO - __main__ - Step 30 Global step 30 Train loss 7.709102 on epoch=14
03/09/2022 03:58:42 - INFO - __main__ - Step 40 Global step 40 Train loss 6.978751 on epoch=19
03/09/2022 03:58:48 - INFO - __main__ - Step 50 Global step 50 Train loss 6.847214 on epoch=24
03/09/2022 03:58:50 - INFO - __main__ - Global step 50 Train loss 10.276325 QA-F1 0.0 on epoch=24
03/09/2022 03:59:25 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 03:59:31 - INFO - __main__ - Step 60 Global step 60 Train loss 6.687833 on epoch=29
03/09/2022 03:59:37 - INFO - __main__ - Step 70 Global step 70 Train loss 5.972879 on epoch=34
03/09/2022 03:59:43 - INFO - __main__ - Step 80 Global step 80 Train loss 5.990162 on epoch=39
03/09/2022 03:59:49 - INFO - __main__ - Step 90 Global step 90 Train loss 5.539386 on epoch=44
03/09/2022 03:59:55 - INFO - __main__ - Step 100 Global step 100 Train loss 5.148571 on epoch=49
03/09/2022 03:59:57 - INFO - __main__ - Global step 100 Train loss 5.867766 QA-F1 0.006944444444444445 on epoch=49
03/09/2022 04:00:32 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.006944444444444445 on epoch=49, global_step=100
03/09/2022 04:00:38 - INFO - __main__ - Step 110 Global step 110 Train loss 5.012373 on epoch=54
03/09/2022 04:00:44 - INFO - __main__ - Step 120 Global step 120 Train loss 4.443606 on epoch=59
03/09/2022 04:00:50 - INFO - __main__ - Step 130 Global step 130 Train loss 4.211223 on epoch=64
03/09/2022 04:00:56 - INFO - __main__ - Step 140 Global step 140 Train loss 4.328286 on epoch=69
03/09/2022 04:01:02 - INFO - __main__ - Step 150 Global step 150 Train loss 3.614793 on epoch=74
03/09/2022 04:01:03 - INFO - __main__ - Global step 150 Train loss 4.322056 QA-F1 0.0 on epoch=74
03/09/2022 04:01:10 - INFO - __main__ - Step 160 Global step 160 Train loss 3.044468 on epoch=79
03/09/2022 04:01:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.683705 on epoch=84
03/09/2022 04:01:22 - INFO - __main__ - Step 180 Global step 180 Train loss 2.391830 on epoch=89
03/09/2022 04:01:28 - INFO - __main__ - Step 190 Global step 190 Train loss 2.306928 on epoch=94
03/09/2022 04:01:34 - INFO - __main__ - Step 200 Global step 200 Train loss 2.179886 on epoch=99
03/09/2022 04:01:35 - INFO - __main__ - Global step 200 Train loss 2.521363 QA-F1 0.015625 on epoch=99
03/09/2022 04:02:10 - INFO - __main__ - Saving model with best QA-F1: 0.006944444444444445 -> 0.015625 on epoch=99, global_step=200
03/09/2022 04:02:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.206490 on epoch=104
03/09/2022 04:02:23 - INFO - __main__ - Step 220 Global step 220 Train loss 2.039606 on epoch=109
03/09/2022 04:02:29 - INFO - __main__ - Step 230 Global step 230 Train loss 1.905023 on epoch=114
03/09/2022 04:02:35 - INFO - __main__ - Step 240 Global step 240 Train loss 1.761439 on epoch=119
03/09/2022 04:02:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.782458 on epoch=124
03/09/2022 04:02:42 - INFO - __main__ - Global step 250 Train loss 1.939003 QA-F1 0.0 on epoch=124
03/09/2022 04:02:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.578213 on epoch=129
03/09/2022 04:02:54 - INFO - __main__ - Step 270 Global step 270 Train loss 1.562556 on epoch=134
03/09/2022 04:03:00 - INFO - __main__ - Step 280 Global step 280 Train loss 1.526061 on epoch=139
03/09/2022 04:03:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.396225 on epoch=144
03/09/2022 04:03:11 - INFO - __main__ - Step 300 Global step 300 Train loss 1.293975 on epoch=149
03/09/2022 04:03:13 - INFO - __main__ - Global step 300 Train loss 1.471406 QA-F1 0.015625 on epoch=149
03/09/2022 04:03:19 - INFO - __main__ - Step 310 Global step 310 Train loss 1.093900 on epoch=154
03/09/2022 04:03:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.299270 on epoch=159
03/09/2022 04:03:31 - INFO - __main__ - Step 330 Global step 330 Train loss 1.125443 on epoch=164
03/09/2022 04:03:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.116819 on epoch=169
03/09/2022 04:03:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.956387 on epoch=174
03/09/2022 04:03:44 - INFO - __main__ - Global step 350 Train loss 1.118364 QA-F1 0.057291666666666664 on epoch=174
03/09/2022 04:04:19 - INFO - __main__ - Saving model with best QA-F1: 0.015625 -> 0.057291666666666664 on epoch=174, global_step=350
03/09/2022 04:04:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.588846 on epoch=179
03/09/2022 04:04:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.358294 on epoch=184
03/09/2022 04:04:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054000 on epoch=189
03/09/2022 04:04:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.051450 on epoch=194
03/09/2022 04:04:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015751 on epoch=199
03/09/2022 04:04:49 - INFO - __main__ - Global step 400 Train loss 0.213668 QA-F1 0.0 on epoch=199
03/09/2022 04:04:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014815 on epoch=204
03/09/2022 04:05:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004316 on epoch=209
03/09/2022 04:05:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.057080 on epoch=214
03/09/2022 04:05:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.009847 on epoch=219
03/09/2022 04:05:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004169 on epoch=224
03/09/2022 04:05:20 - INFO - __main__ - Global step 450 Train loss 0.018045 QA-F1 0.0 on epoch=224
03/09/2022 04:05:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.005209 on epoch=229
03/09/2022 04:05:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.009139 on epoch=234
03/09/2022 04:05:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.042545 on epoch=239
03/09/2022 04:05:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004436 on epoch=244
03/09/2022 04:05:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003942 on epoch=249
03/09/2022 04:05:51 - INFO - __main__ - Global step 500 Train loss 0.013054 QA-F1 0.020833333333333332 on epoch=249
03/09/2022 04:05:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002181 on epoch=254
03/09/2022 04:06:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000829 on epoch=259
03/09/2022 04:06:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001777 on epoch=264
03/09/2022 04:06:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003021 on epoch=269
03/09/2022 04:06:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003329 on epoch=274
03/09/2022 04:06:22 - INFO - __main__ - Global step 550 Train loss 0.002227 QA-F1 0.0 on epoch=274
03/09/2022 04:06:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000979 on epoch=279
03/09/2022 04:06:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001522 on epoch=284
03/09/2022 04:06:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000772 on epoch=289
03/09/2022 04:06:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000575 on epoch=294
03/09/2022 04:06:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000679 on epoch=299
03/09/2022 04:06:53 - INFO - __main__ - Global step 600 Train loss 0.000905 QA-F1 0.0 on epoch=299
03/09/2022 04:06:53 - INFO - __main__ - save last model!
03/09/2022 04:06:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:06:53 - INFO - __main__ - Printing 3 examples
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 04:06:53 - INFO - __main__ - ['Allan']
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 04:06:53 - INFO - __main__ - ['Nigro']
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 04:06:53 - INFO - __main__ - ['Minogue']
03/09/2022 04:06:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:06:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:06:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:06:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:06:53 - INFO - __main__ - Printing 3 examples
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 04:06:53 - INFO - __main__ - ['Alan']
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 04:06:53 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 04:06:53 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 04:06:53 - INFO - __main__ - ['Albert']
03/09/2022 04:06:53 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:06:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:06:53 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:07:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:07:04 - INFO - __main__ - Starting training!
03/09/2022 04:07:34 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:07:35 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:07:35 - INFO - __main__ - Printing 3 examples
03/09/2022 04:07:35 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:07:35 - INFO - __main__ - ['Frankie']
03/09/2022 04:07:35 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:07:35 - INFO - __main__ - ['Frankie']
03/09/2022 04:07:35 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:07:35 - INFO - __main__ - ['Frankie']
03/09/2022 04:07:35 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:07:39 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:07:42 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 04:08:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_21_0.0003_8_predictions.txt
03/09/2022 04:08:59 - INFO - __main__ - QA-F1 on test data: 0.0728
03/09/2022 04:09:00 - INFO - __main__ - prefix=quoref_32_21, lr=0.0003, bsz=8, dev_performance=0.057291666666666664, test_performance=0.07280121312379377
03/09/2022 04:09:00 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.0002, bsz=8 ...
03/09/2022 04:09:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:09:01 - INFO - __main__ - Printing 3 examples
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 04:09:01 - INFO - __main__ - ['Allan']
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 04:09:01 - INFO - __main__ - ['Nigro']
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 04:09:01 - INFO - __main__ - ['Minogue']
03/09/2022 04:09:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 04:09:01 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:09:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:09:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:09:01 - INFO - __main__ - Printing 3 examples
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 04:09:01 - INFO - __main__ - ['Alan']
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 04:09:01 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 04:09:01 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 04:09:01 - INFO - __main__ - ['Albert']
03/09/2022 04:09:01 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:09:01 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:09:01 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:09:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:09:10 - INFO - __main__ - Starting training!
03/09/2022 04:09:15 - INFO - __main__ - Step 10 Global step 10 Train loss 19.016361 on epoch=4
03/09/2022 04:09:20 - INFO - __main__ - Step 20 Global step 20 Train loss 17.536722 on epoch=9
03/09/2022 04:09:26 - INFO - __main__ - Step 30 Global step 30 Train loss 15.399976 on epoch=14
03/09/2022 04:09:32 - INFO - __main__ - Step 40 Global step 40 Train loss 12.427687 on epoch=19
03/09/2022 04:09:38 - INFO - __main__ - Step 50 Global step 50 Train loss 10.606977 on epoch=24
03/09/2022 04:09:39 - INFO - __main__ - Global step 50 Train loss 14.997544 QA-F1 0.0 on epoch=24
03/09/2022 04:10:14 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 04:10:20 - INFO - __main__ - Step 60 Global step 60 Train loss 9.733130 on epoch=29
03/09/2022 04:10:26 - INFO - __main__ - Step 70 Global step 70 Train loss 9.220400 on epoch=34
03/09/2022 04:10:32 - INFO - __main__ - Step 80 Global step 80 Train loss 8.682866 on epoch=39
03/09/2022 04:10:37 - INFO - __main__ - Step 90 Global step 90 Train loss 8.055992 on epoch=44
03/09/2022 04:10:43 - INFO - __main__ - Step 100 Global step 100 Train loss 7.606130 on epoch=49
03/09/2022 04:10:44 - INFO - __main__ - Global step 100 Train loss 8.659703 QA-F1 0.0 on epoch=49
03/09/2022 04:10:50 - INFO - __main__ - Step 110 Global step 110 Train loss 7.089355 on epoch=54
03/09/2022 04:10:56 - INFO - __main__ - Step 120 Global step 120 Train loss 6.739081 on epoch=59
03/09/2022 04:11:02 - INFO - __main__ - Step 130 Global step 130 Train loss 6.289083 on epoch=64
03/09/2022 04:11:08 - INFO - __main__ - Step 140 Global step 140 Train loss 5.947490 on epoch=69
03/09/2022 04:11:14 - INFO - __main__ - Step 150 Global step 150 Train loss 5.651125 on epoch=74
03/09/2022 04:11:15 - INFO - __main__ - Global step 150 Train loss 6.343227 QA-F1 0.0125 on epoch=74
03/09/2022 04:11:51 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.0125 on epoch=74, global_step=150
03/09/2022 04:11:57 - INFO - __main__ - Step 160 Global step 160 Train loss 5.292457 on epoch=79
03/09/2022 04:12:03 - INFO - __main__ - Step 170 Global step 170 Train loss 5.260796 on epoch=84
03/09/2022 04:12:09 - INFO - __main__ - Step 180 Global step 180 Train loss 5.083152 on epoch=89
03/09/2022 04:12:15 - INFO - __main__ - Step 190 Global step 190 Train loss 4.854381 on epoch=94
03/09/2022 04:12:21 - INFO - __main__ - Step 200 Global step 200 Train loss 4.628352 on epoch=99
03/09/2022 04:12:22 - INFO - __main__ - Global step 200 Train loss 5.023828 QA-F1 0.0 on epoch=99
03/09/2022 04:12:28 - INFO - __main__ - Step 210 Global step 210 Train loss 4.565450 on epoch=104
03/09/2022 04:12:34 - INFO - __main__ - Step 220 Global step 220 Train loss 4.205974 on epoch=109
03/09/2022 04:12:40 - INFO - __main__ - Step 230 Global step 230 Train loss 3.785614 on epoch=114
03/09/2022 04:12:46 - INFO - __main__ - Step 240 Global step 240 Train loss 3.691600 on epoch=119
03/09/2022 04:12:52 - INFO - __main__ - Step 250 Global step 250 Train loss 3.679810 on epoch=124
03/09/2022 04:12:53 - INFO - __main__ - Global step 250 Train loss 3.985689 QA-F1 0.0 on epoch=124
03/09/2022 04:12:59 - INFO - __main__ - Step 260 Global step 260 Train loss 3.202742 on epoch=129
03/09/2022 04:13:05 - INFO - __main__ - Step 270 Global step 270 Train loss 2.676499 on epoch=134
03/09/2022 04:13:11 - INFO - __main__ - Step 280 Global step 280 Train loss 2.781090 on epoch=139
03/09/2022 04:13:17 - INFO - __main__ - Step 290 Global step 290 Train loss 2.640070 on epoch=144
03/09/2022 04:13:23 - INFO - __main__ - Step 300 Global step 300 Train loss 2.526134 on epoch=149
03/09/2022 04:13:24 - INFO - __main__ - Global step 300 Train loss 2.765307 QA-F1 0.0 on epoch=149
03/09/2022 04:13:30 - INFO - __main__ - Step 310 Global step 310 Train loss 2.309292 on epoch=154
03/09/2022 04:13:36 - INFO - __main__ - Step 320 Global step 320 Train loss 2.214343 on epoch=159
03/09/2022 04:13:42 - INFO - __main__ - Step 330 Global step 330 Train loss 2.148707 on epoch=164
03/09/2022 04:13:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.973562 on epoch=169
03/09/2022 04:13:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.974932 on epoch=174
03/09/2022 04:13:54 - INFO - __main__ - Global step 350 Train loss 2.124167 QA-F1 0.0 on epoch=174
03/09/2022 04:14:00 - INFO - __main__ - Step 360 Global step 360 Train loss 2.101592 on epoch=179
03/09/2022 04:14:06 - INFO - __main__ - Step 370 Global step 370 Train loss 2.152802 on epoch=184
03/09/2022 04:14:12 - INFO - __main__ - Step 380 Global step 380 Train loss 1.834328 on epoch=189
03/09/2022 04:14:18 - INFO - __main__ - Step 390 Global step 390 Train loss 2.093305 on epoch=194
03/09/2022 04:14:24 - INFO - __main__ - Step 400 Global step 400 Train loss 2.065640 on epoch=199
03/09/2022 04:14:25 - INFO - __main__ - Global step 400 Train loss 2.049534 QA-F1 0.0 on epoch=199
03/09/2022 04:14:31 - INFO - __main__ - Step 410 Global step 410 Train loss 2.111857 on epoch=204
03/09/2022 04:14:37 - INFO - __main__ - Step 420 Global step 420 Train loss 1.755909 on epoch=209
03/09/2022 04:14:43 - INFO - __main__ - Step 430 Global step 430 Train loss 1.875901 on epoch=214
03/09/2022 04:14:49 - INFO - __main__ - Step 440 Global step 440 Train loss 1.745864 on epoch=219
03/09/2022 04:14:55 - INFO - __main__ - Step 450 Global step 450 Train loss 1.764880 on epoch=224
03/09/2022 04:14:56 - INFO - __main__ - Global step 450 Train loss 1.850882 QA-F1 0.04583333333333334 on epoch=224
03/09/2022 04:15:33 - INFO - __main__ - Saving model with best QA-F1: 0.0125 -> 0.04583333333333334 on epoch=224, global_step=450
03/09/2022 04:15:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.694657 on epoch=229
03/09/2022 04:15:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.633037 on epoch=234
03/09/2022 04:15:51 - INFO - __main__ - Step 480 Global step 480 Train loss 1.474739 on epoch=239
03/09/2022 04:15:57 - INFO - __main__ - Step 490 Global step 490 Train loss 1.640452 on epoch=244
03/09/2022 04:16:03 - INFO - __main__ - Step 500 Global step 500 Train loss 1.353491 on epoch=249
03/09/2022 04:16:04 - INFO - __main__ - Global step 500 Train loss 1.559275 QA-F1 0.0 on epoch=249
03/09/2022 04:16:10 - INFO - __main__ - Step 510 Global step 510 Train loss 1.550619 on epoch=254
03/09/2022 04:16:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.488438 on epoch=259
03/09/2022 04:16:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.728600 on epoch=264
03/09/2022 04:16:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.465403 on epoch=269
03/09/2022 04:16:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.552915 on epoch=274
03/09/2022 04:16:35 - INFO - __main__ - Global step 550 Train loss 1.557195 QA-F1 0.0 on epoch=274
03/09/2022 04:16:41 - INFO - __main__ - Step 560 Global step 560 Train loss 1.407390 on epoch=279
03/09/2022 04:16:47 - INFO - __main__ - Step 570 Global step 570 Train loss 1.440298 on epoch=284
03/09/2022 04:16:53 - INFO - __main__ - Step 580 Global step 580 Train loss 1.315536 on epoch=289
03/09/2022 04:16:59 - INFO - __main__ - Step 590 Global step 590 Train loss 1.433103 on epoch=294
03/09/2022 04:17:05 - INFO - __main__ - Step 600 Global step 600 Train loss 1.301218 on epoch=299
03/09/2022 04:17:06 - INFO - __main__ - Global step 600 Train loss 1.379509 QA-F1 0.041666666666666664 on epoch=299
03/09/2022 04:17:06 - INFO - __main__ - save last model!
03/09/2022 04:17:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:17:06 - INFO - __main__ - Printing 3 examples
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 04:17:06 - INFO - __main__ - ['Allan']
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 04:17:06 - INFO - __main__ - ['Nigro']
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 04:17:06 - INFO - __main__ - ['Minogue']
03/09/2022 04:17:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:17:06 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:17:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:17:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:17:06 - INFO - __main__ - Printing 3 examples
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 04:17:06 - INFO - __main__ - ['Alan']
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 04:17:06 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 04:17:06 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 04:17:06 - INFO - __main__ - ['Albert']
03/09/2022 04:17:06 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:17:06 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:17:06 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:17:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:17:17 - INFO - __main__ - Starting training!
03/09/2022 04:17:48 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:17:49 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:17:49 - INFO - __main__ - Printing 3 examples
03/09/2022 04:17:49 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:17:49 - INFO - __main__ - ['Frankie']
03/09/2022 04:17:49 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:17:49 - INFO - __main__ - ['Frankie']
03/09/2022 04:17:49 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:17:49 - INFO - __main__ - ['Frankie']
03/09/2022 04:17:49 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:17:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:17:56 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 04:19:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_21_0.0002_8_predictions.txt
03/09/2022 04:19:10 - INFO - __main__ - QA-F1 on test data: 0.0178
03/09/2022 04:19:10 - INFO - __main__ - prefix=quoref_32_21, lr=0.0002, bsz=8, dev_performance=0.04583333333333334, test_performance=0.017810862972153292
03/09/2022 04:19:10 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.0001, bsz=8 ...
03/09/2022 04:19:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:19:11 - INFO - __main__ - Printing 3 examples
03/09/2022 04:19:11 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/09/2022 04:19:11 - INFO - __main__ - ['Allan']
03/09/2022 04:19:11 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/09/2022 04:19:11 - INFO - __main__ - ['Nigro']
03/09/2022 04:19:11 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/09/2022 04:19:11 - INFO - __main__ - ['Minogue']
03/09/2022 04:19:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 04:19:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:19:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:19:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:19:11 - INFO - __main__ - Printing 3 examples
03/09/2022 04:19:11 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/09/2022 04:19:11 - INFO - __main__ - ['Alan']
03/09/2022 04:19:11 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/09/2022 04:19:12 - INFO - __main__ - ['Santo Tomás de Castilla']
03/09/2022 04:19:12 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/09/2022 04:19:12 - INFO - __main__ - ['Albert']
03/09/2022 04:19:12 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:19:12 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:19:12 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:19:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:19:21 - INFO - __main__ - Starting training!
03/09/2022 04:19:27 - INFO - __main__ - Step 10 Global step 10 Train loss 18.856926 on epoch=4
03/09/2022 04:19:32 - INFO - __main__ - Step 20 Global step 20 Train loss 16.672016 on epoch=9
03/09/2022 04:19:39 - INFO - __main__ - Step 30 Global step 30 Train loss 12.461906 on epoch=14
03/09/2022 04:19:45 - INFO - __main__ - Step 40 Global step 40 Train loss 9.872492 on epoch=19
03/09/2022 04:19:51 - INFO - __main__ - Step 50 Global step 50 Train loss 8.488264 on epoch=24
03/09/2022 04:19:52 - INFO - __main__ - Global step 50 Train loss 13.270322 QA-F1 0.05729166666666666 on epoch=24
03/09/2022 04:20:28 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.05729166666666666 on epoch=24, global_step=50
03/09/2022 04:20:34 - INFO - __main__ - Step 60 Global step 60 Train loss 8.457823 on epoch=29
03/09/2022 04:20:40 - INFO - __main__ - Step 70 Global step 70 Train loss 7.623843 on epoch=34
03/09/2022 04:20:46 - INFO - __main__ - Step 80 Global step 80 Train loss 7.527827 on epoch=39
03/09/2022 04:20:52 - INFO - __main__ - Step 90 Global step 90 Train loss 7.070533 on epoch=44
03/09/2022 04:20:58 - INFO - __main__ - Step 100 Global step 100 Train loss 6.979043 on epoch=49
03/09/2022 04:21:00 - INFO - __main__ - Global step 100 Train loss 7.531813 QA-F1 0.020833333333333332 on epoch=49
03/09/2022 04:21:06 - INFO - __main__ - Step 110 Global step 110 Train loss 6.581533 on epoch=54
03/09/2022 04:21:12 - INFO - __main__ - Step 120 Global step 120 Train loss 6.519031 on epoch=59
03/09/2022 04:21:17 - INFO - __main__ - Step 130 Global step 130 Train loss 6.366955 on epoch=64
03/09/2022 04:21:23 - INFO - __main__ - Step 140 Global step 140 Train loss 7.488215 on epoch=69
03/09/2022 04:21:29 - INFO - __main__ - Step 150 Global step 150 Train loss 8.807292 on epoch=74
03/09/2022 04:21:37 - INFO - __main__ - Global step 150 Train loss 7.152605 QA-F1 0.020833333333333332 on epoch=74
03/09/2022 04:21:43 - INFO - __main__ - Step 160 Global step 160 Train loss 7.590087 on epoch=79
03/09/2022 04:21:49 - INFO - __main__ - Step 170 Global step 170 Train loss 7.817204 on epoch=84
03/09/2022 04:21:55 - INFO - __main__ - Step 180 Global step 180 Train loss 7.040144 on epoch=89
03/09/2022 04:22:01 - INFO - __main__ - Step 190 Global step 190 Train loss 6.676190 on epoch=94
03/09/2022 04:22:07 - INFO - __main__ - Step 200 Global step 200 Train loss 6.714580 on epoch=99
03/09/2022 04:22:08 - INFO - __main__ - Global step 200 Train loss 7.167641 QA-F1 0.0 on epoch=99
03/09/2022 04:22:14 - INFO - __main__ - Step 210 Global step 210 Train loss 6.415392 on epoch=104
03/09/2022 04:22:20 - INFO - __main__ - Step 220 Global step 220 Train loss 6.213933 on epoch=109
03/09/2022 04:22:26 - INFO - __main__ - Step 230 Global step 230 Train loss 5.944695 on epoch=114
03/09/2022 04:22:32 - INFO - __main__ - Step 240 Global step 240 Train loss 5.744016 on epoch=119
03/09/2022 04:22:38 - INFO - __main__ - Step 250 Global step 250 Train loss 5.718256 on epoch=124
03/09/2022 04:22:39 - INFO - __main__ - Global step 250 Train loss 6.007258 QA-F1 0.0 on epoch=124
03/09/2022 04:22:45 - INFO - __main__ - Step 260 Global step 260 Train loss 5.594738 on epoch=129
03/09/2022 04:22:51 - INFO - __main__ - Step 270 Global step 270 Train loss 5.929391 on epoch=134
03/09/2022 04:22:57 - INFO - __main__ - Step 280 Global step 280 Train loss 5.415805 on epoch=139
03/09/2022 04:23:03 - INFO - __main__ - Step 290 Global step 290 Train loss 5.621577 on epoch=144
03/09/2022 04:23:09 - INFO - __main__ - Step 300 Global step 300 Train loss 5.438985 on epoch=149
03/09/2022 04:23:10 - INFO - __main__ - Global step 300 Train loss 5.600099 QA-F1 0.0 on epoch=149
03/09/2022 04:23:16 - INFO - __main__ - Step 310 Global step 310 Train loss 5.543635 on epoch=154
03/09/2022 04:23:22 - INFO - __main__ - Step 320 Global step 320 Train loss 5.214521 on epoch=159
03/09/2022 04:23:28 - INFO - __main__ - Step 330 Global step 330 Train loss 5.209218 on epoch=164
03/09/2022 04:23:34 - INFO - __main__ - Step 340 Global step 340 Train loss 5.083579 on epoch=169
03/09/2022 04:23:40 - INFO - __main__ - Step 350 Global step 350 Train loss 4.855394 on epoch=174
03/09/2022 04:23:41 - INFO - __main__ - Global step 350 Train loss 5.181270 QA-F1 0.0 on epoch=174
03/09/2022 04:23:47 - INFO - __main__ - Step 360 Global step 360 Train loss 4.666231 on epoch=179
03/09/2022 04:23:53 - INFO - __main__ - Step 370 Global step 370 Train loss 4.704041 on epoch=184
03/09/2022 04:23:59 - INFO - __main__ - Step 380 Global step 380 Train loss 4.706507 on epoch=189
03/09/2022 04:24:05 - INFO - __main__ - Step 390 Global step 390 Train loss 4.811140 on epoch=194
03/09/2022 04:24:11 - INFO - __main__ - Step 400 Global step 400 Train loss 4.275525 on epoch=199
03/09/2022 04:24:12 - INFO - __main__ - Global step 400 Train loss 4.632689 QA-F1 0.0 on epoch=199
03/09/2022 04:24:18 - INFO - __main__ - Step 410 Global step 410 Train loss 4.212752 on epoch=204
03/09/2022 04:24:24 - INFO - __main__ - Step 420 Global step 420 Train loss 4.198003 on epoch=209
03/09/2022 04:24:30 - INFO - __main__ - Step 430 Global step 430 Train loss 4.235394 on epoch=214
03/09/2022 04:24:36 - INFO - __main__ - Step 440 Global step 440 Train loss 4.205916 on epoch=219
03/09/2022 04:24:42 - INFO - __main__ - Step 450 Global step 450 Train loss 3.831590 on epoch=224
03/09/2022 04:24:43 - INFO - __main__ - Global step 450 Train loss 4.136731 QA-F1 0.0 on epoch=224
03/09/2022 04:24:49 - INFO - __main__ - Step 460 Global step 460 Train loss 3.903878 on epoch=229
03/09/2022 04:24:55 - INFO - __main__ - Step 470 Global step 470 Train loss 3.366527 on epoch=234
03/09/2022 04:25:01 - INFO - __main__ - Step 480 Global step 480 Train loss 3.085445 on epoch=239
03/09/2022 04:25:07 - INFO - __main__ - Step 490 Global step 490 Train loss 3.232472 on epoch=244
03/09/2022 04:25:13 - INFO - __main__ - Step 500 Global step 500 Train loss 2.782878 on epoch=249
03/09/2022 04:25:14 - INFO - __main__ - Global step 500 Train loss 3.274240 QA-F1 0.0 on epoch=249
03/09/2022 04:25:20 - INFO - __main__ - Step 510 Global step 510 Train loss 2.963545 on epoch=254
03/09/2022 04:25:26 - INFO - __main__ - Step 520 Global step 520 Train loss 2.530789 on epoch=259
03/09/2022 04:25:32 - INFO - __main__ - Step 530 Global step 530 Train loss 2.578232 on epoch=264
03/09/2022 04:25:38 - INFO - __main__ - Step 540 Global step 540 Train loss 2.555505 on epoch=269
03/09/2022 04:25:44 - INFO - __main__ - Step 550 Global step 550 Train loss 2.419734 on epoch=274
03/09/2022 04:25:45 - INFO - __main__ - Global step 550 Train loss 2.609561 QA-F1 0.0 on epoch=274
03/09/2022 04:25:51 - INFO - __main__ - Step 560 Global step 560 Train loss 2.880805 on epoch=279
03/09/2022 04:25:57 - INFO - __main__ - Step 570 Global step 570 Train loss 2.422099 on epoch=284
03/09/2022 04:26:03 - INFO - __main__ - Step 580 Global step 580 Train loss 2.189578 on epoch=289
03/09/2022 04:26:09 - INFO - __main__ - Step 590 Global step 590 Train loss 2.649369 on epoch=294
03/09/2022 04:26:15 - INFO - __main__ - Step 600 Global step 600 Train loss 2.133682 on epoch=299
03/09/2022 04:26:16 - INFO - __main__ - Global step 600 Train loss 2.455106 QA-F1 0.0 on epoch=299
03/09/2022 04:26:16 - INFO - __main__ - save last model!
03/09/2022 04:26:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:26:16 - INFO - __main__ - Printing 3 examples
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:26:16 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:26:16 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:26:16 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:26:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:26:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:26:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:26:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:26:16 - INFO - __main__ - Printing 3 examples
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:26:16 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:26:16 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:26:16 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:26:16 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:26:16 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:26:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:26:16 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:26:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:26:27 - INFO - __main__ - Starting training!
03/09/2022 04:26:57 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:26:58 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:26:58 - INFO - __main__ - Printing 3 examples
03/09/2022 04:26:58 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:26:58 - INFO - __main__ - ['Frankie']
03/09/2022 04:26:58 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:26:58 - INFO - __main__ - ['Frankie']
03/09/2022 04:26:58 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:26:58 - INFO - __main__ - ['Frankie']
03/09/2022 04:26:58 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:27:02 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:27:05 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 04:28:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_21_0.0001_8_predictions.txt
03/09/2022 04:28:42 - INFO - __main__ - QA-F1 on test data: 0.0300
03/09/2022 04:28:42 - INFO - __main__ - prefix=quoref_32_21, lr=0.0001, bsz=8, dev_performance=0.05729166666666666, test_performance=0.029950963015479146
03/09/2022 04:28:42 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.0005, bsz=8 ...
03/09/2022 04:28:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:28:43 - INFO - __main__ - Printing 3 examples
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:28:43 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:28:43 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:28:43 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:28:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 04:28:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:28:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:28:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:28:43 - INFO - __main__ - Printing 3 examples
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:28:43 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:28:43 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:28:43 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:28:43 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:28:43 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:28:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:28:43 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:28:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:28:52 - INFO - __main__ - Starting training!
03/09/2022 04:28:57 - INFO - __main__ - Step 10 Global step 10 Train loss 20.605570 on epoch=4
03/09/2022 04:29:02 - INFO - __main__ - Step 20 Global step 20 Train loss 17.446060 on epoch=9
03/09/2022 04:29:08 - INFO - __main__ - Step 30 Global step 30 Train loss 15.991011 on epoch=14
03/09/2022 04:29:14 - INFO - __main__ - Step 40 Global step 40 Train loss 10.776766 on epoch=19
03/09/2022 04:29:20 - INFO - __main__ - Step 50 Global step 50 Train loss 9.535097 on epoch=24
03/09/2022 04:29:21 - INFO - __main__ - Global step 50 Train loss 14.870899 QA-F1 0.0 on epoch=24
03/09/2022 04:29:58 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 04:30:04 - INFO - __main__ - Step 60 Global step 60 Train loss 8.853569 on epoch=29
03/09/2022 04:30:10 - INFO - __main__ - Step 70 Global step 70 Train loss 7.810922 on epoch=34
03/09/2022 04:30:16 - INFO - __main__ - Step 80 Global step 80 Train loss 7.051402 on epoch=39
03/09/2022 04:30:22 - INFO - __main__ - Step 90 Global step 90 Train loss 6.084879 on epoch=44
03/09/2022 04:30:28 - INFO - __main__ - Step 100 Global step 100 Train loss 5.127444 on epoch=49
03/09/2022 04:30:29 - INFO - __main__ - Global step 100 Train loss 6.985643 QA-F1 0.0 on epoch=49
03/09/2022 04:30:35 - INFO - __main__ - Step 110 Global step 110 Train loss 4.108781 on epoch=54
03/09/2022 04:30:41 - INFO - __main__ - Step 120 Global step 120 Train loss 3.444066 on epoch=59
03/09/2022 04:30:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.807582 on epoch=64
03/09/2022 04:30:53 - INFO - __main__ - Step 140 Global step 140 Train loss 3.006672 on epoch=69
03/09/2022 04:30:59 - INFO - __main__ - Step 150 Global step 150 Train loss 2.846069 on epoch=74
03/09/2022 04:31:00 - INFO - __main__ - Global step 150 Train loss 3.242634 QA-F1 0.0 on epoch=74
03/09/2022 04:31:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.760513 on epoch=79
03/09/2022 04:31:12 - INFO - __main__ - Step 170 Global step 170 Train loss 2.546971 on epoch=84
03/09/2022 04:31:18 - INFO - __main__ - Step 180 Global step 180 Train loss 2.496585 on epoch=89
03/09/2022 04:31:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.623820 on epoch=94
03/09/2022 04:31:30 - INFO - __main__ - Step 200 Global step 200 Train loss 2.279480 on epoch=99
03/09/2022 04:31:31 - INFO - __main__ - Global step 200 Train loss 2.541474 QA-F1 0.0 on epoch=99
03/09/2022 04:31:37 - INFO - __main__ - Step 210 Global step 210 Train loss 1.939153 on epoch=104
03/09/2022 04:31:43 - INFO - __main__ - Step 220 Global step 220 Train loss 2.143943 on epoch=109
03/09/2022 04:31:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.911166 on epoch=114
03/09/2022 04:31:55 - INFO - __main__ - Step 240 Global step 240 Train loss 2.278307 on epoch=119
03/09/2022 04:32:01 - INFO - __main__ - Step 250 Global step 250 Train loss 2.033943 on epoch=124
03/09/2022 04:32:02 - INFO - __main__ - Global step 250 Train loss 2.061303 QA-F1 0.0 on epoch=124
03/09/2022 04:32:08 - INFO - __main__ - Step 260 Global step 260 Train loss 1.782561 on epoch=129
03/09/2022 04:32:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.831095 on epoch=134
03/09/2022 04:32:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.680335 on epoch=139
03/09/2022 04:32:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.912919 on epoch=144
03/09/2022 04:32:32 - INFO - __main__ - Step 300 Global step 300 Train loss 1.877814 on epoch=149
03/09/2022 04:32:33 - INFO - __main__ - Global step 300 Train loss 1.816945 QA-F1 0.0 on epoch=149
03/09/2022 04:32:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.584214 on epoch=154
03/09/2022 04:32:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.540997 on epoch=159
03/09/2022 04:32:51 - INFO - __main__ - Step 330 Global step 330 Train loss 1.479965 on epoch=164
03/09/2022 04:32:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.428892 on epoch=169
03/09/2022 04:33:03 - INFO - __main__ - Step 350 Global step 350 Train loss 1.528244 on epoch=174
03/09/2022 04:33:04 - INFO - __main__ - Global step 350 Train loss 1.512462 QA-F1 0.0 on epoch=174
03/09/2022 04:33:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.332413 on epoch=179
03/09/2022 04:33:16 - INFO - __main__ - Step 370 Global step 370 Train loss 2.036777 on epoch=184
03/09/2022 04:33:22 - INFO - __main__ - Step 380 Global step 380 Train loss 1.127858 on epoch=189
03/09/2022 04:33:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.412604 on epoch=194
03/09/2022 04:33:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.158037 on epoch=199
03/09/2022 04:33:35 - INFO - __main__ - Global step 400 Train loss 1.013538 QA-F1 0.0 on epoch=199
03/09/2022 04:33:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.124239 on epoch=204
03/09/2022 04:33:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.220213 on epoch=209
03/09/2022 04:33:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.109998 on epoch=214
03/09/2022 04:33:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.111889 on epoch=219
03/09/2022 04:34:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.107371 on epoch=224
03/09/2022 04:34:06 - INFO - __main__ - Global step 450 Train loss 0.134742 QA-F1 0.0 on epoch=224
03/09/2022 04:34:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.090756 on epoch=229
03/09/2022 04:34:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116597 on epoch=234
03/09/2022 04:34:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.102506 on epoch=239
03/09/2022 04:34:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.103911 on epoch=244
03/09/2022 04:34:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.084256 on epoch=249
03/09/2022 04:34:37 - INFO - __main__ - Global step 500 Train loss 0.099605 QA-F1 0.0 on epoch=249
03/09/2022 04:34:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.045606 on epoch=254
03/09/2022 04:34:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.092024 on epoch=259
03/09/2022 04:34:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.060087 on epoch=264
03/09/2022 04:35:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.098076 on epoch=269
03/09/2022 04:35:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.056487 on epoch=274
03/09/2022 04:35:08 - INFO - __main__ - Global step 550 Train loss 0.070456 QA-F1 0.03125 on epoch=274
03/09/2022 04:35:45 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.03125 on epoch=274, global_step=550
03/09/2022 04:35:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.074807 on epoch=279
03/09/2022 04:35:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.085573 on epoch=284
03/09/2022 04:36:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.095973 on epoch=289
03/09/2022 04:36:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.062518 on epoch=294
03/09/2022 04:36:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.033969 on epoch=299
03/09/2022 04:36:16 - INFO - __main__ - Global step 600 Train loss 0.070568 QA-F1 0.03125 on epoch=299
03/09/2022 04:36:16 - INFO - __main__ - save last model!
03/09/2022 04:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:36:16 - INFO - __main__ - Printing 3 examples
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:36:16 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:36:16 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:36:16 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:36:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:36:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:36:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:36:16 - INFO - __main__ - Printing 3 examples
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:36:16 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:36:16 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:36:16 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:36:16 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:36:16 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:36:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:36:16 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:36:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:36:27 - INFO - __main__ - Starting training!
03/09/2022 04:36:58 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:36:59 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:36:59 - INFO - __main__ - Printing 3 examples
03/09/2022 04:36:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:36:59 - INFO - __main__ - ['Frankie']
03/09/2022 04:36:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:36:59 - INFO - __main__ - ['Frankie']
03/09/2022 04:36:59 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:36:59 - INFO - __main__ - ['Frankie']
03/09/2022 04:36:59 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:37:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:37:06 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 04:38:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_42_0.0005_8_predictions.txt
03/09/2022 04:38:18 - INFO - __main__ - QA-F1 on test data: 0.0588
03/09/2022 04:38:18 - INFO - __main__ - prefix=quoref_32_42, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.0588089330024814
03/09/2022 04:38:18 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.0003, bsz=8 ...
03/09/2022 04:38:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:38:19 - INFO - __main__ - Printing 3 examples
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:38:19 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:38:19 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:38:19 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:38:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 04:38:19 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:38:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:38:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:38:19 - INFO - __main__ - Printing 3 examples
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:38:19 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:38:19 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:38:19 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:38:19 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:38:19 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:38:19 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:38:19 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:38:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:38:28 - INFO - __main__ - Starting training!
03/09/2022 04:38:33 - INFO - __main__ - Step 10 Global step 10 Train loss 21.218805 on epoch=4
03/09/2022 04:38:39 - INFO - __main__ - Step 20 Global step 20 Train loss 17.880619 on epoch=9
03/09/2022 04:38:45 - INFO - __main__ - Step 30 Global step 30 Train loss 12.193693 on epoch=14
03/09/2022 04:38:51 - INFO - __main__ - Step 40 Global step 40 Train loss 10.157850 on epoch=19
03/09/2022 04:38:57 - INFO - __main__ - Step 50 Global step 50 Train loss 9.757689 on epoch=24
03/09/2022 04:38:58 - INFO - __main__ - Global step 50 Train loss 14.241731 QA-F1 0.0 on epoch=24
03/09/2022 04:39:33 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 04:39:39 - INFO - __main__ - Step 60 Global step 60 Train loss 9.591655 on epoch=29
03/09/2022 04:39:45 - INFO - __main__ - Step 70 Global step 70 Train loss 9.161572 on epoch=34
03/09/2022 04:39:51 - INFO - __main__ - Step 80 Global step 80 Train loss 8.079881 on epoch=39
03/09/2022 04:39:57 - INFO - __main__ - Step 90 Global step 90 Train loss 7.977411 on epoch=44
03/09/2022 04:40:03 - INFO - __main__ - Step 100 Global step 100 Train loss 7.005664 on epoch=49
03/09/2022 04:40:04 - INFO - __main__ - Global step 100 Train loss 8.363236 QA-F1 0.0 on epoch=49
03/09/2022 04:40:10 - INFO - __main__ - Step 110 Global step 110 Train loss 7.078266 on epoch=54
03/09/2022 04:40:16 - INFO - __main__ - Step 120 Global step 120 Train loss 6.929429 on epoch=59
03/09/2022 04:40:22 - INFO - __main__ - Step 130 Global step 130 Train loss 6.022139 on epoch=64
03/09/2022 04:40:28 - INFO - __main__ - Step 140 Global step 140 Train loss 5.382235 on epoch=69
03/09/2022 04:40:34 - INFO - __main__ - Step 150 Global step 150 Train loss 5.248907 on epoch=74
03/09/2022 04:40:35 - INFO - __main__ - Global step 150 Train loss 6.132195 QA-F1 0.0 on epoch=74
03/09/2022 04:40:41 - INFO - __main__ - Step 160 Global step 160 Train loss 3.796105 on epoch=79
03/09/2022 04:40:47 - INFO - __main__ - Step 170 Global step 170 Train loss 3.593807 on epoch=84
03/09/2022 04:40:53 - INFO - __main__ - Step 180 Global step 180 Train loss 3.275761 on epoch=89
03/09/2022 04:40:59 - INFO - __main__ - Step 190 Global step 190 Train loss 3.106665 on epoch=94
03/09/2022 04:41:04 - INFO - __main__ - Step 200 Global step 200 Train loss 3.509873 on epoch=99
03/09/2022 04:41:05 - INFO - __main__ - Global step 200 Train loss 3.456442 QA-F1 0.0 on epoch=99
03/09/2022 04:41:11 - INFO - __main__ - Step 210 Global step 210 Train loss 3.202935 on epoch=104
03/09/2022 04:41:17 - INFO - __main__ - Step 220 Global step 220 Train loss 2.886286 on epoch=109
03/09/2022 04:41:23 - INFO - __main__ - Step 230 Global step 230 Train loss 2.685734 on epoch=114
03/09/2022 04:41:29 - INFO - __main__ - Step 240 Global step 240 Train loss 2.764345 on epoch=119
03/09/2022 04:41:35 - INFO - __main__ - Step 250 Global step 250 Train loss 2.626010 on epoch=124
03/09/2022 04:41:36 - INFO - __main__ - Global step 250 Train loss 2.833062 QA-F1 0.0 on epoch=124
03/09/2022 04:41:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.288015 on epoch=129
03/09/2022 04:41:48 - INFO - __main__ - Step 270 Global step 270 Train loss 2.471281 on epoch=134
03/09/2022 04:41:54 - INFO - __main__ - Step 280 Global step 280 Train loss 2.398849 on epoch=139
03/09/2022 04:42:00 - INFO - __main__ - Step 290 Global step 290 Train loss 2.362809 on epoch=144
03/09/2022 04:42:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.714952 on epoch=149
03/09/2022 04:42:07 - INFO - __main__ - Global step 300 Train loss 2.047181 QA-F1 0.11458333333333333 on epoch=149
03/09/2022 04:42:44 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.11458333333333333 on epoch=149, global_step=300
03/09/2022 04:42:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.103960 on epoch=154
03/09/2022 04:42:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090122 on epoch=159
03/09/2022 04:43:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.098792 on epoch=164
03/09/2022 04:43:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.073801 on epoch=169
03/09/2022 04:43:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.039125 on epoch=174
03/09/2022 04:43:15 - INFO - __main__ - Global step 350 Train loss 0.081160 QA-F1 0.08854166666666666 on epoch=174
03/09/2022 04:43:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.077369 on epoch=179
03/09/2022 04:43:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.048850 on epoch=184
03/09/2022 04:43:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054617 on epoch=189
03/09/2022 04:43:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.012974 on epoch=194
03/09/2022 04:43:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.021307 on epoch=199
03/09/2022 04:43:46 - INFO - __main__ - Global step 400 Train loss 0.043023 QA-F1 0.14583333333333331 on epoch=199
03/09/2022 04:44:23 - INFO - __main__ - Saving model with best QA-F1: 0.11458333333333333 -> 0.14583333333333331 on epoch=199, global_step=400
03/09/2022 04:44:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.022891 on epoch=204
03/09/2022 04:44:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.049135 on epoch=209
03/09/2022 04:44:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.010228 on epoch=214
03/09/2022 04:44:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.037621 on epoch=219
03/09/2022 04:44:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.124001 on epoch=224
03/09/2022 04:44:53 - INFO - __main__ - Global step 450 Train loss 0.048775 QA-F1 0.13541666666666666 on epoch=224
03/09/2022 04:44:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.034758 on epoch=229
03/09/2022 04:45:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.032902 on epoch=234
03/09/2022 04:45:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.036699 on epoch=239
03/09/2022 04:45:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.053530 on epoch=244
03/09/2022 04:45:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.033184 on epoch=249
03/09/2022 04:45:24 - INFO - __main__ - Global step 500 Train loss 0.038215 QA-F1 0.17708333333333331 on epoch=249
03/09/2022 04:46:01 - INFO - __main__ - Saving model with best QA-F1: 0.14583333333333331 -> 0.17708333333333331 on epoch=249, global_step=500
03/09/2022 04:46:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027861 on epoch=254
03/09/2022 04:46:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.044726 on epoch=259
03/09/2022 04:46:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.026547 on epoch=264
03/09/2022 04:46:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.028217 on epoch=269
03/09/2022 04:46:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.049237 on epoch=274
03/09/2022 04:46:32 - INFO - __main__ - Global step 550 Train loss 0.035318 QA-F1 0.14583333333333331 on epoch=274
03/09/2022 04:46:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.022386 on epoch=279
03/09/2022 04:46:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.036843 on epoch=284
03/09/2022 04:46:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016968 on epoch=289
03/09/2022 04:46:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.049219 on epoch=294
03/09/2022 04:47:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.026122 on epoch=299
03/09/2022 04:47:03 - INFO - __main__ - Global step 600 Train loss 0.030308 QA-F1 0.14583333333333331 on epoch=299
03/09/2022 04:47:03 - INFO - __main__ - save last model!
03/09/2022 04:47:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:47:03 - INFO - __main__ - Printing 3 examples
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:47:03 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:47:03 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:47:03 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:47:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:47:03 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:47:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:47:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:47:03 - INFO - __main__ - Printing 3 examples
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:47:03 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:47:03 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:47:03 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:47:03 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:47:03 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:47:03 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:47:03 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:47:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:47:13 - INFO - __main__ - Starting training!
03/09/2022 04:47:44 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:47:45 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:47:45 - INFO - __main__ - Printing 3 examples
03/09/2022 04:47:45 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:47:45 - INFO - __main__ - ['Frankie']
03/09/2022 04:47:45 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:47:45 - INFO - __main__ - ['Frankie']
03/09/2022 04:47:45 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:47:45 - INFO - __main__ - ['Frankie']
03/09/2022 04:47:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:47:49 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:47:52 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 04:49:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_42_0.0003_8_predictions.txt
03/09/2022 04:49:05 - INFO - __main__ - QA-F1 on test data: 0.1971
03/09/2022 04:49:06 - INFO - __main__ - prefix=quoref_32_42, lr=0.0003, bsz=8, dev_performance=0.17708333333333331, test_performance=0.19714640198511163
03/09/2022 04:49:06 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.0002, bsz=8 ...
03/09/2022 04:49:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:49:07 - INFO - __main__ - Printing 3 examples
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:49:07 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:49:07 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:49:07 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:49:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 04:49:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:49:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:49:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:49:07 - INFO - __main__ - Printing 3 examples
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:49:07 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:49:07 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:49:07 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:49:07 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:49:07 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:49:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:49:07 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:49:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:49:16 - INFO - __main__ - Starting training!
03/09/2022 04:49:22 - INFO - __main__ - Step 10 Global step 10 Train loss 21.186558 on epoch=4
03/09/2022 04:49:28 - INFO - __main__ - Step 20 Global step 20 Train loss 17.992313 on epoch=9
03/09/2022 04:49:33 - INFO - __main__ - Step 30 Global step 30 Train loss 18.046082 on epoch=14
03/09/2022 04:49:39 - INFO - __main__ - Step 40 Global step 40 Train loss 15.184698 on epoch=19
03/09/2022 04:49:45 - INFO - __main__ - Step 50 Global step 50 Train loss 12.809848 on epoch=24
03/09/2022 04:49:50 - INFO - __main__ - Global step 50 Train loss 17.043901 QA-F1 0.1016582375957376 on epoch=24
03/09/2022 04:50:27 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.1016582375957376 on epoch=24, global_step=50
03/09/2022 04:50:33 - INFO - __main__ - Step 60 Global step 60 Train loss 11.289198 on epoch=29
03/09/2022 04:50:39 - INFO - __main__ - Step 70 Global step 70 Train loss 10.501111 on epoch=34
03/09/2022 04:50:45 - INFO - __main__ - Step 80 Global step 80 Train loss 10.095521 on epoch=39
03/09/2022 04:50:51 - INFO - __main__ - Step 90 Global step 90 Train loss 9.121645 on epoch=44
03/09/2022 04:50:57 - INFO - __main__ - Step 100 Global step 100 Train loss 9.377235 on epoch=49
03/09/2022 04:51:00 - INFO - __main__ - Global step 100 Train loss 10.076942 QA-F1 0.11458333333333333 on epoch=49
03/09/2022 04:51:37 - INFO - __main__ - Saving model with best QA-F1: 0.1016582375957376 -> 0.11458333333333333 on epoch=49, global_step=100
03/09/2022 04:51:43 - INFO - __main__ - Step 110 Global step 110 Train loss 9.094095 on epoch=54
03/09/2022 04:51:49 - INFO - __main__ - Step 120 Global step 120 Train loss 8.624228 on epoch=59
03/09/2022 04:51:55 - INFO - __main__ - Step 130 Global step 130 Train loss 8.009523 on epoch=64
03/09/2022 04:52:01 - INFO - __main__ - Step 140 Global step 140 Train loss 7.835479 on epoch=69
03/09/2022 04:52:07 - INFO - __main__ - Step 150 Global step 150 Train loss 7.863110 on epoch=74
03/09/2022 04:52:08 - INFO - __main__ - Global step 150 Train loss 8.285287 QA-F1 0.05208333333333333 on epoch=74
03/09/2022 04:52:14 - INFO - __main__ - Step 160 Global step 160 Train loss 7.279473 on epoch=79
03/09/2022 04:52:19 - INFO - __main__ - Step 170 Global step 170 Train loss 7.349289 on epoch=84
03/09/2022 04:52:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.976676 on epoch=89
03/09/2022 04:52:31 - INFO - __main__ - Step 190 Global step 190 Train loss 6.824116 on epoch=94
03/09/2022 04:52:37 - INFO - __main__ - Step 200 Global step 200 Train loss 5.805096 on epoch=99
03/09/2022 04:52:38 - INFO - __main__ - Global step 200 Train loss 6.846930 QA-F1 0.03125 on epoch=99
03/09/2022 04:52:44 - INFO - __main__ - Step 210 Global step 210 Train loss 5.762772 on epoch=104
03/09/2022 04:52:50 - INFO - __main__ - Step 220 Global step 220 Train loss 4.765121 on epoch=109
03/09/2022 04:52:56 - INFO - __main__ - Step 230 Global step 230 Train loss 4.413498 on epoch=114
03/09/2022 04:53:02 - INFO - __main__ - Step 240 Global step 240 Train loss 4.378115 on epoch=119
03/09/2022 04:53:08 - INFO - __main__ - Step 250 Global step 250 Train loss 4.376886 on epoch=124
03/09/2022 04:53:09 - INFO - __main__ - Global step 250 Train loss 4.739278 QA-F1 0.0 on epoch=124
03/09/2022 04:53:15 - INFO - __main__ - Step 260 Global step 260 Train loss 3.552400 on epoch=129
03/09/2022 04:53:21 - INFO - __main__ - Step 270 Global step 270 Train loss 3.480124 on epoch=134
03/09/2022 04:53:27 - INFO - __main__ - Step 280 Global step 280 Train loss 3.602627 on epoch=139
03/09/2022 04:53:33 - INFO - __main__ - Step 290 Global step 290 Train loss 3.278300 on epoch=144
03/09/2022 04:53:39 - INFO - __main__ - Step 300 Global step 300 Train loss 2.987776 on epoch=149
03/09/2022 04:53:40 - INFO - __main__ - Global step 300 Train loss 3.380245 QA-F1 0.03125 on epoch=149
03/09/2022 04:53:46 - INFO - __main__ - Step 310 Global step 310 Train loss 3.188764 on epoch=154
03/09/2022 04:53:52 - INFO - __main__ - Step 320 Global step 320 Train loss 3.335815 on epoch=159
03/09/2022 04:53:58 - INFO - __main__ - Step 330 Global step 330 Train loss 2.980748 on epoch=164
03/09/2022 04:54:03 - INFO - __main__ - Step 340 Global step 340 Train loss 2.898022 on epoch=169
03/09/2022 04:54:09 - INFO - __main__ - Step 350 Global step 350 Train loss 2.913978 on epoch=174
03/09/2022 04:54:10 - INFO - __main__ - Global step 350 Train loss 3.063465 QA-F1 0.0 on epoch=174
03/09/2022 04:54:16 - INFO - __main__ - Step 360 Global step 360 Train loss 2.927902 on epoch=179
03/09/2022 04:54:22 - INFO - __main__ - Step 370 Global step 370 Train loss 2.772742 on epoch=184
03/09/2022 04:54:28 - INFO - __main__ - Step 380 Global step 380 Train loss 2.717322 on epoch=189
03/09/2022 04:54:34 - INFO - __main__ - Step 390 Global step 390 Train loss 2.249293 on epoch=194
03/09/2022 04:54:40 - INFO - __main__ - Step 400 Global step 400 Train loss 2.448764 on epoch=199
03/09/2022 04:54:41 - INFO - __main__ - Global step 400 Train loss 2.623205 QA-F1 0.0 on epoch=199
03/09/2022 04:54:47 - INFO - __main__ - Step 410 Global step 410 Train loss 2.946472 on epoch=204
03/09/2022 04:54:53 - INFO - __main__ - Step 420 Global step 420 Train loss 2.871308 on epoch=209
03/09/2022 04:54:59 - INFO - __main__ - Step 430 Global step 430 Train loss 2.476977 on epoch=214
03/09/2022 04:55:05 - INFO - __main__ - Step 440 Global step 440 Train loss 2.208457 on epoch=219
03/09/2022 04:55:11 - INFO - __main__ - Step 450 Global step 450 Train loss 1.973578 on epoch=224
03/09/2022 04:55:12 - INFO - __main__ - Global step 450 Train loss 2.495358 QA-F1 0.0 on epoch=224
03/09/2022 04:55:17 - INFO - __main__ - Step 460 Global step 460 Train loss 2.182185 on epoch=229
03/09/2022 04:55:23 - INFO - __main__ - Step 470 Global step 470 Train loss 2.573866 on epoch=234
03/09/2022 04:55:29 - INFO - __main__ - Step 480 Global step 480 Train loss 1.974489 on epoch=239
03/09/2022 04:55:35 - INFO - __main__ - Step 490 Global step 490 Train loss 2.454160 on epoch=244
03/09/2022 04:55:41 - INFO - __main__ - Step 500 Global step 500 Train loss 2.128933 on epoch=249
03/09/2022 04:55:42 - INFO - __main__ - Global step 500 Train loss 2.262727 QA-F1 0.0 on epoch=249
03/09/2022 04:55:48 - INFO - __main__ - Step 510 Global step 510 Train loss 2.258228 on epoch=254
03/09/2022 04:55:54 - INFO - __main__ - Step 520 Global step 520 Train loss 2.291729 on epoch=259
03/09/2022 04:56:00 - INFO - __main__ - Step 530 Global step 530 Train loss 2.005675 on epoch=264
03/09/2022 04:56:06 - INFO - __main__ - Step 540 Global step 540 Train loss 1.983849 on epoch=269
03/09/2022 04:56:12 - INFO - __main__ - Step 550 Global step 550 Train loss 1.833838 on epoch=274
03/09/2022 04:56:13 - INFO - __main__ - Global step 550 Train loss 2.074664 QA-F1 0.0 on epoch=274
03/09/2022 04:56:19 - INFO - __main__ - Step 560 Global step 560 Train loss 1.951603 on epoch=279
03/09/2022 04:56:25 - INFO - __main__ - Step 570 Global step 570 Train loss 2.088076 on epoch=284
03/09/2022 04:56:30 - INFO - __main__ - Step 580 Global step 580 Train loss 1.925496 on epoch=289
03/09/2022 04:56:36 - INFO - __main__ - Step 590 Global step 590 Train loss 2.051920 on epoch=294
03/09/2022 04:56:42 - INFO - __main__ - Step 600 Global step 600 Train loss 1.686947 on epoch=299
03/09/2022 04:56:43 - INFO - __main__ - Global step 600 Train loss 1.940809 QA-F1 0.0 on epoch=299
03/09/2022 04:56:43 - INFO - __main__ - save last model!
03/09/2022 04:56:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:56:44 - INFO - __main__ - Printing 3 examples
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 04:56:44 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 04:56:44 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 04:56:44 - INFO - __main__ - ['Sam Harper']
03/09/2022 04:56:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 04:56:44 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:56:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 04:56:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 04:56:44 - INFO - __main__ - Printing 3 examples
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 04:56:44 - INFO - __main__ - ['Red Shadow']
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 04:56:44 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 04:56:44 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 04:56:44 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 04:56:44 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:56:44 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:56:44 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 04:56:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 04:56:53 - INFO - __main__ - Starting training!
03/09/2022 04:57:25 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 04:57:26 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 04:57:26 - INFO - __main__ - Printing 3 examples
03/09/2022 04:57:26 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:57:26 - INFO - __main__ - ['Frankie']
03/09/2022 04:57:26 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:57:26 - INFO - __main__ - ['Frankie']
03/09/2022 04:57:26 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 04:57:26 - INFO - __main__ - ['Frankie']
03/09/2022 04:57:26 - INFO - __main__ - Tokenizing Input ...
03/09/2022 04:57:30 - INFO - __main__ - Tokenizing Output ...
03/09/2022 04:57:33 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:02:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_42_0.0002_8_predictions.txt
03/09/2022 05:02:10 - INFO - __main__ - QA-F1 on test data: 0.1439
03/09/2022 05:02:10 - INFO - __main__ - prefix=quoref_32_42, lr=0.0002, bsz=8, dev_performance=0.11458333333333333, test_performance=0.1439229648172137
03/09/2022 05:02:10 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.0001, bsz=8 ...
03/09/2022 05:02:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:02:11 - INFO - __main__ - Printing 3 examples
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/09/2022 05:02:11 - INFO - __main__ - ['Esteban Pasquale']
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/09/2022 05:02:11 - INFO - __main__ - ['Ronnie J. McGorvey']
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/09/2022 05:02:11 - INFO - __main__ - ['Sam Harper']
03/09/2022 05:02:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:02:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:02:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:02:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:02:11 - INFO - __main__ - Printing 3 examples
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/09/2022 05:02:11 - INFO - __main__ - ['Red Shadow']
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/09/2022 05:02:11 - INFO - __main__ - ['Louder Than You Think Studio']
03/09/2022 05:02:11 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/09/2022 05:02:11 - INFO - __main__ - ['Matt', 'Wendy']
03/09/2022 05:02:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:02:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:02:11 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:02:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:02:21 - INFO - __main__ - Starting training!
03/09/2022 05:02:27 - INFO - __main__ - Step 10 Global step 10 Train loss 20.289610 on epoch=4
03/09/2022 05:02:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.088657 on epoch=9
03/09/2022 05:02:38 - INFO - __main__ - Step 30 Global step 30 Train loss 15.112656 on epoch=14
03/09/2022 05:02:44 - INFO - __main__ - Step 40 Global step 40 Train loss 12.288713 on epoch=19
03/09/2022 05:02:50 - INFO - __main__ - Step 50 Global step 50 Train loss 11.127782 on epoch=24
03/09/2022 05:02:52 - INFO - __main__ - Global step 50 Train loss 15.181484 QA-F1 0.03125 on epoch=24
03/09/2022 05:03:28 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.03125 on epoch=24, global_step=50
03/09/2022 05:03:34 - INFO - __main__ - Step 60 Global step 60 Train loss 11.045410 on epoch=29
03/09/2022 05:03:40 - INFO - __main__ - Step 70 Global step 70 Train loss 10.317043 on epoch=34
03/09/2022 05:03:46 - INFO - __main__ - Step 80 Global step 80 Train loss 9.786708 on epoch=39
03/09/2022 05:03:52 - INFO - __main__ - Step 90 Global step 90 Train loss 10.013494 on epoch=44
03/09/2022 05:03:58 - INFO - __main__ - Step 100 Global step 100 Train loss 10.046338 on epoch=49
03/09/2022 05:03:59 - INFO - __main__ - Global step 100 Train loss 10.241798 QA-F1 0.0 on epoch=49
03/09/2022 05:04:05 - INFO - __main__ - Step 110 Global step 110 Train loss 9.412627 on epoch=54
03/09/2022 05:04:11 - INFO - __main__ - Step 120 Global step 120 Train loss 9.531297 on epoch=59
03/09/2022 05:04:17 - INFO - __main__ - Step 130 Global step 130 Train loss 9.655782 on epoch=64
03/09/2022 05:04:23 - INFO - __main__ - Step 140 Global step 140 Train loss 10.364325 on epoch=69
03/09/2022 05:04:28 - INFO - __main__ - Step 150 Global step 150 Train loss 9.644108 on epoch=74
03/09/2022 05:04:30 - INFO - __main__ - Global step 150 Train loss 9.721628 QA-F1 0.0 on epoch=74
03/09/2022 05:04:36 - INFO - __main__ - Step 160 Global step 160 Train loss 9.226748 on epoch=79
03/09/2022 05:04:42 - INFO - __main__ - Step 170 Global step 170 Train loss 9.427542 on epoch=84
03/09/2022 05:04:48 - INFO - __main__ - Step 180 Global step 180 Train loss 8.966093 on epoch=89
03/09/2022 05:04:54 - INFO - __main__ - Step 190 Global step 190 Train loss 9.316675 on epoch=94
03/09/2022 05:05:00 - INFO - __main__ - Step 200 Global step 200 Train loss 8.724107 on epoch=99
03/09/2022 05:05:01 - INFO - __main__ - Global step 200 Train loss 9.132233 QA-F1 0.0 on epoch=99
03/09/2022 05:05:07 - INFO - __main__ - Step 210 Global step 210 Train loss 8.296824 on epoch=104
03/09/2022 05:05:13 - INFO - __main__ - Step 220 Global step 220 Train loss 8.280027 on epoch=109
03/09/2022 05:05:19 - INFO - __main__ - Step 230 Global step 230 Train loss 8.231585 on epoch=114
03/09/2022 05:05:24 - INFO - __main__ - Step 240 Global step 240 Train loss 7.935962 on epoch=119
03/09/2022 05:05:31 - INFO - __main__ - Step 250 Global step 250 Train loss 7.719283 on epoch=124
03/09/2022 05:05:32 - INFO - __main__ - Global step 250 Train loss 8.092735 QA-F1 0.0 on epoch=124
03/09/2022 05:05:38 - INFO - __main__ - Step 260 Global step 260 Train loss 8.183718 on epoch=129
03/09/2022 05:05:44 - INFO - __main__ - Step 270 Global step 270 Train loss 8.048853 on epoch=134
03/09/2022 05:05:50 - INFO - __main__ - Step 280 Global step 280 Train loss 7.753585 on epoch=139
03/09/2022 05:05:55 - INFO - __main__ - Step 290 Global step 290 Train loss 7.159158 on epoch=144
03/09/2022 05:06:01 - INFO - __main__ - Step 300 Global step 300 Train loss 7.297301 on epoch=149
03/09/2022 05:06:02 - INFO - __main__ - Global step 300 Train loss 7.688524 QA-F1 0.0 on epoch=149
03/09/2022 05:06:08 - INFO - __main__ - Step 310 Global step 310 Train loss 7.438422 on epoch=154
03/09/2022 05:06:14 - INFO - __main__ - Step 320 Global step 320 Train loss 6.959237 on epoch=159
03/09/2022 05:06:20 - INFO - __main__ - Step 330 Global step 330 Train loss 6.894097 on epoch=164
03/09/2022 05:06:26 - INFO - __main__ - Step 340 Global step 340 Train loss 6.989329 on epoch=169
03/09/2022 05:06:32 - INFO - __main__ - Step 350 Global step 350 Train loss 6.877453 on epoch=174
03/09/2022 05:06:33 - INFO - __main__ - Global step 350 Train loss 7.031708 QA-F1 0.0 on epoch=174
03/09/2022 05:06:39 - INFO - __main__ - Step 360 Global step 360 Train loss 6.472697 on epoch=179
03/09/2022 05:06:45 - INFO - __main__ - Step 370 Global step 370 Train loss 6.256424 on epoch=184
03/09/2022 05:06:51 - INFO - __main__ - Step 380 Global step 380 Train loss 5.733524 on epoch=189
03/09/2022 05:06:57 - INFO - __main__ - Step 390 Global step 390 Train loss 5.697164 on epoch=194
03/09/2022 05:07:03 - INFO - __main__ - Step 400 Global step 400 Train loss 6.163833 on epoch=199
03/09/2022 05:07:04 - INFO - __main__ - Global step 400 Train loss 6.064728 QA-F1 0.0 on epoch=199
03/09/2022 05:07:09 - INFO - __main__ - Step 410 Global step 410 Train loss 5.673707 on epoch=204
03/09/2022 05:07:15 - INFO - __main__ - Step 420 Global step 420 Train loss 5.245284 on epoch=209
03/09/2022 05:07:21 - INFO - __main__ - Step 430 Global step 430 Train loss 5.151333 on epoch=214
03/09/2022 05:07:27 - INFO - __main__ - Step 440 Global step 440 Train loss 4.509143 on epoch=219
03/09/2022 05:07:33 - INFO - __main__ - Step 450 Global step 450 Train loss 4.348531 on epoch=224
03/09/2022 05:07:34 - INFO - __main__ - Global step 450 Train loss 4.985600 QA-F1 0.0 on epoch=224
03/09/2022 05:07:40 - INFO - __main__ - Step 460 Global step 460 Train loss 3.923461 on epoch=229
03/09/2022 05:07:46 - INFO - __main__ - Step 470 Global step 470 Train loss 4.628603 on epoch=234
03/09/2022 05:07:52 - INFO - __main__ - Step 480 Global step 480 Train loss 4.195008 on epoch=239
03/09/2022 05:07:58 - INFO - __main__ - Step 490 Global step 490 Train loss 4.236478 on epoch=244
03/09/2022 05:08:04 - INFO - __main__ - Step 500 Global step 500 Train loss 4.005155 on epoch=249
03/09/2022 05:08:05 - INFO - __main__ - Global step 500 Train loss 4.197741 QA-F1 0.0 on epoch=249
03/09/2022 05:08:11 - INFO - __main__ - Step 510 Global step 510 Train loss 3.801283 on epoch=254
03/09/2022 05:08:17 - INFO - __main__ - Step 520 Global step 520 Train loss 3.752299 on epoch=259
03/09/2022 05:08:23 - INFO - __main__ - Step 530 Global step 530 Train loss 3.727045 on epoch=264
03/09/2022 05:08:29 - INFO - __main__ - Step 540 Global step 540 Train loss 3.705891 on epoch=269
03/09/2022 05:08:35 - INFO - __main__ - Step 550 Global step 550 Train loss 3.073268 on epoch=274
03/09/2022 05:08:35 - INFO - __main__ - Global step 550 Train loss 3.611957 QA-F1 0.0 on epoch=274
03/09/2022 05:08:41 - INFO - __main__ - Step 560 Global step 560 Train loss 3.533192 on epoch=279
03/09/2022 05:08:47 - INFO - __main__ - Step 570 Global step 570 Train loss 3.531981 on epoch=284
03/09/2022 05:08:53 - INFO - __main__ - Step 580 Global step 580 Train loss 3.059793 on epoch=289
03/09/2022 05:08:59 - INFO - __main__ - Step 590 Global step 590 Train loss 3.600706 on epoch=294
03/09/2022 05:09:05 - INFO - __main__ - Step 600 Global step 600 Train loss 2.974388 on epoch=299
03/09/2022 05:09:06 - INFO - __main__ - Global step 600 Train loss 3.340012 QA-F1 0.0 on epoch=299
03/09/2022 05:09:06 - INFO - __main__ - save last model!
03/09/2022 05:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:09:07 - INFO - __main__ - Printing 3 examples
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:09:07 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:09:07 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:09:07 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:09:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 05:09:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:09:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:09:07 - INFO - __main__ - Printing 3 examples
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:09:07 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:09:07 - INFO - __main__ - ['Captain England']
03/09/2022 05:09:07 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:09:07 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:09:07 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:09:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:09:07 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:09:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:09:17 - INFO - __main__ - Starting training!
03/09/2022 05:09:50 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 05:09:51 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 05:09:51 - INFO - __main__ - Printing 3 examples
03/09/2022 05:09:51 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:09:51 - INFO - __main__ - ['Frankie']
03/09/2022 05:09:51 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:09:51 - INFO - __main__ - ['Frankie']
03/09/2022 05:09:51 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:09:51 - INFO - __main__ - ['Frankie']
03/09/2022 05:09:51 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:09:55 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:09:58 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:11:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_42_0.0001_8_predictions.txt
03/09/2022 05:11:41 - INFO - __main__ - QA-F1 on test data: 0.0333
03/09/2022 05:11:41 - INFO - __main__ - prefix=quoref_32_42, lr=0.0001, bsz=8, dev_performance=0.03125, test_performance=0.033306525858708025
03/09/2022 05:11:41 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.0005, bsz=8 ...
03/09/2022 05:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:11:42 - INFO - __main__ - Printing 3 examples
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:11:42 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:11:42 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:11:42 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:11:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:11:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:11:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:11:42 - INFO - __main__ - Printing 3 examples
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:11:42 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:11:42 - INFO - __main__ - ['Captain England']
03/09/2022 05:11:42 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:11:42 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:11:42 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:11:43 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:11:43 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:11:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:11:53 - INFO - __main__ - Starting training!
03/09/2022 05:11:58 - INFO - __main__ - Step 10 Global step 10 Train loss 19.575432 on epoch=4
03/09/2022 05:12:04 - INFO - __main__ - Step 20 Global step 20 Train loss 15.116941 on epoch=9
03/09/2022 05:12:10 - INFO - __main__ - Step 30 Global step 30 Train loss 9.869137 on epoch=14
03/09/2022 05:12:16 - INFO - __main__ - Step 40 Global step 40 Train loss 7.427775 on epoch=19
03/09/2022 05:12:22 - INFO - __main__ - Step 50 Global step 50 Train loss 6.831258 on epoch=24
03/09/2022 05:12:24 - INFO - __main__ - Global step 50 Train loss 11.764109 QA-F1 0.0 on epoch=24
03/09/2022 05:12:59 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 05:13:05 - INFO - __main__ - Step 60 Global step 60 Train loss 5.881715 on epoch=29
03/09/2022 05:13:11 - INFO - __main__ - Step 70 Global step 70 Train loss 5.561713 on epoch=34
03/09/2022 05:13:17 - INFO - __main__ - Step 80 Global step 80 Train loss 4.802261 on epoch=39
03/09/2022 05:13:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.361733 on epoch=44
03/09/2022 05:13:30 - INFO - __main__ - Step 100 Global step 100 Train loss 3.866281 on epoch=49
03/09/2022 05:13:31 - INFO - __main__ - Global step 100 Train loss 4.894741 QA-F1 0.0 on epoch=49
03/09/2022 05:13:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.911698 on epoch=54
03/09/2022 05:13:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.495166 on epoch=59
03/09/2022 05:13:49 - INFO - __main__ - Step 130 Global step 130 Train loss 2.289670 on epoch=64
03/09/2022 05:13:55 - INFO - __main__ - Step 140 Global step 140 Train loss 2.188492 on epoch=69
03/09/2022 05:14:01 - INFO - __main__ - Step 150 Global step 150 Train loss 2.141579 on epoch=74
03/09/2022 05:14:02 - INFO - __main__ - Global step 150 Train loss 2.405321 QA-F1 0.0 on epoch=74
03/09/2022 05:14:08 - INFO - __main__ - Step 160 Global step 160 Train loss 1.946929 on epoch=79
03/09/2022 05:14:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.930941 on epoch=84
03/09/2022 05:14:20 - INFO - __main__ - Step 180 Global step 180 Train loss 1.977131 on epoch=89
03/09/2022 05:14:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.685679 on epoch=94
03/09/2022 05:14:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.862485 on epoch=99
03/09/2022 05:14:33 - INFO - __main__ - Global step 200 Train loss 1.880633 QA-F1 0.0 on epoch=99
03/09/2022 05:14:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.538929 on epoch=104
03/09/2022 05:14:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.553459 on epoch=109
03/09/2022 05:14:51 - INFO - __main__ - Step 230 Global step 230 Train loss 1.519212 on epoch=114
03/09/2022 05:14:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.368838 on epoch=119
03/09/2022 05:15:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.477653 on epoch=124
03/09/2022 05:15:04 - INFO - __main__ - Global step 250 Train loss 1.491618 QA-F1 0.0 on epoch=124
03/09/2022 05:15:10 - INFO - __main__ - Step 260 Global step 260 Train loss 1.397413 on epoch=129
03/09/2022 05:15:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.473849 on epoch=134
03/09/2022 05:15:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.456009 on epoch=139
03/09/2022 05:15:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.339575 on epoch=144
03/09/2022 05:15:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.503605 on epoch=149
03/09/2022 05:15:35 - INFO - __main__ - Global step 300 Train loss 1.434090 QA-F1 0.0 on epoch=149
03/09/2022 05:15:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.161422 on epoch=154
03/09/2022 05:15:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.108270 on epoch=159
03/09/2022 05:15:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.089694 on epoch=164
03/09/2022 05:15:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.161547 on epoch=169
03/09/2022 05:16:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.931167 on epoch=174
03/09/2022 05:16:06 - INFO - __main__ - Global step 350 Train loss 1.090420 QA-F1 0.0 on epoch=174
03/09/2022 05:16:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.674768 on epoch=179
03/09/2022 05:16:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.493917 on epoch=184
03/09/2022 05:16:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.587463 on epoch=189
03/09/2022 05:16:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.224119 on epoch=194
03/09/2022 05:16:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.134858 on epoch=199
03/09/2022 05:16:37 - INFO - __main__ - Global step 400 Train loss 0.423025 QA-F1 0.0 on epoch=199
03/09/2022 05:16:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.144380 on epoch=204
03/09/2022 05:16:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.246753 on epoch=209
03/09/2022 05:16:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.192622 on epoch=214
03/09/2022 05:17:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.120034 on epoch=219
03/09/2022 05:17:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.056122 on epoch=224
03/09/2022 05:17:09 - INFO - __main__ - Global step 450 Train loss 0.151982 QA-F1 0.03125 on epoch=224
03/09/2022 05:17:45 - INFO - __main__ - Saving model with best QA-F1: 0.0 -> 0.03125 on epoch=224, global_step=450
03/09/2022 05:17:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.094958 on epoch=229
03/09/2022 05:17:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.095848 on epoch=234
03/09/2022 05:18:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.071455 on epoch=239
03/09/2022 05:18:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.053442 on epoch=244
03/09/2022 05:18:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.042959 on epoch=249
03/09/2022 05:18:15 - INFO - __main__ - Global step 500 Train loss 0.071732 QA-F1 0.0 on epoch=249
03/09/2022 05:18:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.047122 on epoch=254
03/09/2022 05:18:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.074208 on epoch=259
03/09/2022 05:18:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.051952 on epoch=264
03/09/2022 05:18:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.036826 on epoch=269
03/09/2022 05:18:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.061936 on epoch=274
03/09/2022 05:18:47 - INFO - __main__ - Global step 550 Train loss 0.054409 QA-F1 0.0 on epoch=274
03/09/2022 05:18:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.050760 on epoch=279
03/09/2022 05:18:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.055564 on epoch=284
03/09/2022 05:19:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.055585 on epoch=289
03/09/2022 05:19:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.065099 on epoch=294
03/09/2022 05:19:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.058397 on epoch=299
03/09/2022 05:19:17 - INFO - __main__ - Global step 600 Train loss 0.057081 QA-F1 0.0 on epoch=299
03/09/2022 05:19:17 - INFO - __main__ - save last model!
03/09/2022 05:19:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:19:17 - INFO - __main__ - Printing 3 examples
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:19:17 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:19:17 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:19:17 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:19:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 05:19:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:19:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:19:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:19:17 - INFO - __main__ - Printing 3 examples
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:19:17 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:19:17 - INFO - __main__ - ['Captain England']
03/09/2022 05:19:17 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:19:17 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:19:17 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:19:18 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:19:18 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:19:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:19:28 - INFO - __main__ - Starting training!
03/09/2022 05:19:59 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 05:19:59 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 05:19:59 - INFO - __main__ - Printing 3 examples
03/09/2022 05:19:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:19:59 - INFO - __main__ - ['Frankie']
03/09/2022 05:19:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:19:59 - INFO - __main__ - ['Frankie']
03/09/2022 05:19:59 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:19:59 - INFO - __main__ - ['Frankie']
03/09/2022 05:19:59 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:20:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:20:06 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:21:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_87_0.0005_8_predictions.txt
03/09/2022 05:21:28 - INFO - __main__ - QA-F1 on test data: 0.0357
03/09/2022 05:21:28 - INFO - __main__ - prefix=quoref_32_87, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.03566833405543083
03/09/2022 05:21:28 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.0003, bsz=8 ...
03/09/2022 05:21:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:21:29 - INFO - __main__ - Printing 3 examples
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:21:29 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:21:29 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:21:29 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:21:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:21:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:21:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:21:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:21:29 - INFO - __main__ - Printing 3 examples
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:21:29 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:21:29 - INFO - __main__ - ['Captain England']
03/09/2022 05:21:29 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:21:29 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:21:29 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:21:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:21:30 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:21:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:21:38 - INFO - __main__ - Starting training!
03/09/2022 05:21:44 - INFO - __main__ - Step 10 Global step 10 Train loss 20.516785 on epoch=4
03/09/2022 05:21:49 - INFO - __main__ - Step 20 Global step 20 Train loss 17.106071 on epoch=9
03/09/2022 05:21:54 - INFO - __main__ - Step 30 Global step 30 Train loss 12.745146 on epoch=14
03/09/2022 05:22:00 - INFO - __main__ - Step 40 Global step 40 Train loss 10.808120 on epoch=19
03/09/2022 05:22:06 - INFO - __main__ - Step 50 Global step 50 Train loss 9.234779 on epoch=24
03/09/2022 05:22:10 - INFO - __main__ - Global step 50 Train loss 14.082180 QA-F1 0.17418478260869563 on epoch=24
03/09/2022 05:22:46 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.17418478260869563 on epoch=24, global_step=50
03/09/2022 05:22:51 - INFO - __main__ - Step 60 Global step 60 Train loss 8.611952 on epoch=29
03/09/2022 05:22:57 - INFO - __main__ - Step 70 Global step 70 Train loss 8.130554 on epoch=34
03/09/2022 05:23:03 - INFO - __main__ - Step 80 Global step 80 Train loss 7.851463 on epoch=39
03/09/2022 05:23:09 - INFO - __main__ - Step 90 Global step 90 Train loss 7.896091 on epoch=44
03/09/2022 05:23:15 - INFO - __main__ - Step 100 Global step 100 Train loss 6.596982 on epoch=49
03/09/2022 05:23:16 - INFO - __main__ - Global step 100 Train loss 7.817409 QA-F1 0.0 on epoch=49
03/09/2022 05:23:22 - INFO - __main__ - Step 110 Global step 110 Train loss 6.248035 on epoch=54
03/09/2022 05:23:28 - INFO - __main__ - Step 120 Global step 120 Train loss 5.719485 on epoch=59
03/09/2022 05:23:34 - INFO - __main__ - Step 130 Global step 130 Train loss 5.521852 on epoch=64
03/09/2022 05:23:39 - INFO - __main__ - Step 140 Global step 140 Train loss 4.754719 on epoch=69
03/09/2022 05:23:45 - INFO - __main__ - Step 150 Global step 150 Train loss 4.825612 on epoch=74
03/09/2022 05:23:47 - INFO - __main__ - Global step 150 Train loss 5.413940 QA-F1 0.0 on epoch=74
03/09/2022 05:23:53 - INFO - __main__ - Step 160 Global step 160 Train loss 4.460968 on epoch=79
03/09/2022 05:23:59 - INFO - __main__ - Step 170 Global step 170 Train loss 3.795222 on epoch=84
03/09/2022 05:24:05 - INFO - __main__ - Step 180 Global step 180 Train loss 3.748608 on epoch=89
03/09/2022 05:24:10 - INFO - __main__ - Step 190 Global step 190 Train loss 3.454470 on epoch=94
03/09/2022 05:24:16 - INFO - __main__ - Step 200 Global step 200 Train loss 3.090650 on epoch=99
03/09/2022 05:24:17 - INFO - __main__ - Global step 200 Train loss 3.709984 QA-F1 0.0 on epoch=99
03/09/2022 05:24:23 - INFO - __main__ - Step 210 Global step 210 Train loss 2.692289 on epoch=104
03/09/2022 05:24:29 - INFO - __main__ - Step 220 Global step 220 Train loss 2.639028 on epoch=109
03/09/2022 05:24:35 - INFO - __main__ - Step 230 Global step 230 Train loss 2.407462 on epoch=114
03/09/2022 05:24:41 - INFO - __main__ - Step 240 Global step 240 Train loss 2.363686 on epoch=119
03/09/2022 05:24:47 - INFO - __main__ - Step 250 Global step 250 Train loss 2.424944 on epoch=124
03/09/2022 05:24:47 - INFO - __main__ - Global step 250 Train loss 2.505482 QA-F1 0.0 on epoch=124
03/09/2022 05:24:53 - INFO - __main__ - Step 260 Global step 260 Train loss 2.042796 on epoch=129
03/09/2022 05:24:59 - INFO - __main__ - Step 270 Global step 270 Train loss 2.109064 on epoch=134
03/09/2022 05:25:05 - INFO - __main__ - Step 280 Global step 280 Train loss 2.227969 on epoch=139
03/09/2022 05:25:11 - INFO - __main__ - Step 290 Global step 290 Train loss 2.000789 on epoch=144
03/09/2022 05:25:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.932517 on epoch=149
03/09/2022 05:25:18 - INFO - __main__ - Global step 300 Train loss 2.062627 QA-F1 0.0 on epoch=149
03/09/2022 05:25:23 - INFO - __main__ - Step 310 Global step 310 Train loss 1.742976 on epoch=154
03/09/2022 05:25:29 - INFO - __main__ - Step 320 Global step 320 Train loss 2.181209 on epoch=159
03/09/2022 05:25:35 - INFO - __main__ - Step 330 Global step 330 Train loss 1.948760 on epoch=164
03/09/2022 05:25:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.735038 on epoch=169
03/09/2022 05:25:47 - INFO - __main__ - Step 350 Global step 350 Train loss 1.887993 on epoch=174
03/09/2022 05:25:48 - INFO - __main__ - Global step 350 Train loss 1.899195 QA-F1 0.0 on epoch=174
03/09/2022 05:25:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.643149 on epoch=179
03/09/2022 05:26:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.582565 on epoch=184
03/09/2022 05:26:06 - INFO - __main__ - Step 380 Global step 380 Train loss 1.800263 on epoch=189
03/09/2022 05:26:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.664863 on epoch=194
03/09/2022 05:26:17 - INFO - __main__ - Step 400 Global step 400 Train loss 1.331096 on epoch=199
03/09/2022 05:26:18 - INFO - __main__ - Global step 400 Train loss 1.604387 QA-F1 0.0 on epoch=199
03/09/2022 05:26:24 - INFO - __main__ - Step 410 Global step 410 Train loss 1.615985 on epoch=204
03/09/2022 05:26:30 - INFO - __main__ - Step 420 Global step 420 Train loss 1.498066 on epoch=209
03/09/2022 05:26:36 - INFO - __main__ - Step 430 Global step 430 Train loss 1.533777 on epoch=214
03/09/2022 05:26:42 - INFO - __main__ - Step 440 Global step 440 Train loss 1.529266 on epoch=219
03/09/2022 05:26:48 - INFO - __main__ - Step 450 Global step 450 Train loss 1.380805 on epoch=224
03/09/2022 05:26:49 - INFO - __main__ - Global step 450 Train loss 1.511580 QA-F1 0.0 on epoch=224
03/09/2022 05:26:55 - INFO - __main__ - Step 460 Global step 460 Train loss 1.423247 on epoch=229
03/09/2022 05:27:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.468124 on epoch=234
03/09/2022 05:27:06 - INFO - __main__ - Step 480 Global step 480 Train loss 1.317011 on epoch=239
03/09/2022 05:27:12 - INFO - __main__ - Step 490 Global step 490 Train loss 1.335938 on epoch=244
03/09/2022 05:27:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.324062 on epoch=249
03/09/2022 05:27:19 - INFO - __main__ - Global step 500 Train loss 1.373676 QA-F1 0.0 on epoch=249
03/09/2022 05:27:25 - INFO - __main__ - Step 510 Global step 510 Train loss 1.304831 on epoch=254
03/09/2022 05:27:31 - INFO - __main__ - Step 520 Global step 520 Train loss 1.256001 on epoch=259
03/09/2022 05:27:37 - INFO - __main__ - Step 530 Global step 530 Train loss 1.208933 on epoch=264
03/09/2022 05:27:43 - INFO - __main__ - Step 540 Global step 540 Train loss 1.335769 on epoch=269
03/09/2022 05:27:49 - INFO - __main__ - Step 550 Global step 550 Train loss 1.128602 on epoch=274
03/09/2022 05:27:49 - INFO - __main__ - Global step 550 Train loss 1.246827 QA-F1 0.0 on epoch=274
03/09/2022 05:27:55 - INFO - __main__ - Step 560 Global step 560 Train loss 1.270474 on epoch=279
03/09/2022 05:28:01 - INFO - __main__ - Step 570 Global step 570 Train loss 1.289856 on epoch=284
03/09/2022 05:28:07 - INFO - __main__ - Step 580 Global step 580 Train loss 1.232073 on epoch=289
03/09/2022 05:28:13 - INFO - __main__ - Step 590 Global step 590 Train loss 1.129218 on epoch=294
03/09/2022 05:28:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.183479 on epoch=299
03/09/2022 05:28:19 - INFO - __main__ - Global step 600 Train loss 1.221020 QA-F1 0.0 on epoch=299
03/09/2022 05:28:19 - INFO - __main__ - save last model!
03/09/2022 05:28:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:28:21 - INFO - __main__ - Printing 3 examples
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:28:21 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:28:21 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:28:21 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:28:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 05:28:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:28:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:28:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:28:21 - INFO - __main__ - Printing 3 examples
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:28:21 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:28:21 - INFO - __main__ - ['Captain England']
03/09/2022 05:28:21 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:28:21 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:28:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:28:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:28:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:28:31 - INFO - __main__ - Starting training!
03/09/2022 05:29:01 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 05:29:02 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 05:29:02 - INFO - __main__ - Printing 3 examples
03/09/2022 05:29:02 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:29:02 - INFO - __main__ - ['Frankie']
03/09/2022 05:29:02 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:29:02 - INFO - __main__ - ['Frankie']
03/09/2022 05:29:02 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:29:02 - INFO - __main__ - ['Frankie']
03/09/2022 05:29:02 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:29:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:29:09 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:32:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_87_0.0003_8_predictions.txt
03/09/2022 05:32:40 - INFO - __main__ - QA-F1 on test data: 0.1554
03/09/2022 05:32:41 - INFO - __main__ - prefix=quoref_32_87, lr=0.0003, bsz=8, dev_performance=0.17418478260869563, test_performance=0.1553522715392026
03/09/2022 05:32:41 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.0002, bsz=8 ...
03/09/2022 05:32:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:32:41 - INFO - __main__ - Printing 3 examples
03/09/2022 05:32:41 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:32:41 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:32:41 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:32:41 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:32:41 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:32:41 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:32:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:32:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:32:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:32:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:32:42 - INFO - __main__ - Printing 3 examples
03/09/2022 05:32:42 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:32:42 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:32:42 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:32:42 - INFO - __main__ - ['Captain England']
03/09/2022 05:32:42 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:32:42 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:32:42 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:32:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:32:42 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:32:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:32:52 - INFO - __main__ - Starting training!
03/09/2022 05:32:57 - INFO - __main__ - Step 10 Global step 10 Train loss 19.628277 on epoch=4
03/09/2022 05:33:03 - INFO - __main__ - Step 20 Global step 20 Train loss 14.866035 on epoch=9
03/09/2022 05:33:09 - INFO - __main__ - Step 30 Global step 30 Train loss 9.483915 on epoch=14
03/09/2022 05:33:15 - INFO - __main__ - Step 40 Global step 40 Train loss 8.470458 on epoch=19
03/09/2022 05:33:21 - INFO - __main__ - Step 50 Global step 50 Train loss 9.801353 on epoch=24
03/09/2022 05:33:23 - INFO - __main__ - Global step 50 Train loss 12.450007 QA-F1 0.03125 on epoch=24
03/09/2022 05:34:00 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.03125 on epoch=24, global_step=50
03/09/2022 05:34:06 - INFO - __main__ - Step 60 Global step 60 Train loss 9.206266 on epoch=29
03/09/2022 05:34:12 - INFO - __main__ - Step 70 Global step 70 Train loss 8.455765 on epoch=34
03/09/2022 05:34:18 - INFO - __main__ - Step 80 Global step 80 Train loss 7.719354 on epoch=39
03/09/2022 05:34:23 - INFO - __main__ - Step 90 Global step 90 Train loss 7.601840 on epoch=44
03/09/2022 05:34:29 - INFO - __main__ - Step 100 Global step 100 Train loss 7.572375 on epoch=49
03/09/2022 05:34:31 - INFO - __main__ - Global step 100 Train loss 8.111120 QA-F1 0.0 on epoch=49
03/09/2022 05:34:36 - INFO - __main__ - Step 110 Global step 110 Train loss 6.785839 on epoch=54
03/09/2022 05:34:42 - INFO - __main__ - Step 120 Global step 120 Train loss 6.244575 on epoch=59
03/09/2022 05:34:48 - INFO - __main__ - Step 130 Global step 130 Train loss 6.609071 on epoch=64
03/09/2022 05:34:54 - INFO - __main__ - Step 140 Global step 140 Train loss 6.201789 on epoch=69
03/09/2022 05:35:00 - INFO - __main__ - Step 150 Global step 150 Train loss 5.883893 on epoch=74
03/09/2022 05:35:01 - INFO - __main__ - Global step 150 Train loss 6.345033 QA-F1 0.0 on epoch=74
03/09/2022 05:35:07 - INFO - __main__ - Step 160 Global step 160 Train loss 5.859441 on epoch=79
03/09/2022 05:35:13 - INFO - __main__ - Step 170 Global step 170 Train loss 5.555279 on epoch=84
03/09/2022 05:35:18 - INFO - __main__ - Step 180 Global step 180 Train loss 5.377297 on epoch=89
03/09/2022 05:35:24 - INFO - __main__ - Step 190 Global step 190 Train loss 5.569384 on epoch=94
03/09/2022 05:35:30 - INFO - __main__ - Step 200 Global step 200 Train loss 5.367131 on epoch=99
03/09/2022 05:35:31 - INFO - __main__ - Global step 200 Train loss 5.545707 QA-F1 0.0 on epoch=99
03/09/2022 05:35:37 - INFO - __main__ - Step 210 Global step 210 Train loss 5.104907 on epoch=104
03/09/2022 05:35:43 - INFO - __main__ - Step 220 Global step 220 Train loss 4.656850 on epoch=109
03/09/2022 05:35:49 - INFO - __main__ - Step 230 Global step 230 Train loss 4.584580 on epoch=114
03/09/2022 05:35:55 - INFO - __main__ - Step 240 Global step 240 Train loss 4.129878 on epoch=119
03/09/2022 05:36:01 - INFO - __main__ - Step 250 Global step 250 Train loss 3.987413 on epoch=124
03/09/2022 05:36:02 - INFO - __main__ - Global step 250 Train loss 4.492725 QA-F1 0.0 on epoch=124
03/09/2022 05:36:08 - INFO - __main__ - Step 260 Global step 260 Train loss 3.813276 on epoch=129
03/09/2022 05:36:14 - INFO - __main__ - Step 270 Global step 270 Train loss 3.294289 on epoch=134
03/09/2022 05:36:19 - INFO - __main__ - Step 280 Global step 280 Train loss 3.148108 on epoch=139
03/09/2022 05:36:25 - INFO - __main__ - Step 290 Global step 290 Train loss 3.096838 on epoch=144
03/09/2022 05:36:31 - INFO - __main__ - Step 300 Global step 300 Train loss 2.951495 on epoch=149
03/09/2022 05:36:32 - INFO - __main__ - Global step 300 Train loss 3.260801 QA-F1 0.0 on epoch=149
03/09/2022 05:36:38 - INFO - __main__ - Step 310 Global step 310 Train loss 2.703992 on epoch=154
03/09/2022 05:36:44 - INFO - __main__ - Step 320 Global step 320 Train loss 2.435605 on epoch=159
03/09/2022 05:36:50 - INFO - __main__ - Step 330 Global step 330 Train loss 2.282870 on epoch=164
03/09/2022 05:36:56 - INFO - __main__ - Step 340 Global step 340 Train loss 2.524250 on epoch=169
03/09/2022 05:37:02 - INFO - __main__ - Step 350 Global step 350 Train loss 2.237490 on epoch=174
03/09/2022 05:37:03 - INFO - __main__ - Global step 350 Train loss 2.436841 QA-F1 0.0 on epoch=174
03/09/2022 05:37:09 - INFO - __main__ - Step 360 Global step 360 Train loss 2.191478 on epoch=179
03/09/2022 05:37:15 - INFO - __main__ - Step 370 Global step 370 Train loss 2.016362 on epoch=184
03/09/2022 05:37:21 - INFO - __main__ - Step 380 Global step 380 Train loss 2.078559 on epoch=189
03/09/2022 05:37:26 - INFO - __main__ - Step 390 Global step 390 Train loss 2.355678 on epoch=194
03/09/2022 05:37:32 - INFO - __main__ - Step 400 Global step 400 Train loss 2.189086 on epoch=199
03/09/2022 05:37:33 - INFO - __main__ - Global step 400 Train loss 2.166233 QA-F1 0.0 on epoch=199
03/09/2022 05:37:39 - INFO - __main__ - Step 410 Global step 410 Train loss 2.042010 on epoch=204
03/09/2022 05:37:45 - INFO - __main__ - Step 420 Global step 420 Train loss 1.943957 on epoch=209
03/09/2022 05:37:51 - INFO - __main__ - Step 430 Global step 430 Train loss 2.112341 on epoch=214
03/09/2022 05:37:57 - INFO - __main__ - Step 440 Global step 440 Train loss 2.003211 on epoch=219
03/09/2022 05:38:03 - INFO - __main__ - Step 450 Global step 450 Train loss 2.124037 on epoch=224
03/09/2022 05:38:04 - INFO - __main__ - Global step 450 Train loss 2.045111 QA-F1 0.0 on epoch=224
03/09/2022 05:38:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.950469 on epoch=229
03/09/2022 05:38:16 - INFO - __main__ - Step 470 Global step 470 Train loss 1.633680 on epoch=234
03/09/2022 05:38:22 - INFO - __main__ - Step 480 Global step 480 Train loss 1.889720 on epoch=239
03/09/2022 05:38:28 - INFO - __main__ - Step 490 Global step 490 Train loss 1.651527 on epoch=244
03/09/2022 05:38:34 - INFO - __main__ - Step 500 Global step 500 Train loss 1.731787 on epoch=249
03/09/2022 05:38:36 - INFO - __main__ - Global step 500 Train loss 1.771436 QA-F1 0.0 on epoch=249
03/09/2022 05:38:41 - INFO - __main__ - Step 510 Global step 510 Train loss 1.821306 on epoch=254
03/09/2022 05:38:47 - INFO - __main__ - Step 520 Global step 520 Train loss 1.557337 on epoch=259
03/09/2022 05:38:53 - INFO - __main__ - Step 530 Global step 530 Train loss 1.580449 on epoch=264
03/09/2022 05:38:59 - INFO - __main__ - Step 540 Global step 540 Train loss 1.795176 on epoch=269
03/09/2022 05:39:05 - INFO - __main__ - Step 550 Global step 550 Train loss 1.617706 on epoch=274
03/09/2022 05:39:06 - INFO - __main__ - Global step 550 Train loss 1.674395 QA-F1 0.0 on epoch=274
03/09/2022 05:39:12 - INFO - __main__ - Step 560 Global step 560 Train loss 1.677685 on epoch=279
03/09/2022 05:39:18 - INFO - __main__ - Step 570 Global step 570 Train loss 1.470004 on epoch=284
03/09/2022 05:39:24 - INFO - __main__ - Step 580 Global step 580 Train loss 1.524921 on epoch=289
03/09/2022 05:39:30 - INFO - __main__ - Step 590 Global step 590 Train loss 1.642112 on epoch=294
03/09/2022 05:39:36 - INFO - __main__ - Step 600 Global step 600 Train loss 1.422824 on epoch=299
03/09/2022 05:39:37 - INFO - __main__ - Global step 600 Train loss 1.547509 QA-F1 0.0 on epoch=299
03/09/2022 05:39:37 - INFO - __main__ - save last model!
03/09/2022 05:39:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:39:37 - INFO - __main__ - Printing 3 examples
03/09/2022 05:39:37 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:39:37 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:39:37 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:39:37 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:39:37 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:39:37 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:39:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 05:39:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:39:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:39:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:39:38 - INFO - __main__ - Printing 3 examples
03/09/2022 05:39:38 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:39:38 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:39:38 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:39:38 - INFO - __main__ - ['Captain England']
03/09/2022 05:39:38 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:39:38 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:39:38 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:39:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:39:38 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:39:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:39:47 - INFO - __main__ - Starting training!
03/09/2022 05:40:19 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 05:40:20 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 05:40:20 - INFO - __main__ - Printing 3 examples
03/09/2022 05:40:20 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:40:20 - INFO - __main__ - ['Frankie']
03/09/2022 05:40:20 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:40:20 - INFO - __main__ - ['Frankie']
03/09/2022 05:40:20 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:40:20 - INFO - __main__ - ['Frankie']
03/09/2022 05:40:20 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:40:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:40:27 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:41:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_87_0.0002_8_predictions.txt
03/09/2022 05:41:38 - INFO - __main__ - QA-F1 on test data: 0.0019
03/09/2022 05:41:39 - INFO - __main__ - prefix=quoref_32_87, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.001929969671905156
03/09/2022 05:41:39 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.0001, bsz=8 ...
03/09/2022 05:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:41:40 - INFO - __main__ - Printing 3 examples
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/09/2022 05:41:40 - INFO - __main__ - ['Elaine Benson']
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/09/2022 05:41:40 - INFO - __main__ - ['Richard Boston']
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/09/2022 05:41:40 - INFO - __main__ - ['The Knight Errant']
03/09/2022 05:41:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:41:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:41:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:41:40 - INFO - __main__ - Printing 3 examples
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/09/2022 05:41:40 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/09/2022 05:41:40 - INFO - __main__ - ['Captain England']
03/09/2022 05:41:40 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/09/2022 05:41:40 - INFO - __main__ - ['Saint-Saëns']
03/09/2022 05:41:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:41:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:41:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:41:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:41:49 - INFO - __main__ - Starting training!
03/09/2022 05:41:56 - INFO - __main__ - Step 10 Global step 10 Train loss 20.057089 on epoch=4
03/09/2022 05:42:02 - INFO - __main__ - Step 20 Global step 20 Train loss 18.166452 on epoch=9
03/09/2022 05:42:08 - INFO - __main__ - Step 30 Global step 30 Train loss 15.849751 on epoch=14
03/09/2022 05:42:14 - INFO - __main__ - Step 40 Global step 40 Train loss 15.337064 on epoch=19
03/09/2022 05:42:20 - INFO - __main__ - Step 50 Global step 50 Train loss 12.871837 on epoch=24
03/09/2022 05:42:24 - INFO - __main__ - Global step 50 Train loss 16.456440 QA-F1 0.07532894736842105 on epoch=24
03/09/2022 05:43:00 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.07532894736842105 on epoch=24, global_step=50
03/09/2022 05:43:06 - INFO - __main__ - Step 60 Global step 60 Train loss 12.317796 on epoch=29
03/09/2022 05:43:12 - INFO - __main__ - Step 70 Global step 70 Train loss 11.493568 on epoch=34
03/09/2022 05:43:18 - INFO - __main__ - Step 80 Global step 80 Train loss 9.766035 on epoch=39
03/09/2022 05:43:24 - INFO - __main__ - Step 90 Global step 90 Train loss 9.415823 on epoch=44
03/09/2022 05:43:30 - INFO - __main__ - Step 100 Global step 100 Train loss 9.379128 on epoch=49
03/09/2022 05:43:31 - INFO - __main__ - Global step 100 Train loss 10.474470 QA-F1 0.026785714285714284 on epoch=49
03/09/2022 05:43:37 - INFO - __main__ - Step 110 Global step 110 Train loss 8.310825 on epoch=54
03/09/2022 05:43:43 - INFO - __main__ - Step 120 Global step 120 Train loss 8.177399 on epoch=59
03/09/2022 05:43:49 - INFO - __main__ - Step 130 Global step 130 Train loss 7.860756 on epoch=64
03/09/2022 05:43:55 - INFO - __main__ - Step 140 Global step 140 Train loss 7.746880 on epoch=69
03/09/2022 05:44:01 - INFO - __main__ - Step 150 Global step 150 Train loss 7.638233 on epoch=74
03/09/2022 05:44:02 - INFO - __main__ - Global step 150 Train loss 7.946818 QA-F1 0.03125 on epoch=74
03/09/2022 05:44:08 - INFO - __main__ - Step 160 Global step 160 Train loss 7.602502 on epoch=79
03/09/2022 05:44:14 - INFO - __main__ - Step 170 Global step 170 Train loss 7.323495 on epoch=84
03/09/2022 05:44:20 - INFO - __main__ - Step 180 Global step 180 Train loss 7.310252 on epoch=89
03/09/2022 05:44:26 - INFO - __main__ - Step 190 Global step 190 Train loss 7.000025 on epoch=94
03/09/2022 05:44:32 - INFO - __main__ - Step 200 Global step 200 Train loss 6.545555 on epoch=99
03/09/2022 05:44:33 - INFO - __main__ - Global step 200 Train loss 7.156366 QA-F1 0.0 on epoch=99
03/09/2022 05:44:38 - INFO - __main__ - Step 210 Global step 210 Train loss 7.234091 on epoch=104
03/09/2022 05:44:44 - INFO - __main__ - Step 220 Global step 220 Train loss 7.549142 on epoch=109
03/09/2022 05:44:50 - INFO - __main__ - Step 230 Global step 230 Train loss 7.196799 on epoch=114
03/09/2022 05:44:56 - INFO - __main__ - Step 240 Global step 240 Train loss 6.941808 on epoch=119
03/09/2022 05:45:02 - INFO - __main__ - Step 250 Global step 250 Train loss 6.668073 on epoch=124
03/09/2022 05:45:04 - INFO - __main__ - Global step 250 Train loss 7.117983 QA-F1 0.020833333333333332 on epoch=124
03/09/2022 05:45:10 - INFO - __main__ - Step 260 Global step 260 Train loss 6.370610 on epoch=129
03/09/2022 05:45:16 - INFO - __main__ - Step 270 Global step 270 Train loss 6.348714 on epoch=134
03/09/2022 05:45:22 - INFO - __main__ - Step 280 Global step 280 Train loss 6.233778 on epoch=139
03/09/2022 05:45:27 - INFO - __main__ - Step 290 Global step 290 Train loss 6.460138 on epoch=144
03/09/2022 05:45:33 - INFO - __main__ - Step 300 Global step 300 Train loss 6.072008 on epoch=149
03/09/2022 05:45:35 - INFO - __main__ - Global step 300 Train loss 6.297050 QA-F1 0.0 on epoch=149
03/09/2022 05:45:41 - INFO - __main__ - Step 310 Global step 310 Train loss 6.206586 on epoch=154
03/09/2022 05:45:47 - INFO - __main__ - Step 320 Global step 320 Train loss 6.093820 on epoch=159
03/09/2022 05:45:53 - INFO - __main__ - Step 330 Global step 330 Train loss 6.091877 on epoch=164
03/09/2022 05:45:59 - INFO - __main__ - Step 340 Global step 340 Train loss 5.926780 on epoch=169
03/09/2022 05:46:05 - INFO - __main__ - Step 350 Global step 350 Train loss 5.500691 on epoch=174
03/09/2022 05:46:07 - INFO - __main__ - Global step 350 Train loss 5.963952 QA-F1 0.0 on epoch=174
03/09/2022 05:46:13 - INFO - __main__ - Step 360 Global step 360 Train loss 5.569926 on epoch=179
03/09/2022 05:46:19 - INFO - __main__ - Step 370 Global step 370 Train loss 5.369431 on epoch=184
03/09/2022 05:46:25 - INFO - __main__ - Step 380 Global step 380 Train loss 4.785861 on epoch=189
03/09/2022 05:46:31 - INFO - __main__ - Step 390 Global step 390 Train loss 5.125502 on epoch=194
03/09/2022 05:46:37 - INFO - __main__ - Step 400 Global step 400 Train loss 4.858932 on epoch=199
03/09/2022 05:46:38 - INFO - __main__ - Global step 400 Train loss 5.141930 QA-F1 0.0 on epoch=199
03/09/2022 05:46:44 - INFO - __main__ - Step 410 Global step 410 Train loss 4.887594 on epoch=204
03/09/2022 05:46:50 - INFO - __main__ - Step 420 Global step 420 Train loss 4.729108 on epoch=209
03/09/2022 05:46:56 - INFO - __main__ - Step 430 Global step 430 Train loss 4.700627 on epoch=214
03/09/2022 05:47:02 - INFO - __main__ - Step 440 Global step 440 Train loss 4.851336 on epoch=219
03/09/2022 05:47:08 - INFO - __main__ - Step 450 Global step 450 Train loss 4.274252 on epoch=224
03/09/2022 05:47:10 - INFO - __main__ - Global step 450 Train loss 4.688584 QA-F1 0.0 on epoch=224
03/09/2022 05:47:15 - INFO - __main__ - Step 460 Global step 460 Train loss 4.187916 on epoch=229
03/09/2022 05:47:21 - INFO - __main__ - Step 470 Global step 470 Train loss 4.259673 on epoch=234
03/09/2022 05:47:27 - INFO - __main__ - Step 480 Global step 480 Train loss 4.193449 on epoch=239
03/09/2022 05:47:33 - INFO - __main__ - Step 490 Global step 490 Train loss 3.907923 on epoch=244
03/09/2022 05:47:39 - INFO - __main__ - Step 500 Global step 500 Train loss 3.842250 on epoch=249
03/09/2022 05:47:41 - INFO - __main__ - Global step 500 Train loss 4.078242 QA-F1 0.0 on epoch=249
03/09/2022 05:47:47 - INFO - __main__ - Step 510 Global step 510 Train loss 3.929593 on epoch=254
03/09/2022 05:47:53 - INFO - __main__ - Step 520 Global step 520 Train loss 3.616871 on epoch=259
03/09/2022 05:47:59 - INFO - __main__ - Step 530 Global step 530 Train loss 3.335164 on epoch=264
03/09/2022 05:48:04 - INFO - __main__ - Step 540 Global step 540 Train loss 3.235898 on epoch=269
03/09/2022 05:48:10 - INFO - __main__ - Step 550 Global step 550 Train loss 3.021636 on epoch=274
03/09/2022 05:48:12 - INFO - __main__ - Global step 550 Train loss 3.427833 QA-F1 0.0 on epoch=274
03/09/2022 05:48:18 - INFO - __main__ - Step 560 Global step 560 Train loss 2.842910 on epoch=279
03/09/2022 05:48:24 - INFO - __main__ - Step 570 Global step 570 Train loss 3.167325 on epoch=284
03/09/2022 05:48:30 - INFO - __main__ - Step 580 Global step 580 Train loss 2.679682 on epoch=289
03/09/2022 05:48:36 - INFO - __main__ - Step 590 Global step 590 Train loss 2.764678 on epoch=294
03/09/2022 05:48:42 - INFO - __main__ - Step 600 Global step 600 Train loss 2.938367 on epoch=299
03/09/2022 05:48:44 - INFO - __main__ - Global step 600 Train loss 2.878592 QA-F1 0.0 on epoch=299
03/09/2022 05:48:44 - INFO - __main__ - save last model!
03/09/2022 05:49:26 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 05:49:27 - INFO - __main__ - Start tokenizing ... 2418 instances
03/09/2022 05:49:27 - INFO - __main__ - Printing 3 examples
03/09/2022 05:49:27 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:49:27 - INFO - __main__ - ['Frankie']
03/09/2022 05:49:27 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:49:27 - INFO - __main__ - ['Frankie']
03/09/2022 05:49:27 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/09/2022 05:49:27 - INFO - __main__ - ['Frankie']
03/09/2022 05:49:27 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:49:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:49:34 - INFO - __main__ - Loaded 2418 examples from test data
03/09/2022 05:53:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-quoref/quoref_32_87_0.0001_8_predictions.txt
03/09/2022 05:53:45 - INFO - __main__ - QA-F1 on test data: 0.0364
03/09/2022 05:53:46 - INFO - __main__ - prefix=quoref_32_87, lr=0.0001, bsz=8, dev_performance=0.07532894736842105, test_performance=0.036420405661105215
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.000518798828125 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13543", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12437, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "13544", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12437, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 12437, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (13631): No such process
Task: amazon_polarity, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_q4r12_kt/none_dqcxwquj
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_q4r12_kt/none_dqcxwquj/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_q4r12_kt/none_dqcxwquj/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/09/2022 05:53:53 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 05:53:53 - INFO - __main__ - models/T5-large-ft-random/singletask-amazon_polarity
03/09/2022 05:53:53 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 05:53:53 - INFO - __main__ - models/T5-large-ft-random/singletask-amazon_polarity
03/09/2022 05:53:53 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/09/2022 05:53:53 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/09/2022 05:53:53 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/09/2022 05:53:53 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/09/2022 05:53:53 - INFO - __main__ - args.device: cuda:0
03/09/2022 05:53:53 - INFO - __main__ - args.device: cuda:1
03/09/2022 05:53:53 - INFO - __main__ - Using 2 gpus
03/09/2022 05:53:53 - INFO - __main__ - Using 2 gpus
03/09/2022 05:53:53 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/09/2022 05:53:53 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/09/2022 05:53:58 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0005, bsz=8 ...
03/09/2022 05:53:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:53:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:53:59 - INFO - __main__ - Printing 3 examples
03/09/2022 05:53:59 - INFO - __main__ - Printing 3 examples
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:53:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:53:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:53:59 - INFO - __main__ - Printing 3 examples
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:53:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 05:53:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 05:53:59 - INFO - __main__ - Printing 3 examples
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 05:53:59 - INFO - __main__ - ['positive']
03/09/2022 05:53:59 - INFO - __main__ - Tokenizing Input ...
03/09/2022 05:54:00 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:54:00 - INFO - __main__ - Tokenizing Output ...
03/09/2022 05:54:00 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 05:54:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:54:11 - INFO - __main__ - Starting training!
03/09/2022 05:54:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 05:54:11 - INFO - __main__ - Starting training!
03/09/2022 05:54:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.088821 on epoch=4
03/09/2022 05:54:20 - INFO - __main__ - Step 20 Global step 20 Train loss 16.699091 on epoch=9
03/09/2022 05:54:25 - INFO - __main__ - Step 30 Global step 30 Train loss 14.897072 on epoch=14
03/09/2022 05:54:29 - INFO - __main__ - Step 40 Global step 40 Train loss 13.574679 on epoch=19
03/09/2022 05:54:34 - INFO - __main__ - Step 50 Global step 50 Train loss 12.176883 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/09/2022 05:54:37 - INFO - __main__ - Global step 50 Train loss 16.087309 Classification-F1 0.0 on epoch=24
03/09/2022 05:54:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 05:54:54 - INFO - __main__ - Step 60 Global step 60 Train loss 9.118742 on epoch=29
03/09/2022 05:54:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.583767 on epoch=34
03/09/2022 05:55:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.432294 on epoch=39
03/09/2022 05:55:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.496052 on epoch=44
03/09/2022 05:55:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.331646 on epoch=49
03/09/2022 05:55:14 - INFO - __main__ - Global step 100 Train loss 2.792500 Classification-F1 0.6536796536796536 on epoch=49
03/09/2022 05:55:51 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.6536796536796536 on epoch=49, global_step=100
03/09/2022 05:55:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.229247 on epoch=54
03/09/2022 05:56:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.212397 on epoch=59
03/09/2022 05:56:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.567583 on epoch=64
03/09/2022 05:56:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.028654 on epoch=69
03/09/2022 05:56:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.039273 on epoch=74
03/09/2022 05:56:15 - INFO - __main__ - Global step 150 Train loss 0.215431 Classification-F1 0.9375 on epoch=74
03/09/2022 05:56:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6536796536796536 -> 0.9375 on epoch=74, global_step=150
03/09/2022 05:56:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005189 on epoch=79
03/09/2022 05:57:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.006823 on epoch=84
03/09/2022 05:57:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.001738 on epoch=89
03/09/2022 05:57:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.043085 on epoch=94
03/09/2022 05:57:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001441 on epoch=99
03/09/2022 05:57:15 - INFO - __main__ - Global step 200 Train loss 0.011655 Classification-F1 0.9687194525904204 on epoch=99
03/09/2022 05:57:54 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=99, global_step=200
03/09/2022 05:57:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001764 on epoch=104
03/09/2022 05:58:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004115 on epoch=109
03/09/2022 05:58:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003064 on epoch=114
03/09/2022 05:58:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001775 on epoch=119
03/09/2022 05:58:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001234 on epoch=124
03/09/2022 05:58:18 - INFO - __main__ - Global step 250 Train loss 0.002391 Classification-F1 0.906158357771261 on epoch=124
03/09/2022 05:58:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000722 on epoch=129
03/09/2022 05:58:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000513 on epoch=134
03/09/2022 05:58:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.028706 on epoch=139
03/09/2022 05:58:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002052 on epoch=144
03/09/2022 05:58:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000281 on epoch=149
03/09/2022 05:58:42 - INFO - __main__ - Global step 300 Train loss 0.006455 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 05:58:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000246 on epoch=154
03/09/2022 05:58:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000174 on epoch=159
03/09/2022 05:58:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001361 on epoch=164
03/09/2022 05:59:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000143 on epoch=169
03/09/2022 05:59:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000208 on epoch=174
03/09/2022 05:59:06 - INFO - __main__ - Global step 350 Train loss 0.000426 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 05:59:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000092 on epoch=179
03/09/2022 05:59:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000067 on epoch=184
03/09/2022 05:59:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002285 on epoch=189
03/09/2022 05:59:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000140 on epoch=194
03/09/2022 05:59:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000085 on epoch=199
03/09/2022 05:59:30 - INFO - __main__ - Global step 400 Train loss 0.000534 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 05:59:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000089 on epoch=204
03/09/2022 05:59:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000236 on epoch=209
03/09/2022 05:59:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000027 on epoch=214
03/09/2022 05:59:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000026 on epoch=219
03/09/2022 05:59:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000069 on epoch=224
03/09/2022 05:59:54 - INFO - __main__ - Global step 450 Train loss 0.000090 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 05:59:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001198 on epoch=229
03/09/2022 06:00:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000148 on epoch=234
03/09/2022 06:00:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000019 on epoch=239
03/09/2022 06:00:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000208 on epoch=244
03/09/2022 06:00:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000023 on epoch=249
03/09/2022 06:00:18 - INFO - __main__ - Global step 500 Train loss 0.000319 Classification-F1 0.906158357771261 on epoch=249
03/09/2022 06:00:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000008 on epoch=254
03/09/2022 06:00:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000013 on epoch=259
03/09/2022 06:00:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000017 on epoch=264
03/09/2022 06:00:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000012 on epoch=269
03/09/2022 06:00:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000014 on epoch=274
03/09/2022 06:00:42 - INFO - __main__ - Global step 550 Train loss 0.000013 Classification-F1 0.906158357771261 on epoch=274
03/09/2022 06:00:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.043785 on epoch=279
03/09/2022 06:00:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000197 on epoch=284
03/09/2022 06:00:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000166 on epoch=289
03/09/2022 06:01:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000080 on epoch=294
03/09/2022 06:01:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000094 on epoch=299
03/09/2022 06:01:06 - INFO - __main__ - Global step 600 Train loss 0.008864 Classification-F1 0.9372549019607843 on epoch=299
03/09/2022 06:01:06 - INFO - __main__ - save last model!
03/09/2022 06:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:01:07 - INFO - __main__ - Printing 3 examples
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:01:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:01:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:01:07 - INFO - __main__ - Printing 3 examples
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:01:07 - INFO - __main__ - ['positive']
03/09/2022 06:01:07 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:01:07 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:01:07 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:01:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:01:17 - INFO - __main__ - Starting training!
03/09/2022 06:01:28 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:01:28 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:01:28 - INFO - __main__ - Printing 3 examples
03/09/2022 06:01:28 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:01:28 - INFO - __main__ - ['negative']
03/09/2022 06:01:28 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:01:28 - INFO - __main__ - ['negative']
03/09/2022 06:01:28 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:01:28 - INFO - __main__ - ['negative']
03/09/2022 06:01:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:01:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:01:30 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:01:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0005_8_predictions.txt
03/09/2022 06:01:44 - INFO - __main__ - Classification-F1 on test data: 0.9089
03/09/2022 06:01:44 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9089234045832545
03/09/2022 06:01:44 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0003, bsz=8 ...
03/09/2022 06:01:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:01:45 - INFO - __main__ - Printing 3 examples
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:01:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:01:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:01:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:01:45 - INFO - __main__ - Printing 3 examples
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:01:45 - INFO - __main__ - ['positive']
03/09/2022 06:01:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:01:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:01:45 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:01:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:01:54 - INFO - __main__ - Starting training!
03/09/2022 06:01:59 - INFO - __main__ - Step 10 Global step 10 Train loss 22.450546 on epoch=4
03/09/2022 06:02:04 - INFO - __main__ - Step 20 Global step 20 Train loss 17.266712 on epoch=9
03/09/2022 06:02:08 - INFO - __main__ - Step 30 Global step 30 Train loss 16.416533 on epoch=14
03/09/2022 06:02:13 - INFO - __main__ - Step 40 Global step 40 Train loss 14.255020 on epoch=19
03/09/2022 06:02:18 - INFO - __main__ - Step 50 Global step 50 Train loss 13.355293 on epoch=24
03/09/2022 06:02:28 - INFO - __main__ - Global step 50 Train loss 16.748821 Classification-F1 0.0 on epoch=24
03/09/2022 06:03:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:03:10 - INFO - __main__ - Step 60 Global step 60 Train loss 12.151666 on epoch=29
03/09/2022 06:03:15 - INFO - __main__ - Step 70 Global step 70 Train loss 11.541567 on epoch=34
03/09/2022 06:03:19 - INFO - __main__ - Step 80 Global step 80 Train loss 9.853167 on epoch=39
03/09/2022 06:03:24 - INFO - __main__ - Step 90 Global step 90 Train loss 3.807027 on epoch=44
03/09/2022 06:03:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.503125 on epoch=49
03/09/2022 06:03:29 - INFO - __main__ - Global step 100 Train loss 7.771310 Classification-F1 0.3333333333333333 on epoch=49
03/09/2022 06:04:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
03/09/2022 06:04:11 - INFO - __main__ - Step 110 Global step 110 Train loss 1.056018 on epoch=54
03/09/2022 06:04:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.696585 on epoch=59
03/09/2022 06:04:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.878008 on epoch=64
03/09/2022 06:04:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.298288 on epoch=69
03/09/2022 06:04:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.275002 on epoch=74
03/09/2022 06:04:30 - INFO - __main__ - Global step 150 Train loss 0.640780 Classification-F1 0.36374269005847953 on epoch=74
03/09/2022 06:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=74, global_step=150
03/09/2022 06:05:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.339806 on epoch=79
03/09/2022 06:05:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.311447 on epoch=84
03/09/2022 06:05:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.297865 on epoch=89
03/09/2022 06:05:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.322350 on epoch=94
03/09/2022 06:05:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.259252 on epoch=99
03/09/2022 06:05:32 - INFO - __main__ - Global step 200 Train loss 0.306144 Classification-F1 0.4920634920634921 on epoch=99
03/09/2022 06:06:09 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.4920634920634921 on epoch=99, global_step=200
03/09/2022 06:06:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.320352 on epoch=104
03/09/2022 06:06:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.493819 on epoch=109
03/09/2022 06:06:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.655245 on epoch=114
03/09/2022 06:06:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.335450 on epoch=119
03/09/2022 06:06:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.347648 on epoch=124
03/09/2022 06:06:33 - INFO - __main__ - Global step 250 Train loss 0.430503 Classification-F1 0.5134502923976608 on epoch=124
03/09/2022 06:07:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5134502923976608 on epoch=124, global_step=250
03/09/2022 06:07:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.331173 on epoch=129
03/09/2022 06:07:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.301956 on epoch=134
03/09/2022 06:07:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.399953 on epoch=139
03/09/2022 06:07:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.327192 on epoch=144
03/09/2022 06:07:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.349462 on epoch=149
03/09/2022 06:07:34 - INFO - __main__ - Global step 300 Train loss 0.341947 Classification-F1 0.4181818181818182 on epoch=149
03/09/2022 06:07:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.277174 on epoch=154
03/09/2022 06:07:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.341923 on epoch=159
03/09/2022 06:07:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.342109 on epoch=164
03/09/2022 06:07:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.340683 on epoch=169
03/09/2022 06:07:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.348636 on epoch=174
03/09/2022 06:07:59 - INFO - __main__ - Global step 350 Train loss 0.330105 Classification-F1 0.3992490613266583 on epoch=174
03/09/2022 06:08:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.313679 on epoch=179
03/09/2022 06:08:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.316938 on epoch=184
03/09/2022 06:08:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.333955 on epoch=189
03/09/2022 06:08:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.325551 on epoch=194
03/09/2022 06:08:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.319121 on epoch=199
03/09/2022 06:08:23 - INFO - __main__ - Global step 400 Train loss 0.321849 Classification-F1 0.5333333333333333 on epoch=199
03/09/2022 06:09:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5333333333333333 on epoch=199, global_step=400
03/09/2022 06:09:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.310184 on epoch=204
03/09/2022 06:09:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.286815 on epoch=209
03/09/2022 06:09:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.255326 on epoch=214
03/09/2022 06:09:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.293251 on epoch=219
03/09/2022 06:09:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.310589 on epoch=224
03/09/2022 06:09:25 - INFO - __main__ - Global step 450 Train loss 0.291233 Classification-F1 0.37254901960784315 on epoch=224
03/09/2022 06:09:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.290296 on epoch=229
03/09/2022 06:09:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.273337 on epoch=234
03/09/2022 06:09:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.249093 on epoch=239
03/09/2022 06:09:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.269030 on epoch=244
03/09/2022 06:09:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.284494 on epoch=249
03/09/2022 06:09:49 - INFO - __main__ - Global step 500 Train loss 0.273250 Classification-F1 0.5835835835835835 on epoch=249
03/09/2022 06:10:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5835835835835835 on epoch=249, global_step=500
03/09/2022 06:10:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.268593 on epoch=254
03/09/2022 06:10:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.317596 on epoch=259
03/09/2022 06:10:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.280769 on epoch=264
03/09/2022 06:10:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.276138 on epoch=269
03/09/2022 06:10:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.252422 on epoch=274
03/09/2022 06:10:51 - INFO - __main__ - Global step 550 Train loss 0.279103 Classification-F1 0.6532019704433498 on epoch=274
03/09/2022 06:11:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5835835835835835 -> 0.6532019704433498 on epoch=274, global_step=550
03/09/2022 06:11:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.177254 on epoch=279
03/09/2022 06:11:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.196359 on epoch=284
03/09/2022 06:11:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.264927 on epoch=289
03/09/2022 06:11:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.223306 on epoch=294
03/09/2022 06:11:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.205151 on epoch=299
03/09/2022 06:11:51 - INFO - __main__ - Global step 600 Train loss 0.213400 Classification-F1 0.6825396825396826 on epoch=299
03/09/2022 06:11:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:11:53 - INFO - __main__ - Printing 3 examples
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:11:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:11:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:11:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:11:53 - INFO - __main__ - Printing 3 examples
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:11:53 - INFO - __main__ - ['positive']
03/09/2022 06:11:53 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:11:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:11:53 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:12:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:12:03 - INFO - __main__ - Starting training!
03/09/2022 06:12:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6532019704433498 -> 0.6825396825396826 on epoch=299, global_step=600
03/09/2022 06:12:28 - INFO - __main__ - save last model!
03/09/2022 06:13:10 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:13:11 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:13:11 - INFO - __main__ - Printing 3 examples
03/09/2022 06:13:11 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:13:11 - INFO - __main__ - ['negative']
03/09/2022 06:13:11 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:13:11 - INFO - __main__ - ['negative']
03/09/2022 06:13:11 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:13:11 - INFO - __main__ - ['negative']
03/09/2022 06:13:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:13:12 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:13:13 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:13:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0003_8_predictions.txt
03/09/2022 06:13:27 - INFO - __main__ - Classification-F1 on test data: 0.6449
03/09/2022 06:13:28 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.6825396825396826, test_performance=0.6448945799646102
03/09/2022 06:13:28 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0002, bsz=8 ...
03/09/2022 06:13:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:13:29 - INFO - __main__ - Printing 3 examples
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:13:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:13:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:13:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:13:29 - INFO - __main__ - Printing 3 examples
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:13:29 - INFO - __main__ - ['positive']
03/09/2022 06:13:29 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:13:29 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:13:29 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:13:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:13:38 - INFO - __main__ - Starting training!
03/09/2022 06:13:42 - INFO - __main__ - Step 10 Global step 10 Train loss 22.182211 on epoch=4
03/09/2022 06:13:47 - INFO - __main__ - Step 20 Global step 20 Train loss 19.298891 on epoch=9
03/09/2022 06:13:51 - INFO - __main__ - Step 30 Global step 30 Train loss 16.924904 on epoch=14
03/09/2022 06:13:56 - INFO - __main__ - Step 40 Global step 40 Train loss 16.268661 on epoch=19
03/09/2022 06:14:01 - INFO - __main__ - Step 50 Global step 50 Train loss 14.967832 on epoch=24
03/09/2022 06:14:02 - INFO - __main__ - Global step 50 Train loss 17.928499 Classification-F1 0.0 on epoch=24
03/09/2022 06:14:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:14:42 - INFO - __main__ - Step 60 Global step 60 Train loss 14.611003 on epoch=29
03/09/2022 06:14:47 - INFO - __main__ - Step 70 Global step 70 Train loss 12.842792 on epoch=34
03/09/2022 06:14:51 - INFO - __main__ - Step 80 Global step 80 Train loss 12.892672 on epoch=39
03/09/2022 06:14:56 - INFO - __main__ - Step 90 Global step 90 Train loss 11.882254 on epoch=44
03/09/2022 06:15:01 - INFO - __main__ - Step 100 Global step 100 Train loss 11.061594 on epoch=49
03/09/2022 06:15:01 - INFO - __main__ - Global step 100 Train loss 12.658064 Classification-F1 0.0 on epoch=49
03/09/2022 06:15:06 - INFO - __main__ - Step 110 Global step 110 Train loss 9.695046 on epoch=54
03/09/2022 06:15:11 - INFO - __main__ - Step 120 Global step 120 Train loss 7.658845 on epoch=59
03/09/2022 06:15:16 - INFO - __main__ - Step 130 Global step 130 Train loss 4.888590 on epoch=64
03/09/2022 06:15:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.477153 on epoch=69
03/09/2022 06:15:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.232917 on epoch=74
03/09/2022 06:15:26 - INFO - __main__ - Global step 150 Train loss 4.590510 Classification-F1 0.9375 on epoch=74
03/09/2022 06:16:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.9375 on epoch=74, global_step=150
03/09/2022 06:16:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.436172 on epoch=79
03/09/2022 06:16:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.542928 on epoch=84
03/09/2022 06:16:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.861578 on epoch=89
03/09/2022 06:16:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.112195 on epoch=94
03/09/2022 06:16:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.611054 on epoch=99
03/09/2022 06:16:24 - INFO - __main__ - Global step 200 Train loss 1.512785 Classification-F1 0.3992490613266583 on epoch=99
03/09/2022 06:16:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.474309 on epoch=104
03/09/2022 06:16:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.524734 on epoch=109
03/09/2022 06:16:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.423920 on epoch=114
03/09/2022 06:16:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.343409 on epoch=119
03/09/2022 06:16:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.449154 on epoch=124
03/09/2022 06:16:48 - INFO - __main__ - Global step 250 Train loss 0.443105 Classification-F1 0.4682306940371457 on epoch=124
03/09/2022 06:16:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.400022 on epoch=129
03/09/2022 06:16:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.432462 on epoch=134
03/09/2022 06:17:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.367010 on epoch=139
03/09/2022 06:17:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.405357 on epoch=144
03/09/2022 06:17:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.345616 on epoch=149
03/09/2022 06:17:13 - INFO - __main__ - Global step 300 Train loss 0.390093 Classification-F1 0.5151515151515151 on epoch=149
03/09/2022 06:17:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.338864 on epoch=154
03/09/2022 06:17:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.357364 on epoch=159
03/09/2022 06:17:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.334703 on epoch=164
03/09/2022 06:17:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360634 on epoch=169
03/09/2022 06:17:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.379042 on epoch=174
03/09/2022 06:17:37 - INFO - __main__ - Global step 350 Train loss 0.354121 Classification-F1 0.4385964912280702 on epoch=174
03/09/2022 06:17:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.323665 on epoch=179
03/09/2022 06:17:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331689 on epoch=184
03/09/2022 06:17:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.313764 on epoch=189
03/09/2022 06:17:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.296254 on epoch=194
03/09/2022 06:18:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.361726 on epoch=199
03/09/2022 06:18:01 - INFO - __main__ - Global step 400 Train loss 0.325420 Classification-F1 0.6862745098039216 on epoch=199
03/09/2022 06:18:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.357778 on epoch=204
03/09/2022 06:18:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.415190 on epoch=209
03/09/2022 06:18:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.349806 on epoch=214
03/09/2022 06:18:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.318413 on epoch=219
03/09/2022 06:18:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.346564 on epoch=224
03/09/2022 06:18:25 - INFO - __main__ - Global step 450 Train loss 0.357550 Classification-F1 0.4817813765182186 on epoch=224
03/09/2022 06:18:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.350521 on epoch=229
03/09/2022 06:18:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.340484 on epoch=234
03/09/2022 06:18:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.345979 on epoch=239
03/09/2022 06:18:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.332773 on epoch=244
03/09/2022 06:18:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.347063 on epoch=249
03/09/2022 06:18:49 - INFO - __main__ - Global step 500 Train loss 0.343364 Classification-F1 0.4589371980676329 on epoch=249
03/09/2022 06:18:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.317397 on epoch=254
03/09/2022 06:18:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.356903 on epoch=259
03/09/2022 06:19:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.329633 on epoch=264
03/09/2022 06:19:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.309777 on epoch=269
03/09/2022 06:19:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.298435 on epoch=274
03/09/2022 06:19:14 - INFO - __main__ - Global step 550 Train loss 0.322429 Classification-F1 0.5835835835835835 on epoch=274
03/09/2022 06:19:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.357353 on epoch=279
03/09/2022 06:19:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.329751 on epoch=284
03/09/2022 06:19:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.318213 on epoch=289
03/09/2022 06:19:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.300330 on epoch=294
03/09/2022 06:19:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.323384 on epoch=299
03/09/2022 06:19:38 - INFO - __main__ - Global step 600 Train loss 0.325806 Classification-F1 0.39756367663344405 on epoch=299
03/09/2022 06:19:38 - INFO - __main__ - save last model!
03/09/2022 06:19:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:19:39 - INFO - __main__ - Printing 3 examples
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:19:39 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:19:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:19:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:19:39 - INFO - __main__ - Printing 3 examples
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:19:39 - INFO - __main__ - ['positive']
03/09/2022 06:19:39 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:19:39 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:19:39 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:19:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:19:48 - INFO - __main__ - Starting training!
03/09/2022 06:20:21 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:20:21 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:20:21 - INFO - __main__ - Printing 3 examples
03/09/2022 06:20:21 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:20:21 - INFO - __main__ - ['negative']
03/09/2022 06:20:21 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:20:21 - INFO - __main__ - ['negative']
03/09/2022 06:20:21 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:20:21 - INFO - __main__ - ['negative']
03/09/2022 06:20:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:20:22 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:20:23 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:20:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0002_8_predictions.txt
03/09/2022 06:20:37 - INFO - __main__ - Classification-F1 on test data: 0.9460
03/09/2022 06:20:37 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.9459991359861757
03/09/2022 06:20:37 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0001, bsz=8 ...
03/09/2022 06:20:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:20:38 - INFO - __main__ - Printing 3 examples
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:20:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:20:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:20:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:20:38 - INFO - __main__ - Printing 3 examples
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/09/2022 06:20:38 - INFO - __main__ - ['positive']
03/09/2022 06:20:38 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:20:38 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:20:38 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:20:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:20:47 - INFO - __main__ - Starting training!
03/09/2022 06:20:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.350376 on epoch=4
03/09/2022 06:20:57 - INFO - __main__ - Step 20 Global step 20 Train loss 21.112301 on epoch=9
03/09/2022 06:21:02 - INFO - __main__ - Step 30 Global step 30 Train loss 18.029680 on epoch=14
03/09/2022 06:21:07 - INFO - __main__ - Step 40 Global step 40 Train loss 16.769276 on epoch=19
03/09/2022 06:21:11 - INFO - __main__ - Step 50 Global step 50 Train loss 17.164574 on epoch=24
03/09/2022 06:21:23 - INFO - __main__ - Global step 50 Train loss 19.285242 Classification-F1 0.0 on epoch=24
03/09/2022 06:22:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:22:05 - INFO - __main__ - Step 60 Global step 60 Train loss 15.913518 on epoch=29
03/09/2022 06:22:09 - INFO - __main__ - Step 70 Global step 70 Train loss 15.516785 on epoch=34
03/09/2022 06:22:14 - INFO - __main__ - Step 80 Global step 80 Train loss 15.510806 on epoch=39
03/09/2022 06:22:19 - INFO - __main__ - Step 90 Global step 90 Train loss 15.276628 on epoch=44
03/09/2022 06:22:24 - INFO - __main__ - Step 100 Global step 100 Train loss 14.036064 on epoch=49
03/09/2022 06:22:32 - INFO - __main__ - Global step 100 Train loss 15.250762 Classification-F1 0.0 on epoch=49
03/09/2022 06:22:37 - INFO - __main__ - Step 110 Global step 110 Train loss 13.422679 on epoch=54
03/09/2022 06:22:42 - INFO - __main__ - Step 120 Global step 120 Train loss 13.748500 on epoch=59
03/09/2022 06:22:46 - INFO - __main__ - Step 130 Global step 130 Train loss 13.225159 on epoch=64
03/09/2022 06:22:51 - INFO - __main__ - Step 140 Global step 140 Train loss 13.049443 on epoch=69
03/09/2022 06:22:56 - INFO - __main__ - Step 150 Global step 150 Train loss 12.540441 on epoch=74
03/09/2022 06:23:00 - INFO - __main__ - Global step 150 Train loss 13.197244 Classification-F1 0.0 on epoch=74
03/09/2022 06:23:05 - INFO - __main__ - Step 160 Global step 160 Train loss 12.538486 on epoch=79
03/09/2022 06:23:10 - INFO - __main__ - Step 170 Global step 170 Train loss 11.412712 on epoch=84
03/09/2022 06:23:15 - INFO - __main__ - Step 180 Global step 180 Train loss 10.855384 on epoch=89
03/09/2022 06:23:20 - INFO - __main__ - Step 190 Global step 190 Train loss 6.536744 on epoch=94
03/09/2022 06:23:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.761459 on epoch=99
03/09/2022 06:23:25 - INFO - __main__ - Global step 200 Train loss 8.420957 Classification-F1 0.7408906882591093 on epoch=99
03/09/2022 06:24:03 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.7408906882591093 on epoch=99, global_step=200
03/09/2022 06:24:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.387333 on epoch=104
03/09/2022 06:24:12 - INFO - __main__ - Step 220 Global step 220 Train loss 1.762463 on epoch=109
03/09/2022 06:24:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.262734 on epoch=114
03/09/2022 06:24:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.494995 on epoch=119
03/09/2022 06:24:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.214649 on epoch=124
03/09/2022 06:24:27 - INFO - __main__ - Global step 250 Train loss 0.624435 Classification-F1 0.9375 on epoch=124
03/09/2022 06:25:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7408906882591093 -> 0.9375 on epoch=124, global_step=250
03/09/2022 06:25:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.292797 on epoch=129
03/09/2022 06:25:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.160868 on epoch=134
03/09/2022 06:25:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.123253 on epoch=139
03/09/2022 06:25:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.117365 on epoch=144
03/09/2022 06:25:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.065113 on epoch=149
03/09/2022 06:25:30 - INFO - __main__ - Global step 300 Train loss 0.151879 Classification-F1 0.9372549019607843 on epoch=149
03/09/2022 06:25:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.068167 on epoch=154
03/09/2022 06:25:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.048344 on epoch=159
03/09/2022 06:25:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.026037 on epoch=164
03/09/2022 06:25:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017085 on epoch=169
03/09/2022 06:25:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.015146 on epoch=174
03/09/2022 06:25:54 - INFO - __main__ - Global step 350 Train loss 0.034956 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 06:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=174, global_step=350
03/09/2022 06:26:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.012050 on epoch=179
03/09/2022 06:26:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.013543 on epoch=184
03/09/2022 06:26:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025599 on epoch=189
03/09/2022 06:26:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.007921 on epoch=194
03/09/2022 06:26:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.027348 on epoch=199
03/09/2022 06:26:56 - INFO - __main__ - Global step 400 Train loss 0.017292 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 06:27:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001352 on epoch=204
03/09/2022 06:27:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001815 on epoch=209
03/09/2022 06:27:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001272 on epoch=214
03/09/2022 06:27:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002182 on epoch=219
03/09/2022 06:27:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000469 on epoch=224
03/09/2022 06:27:20 - INFO - __main__ - Global step 450 Train loss 0.001418 Classification-F1 1.0 on epoch=224
03/09/2022 06:27:57 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=224, global_step=450
03/09/2022 06:28:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020404 on epoch=229
03/09/2022 06:28:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001926 on epoch=234
03/09/2022 06:28:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001378 on epoch=239
03/09/2022 06:28:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002929 on epoch=244
03/09/2022 06:28:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000334 on epoch=249
03/09/2022 06:28:21 - INFO - __main__ - Global step 500 Train loss 0.005394 Classification-F1 1.0 on epoch=249
03/09/2022 06:28:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000824 on epoch=254
03/09/2022 06:28:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003451 on epoch=259
03/09/2022 06:28:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001658 on epoch=264
03/09/2022 06:28:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000369 on epoch=269
03/09/2022 06:28:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000462 on epoch=274
03/09/2022 06:28:46 - INFO - __main__ - Global step 550 Train loss 0.001353 Classification-F1 1.0 on epoch=274
03/09/2022 06:28:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000860 on epoch=279
03/09/2022 06:28:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001606 on epoch=284
03/09/2022 06:29:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000912 on epoch=289
03/09/2022 06:29:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000324 on epoch=294
03/09/2022 06:29:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000197 on epoch=299
03/09/2022 06:29:10 - INFO - __main__ - Global step 600 Train loss 0.000780 Classification-F1 1.0 on epoch=299
03/09/2022 06:29:10 - INFO - __main__ - save last model!
03/09/2022 06:29:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:29:11 - INFO - __main__ - Printing 3 examples
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:29:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:29:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:29:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:29:11 - INFO - __main__ - Printing 3 examples
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:29:11 - INFO - __main__ - ['negative']
03/09/2022 06:29:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:29:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:29:11 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:29:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:29:20 - INFO - __main__ - Starting training!
03/09/2022 06:29:50 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:29:51 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:29:51 - INFO - __main__ - Printing 3 examples
03/09/2022 06:29:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:29:51 - INFO - __main__ - ['negative']
03/09/2022 06:29:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:29:51 - INFO - __main__ - ['negative']
03/09/2022 06:29:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:29:51 - INFO - __main__ - ['negative']
03/09/2022 06:29:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:29:51 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:29:52 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:30:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_100_0.0001_8_predictions.txt
03/09/2022 06:30:16 - INFO - __main__ - Classification-F1 on test data: 0.4569
03/09/2022 06:30:16 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.4569136537097226
03/09/2022 06:30:16 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0005, bsz=8 ...
03/09/2022 06:30:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:30:17 - INFO - __main__ - Printing 3 examples
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:30:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:30:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:30:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:30:17 - INFO - __main__ - Printing 3 examples
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:30:17 - INFO - __main__ - ['negative']
03/09/2022 06:30:17 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:30:17 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:30:17 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:30:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:30:27 - INFO - __main__ - Starting training!
03/09/2022 06:30:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.678394 on epoch=4
03/09/2022 06:30:35 - INFO - __main__ - Step 20 Global step 20 Train loss 17.665001 on epoch=9
03/09/2022 06:30:39 - INFO - __main__ - Step 30 Global step 30 Train loss 16.538038 on epoch=14
03/09/2022 06:30:44 - INFO - __main__ - Step 40 Global step 40 Train loss 14.117441 on epoch=19
03/09/2022 06:30:48 - INFO - __main__ - Step 50 Global step 50 Train loss 11.824023 on epoch=24
03/09/2022 06:30:56 - INFO - __main__ - Global step 50 Train loss 16.764580 Classification-F1 0.0 on epoch=24
03/09/2022 06:31:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:31:37 - INFO - __main__ - Step 60 Global step 60 Train loss 7.677657 on epoch=29
03/09/2022 06:31:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.796773 on epoch=34
03/09/2022 06:31:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.398216 on epoch=39
03/09/2022 06:31:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.449718 on epoch=44
03/09/2022 06:31:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.423573 on epoch=49
03/09/2022 06:31:58 - INFO - __main__ - Global step 100 Train loss 2.349187 Classification-F1 0.0 on epoch=49
03/09/2022 06:32:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.380036 on epoch=54
03/09/2022 06:32:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.489807 on epoch=59
03/09/2022 06:32:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.894923 on epoch=64
03/09/2022 06:32:17 - INFO - __main__ - Step 140 Global step 140 Train loss 1.748891 on epoch=69
03/09/2022 06:32:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.463929 on epoch=74
03/09/2022 06:32:22 - INFO - __main__ - Global step 150 Train loss 0.795517 Classification-F1 0.6945917285259808 on epoch=74
03/09/2022 06:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.6945917285259808 on epoch=74, global_step=150
03/09/2022 06:33:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.137712 on epoch=79
03/09/2022 06:33:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.146729 on epoch=84
03/09/2022 06:33:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.157339 on epoch=89
03/09/2022 06:33:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.240239 on epoch=94
03/09/2022 06:33:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.404249 on epoch=99
03/09/2022 06:33:23 - INFO - __main__ - Global step 200 Train loss 0.217254 Classification-F1 0.6267232237539766 on epoch=99
03/09/2022 06:33:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.361187 on epoch=104
03/09/2022 06:33:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.351794 on epoch=109
03/09/2022 06:33:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.358448 on epoch=114
03/09/2022 06:33:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.333869 on epoch=119
03/09/2022 06:33:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.376546 on epoch=124
03/09/2022 06:33:46 - INFO - __main__ - Global step 250 Train loss 0.356369 Classification-F1 0.539313399778516 on epoch=124
03/09/2022 06:33:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.355392 on epoch=129
03/09/2022 06:33:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.336506 on epoch=134
03/09/2022 06:34:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.378076 on epoch=139
03/09/2022 06:34:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.391015 on epoch=144
03/09/2022 06:34:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.375728 on epoch=149
03/09/2022 06:34:10 - INFO - __main__ - Global step 300 Train loss 0.367343 Classification-F1 0.3191489361702127 on epoch=149
03/09/2022 06:34:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.350861 on epoch=154
03/09/2022 06:34:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.345288 on epoch=159
03/09/2022 06:34:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.326039 on epoch=164
03/09/2022 06:34:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.345468 on epoch=169
03/09/2022 06:34:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.355583 on epoch=174
03/09/2022 06:34:33 - INFO - __main__ - Global step 350 Train loss 0.344648 Classification-F1 0.3333333333333333 on epoch=174
03/09/2022 06:34:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.344189 on epoch=179
03/09/2022 06:34:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.363120 on epoch=184
03/09/2022 06:34:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342415 on epoch=189
03/09/2022 06:34:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.353160 on epoch=194
03/09/2022 06:34:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.341650 on epoch=199
03/09/2022 06:34:57 - INFO - __main__ - Global step 400 Train loss 0.348907 Classification-F1 0.3333333333333333 on epoch=199
03/09/2022 06:35:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.352127 on epoch=204
03/09/2022 06:35:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.354942 on epoch=209
03/09/2022 06:35:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.379469 on epoch=214
03/09/2022 06:35:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.352136 on epoch=219
03/09/2022 06:35:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.332392 on epoch=224
03/09/2022 06:35:21 - INFO - __main__ - Global step 450 Train loss 0.354213 Classification-F1 0.5636363636363637 on epoch=224
03/09/2022 06:35:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.330716 on epoch=229
03/09/2022 06:35:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.351570 on epoch=234
03/09/2022 06:35:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.356971 on epoch=239
03/09/2022 06:35:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.340944 on epoch=244
03/09/2022 06:35:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.357734 on epoch=249
03/09/2022 06:35:45 - INFO - __main__ - Global step 500 Train loss 0.347587 Classification-F1 0.7810361681329424 on epoch=249
03/09/2022 06:36:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6945917285259808 -> 0.7810361681329424 on epoch=249, global_step=500
03/09/2022 06:36:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.367456 on epoch=254
03/09/2022 06:36:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.359235 on epoch=259
03/09/2022 06:36:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.336806 on epoch=264
03/09/2022 06:36:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.360546 on epoch=269
03/09/2022 06:36:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.353456 on epoch=274
03/09/2022 06:36:46 - INFO - __main__ - Global step 550 Train loss 0.355500 Classification-F1 0.3992490613266583 on epoch=274
03/09/2022 06:36:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.384417 on epoch=279
03/09/2022 06:36:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.329745 on epoch=284
03/09/2022 06:37:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.365269 on epoch=289
03/09/2022 06:37:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.346752 on epoch=294
03/09/2022 06:37:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.343598 on epoch=299
03/09/2022 06:37:10 - INFO - __main__ - Global step 600 Train loss 0.353956 Classification-F1 0.3333333333333333 on epoch=299
03/09/2022 06:37:10 - INFO - __main__ - save last model!
03/09/2022 06:37:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:37:10 - INFO - __main__ - Printing 3 examples
03/09/2022 06:37:10 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:37:10 - INFO - __main__ - ['negative']
03/09/2022 06:37:10 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:37:10 - INFO - __main__ - ['negative']
03/09/2022 06:37:10 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:37:10 - INFO - __main__ - ['negative']
03/09/2022 06:37:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:37:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:37:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:37:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:37:11 - INFO - __main__ - Printing 3 examples
03/09/2022 06:37:11 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:37:11 - INFO - __main__ - ['negative']
03/09/2022 06:37:11 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:37:11 - INFO - __main__ - ['negative']
03/09/2022 06:37:11 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:37:11 - INFO - __main__ - ['negative']
03/09/2022 06:37:11 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:37:11 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:37:11 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:37:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:37:20 - INFO - __main__ - Starting training!
03/09/2022 06:37:52 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:37:53 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:37:53 - INFO - __main__ - Printing 3 examples
03/09/2022 06:37:53 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:37:53 - INFO - __main__ - ['negative']
03/09/2022 06:37:53 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:37:53 - INFO - __main__ - ['negative']
03/09/2022 06:37:53 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:37:53 - INFO - __main__ - ['negative']
03/09/2022 06:37:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:37:53 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:37:54 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:38:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0005_8_predictions.txt
03/09/2022 06:38:27 - INFO - __main__ - Classification-F1 on test data: 0.2739
03/09/2022 06:38:27 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.7810361681329424, test_performance=0.2739054105503617
03/09/2022 06:38:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0003, bsz=8 ...
03/09/2022 06:38:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:38:28 - INFO - __main__ - Printing 3 examples
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:38:28 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:38:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:38:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:38:28 - INFO - __main__ - Printing 3 examples
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:38:28 - INFO - __main__ - ['negative']
03/09/2022 06:38:28 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:38:28 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:38:29 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:38:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:38:39 - INFO - __main__ - Starting training!
03/09/2022 06:38:43 - INFO - __main__ - Step 10 Global step 10 Train loss 22.014746 on epoch=4
03/09/2022 06:38:48 - INFO - __main__ - Step 20 Global step 20 Train loss 18.429630 on epoch=9
03/09/2022 06:38:53 - INFO - __main__ - Step 30 Global step 30 Train loss 16.211752 on epoch=14
03/09/2022 06:38:58 - INFO - __main__ - Step 40 Global step 40 Train loss 14.851277 on epoch=19
03/09/2022 06:39:02 - INFO - __main__ - Step 50 Global step 50 Train loss 14.018236 on epoch=24
03/09/2022 06:39:12 - INFO - __main__ - Global step 50 Train loss 17.105129 Classification-F1 0.0 on epoch=24
03/09/2022 06:39:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:39:53 - INFO - __main__ - Step 60 Global step 60 Train loss 13.002149 on epoch=29
03/09/2022 06:39:58 - INFO - __main__ - Step 70 Global step 70 Train loss 12.342925 on epoch=34
03/09/2022 06:40:02 - INFO - __main__ - Step 80 Global step 80 Train loss 9.267306 on epoch=39
03/09/2022 06:40:07 - INFO - __main__ - Step 90 Global step 90 Train loss 6.649744 on epoch=44
03/09/2022 06:40:12 - INFO - __main__ - Step 100 Global step 100 Train loss 7.306134 on epoch=49
03/09/2022 06:40:12 - INFO - __main__ - Global step 100 Train loss 9.713651 Classification-F1 0.0 on epoch=49
03/09/2022 06:40:17 - INFO - __main__ - Step 110 Global step 110 Train loss 3.337065 on epoch=54
03/09/2022 06:40:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.460373 on epoch=59
03/09/2022 06:40:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.177398 on epoch=64
03/09/2022 06:40:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.132874 on epoch=69
03/09/2022 06:40:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.050293 on epoch=74
03/09/2022 06:40:37 - INFO - __main__ - Global step 150 Train loss 0.831601 Classification-F1 0.875 on epoch=74
03/09/2022 06:41:13 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.875 on epoch=74, global_step=150
03/09/2022 06:41:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.028848 on epoch=79
03/09/2022 06:41:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.010855 on epoch=84
03/09/2022 06:41:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.010520 on epoch=89
03/09/2022 06:41:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.012375 on epoch=94
03/09/2022 06:41:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.024895 on epoch=99
03/09/2022 06:41:38 - INFO - __main__ - Global step 200 Train loss 0.017499 Classification-F1 0.9054187192118226 on epoch=99
03/09/2022 06:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.875 -> 0.9054187192118226 on epoch=99, global_step=200
03/09/2022 06:42:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005826 on epoch=104
03/09/2022 06:42:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002635 on epoch=109
03/09/2022 06:42:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001569 on epoch=114
03/09/2022 06:42:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.004648 on epoch=119
03/09/2022 06:42:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.003260 on epoch=124
03/09/2022 06:42:39 - INFO - __main__ - Global step 250 Train loss 0.003587 Classification-F1 0.9054187192118226 on epoch=124
03/09/2022 06:42:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.022348 on epoch=129
03/09/2022 06:42:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002365 on epoch=134
03/09/2022 06:42:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003973 on epoch=139
03/09/2022 06:42:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.014832 on epoch=144
03/09/2022 06:43:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.007792 on epoch=149
03/09/2022 06:43:04 - INFO - __main__ - Global step 300 Train loss 0.010262 Classification-F1 0.8745098039215686 on epoch=149
03/09/2022 06:43:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.042268 on epoch=154
03/09/2022 06:43:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.047188 on epoch=159
03/09/2022 06:43:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000785 on epoch=164
03/09/2022 06:43:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001237 on epoch=169
03/09/2022 06:43:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002109 on epoch=174
03/09/2022 06:43:28 - INFO - __main__ - Global step 350 Train loss 0.018717 Classification-F1 0.9054187192118226 on epoch=174
03/09/2022 06:43:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000878 on epoch=179
03/09/2022 06:43:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001043 on epoch=184
03/09/2022 06:43:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000724 on epoch=189
03/09/2022 06:43:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000674 on epoch=194
03/09/2022 06:43:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001088 on epoch=199
03/09/2022 06:43:53 - INFO - __main__ - Global step 400 Train loss 0.000882 Classification-F1 0.9054187192118226 on epoch=199
03/09/2022 06:43:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000660 on epoch=204
03/09/2022 06:44:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000545 on epoch=209
03/09/2022 06:44:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.031545 on epoch=214
03/09/2022 06:44:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001008 on epoch=219
03/09/2022 06:44:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000806 on epoch=224
03/09/2022 06:44:18 - INFO - __main__ - Global step 450 Train loss 0.006913 Classification-F1 0.9372549019607843 on epoch=224
03/09/2022 06:44:54 - INFO - __main__ - Saving model with best Classification-F1: 0.9054187192118226 -> 0.9372549019607843 on epoch=224, global_step=450
03/09/2022 06:44:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.009456 on epoch=229
03/09/2022 06:45:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001348 on epoch=234
03/09/2022 06:45:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012841 on epoch=239
03/09/2022 06:45:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000880 on epoch=244
03/09/2022 06:45:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003164 on epoch=249
03/09/2022 06:45:18 - INFO - __main__ - Global step 500 Train loss 0.005538 Classification-F1 0.9372549019607843 on epoch=249
03/09/2022 06:45:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000131 on epoch=254
03/09/2022 06:45:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002994 on epoch=259
03/09/2022 06:45:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000193 on epoch=264
03/09/2022 06:45:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001018 on epoch=269
03/09/2022 06:45:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004282 on epoch=274
03/09/2022 06:45:42 - INFO - __main__ - Global step 550 Train loss 0.001724 Classification-F1 0.9054187192118226 on epoch=274
03/09/2022 06:45:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000229 on epoch=279
03/09/2022 06:45:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000235 on epoch=284
03/09/2022 06:45:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000521 on epoch=289
03/09/2022 06:46:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000028 on epoch=294
03/09/2022 06:46:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000697 on epoch=299
03/09/2022 06:46:07 - INFO - __main__ - Global step 600 Train loss 0.000342 Classification-F1 0.9054187192118226 on epoch=299
03/09/2022 06:46:07 - INFO - __main__ - save last model!
03/09/2022 06:46:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:46:07 - INFO - __main__ - Printing 3 examples
03/09/2022 06:46:07 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:46:07 - INFO - __main__ - ['negative']
03/09/2022 06:46:07 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:46:07 - INFO - __main__ - ['negative']
03/09/2022 06:46:07 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:46:07 - INFO - __main__ - ['negative']
03/09/2022 06:46:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:46:08 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:46:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:46:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:46:08 - INFO - __main__ - Printing 3 examples
03/09/2022 06:46:08 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:46:08 - INFO - __main__ - ['negative']
03/09/2022 06:46:08 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:46:08 - INFO - __main__ - ['negative']
03/09/2022 06:46:08 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:46:08 - INFO - __main__ - ['negative']
03/09/2022 06:46:08 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:46:08 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:46:08 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:46:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:46:18 - INFO - __main__ - Starting training!
03/09/2022 06:46:48 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:46:49 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:46:49 - INFO - __main__ - Printing 3 examples
03/09/2022 06:46:49 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:46:49 - INFO - __main__ - ['negative']
03/09/2022 06:46:49 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:46:49 - INFO - __main__ - ['negative']
03/09/2022 06:46:49 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:46:49 - INFO - __main__ - ['negative']
03/09/2022 06:46:49 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:46:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:46:51 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:47:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0003_8_predictions.txt
03/09/2022 06:47:05 - INFO - __main__ - Classification-F1 on test data: 0.6040
03/09/2022 06:47:05 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.603993789494431
03/09/2022 06:47:05 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0002, bsz=8 ...
03/09/2022 06:47:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:47:06 - INFO - __main__ - Printing 3 examples
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:47:06 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:47:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:47:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:47:06 - INFO - __main__ - Printing 3 examples
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:47:06 - INFO - __main__ - ['negative']
03/09/2022 06:47:06 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:47:06 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:47:06 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:47:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:47:17 - INFO - __main__ - Starting training!
03/09/2022 06:47:23 - INFO - __main__ - Step 10 Global step 10 Train loss 22.657948 on epoch=4
03/09/2022 06:47:27 - INFO - __main__ - Step 20 Global step 20 Train loss 17.918150 on epoch=9
03/09/2022 06:47:32 - INFO - __main__ - Step 30 Global step 30 Train loss 16.446228 on epoch=14
03/09/2022 06:47:37 - INFO - __main__ - Step 40 Global step 40 Train loss 15.591855 on epoch=19
03/09/2022 06:47:42 - INFO - __main__ - Step 50 Global step 50 Train loss 13.948126 on epoch=24
03/09/2022 06:47:44 - INFO - __main__ - Global step 50 Train loss 17.312460 Classification-F1 0.0 on epoch=24
03/09/2022 06:48:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:48:25 - INFO - __main__ - Step 60 Global step 60 Train loss 13.871733 on epoch=29
03/09/2022 06:48:30 - INFO - __main__ - Step 70 Global step 70 Train loss 12.820196 on epoch=34
03/09/2022 06:48:35 - INFO - __main__ - Step 80 Global step 80 Train loss 12.306083 on epoch=39
03/09/2022 06:48:40 - INFO - __main__ - Step 90 Global step 90 Train loss 11.776954 on epoch=44
03/09/2022 06:48:44 - INFO - __main__ - Step 100 Global step 100 Train loss 8.765771 on epoch=49
03/09/2022 06:48:45 - INFO - __main__ - Global step 100 Train loss 11.908147 Classification-F1 0.0 on epoch=49
03/09/2022 06:48:50 - INFO - __main__ - Step 110 Global step 110 Train loss 4.962437 on epoch=54
03/09/2022 06:48:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.908227 on epoch=59
03/09/2022 06:48:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.135192 on epoch=64
03/09/2022 06:49:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.065525 on epoch=69
03/09/2022 06:49:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.046343 on epoch=74
03/09/2022 06:49:09 - INFO - __main__ - Global step 150 Train loss 1.223545 Classification-F1 0.9375 on epoch=74
03/09/2022 06:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.9375 on epoch=74, global_step=150
03/09/2022 06:49:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.198479 on epoch=79
03/09/2022 06:49:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.089561 on epoch=84
03/09/2022 06:50:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.006425 on epoch=89
03/09/2022 06:50:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.006005 on epoch=94
03/09/2022 06:50:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.023701 on epoch=99
03/09/2022 06:50:11 - INFO - __main__ - Global step 200 Train loss 0.064834 Classification-F1 0.9687194525904204 on epoch=99
03/09/2022 06:50:46 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=99, global_step=200
03/09/2022 06:50:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.002625 on epoch=104
03/09/2022 06:50:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.001582 on epoch=109
03/09/2022 06:51:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000706 on epoch=114
03/09/2022 06:51:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.317918 on epoch=119
03/09/2022 06:51:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.074913 on epoch=124
03/09/2022 06:51:11 - INFO - __main__ - Global step 250 Train loss 0.079549 Classification-F1 0.9687194525904204 on epoch=124
03/09/2022 06:51:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.028309 on epoch=129
03/09/2022 06:51:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001288 on epoch=134
03/09/2022 06:51:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000392 on epoch=139
03/09/2022 06:51:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000218 on epoch=144
03/09/2022 06:51:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000174 on epoch=149
03/09/2022 06:51:36 - INFO - __main__ - Global step 300 Train loss 0.006076 Classification-F1 1.0 on epoch=149
03/09/2022 06:52:12 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=149, global_step=300
03/09/2022 06:52:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.195121 on epoch=154
03/09/2022 06:52:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000184 on epoch=159
03/09/2022 06:52:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.077040 on epoch=164
03/09/2022 06:52:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002218 on epoch=169
03/09/2022 06:52:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.030343 on epoch=174
03/09/2022 06:52:37 - INFO - __main__ - Global step 350 Train loss 0.060981 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 06:52:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001556 on epoch=179
03/09/2022 06:52:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.064016 on epoch=184
03/09/2022 06:52:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.010448 on epoch=189
03/09/2022 06:52:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000172 on epoch=194
03/09/2022 06:53:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000056 on epoch=199
03/09/2022 06:53:01 - INFO - __main__ - Global step 400 Train loss 0.015250 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 06:53:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000192 on epoch=204
03/09/2022 06:53:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000238 on epoch=209
03/09/2022 06:53:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000030 on epoch=214
03/09/2022 06:53:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000327 on epoch=219
03/09/2022 06:53:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000022 on epoch=224
03/09/2022 06:53:26 - INFO - __main__ - Global step 450 Train loss 0.000162 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 06:53:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.134353 on epoch=229
03/09/2022 06:53:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000545 on epoch=234
03/09/2022 06:53:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.345149 on epoch=239
03/09/2022 06:53:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.067909 on epoch=244
03/09/2022 06:53:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000246 on epoch=249
03/09/2022 06:53:51 - INFO - __main__ - Global step 500 Train loss 0.109640 Classification-F1 0.906158357771261 on epoch=249
03/09/2022 06:53:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.023861 on epoch=254
03/09/2022 06:54:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000854 on epoch=259
03/09/2022 06:54:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000109 on epoch=264
03/09/2022 06:54:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001172 on epoch=269
03/09/2022 06:54:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000976 on epoch=274
03/09/2022 06:54:16 - INFO - __main__ - Global step 550 Train loss 0.005394 Classification-F1 0.9375 on epoch=274
03/09/2022 06:54:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000323 on epoch=279
03/09/2022 06:54:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.056509 on epoch=284
03/09/2022 06:54:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000110 on epoch=289
03/09/2022 06:54:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000076 on epoch=294
03/09/2022 06:54:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000153 on epoch=299
03/09/2022 06:54:41 - INFO - __main__ - Global step 600 Train loss 0.011434 Classification-F1 0.9375 on epoch=299
03/09/2022 06:54:41 - INFO - __main__ - save last model!
03/09/2022 06:54:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:54:41 - INFO - __main__ - Printing 3 examples
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 06:54:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:54:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:54:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:54:41 - INFO - __main__ - Printing 3 examples
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:54:41 - INFO - __main__ - ['negative']
03/09/2022 06:54:41 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:54:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:54:42 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:54:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:54:51 - INFO - __main__ - Starting training!
03/09/2022 06:55:23 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 06:55:23 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 06:55:23 - INFO - __main__ - Printing 3 examples
03/09/2022 06:55:23 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 06:55:23 - INFO - __main__ - ['negative']
03/09/2022 06:55:23 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 06:55:23 - INFO - __main__ - ['negative']
03/09/2022 06:55:23 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 06:55:23 - INFO - __main__ - ['negative']
03/09/2022 06:55:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:55:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:55:25 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 06:55:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0002_8_predictions.txt
03/09/2022 06:55:39 - INFO - __main__ - Classification-F1 on test data: 0.9389
03/09/2022 06:55:39 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9389070776651285
03/09/2022 06:55:39 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0001, bsz=8 ...
03/09/2022 06:55:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:55:40 - INFO - __main__ - Printing 3 examples
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 06:55:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:55:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 06:55:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 06:55:40 - INFO - __main__ - Printing 3 examples
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/09/2022 06:55:40 - INFO - __main__ - ['negative']
03/09/2022 06:55:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 06:55:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 06:55:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 06:55:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 06:55:49 - INFO - __main__ - Starting training!
03/09/2022 06:55:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.670954 on epoch=4
03/09/2022 06:55:59 - INFO - __main__ - Step 20 Global step 20 Train loss 18.377687 on epoch=9
03/09/2022 06:56:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.213085 on epoch=14
03/09/2022 06:56:09 - INFO - __main__ - Step 40 Global step 40 Train loss 17.102783 on epoch=19
03/09/2022 06:56:13 - INFO - __main__ - Step 50 Global step 50 Train loss 15.146101 on epoch=24
03/09/2022 06:56:23 - INFO - __main__ - Global step 50 Train loss 18.302122 Classification-F1 0.0 on epoch=24
03/09/2022 06:56:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 06:57:04 - INFO - __main__ - Step 60 Global step 60 Train loss 15.745010 on epoch=29
03/09/2022 06:57:08 - INFO - __main__ - Step 70 Global step 70 Train loss 14.847064 on epoch=34
03/09/2022 06:57:13 - INFO - __main__ - Step 80 Global step 80 Train loss 15.135315 on epoch=39
03/09/2022 06:57:18 - INFO - __main__ - Step 90 Global step 90 Train loss 15.249496 on epoch=44
03/09/2022 06:57:23 - INFO - __main__ - Step 100 Global step 100 Train loss 14.161306 on epoch=49
03/09/2022 06:57:29 - INFO - __main__ - Global step 100 Train loss 15.027639 Classification-F1 0.0 on epoch=49
03/09/2022 06:57:33 - INFO - __main__ - Step 110 Global step 110 Train loss 13.537671 on epoch=54
03/09/2022 06:57:38 - INFO - __main__ - Step 120 Global step 120 Train loss 13.830145 on epoch=59
03/09/2022 06:57:43 - INFO - __main__ - Step 130 Global step 130 Train loss 13.507220 on epoch=64
03/09/2022 06:57:48 - INFO - __main__ - Step 140 Global step 140 Train loss 12.799348 on epoch=69
03/09/2022 06:57:52 - INFO - __main__ - Step 150 Global step 150 Train loss 12.550844 on epoch=74
03/09/2022 06:57:57 - INFO - __main__ - Global step 150 Train loss 13.245045 Classification-F1 0.0 on epoch=74
03/09/2022 06:58:02 - INFO - __main__ - Step 160 Global step 160 Train loss 11.868431 on epoch=79
03/09/2022 06:58:06 - INFO - __main__ - Step 170 Global step 170 Train loss 11.599307 on epoch=84
03/09/2022 06:58:11 - INFO - __main__ - Step 180 Global step 180 Train loss 11.135634 on epoch=89
03/09/2022 06:58:16 - INFO - __main__ - Step 190 Global step 190 Train loss 10.037599 on epoch=94
03/09/2022 06:58:20 - INFO - __main__ - Step 200 Global step 200 Train loss 8.290104 on epoch=99
03/09/2022 06:58:23 - INFO - __main__ - Global step 200 Train loss 10.586216 Classification-F1 0.041666666666666664 on epoch=99
03/09/2022 06:58:59 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.041666666666666664 on epoch=99, global_step=200
03/09/2022 06:59:03 - INFO - __main__ - Step 210 Global step 210 Train loss 6.357329 on epoch=104
03/09/2022 06:59:08 - INFO - __main__ - Step 220 Global step 220 Train loss 3.497171 on epoch=109
03/09/2022 06:59:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.432600 on epoch=114
03/09/2022 06:59:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.006384 on epoch=119
03/09/2022 06:59:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.967502 on epoch=124
03/09/2022 06:59:23 - INFO - __main__ - Global step 250 Train loss 2.652197 Classification-F1 0.15360983102918588 on epoch=124
03/09/2022 07:00:00 - INFO - __main__ - Saving model with best Classification-F1: 0.041666666666666664 -> 0.15360983102918588 on epoch=124, global_step=250
03/09/2022 07:00:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.305800 on epoch=129
03/09/2022 07:00:09 - INFO - __main__ - Step 270 Global step 270 Train loss 1.089291 on epoch=134
03/09/2022 07:00:14 - INFO - __main__ - Step 280 Global step 280 Train loss 1.071935 on epoch=139
03/09/2022 07:00:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.713537 on epoch=144
03/09/2022 07:00:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.629506 on epoch=149
03/09/2022 07:00:27 - INFO - __main__ - Global step 300 Train loss 0.962014 Classification-F1 0.13043478260869562 on epoch=149
03/09/2022 07:00:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.777235 on epoch=154
03/09/2022 07:00:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.759668 on epoch=159
03/09/2022 07:00:41 - INFO - __main__ - Step 330 Global step 330 Train loss 1.030035 on epoch=164
03/09/2022 07:00:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.587639 on epoch=169
03/09/2022 07:00:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.385206 on epoch=174
03/09/2022 07:00:52 - INFO - __main__ - Global step 350 Train loss 0.707957 Classification-F1 0.49090909090909085 on epoch=174
03/09/2022 07:01:28 - INFO - __main__ - Saving model with best Classification-F1: 0.15360983102918588 -> 0.49090909090909085 on epoch=174, global_step=350
03/09/2022 07:01:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.368787 on epoch=179
03/09/2022 07:01:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.378612 on epoch=184
03/09/2022 07:01:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.344907 on epoch=189
03/09/2022 07:01:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.345426 on epoch=194
03/09/2022 07:01:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.311621 on epoch=199
03/09/2022 07:01:53 - INFO - __main__ - Global step 400 Train loss 0.349871 Classification-F1 0.6825396825396826 on epoch=199
03/09/2022 07:02:30 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.6825396825396826 on epoch=199, global_step=400
03/09/2022 07:02:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.502050 on epoch=204
03/09/2022 07:02:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.458955 on epoch=209
03/09/2022 07:02:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.305438 on epoch=214
03/09/2022 07:02:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.278876 on epoch=219
03/09/2022 07:02:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.354281 on epoch=224
03/09/2022 07:02:55 - INFO - __main__ - Global step 450 Train loss 0.379920 Classification-F1 0.6113360323886641 on epoch=224
03/09/2022 07:03:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.292672 on epoch=229
03/09/2022 07:03:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.253416 on epoch=234
03/09/2022 07:03:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.272510 on epoch=239
03/09/2022 07:03:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.304299 on epoch=244
03/09/2022 07:03:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276426 on epoch=249
03/09/2022 07:03:19 - INFO - __main__ - Global step 500 Train loss 0.279865 Classification-F1 0.875 on epoch=249
03/09/2022 07:03:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6825396825396826 -> 0.875 on epoch=249, global_step=500
03/09/2022 07:04:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.278950 on epoch=254
03/09/2022 07:04:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.289842 on epoch=259
03/09/2022 07:04:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.265846 on epoch=264
03/09/2022 07:04:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.220781 on epoch=269
03/09/2022 07:04:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.227364 on epoch=274
03/09/2022 07:04:20 - INFO - __main__ - Global step 550 Train loss 0.256557 Classification-F1 0.7793103448275862 on epoch=274
03/09/2022 07:04:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.223501 on epoch=279
03/09/2022 07:04:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.208241 on epoch=284
03/09/2022 07:04:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167019 on epoch=289
03/09/2022 07:04:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.195511 on epoch=294
03/09/2022 07:04:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.211189 on epoch=299
03/09/2022 07:04:44 - INFO - __main__ - Global step 600 Train loss 0.201092 Classification-F1 0.8435972629521017 on epoch=299
03/09/2022 07:04:44 - INFO - __main__ - save last model!
03/09/2022 07:04:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:04:45 - INFO - __main__ - Printing 3 examples
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:04:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:04:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:04:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:04:45 - INFO - __main__ - Printing 3 examples
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:04:45 - INFO - __main__ - ['positive']
03/09/2022 07:04:45 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:04:45 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:04:45 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:04:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:04:55 - INFO - __main__ - Starting training!
03/09/2022 07:05:24 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:05:25 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:05:25 - INFO - __main__ - Printing 3 examples
03/09/2022 07:05:25 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:05:25 - INFO - __main__ - ['negative']
03/09/2022 07:05:25 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:05:25 - INFO - __main__ - ['negative']
03/09/2022 07:05:25 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:05:25 - INFO - __main__ - ['negative']
03/09/2022 07:05:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:05:25 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:05:26 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:05:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_13_0.0001_8_predictions.txt
03/09/2022 07:05:41 - INFO - __main__ - Classification-F1 on test data: 0.7874
03/09/2022 07:05:41 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.875, test_performance=0.787350725805549
03/09/2022 07:05:41 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0005, bsz=8 ...
03/09/2022 07:05:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:05:42 - INFO - __main__ - Printing 3 examples
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:05:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:05:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:05:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:05:42 - INFO - __main__ - Printing 3 examples
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:05:42 - INFO - __main__ - ['positive']
03/09/2022 07:05:42 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:05:42 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:05:42 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:05:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:05:51 - INFO - __main__ - Starting training!
03/09/2022 07:05:56 - INFO - __main__ - Step 10 Global step 10 Train loss 23.465708 on epoch=4
03/09/2022 07:05:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.225107 on epoch=9
03/09/2022 07:06:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.374500 on epoch=14
03/09/2022 07:06:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.440104 on epoch=19
03/09/2022 07:06:14 - INFO - __main__ - Step 50 Global step 50 Train loss 12.645704 on epoch=24
03/09/2022 07:06:24 - INFO - __main__ - Global step 50 Train loss 17.230225 Classification-F1 0.0 on epoch=24
03/09/2022 07:07:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:07:07 - INFO - __main__ - Step 60 Global step 60 Train loss 9.783524 on epoch=29
03/09/2022 07:07:11 - INFO - __main__ - Step 70 Global step 70 Train loss 3.709040 on epoch=34
03/09/2022 07:07:16 - INFO - __main__ - Step 80 Global step 80 Train loss 2.599068 on epoch=39
03/09/2022 07:07:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.379314 on epoch=44
03/09/2022 07:07:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.494376 on epoch=49
03/09/2022 07:07:26 - INFO - __main__ - Global step 100 Train loss 3.993065 Classification-F1 0.6113360323886641 on epoch=49
03/09/2022 07:08:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.6113360323886641 on epoch=49, global_step=100
03/09/2022 07:08:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.267591 on epoch=54
03/09/2022 07:08:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.732680 on epoch=59
03/09/2022 07:08:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.559440 on epoch=64
03/09/2022 07:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.275660 on epoch=69
03/09/2022 07:08:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.183720 on epoch=74
03/09/2022 07:08:27 - INFO - __main__ - Global step 150 Train loss 0.603818 Classification-F1 0.8435972629521017 on epoch=74
03/09/2022 07:09:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6113360323886641 -> 0.8435972629521017 on epoch=74, global_step=150
03/09/2022 07:09:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.125530 on epoch=79
03/09/2022 07:09:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.117123 on epoch=84
03/09/2022 07:09:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.168275 on epoch=89
03/09/2022 07:09:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.121782 on epoch=94
03/09/2022 07:09:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.026667 on epoch=99
03/09/2022 07:09:28 - INFO - __main__ - Global step 200 Train loss 0.111875 Classification-F1 0.9372549019607843 on epoch=99
03/09/2022 07:10:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8435972629521017 -> 0.9372549019607843 on epoch=99, global_step=200
03/09/2022 07:10:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.038052 on epoch=104
03/09/2022 07:10:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.026163 on epoch=109
03/09/2022 07:10:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.350606 on epoch=114
03/09/2022 07:10:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.241416 on epoch=119
03/09/2022 07:10:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.099874 on epoch=124
03/09/2022 07:10:30 - INFO - __main__ - Global step 250 Train loss 0.151222 Classification-F1 0.7757757757757757 on epoch=124
03/09/2022 07:10:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.057261 on epoch=129
03/09/2022 07:10:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.188531 on epoch=134
03/09/2022 07:10:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027228 on epoch=139
03/09/2022 07:10:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.072980 on epoch=144
03/09/2022 07:10:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.045355 on epoch=149
03/09/2022 07:10:55 - INFO - __main__ - Global step 300 Train loss 0.078271 Classification-F1 0.873015873015873 on epoch=149
03/09/2022 07:10:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.041676 on epoch=154
03/09/2022 07:11:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.052644 on epoch=159
03/09/2022 07:11:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.025254 on epoch=164
03/09/2022 07:11:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.014648 on epoch=169
03/09/2022 07:11:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.072520 on epoch=174
03/09/2022 07:11:19 - INFO - __main__ - Global step 350 Train loss 0.041348 Classification-F1 0.3992490613266583 on epoch=174
03/09/2022 07:11:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.107682 on epoch=179
03/09/2022 07:11:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.042138 on epoch=184
03/09/2022 07:11:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047339 on epoch=189
03/09/2022 07:11:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.078599 on epoch=194
03/09/2022 07:11:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.035375 on epoch=199
03/09/2022 07:11:44 - INFO - __main__ - Global step 400 Train loss 0.062227 Classification-F1 0.7810361681329424 on epoch=199
03/09/2022 07:11:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.060592 on epoch=204
03/09/2022 07:11:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.013520 on epoch=209
03/09/2022 07:11:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004584 on epoch=214
03/09/2022 07:12:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001038 on epoch=219
03/09/2022 07:12:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007612 on epoch=224
03/09/2022 07:12:08 - INFO - __main__ - Global step 450 Train loss 0.017469 Classification-F1 0.7757757757757757 on epoch=224
03/09/2022 07:12:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008954 on epoch=229
03/09/2022 07:12:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002035 on epoch=234
03/09/2022 07:12:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003194 on epoch=239
03/09/2022 07:12:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002617 on epoch=244
03/09/2022 07:12:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000474 on epoch=249
03/09/2022 07:12:32 - INFO - __main__ - Global step 500 Train loss 0.003455 Classification-F1 0.8435972629521017 on epoch=249
03/09/2022 07:12:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.031074 on epoch=254
03/09/2022 07:12:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000237 on epoch=259
03/09/2022 07:12:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000267 on epoch=264
03/09/2022 07:12:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004413 on epoch=269
03/09/2022 07:12:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000198 on epoch=274
03/09/2022 07:12:57 - INFO - __main__ - Global step 550 Train loss 0.007238 Classification-F1 0.8117647058823529 on epoch=274
03/09/2022 07:13:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000063 on epoch=279
03/09/2022 07:13:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000321 on epoch=284
03/09/2022 07:13:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000761 on epoch=289
03/09/2022 07:13:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000615 on epoch=294
03/09/2022 07:13:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002438 on epoch=299
03/09/2022 07:13:21 - INFO - __main__ - Global step 600 Train loss 0.000840 Classification-F1 0.8435972629521017 on epoch=299
03/09/2022 07:13:21 - INFO - __main__ - save last model!
03/09/2022 07:13:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:13:22 - INFO - __main__ - Printing 3 examples
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:13:22 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:13:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:13:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:13:22 - INFO - __main__ - Printing 3 examples
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:13:22 - INFO - __main__ - ['positive']
03/09/2022 07:13:22 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:13:22 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:13:22 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:13:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:13:32 - INFO - __main__ - Starting training!
03/09/2022 07:14:03 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:14:04 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:14:04 - INFO - __main__ - Printing 3 examples
03/09/2022 07:14:04 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:14:04 - INFO - __main__ - ['negative']
03/09/2022 07:14:04 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:14:04 - INFO - __main__ - ['negative']
03/09/2022 07:14:04 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:14:04 - INFO - __main__ - ['negative']
03/09/2022 07:14:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:14:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:14:05 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:14:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0005_8_predictions.txt
03/09/2022 07:14:20 - INFO - __main__ - Classification-F1 on test data: 0.9169
03/09/2022 07:14:20 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9169394488582177
03/09/2022 07:14:20 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0003, bsz=8 ...
03/09/2022 07:14:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:14:21 - INFO - __main__ - Printing 3 examples
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:14:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:14:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:14:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:14:21 - INFO - __main__ - Printing 3 examples
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:14:21 - INFO - __main__ - ['positive']
03/09/2022 07:14:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:14:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:14:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:14:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:14:30 - INFO - __main__ - Starting training!
03/09/2022 07:14:35 - INFO - __main__ - Step 10 Global step 10 Train loss 22.212280 on epoch=4
03/09/2022 07:14:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.346857 on epoch=9
03/09/2022 07:14:44 - INFO - __main__ - Step 30 Global step 30 Train loss 17.040716 on epoch=14
03/09/2022 07:14:49 - INFO - __main__ - Step 40 Global step 40 Train loss 15.513784 on epoch=19
03/09/2022 07:14:54 - INFO - __main__ - Step 50 Global step 50 Train loss 13.769196 on epoch=24
03/09/2022 07:15:01 - INFO - __main__ - Global step 50 Train loss 17.376568 Classification-F1 0.0 on epoch=24
03/09/2022 07:15:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:15:43 - INFO - __main__ - Step 60 Global step 60 Train loss 12.946634 on epoch=29
03/09/2022 07:15:48 - INFO - __main__ - Step 70 Global step 70 Train loss 11.622050 on epoch=34
03/09/2022 07:15:52 - INFO - __main__ - Step 80 Global step 80 Train loss 10.160978 on epoch=39
03/09/2022 07:15:57 - INFO - __main__ - Step 90 Global step 90 Train loss 3.479689 on epoch=44
03/09/2022 07:16:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.432134 on epoch=49
03/09/2022 07:16:03 - INFO - __main__ - Global step 100 Train loss 7.728297 Classification-F1 0.9687194525904204 on epoch=49
03/09/2022 07:16:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.9687194525904204 on epoch=49, global_step=100
03/09/2022 07:16:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.157234 on epoch=54
03/09/2022 07:16:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.068805 on epoch=59
03/09/2022 07:16:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.075144 on epoch=64
03/09/2022 07:16:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.024177 on epoch=69
03/09/2022 07:17:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.010447 on epoch=74
03/09/2022 07:17:02 - INFO - __main__ - Global step 150 Train loss 0.067161 Classification-F1 0.9372549019607843 on epoch=74
03/09/2022 07:17:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.017471 on epoch=79
03/09/2022 07:17:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.016210 on epoch=84
03/09/2022 07:17:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.008841 on epoch=89
03/09/2022 07:17:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.034360 on epoch=94
03/09/2022 07:17:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003390 on epoch=99
03/09/2022 07:17:26 - INFO - __main__ - Global step 200 Train loss 0.016054 Classification-F1 0.9687194525904204 on epoch=99
03/09/2022 07:17:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.003355 on epoch=104
03/09/2022 07:17:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002604 on epoch=109
03/09/2022 07:17:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001294 on epoch=114
03/09/2022 07:17:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001788 on epoch=119
03/09/2022 07:17:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001120 on epoch=124
03/09/2022 07:17:50 - INFO - __main__ - Global step 250 Train loss 0.002032 Classification-F1 0.9687194525904204 on epoch=124
03/09/2022 07:17:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.006167 on epoch=129
03/09/2022 07:18:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001024 on epoch=134
03/09/2022 07:18:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000610 on epoch=139
03/09/2022 07:18:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.016164 on epoch=144
03/09/2022 07:18:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.109547 on epoch=149
03/09/2022 07:18:14 - INFO - __main__ - Global step 300 Train loss 0.026703 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 07:18:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.032580 on epoch=154
03/09/2022 07:18:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.080153 on epoch=159
03/09/2022 07:18:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.433920 on epoch=164
03/09/2022 07:18:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.374850 on epoch=169
03/09/2022 07:18:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.271681 on epoch=174
03/09/2022 07:18:39 - INFO - __main__ - Global step 350 Train loss 0.238637 Classification-F1 0.7793103448275862 on epoch=174
03/09/2022 07:18:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.315837 on epoch=179
03/09/2022 07:18:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.387604 on epoch=184
03/09/2022 07:18:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.303830 on epoch=189
03/09/2022 07:18:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.323788 on epoch=194
03/09/2022 07:19:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.232843 on epoch=199
03/09/2022 07:19:03 - INFO - __main__ - Global step 400 Train loss 0.312781 Classification-F1 0.906158357771261 on epoch=199
03/09/2022 07:19:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.214381 on epoch=204
03/09/2022 07:19:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.190507 on epoch=209
03/09/2022 07:19:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.170226 on epoch=214
03/09/2022 07:19:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.155957 on epoch=219
03/09/2022 07:19:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.109316 on epoch=224
03/09/2022 07:19:27 - INFO - __main__ - Global step 450 Train loss 0.168077 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 07:19:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.311170 on epoch=229
03/09/2022 07:19:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.145068 on epoch=234
03/09/2022 07:19:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.164105 on epoch=239
03/09/2022 07:19:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.065611 on epoch=244
03/09/2022 07:19:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053580 on epoch=249
03/09/2022 07:19:51 - INFO - __main__ - Global step 500 Train loss 0.147907 Classification-F1 0.7702564102564102 on epoch=249
03/09/2022 07:19:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.055702 on epoch=254
03/09/2022 07:20:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.072138 on epoch=259
03/09/2022 07:20:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.073889 on epoch=264
03/09/2022 07:20:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.144095 on epoch=269
03/09/2022 07:20:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.189512 on epoch=274
03/09/2022 07:20:15 - INFO - __main__ - Global step 550 Train loss 0.107067 Classification-F1 0.9687194525904204 on epoch=274
03/09/2022 07:20:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.045805 on epoch=279
03/09/2022 07:20:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.155834 on epoch=284
03/09/2022 07:20:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.122931 on epoch=289
03/09/2022 07:20:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.121118 on epoch=294
03/09/2022 07:20:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.230752 on epoch=299
03/09/2022 07:20:39 - INFO - __main__ - Global step 600 Train loss 0.135288 Classification-F1 0.6945917285259808 on epoch=299
03/09/2022 07:20:39 - INFO - __main__ - save last model!
03/09/2022 07:20:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:20:39 - INFO - __main__ - Printing 3 examples
03/09/2022 07:20:39 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:20:39 - INFO - __main__ - ['positive']
03/09/2022 07:20:39 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:20:39 - INFO - __main__ - ['positive']
03/09/2022 07:20:39 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:20:39 - INFO - __main__ - ['positive']
03/09/2022 07:20:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:20:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:20:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:20:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:20:40 - INFO - __main__ - Printing 3 examples
03/09/2022 07:20:40 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:20:40 - INFO - __main__ - ['positive']
03/09/2022 07:20:40 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:20:40 - INFO - __main__ - ['positive']
03/09/2022 07:20:40 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:20:40 - INFO - __main__ - ['positive']
03/09/2022 07:20:40 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:20:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:20:40 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:20:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:20:50 - INFO - __main__ - Starting training!
03/09/2022 07:21:19 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:21:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:21:20 - INFO - __main__ - Printing 3 examples
03/09/2022 07:21:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:21:20 - INFO - __main__ - ['negative']
03/09/2022 07:21:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:21:20 - INFO - __main__ - ['negative']
03/09/2022 07:21:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:21:20 - INFO - __main__ - ['negative']
03/09/2022 07:21:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:21:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:21:21 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:21:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0003_8_predictions.txt
03/09/2022 07:21:35 - INFO - __main__ - Classification-F1 on test data: 0.9290
03/09/2022 07:21:36 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9289982249556239
03/09/2022 07:21:36 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0002, bsz=8 ...
03/09/2022 07:21:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:21:37 - INFO - __main__ - Printing 3 examples
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:21:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:21:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:21:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:21:37 - INFO - __main__ - Printing 3 examples
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:21:37 - INFO - __main__ - ['positive']
03/09/2022 07:21:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:21:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:21:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:21:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:21:46 - INFO - __main__ - Starting training!
03/09/2022 07:21:50 - INFO - __main__ - Step 10 Global step 10 Train loss 23.906343 on epoch=4
03/09/2022 07:21:55 - INFO - __main__ - Step 20 Global step 20 Train loss 19.968283 on epoch=9
03/09/2022 07:21:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.506687 on epoch=14
03/09/2022 07:22:04 - INFO - __main__ - Step 40 Global step 40 Train loss 16.257349 on epoch=19
03/09/2022 07:22:09 - INFO - __main__ - Step 50 Global step 50 Train loss 15.577960 on epoch=24
03/09/2022 07:22:15 - INFO - __main__ - Global step 50 Train loss 18.643324 Classification-F1 0.0 on epoch=24
03/09/2022 07:22:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:22:56 - INFO - __main__ - Step 60 Global step 60 Train loss 15.149335 on epoch=29
03/09/2022 07:23:01 - INFO - __main__ - Step 70 Global step 70 Train loss 13.568758 on epoch=34
03/09/2022 07:23:06 - INFO - __main__ - Step 80 Global step 80 Train loss 13.312548 on epoch=39
03/09/2022 07:23:10 - INFO - __main__ - Step 90 Global step 90 Train loss 12.791578 on epoch=44
03/09/2022 07:23:15 - INFO - __main__ - Step 100 Global step 100 Train loss 11.714728 on epoch=49
03/09/2022 07:23:18 - INFO - __main__ - Global step 100 Train loss 13.307390 Classification-F1 0.008403361344537815 on epoch=49
03/09/2022 07:23:55 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.008403361344537815 on epoch=49, global_step=100
03/09/2022 07:24:00 - INFO - __main__ - Step 110 Global step 110 Train loss 10.746004 on epoch=54
03/09/2022 07:24:05 - INFO - __main__ - Step 120 Global step 120 Train loss 9.381700 on epoch=59
03/09/2022 07:24:09 - INFO - __main__ - Step 130 Global step 130 Train loss 5.223197 on epoch=64
03/09/2022 07:24:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.617856 on epoch=69
03/09/2022 07:24:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.418817 on epoch=74
03/09/2022 07:24:19 - INFO - __main__ - Global step 150 Train loss 5.877514 Classification-F1 0.3191489361702127 on epoch=74
03/09/2022 07:24:56 - INFO - __main__ - Saving model with best Classification-F1: 0.008403361344537815 -> 0.3191489361702127 on epoch=74, global_step=150
03/09/2022 07:25:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.086251 on epoch=79
03/09/2022 07:25:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.799559 on epoch=84
03/09/2022 07:25:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.682189 on epoch=89
03/09/2022 07:25:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.462374 on epoch=94
03/09/2022 07:25:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.408528 on epoch=99
03/09/2022 07:25:21 - INFO - __main__ - Global step 200 Train loss 0.687780 Classification-F1 0.6389743589743591 on epoch=99
03/09/2022 07:25:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.6389743589743591 on epoch=99, global_step=200
03/09/2022 07:26:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.461762 on epoch=104
03/09/2022 07:26:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.547941 on epoch=109
03/09/2022 07:26:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.550324 on epoch=114
03/09/2022 07:26:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.415617 on epoch=119
03/09/2022 07:26:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.488420 on epoch=124
03/09/2022 07:26:21 - INFO - __main__ - Global step 250 Train loss 0.492813 Classification-F1 0.4589371980676329 on epoch=124
03/09/2022 07:26:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.623583 on epoch=129
03/09/2022 07:26:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.415308 on epoch=134
03/09/2022 07:26:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.299497 on epoch=139
03/09/2022 07:26:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.302867 on epoch=144
03/09/2022 07:26:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.335746 on epoch=149
03/09/2022 07:26:46 - INFO - __main__ - Global step 300 Train loss 0.395400 Classification-F1 0.5607843137254902 on epoch=149
03/09/2022 07:26:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.335959 on epoch=154
03/09/2022 07:26:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.352104 on epoch=159
03/09/2022 07:27:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.257370 on epoch=164
03/09/2022 07:27:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.440723 on epoch=169
03/09/2022 07:27:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.320892 on epoch=174
03/09/2022 07:27:10 - INFO - __main__ - Global step 350 Train loss 0.341410 Classification-F1 0.5555555555555556 on epoch=174
03/09/2022 07:27:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.299931 on epoch=179
03/09/2022 07:27:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.305630 on epoch=184
03/09/2022 07:27:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.238483 on epoch=189
03/09/2022 07:27:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.220185 on epoch=194
03/09/2022 07:27:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.222169 on epoch=199
03/09/2022 07:27:34 - INFO - __main__ - Global step 400 Train loss 0.257280 Classification-F1 0.6532019704433498 on epoch=199
03/09/2022 07:28:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6389743589743591 -> 0.6532019704433498 on epoch=199, global_step=400
03/09/2022 07:28:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.180762 on epoch=204
03/09/2022 07:28:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.230028 on epoch=209
03/09/2022 07:28:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.195424 on epoch=214
03/09/2022 07:28:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.189841 on epoch=219
03/09/2022 07:28:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.137528 on epoch=224
03/09/2022 07:28:35 - INFO - __main__ - Global step 450 Train loss 0.186716 Classification-F1 0.6532019704433498 on epoch=224
03/09/2022 07:28:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.150664 on epoch=229
03/09/2022 07:28:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.140017 on epoch=234
03/09/2022 07:28:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.136042 on epoch=239
03/09/2022 07:28:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.097711 on epoch=244
03/09/2022 07:28:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.172387 on epoch=249
03/09/2022 07:28:59 - INFO - __main__ - Global step 500 Train loss 0.139364 Classification-F1 0.6235294117647059 on epoch=249
03/09/2022 07:29:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.085302 on epoch=254
03/09/2022 07:29:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.118532 on epoch=259
03/09/2022 07:29:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.089938 on epoch=264
03/09/2022 07:29:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.052510 on epoch=269
03/09/2022 07:29:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.113905 on epoch=274
03/09/2022 07:29:24 - INFO - __main__ - Global step 550 Train loss 0.092038 Classification-F1 0.5901477832512315 on epoch=274
03/09/2022 07:29:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026368 on epoch=279
03/09/2022 07:29:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.057810 on epoch=284
03/09/2022 07:29:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036346 on epoch=289
03/09/2022 07:29:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021039 on epoch=294
03/09/2022 07:29:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037295 on epoch=299
03/09/2022 07:29:48 - INFO - __main__ - Global step 600 Train loss 0.035772 Classification-F1 0.6113360323886641 on epoch=299
03/09/2022 07:29:48 - INFO - __main__ - save last model!
03/09/2022 07:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:29:50 - INFO - __main__ - Printing 3 examples
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:29:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:29:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:29:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:29:50 - INFO - __main__ - Printing 3 examples
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:29:50 - INFO - __main__ - ['positive']
03/09/2022 07:29:50 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:29:50 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:29:50 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:30:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:30:00 - INFO - __main__ - Starting training!
03/09/2022 07:30:30 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:30:31 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:30:31 - INFO - __main__ - Printing 3 examples
03/09/2022 07:30:31 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:30:31 - INFO - __main__ - ['negative']
03/09/2022 07:30:31 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:30:31 - INFO - __main__ - ['negative']
03/09/2022 07:30:31 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:30:31 - INFO - __main__ - ['negative']
03/09/2022 07:30:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:30:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:30:32 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:30:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0002_8_predictions.txt
03/09/2022 07:30:46 - INFO - __main__ - Classification-F1 on test data: 0.6647
03/09/2022 07:30:47 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.6532019704433498, test_performance=0.6646764025671561
03/09/2022 07:30:47 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0001, bsz=8 ...
03/09/2022 07:30:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:30:48 - INFO - __main__ - Printing 3 examples
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:30:48 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:30:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:30:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:30:48 - INFO - __main__ - Printing 3 examples
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/09/2022 07:30:48 - INFO - __main__ - ['positive']
03/09/2022 07:30:48 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:30:48 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:30:48 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:30:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:30:57 - INFO - __main__ - Starting training!
03/09/2022 07:31:01 - INFO - __main__ - Step 10 Global step 10 Train loss 23.357590 on epoch=4
03/09/2022 07:31:06 - INFO - __main__ - Step 20 Global step 20 Train loss 19.941040 on epoch=9
03/09/2022 07:31:10 - INFO - __main__ - Step 30 Global step 30 Train loss 17.896994 on epoch=14
03/09/2022 07:31:15 - INFO - __main__ - Step 40 Global step 40 Train loss 17.781626 on epoch=19
03/09/2022 07:31:20 - INFO - __main__ - Step 50 Global step 50 Train loss 16.401131 on epoch=24
03/09/2022 07:31:28 - INFO - __main__ - Global step 50 Train loss 19.075676 Classification-F1 0.0 on epoch=24
03/09/2022 07:32:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:32:09 - INFO - __main__ - Step 60 Global step 60 Train loss 15.949445 on epoch=29
03/09/2022 07:32:14 - INFO - __main__ - Step 70 Global step 70 Train loss 15.811399 on epoch=34
03/09/2022 07:32:19 - INFO - __main__ - Step 80 Global step 80 Train loss 16.285486 on epoch=39
03/09/2022 07:32:23 - INFO - __main__ - Step 90 Global step 90 Train loss 14.861842 on epoch=44
03/09/2022 07:32:28 - INFO - __main__ - Step 100 Global step 100 Train loss 14.386148 on epoch=49
03/09/2022 07:32:34 - INFO - __main__ - Global step 100 Train loss 15.458865 Classification-F1 0.0 on epoch=49
03/09/2022 07:32:39 - INFO - __main__ - Step 110 Global step 110 Train loss 14.836618 on epoch=54
03/09/2022 07:32:44 - INFO - __main__ - Step 120 Global step 120 Train loss 14.468143 on epoch=59
03/09/2022 07:32:49 - INFO - __main__ - Step 130 Global step 130 Train loss 13.739863 on epoch=64
03/09/2022 07:32:53 - INFO - __main__ - Step 140 Global step 140 Train loss 13.181025 on epoch=69
03/09/2022 07:32:58 - INFO - __main__ - Step 150 Global step 150 Train loss 12.522114 on epoch=74
03/09/2022 07:33:03 - INFO - __main__ - Global step 150 Train loss 13.749552 Classification-F1 0.0 on epoch=74
03/09/2022 07:33:08 - INFO - __main__ - Step 160 Global step 160 Train loss 12.301744 on epoch=79
03/09/2022 07:33:13 - INFO - __main__ - Step 170 Global step 170 Train loss 10.912993 on epoch=84
03/09/2022 07:33:17 - INFO - __main__ - Step 180 Global step 180 Train loss 11.158826 on epoch=89
03/09/2022 07:33:22 - INFO - __main__ - Step 190 Global step 190 Train loss 10.687723 on epoch=94
03/09/2022 07:33:27 - INFO - __main__ - Step 200 Global step 200 Train loss 9.587111 on epoch=99
03/09/2022 07:33:29 - INFO - __main__ - Global step 200 Train loss 10.929679 Classification-F1 0.0 on epoch=99
03/09/2022 07:33:34 - INFO - __main__ - Step 210 Global step 210 Train loss 8.420752 on epoch=104
03/09/2022 07:33:38 - INFO - __main__ - Step 220 Global step 220 Train loss 2.494663 on epoch=109
03/09/2022 07:33:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.845907 on epoch=114
03/09/2022 07:33:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.640872 on epoch=119
03/09/2022 07:33:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.329459 on epoch=124
03/09/2022 07:33:53 - INFO - __main__ - Global step 250 Train loss 2.546330 Classification-F1 0.9687194525904204 on epoch=124
03/09/2022 07:34:29 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.9687194525904204 on epoch=124, global_step=250
03/09/2022 07:34:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.204961 on epoch=129
03/09/2022 07:34:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.163022 on epoch=134
03/09/2022 07:34:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100515 on epoch=139
03/09/2022 07:34:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.047267 on epoch=144
03/09/2022 07:34:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.050518 on epoch=149
03/09/2022 07:34:53 - INFO - __main__ - Global step 300 Train loss 0.113257 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 07:34:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.135147 on epoch=154
03/09/2022 07:35:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.109953 on epoch=159
03/09/2022 07:35:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.106206 on epoch=164
03/09/2022 07:35:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022588 on epoch=169
03/09/2022 07:35:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011059 on epoch=174
03/09/2022 07:35:17 - INFO - __main__ - Global step 350 Train loss 0.076991 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 07:35:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.013238 on epoch=179
03/09/2022 07:35:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009443 on epoch=184
03/09/2022 07:35:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.012456 on epoch=189
03/09/2022 07:35:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001454 on epoch=194
03/09/2022 07:35:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.004133 on epoch=199
03/09/2022 07:35:42 - INFO - __main__ - Global step 400 Train loss 0.008145 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 07:35:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002381 on epoch=204
03/09/2022 07:35:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004236 on epoch=209
03/09/2022 07:35:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.002367 on epoch=214
03/09/2022 07:36:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001175 on epoch=219
03/09/2022 07:36:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.221249 on epoch=224
03/09/2022 07:36:06 - INFO - __main__ - Global step 450 Train loss 0.046282 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 07:36:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001899 on epoch=229
03/09/2022 07:36:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010780 on epoch=234
03/09/2022 07:36:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001764 on epoch=239
03/09/2022 07:36:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009782 on epoch=244
03/09/2022 07:36:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.015932 on epoch=249
03/09/2022 07:36:30 - INFO - __main__ - Global step 500 Train loss 0.008031 Classification-F1 0.9687194525904204 on epoch=249
03/09/2022 07:36:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001268 on epoch=254
03/09/2022 07:36:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003227 on epoch=259
03/09/2022 07:36:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001228 on epoch=264
03/09/2022 07:36:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000527 on epoch=269
03/09/2022 07:36:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000422 on epoch=274
03/09/2022 07:36:54 - INFO - __main__ - Global step 550 Train loss 0.001334 Classification-F1 0.9687194525904204 on epoch=274
03/09/2022 07:36:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000954 on epoch=279
03/09/2022 07:37:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000638 on epoch=284
03/09/2022 07:37:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.008466 on epoch=289
03/09/2022 07:37:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001087 on epoch=294
03/09/2022 07:37:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001791 on epoch=299
03/09/2022 07:37:18 - INFO - __main__ - Global step 600 Train loss 0.002587 Classification-F1 0.9687194525904204 on epoch=299
03/09/2022 07:37:18 - INFO - __main__ - save last model!
03/09/2022 07:37:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:37:19 - INFO - __main__ - Printing 3 examples
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:37:19 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:37:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:37:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:37:19 - INFO - __main__ - Printing 3 examples
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:37:19 - INFO - __main__ - ['negative']
03/09/2022 07:37:19 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:37:19 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:37:19 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:37:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:37:28 - INFO - __main__ - Starting training!
03/09/2022 07:37:59 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:38:00 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:38:00 - INFO - __main__ - Printing 3 examples
03/09/2022 07:38:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:38:00 - INFO - __main__ - ['negative']
03/09/2022 07:38:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:38:00 - INFO - __main__ - ['negative']
03/09/2022 07:38:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:38:00 - INFO - __main__ - ['negative']
03/09/2022 07:38:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:38:00 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:38:01 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:38:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_21_0.0001_8_predictions.txt
03/09/2022 07:38:15 - INFO - __main__ - Classification-F1 on test data: 0.9399
03/09/2022 07:38:15 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9399459513562205
03/09/2022 07:38:15 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0005, bsz=8 ...
03/09/2022 07:38:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:38:16 - INFO - __main__ - Printing 3 examples
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:38:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:38:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:38:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:38:16 - INFO - __main__ - Printing 3 examples
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:38:16 - INFO - __main__ - ['negative']
03/09/2022 07:38:16 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:38:16 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:38:16 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:38:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:38:25 - INFO - __main__ - Starting training!
03/09/2022 07:38:29 - INFO - __main__ - Step 10 Global step 10 Train loss 23.863840 on epoch=4
03/09/2022 07:38:34 - INFO - __main__ - Step 20 Global step 20 Train loss 16.435137 on epoch=9
03/09/2022 07:38:39 - INFO - __main__ - Step 30 Global step 30 Train loss 14.580835 on epoch=14
03/09/2022 07:38:43 - INFO - __main__ - Step 40 Global step 40 Train loss 11.276888 on epoch=19
03/09/2022 07:38:48 - INFO - __main__ - Step 50 Global step 50 Train loss 9.350591 on epoch=24
03/09/2022 07:38:49 - INFO - __main__ - Global step 50 Train loss 15.101458 Classification-F1 0.19393939393939394 on epoch=24
03/09/2022 07:39:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19393939393939394 on epoch=24, global_step=50
03/09/2022 07:39:29 - INFO - __main__ - Step 60 Global step 60 Train loss 4.644236 on epoch=29
03/09/2022 07:39:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.883618 on epoch=34
03/09/2022 07:39:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.817116 on epoch=39
03/09/2022 07:39:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.388940 on epoch=44
03/09/2022 07:39:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.531934 on epoch=49
03/09/2022 07:39:49 - INFO - __main__ - Global step 100 Train loss 2.453169 Classification-F1 0.3333333333333333 on epoch=49
03/09/2022 07:40:26 - INFO - __main__ - Saving model with best Classification-F1: 0.19393939393939394 -> 0.3333333333333333 on epoch=49, global_step=100
03/09/2022 07:40:31 - INFO - __main__ - Step 110 Global step 110 Train loss 1.278857 on epoch=54
03/09/2022 07:40:36 - INFO - __main__ - Step 120 Global step 120 Train loss 1.393622 on epoch=59
03/09/2022 07:40:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.400301 on epoch=64
03/09/2022 07:40:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.383423 on epoch=69
03/09/2022 07:40:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.754945 on epoch=74
03/09/2022 07:40:50 - INFO - __main__ - Global step 150 Train loss 1.242230 Classification-F1 0.5134502923976608 on epoch=74
03/09/2022 07:41:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5134502923976608 on epoch=74, global_step=150
03/09/2022 07:41:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.198871 on epoch=79
03/09/2022 07:41:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.032141 on epoch=84
03/09/2022 07:41:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.013867 on epoch=89
03/09/2022 07:41:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.005097 on epoch=94
03/09/2022 07:41:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.002021 on epoch=99
03/09/2022 07:41:55 - INFO - __main__ - Global step 200 Train loss 0.050399 Classification-F1 0.9687194525904204 on epoch=99
03/09/2022 07:42:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.9687194525904204 on epoch=99, global_step=200
03/09/2022 07:42:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.054340 on epoch=104
03/09/2022 07:42:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.003751 on epoch=109
03/09/2022 07:42:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003910 on epoch=114
03/09/2022 07:42:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.007776 on epoch=119
03/09/2022 07:42:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000652 on epoch=124
03/09/2022 07:42:56 - INFO - __main__ - Global step 250 Train loss 0.014086 Classification-F1 1.0 on epoch=124
03/09/2022 07:43:34 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=124, global_step=250
03/09/2022 07:43:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000507 on epoch=129
03/09/2022 07:43:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000181 on epoch=134
03/09/2022 07:43:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.015663 on epoch=139
03/09/2022 07:43:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001107 on epoch=144
03/09/2022 07:43:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000411 on epoch=149
03/09/2022 07:43:59 - INFO - __main__ - Global step 300 Train loss 0.003574 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 07:44:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000237 on epoch=154
03/09/2022 07:44:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000126 on epoch=159
03/09/2022 07:44:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000148 on epoch=164
03/09/2022 07:44:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000059 on epoch=169
03/09/2022 07:44:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000086 on epoch=174
03/09/2022 07:44:23 - INFO - __main__ - Global step 350 Train loss 0.000131 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 07:44:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000108 on epoch=179
03/09/2022 07:44:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000354 on epoch=184
03/09/2022 07:44:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000044 on epoch=189
03/09/2022 07:44:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000163 on epoch=194
03/09/2022 07:44:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000070 on epoch=199
03/09/2022 07:44:47 - INFO - __main__ - Global step 400 Train loss 0.000148 Classification-F1 0.9372549019607843 on epoch=199
03/09/2022 07:44:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000078 on epoch=204
03/09/2022 07:44:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000104 on epoch=209
03/09/2022 07:45:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001411 on epoch=214
03/09/2022 07:45:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000144 on epoch=219
03/09/2022 07:45:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008945 on epoch=224
03/09/2022 07:45:11 - INFO - __main__ - Global step 450 Train loss 0.002136 Classification-F1 1.0 on epoch=224
03/09/2022 07:45:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.095214 on epoch=229
03/09/2022 07:45:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.117520 on epoch=234
03/09/2022 07:45:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.125420 on epoch=239
03/09/2022 07:45:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002036 on epoch=244
03/09/2022 07:45:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000646 on epoch=249
03/09/2022 07:45:35 - INFO - __main__ - Global step 500 Train loss 0.068167 Classification-F1 0.9372549019607843 on epoch=249
03/09/2022 07:45:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011183 on epoch=254
03/09/2022 07:45:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000514 on epoch=259
03/09/2022 07:45:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004135 on epoch=264
03/09/2022 07:45:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.050202 on epoch=269
03/09/2022 07:45:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001317 on epoch=274
03/09/2022 07:45:59 - INFO - __main__ - Global step 550 Train loss 0.013470 Classification-F1 1.0 on epoch=274
03/09/2022 07:46:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000332 on epoch=279
03/09/2022 07:46:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000218 on epoch=284
03/09/2022 07:46:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000123 on epoch=289
03/09/2022 07:46:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000290 on epoch=294
03/09/2022 07:46:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000118 on epoch=299
03/09/2022 07:46:23 - INFO - __main__ - Global step 600 Train loss 0.000216 Classification-F1 1.0 on epoch=299
03/09/2022 07:46:23 - INFO - __main__ - save last model!
03/09/2022 07:46:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:46:24 - INFO - __main__ - Printing 3 examples
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:46:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:46:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:46:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:46:24 - INFO - __main__ - Printing 3 examples
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:46:24 - INFO - __main__ - ['negative']
03/09/2022 07:46:24 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:46:24 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:46:24 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:46:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:46:35 - INFO - __main__ - Starting training!
03/09/2022 07:47:05 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:47:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:47:06 - INFO - __main__ - Printing 3 examples
03/09/2022 07:47:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:47:06 - INFO - __main__ - ['negative']
03/09/2022 07:47:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:47:06 - INFO - __main__ - ['negative']
03/09/2022 07:47:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:47:06 - INFO - __main__ - ['negative']
03/09/2022 07:47:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:47:06 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:47:07 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:47:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0005_8_predictions.txt
03/09/2022 07:47:21 - INFO - __main__ - Classification-F1 on test data: 0.9510
03/09/2022 07:47:22 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.950999558996031
03/09/2022 07:47:22 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0003, bsz=8 ...
03/09/2022 07:47:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:47:23 - INFO - __main__ - Printing 3 examples
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:47:23 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:47:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:47:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:47:23 - INFO - __main__ - Printing 3 examples
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:47:23 - INFO - __main__ - ['negative']
03/09/2022 07:47:23 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:47:23 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:47:23 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:47:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:47:32 - INFO - __main__ - Starting training!
03/09/2022 07:47:36 - INFO - __main__ - Step 10 Global step 10 Train loss 22.384933 on epoch=4
03/09/2022 07:47:41 - INFO - __main__ - Step 20 Global step 20 Train loss 16.365101 on epoch=9
03/09/2022 07:47:46 - INFO - __main__ - Step 30 Global step 30 Train loss 15.862419 on epoch=14
03/09/2022 07:47:50 - INFO - __main__ - Step 40 Global step 40 Train loss 14.650485 on epoch=19
03/09/2022 07:47:55 - INFO - __main__ - Step 50 Global step 50 Train loss 13.043261 on epoch=24
03/09/2022 07:48:04 - INFO - __main__ - Global step 50 Train loss 16.461241 Classification-F1 0.0 on epoch=24
03/09/2022 07:48:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:48:45 - INFO - __main__ - Step 60 Global step 60 Train loss 12.311401 on epoch=29
03/09/2022 07:48:50 - INFO - __main__ - Step 70 Global step 70 Train loss 12.148779 on epoch=34
03/09/2022 07:48:55 - INFO - __main__ - Step 80 Global step 80 Train loss 9.549019 on epoch=39
03/09/2022 07:48:59 - INFO - __main__ - Step 90 Global step 90 Train loss 4.959825 on epoch=44
03/09/2022 07:49:04 - INFO - __main__ - Step 100 Global step 100 Train loss 1.506199 on epoch=49
03/09/2022 07:49:05 - INFO - __main__ - Global step 100 Train loss 8.095045 Classification-F1 0.3333333333333333 on epoch=49
03/09/2022 07:49:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=49, global_step=100
03/09/2022 07:49:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.318810 on epoch=54
03/09/2022 07:49:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.149518 on epoch=59
03/09/2022 07:49:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.078661 on epoch=64
03/09/2022 07:49:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.027722 on epoch=69
03/09/2022 07:50:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.014712 on epoch=74
03/09/2022 07:50:03 - INFO - __main__ - Global step 150 Train loss 0.117884 Classification-F1 0.873015873015873 on epoch=74
03/09/2022 07:50:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.873015873015873 on epoch=74, global_step=150
03/09/2022 07:50:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.031640 on epoch=79
03/09/2022 07:50:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.005121 on epoch=84
03/09/2022 07:50:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.011141 on epoch=89
03/09/2022 07:50:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.009484 on epoch=94
03/09/2022 07:51:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.054101 on epoch=99
03/09/2022 07:51:04 - INFO - __main__ - Global step 200 Train loss 0.022298 Classification-F1 0.9372549019607843 on epoch=99
03/09/2022 07:51:40 - INFO - __main__ - Saving model with best Classification-F1: 0.873015873015873 -> 0.9372549019607843 on epoch=99, global_step=200
03/09/2022 07:51:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001818 on epoch=104
03/09/2022 07:51:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000835 on epoch=109
03/09/2022 07:51:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001030 on epoch=114
03/09/2022 07:52:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000991 on epoch=119
03/09/2022 07:52:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001488 on epoch=124
03/09/2022 07:52:05 - INFO - __main__ - Global step 250 Train loss 0.001233 Classification-F1 0.9372549019607843 on epoch=124
03/09/2022 07:52:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000928 on epoch=129
03/09/2022 07:52:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.003781 on epoch=134
03/09/2022 07:52:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002514 on epoch=139
03/09/2022 07:52:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000590 on epoch=144
03/09/2022 07:52:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000347 on epoch=149
03/09/2022 07:52:29 - INFO - __main__ - Global step 300 Train loss 0.001632 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 07:53:04 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=149, global_step=300
03/09/2022 07:53:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000056 on epoch=154
03/09/2022 07:53:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000185 on epoch=159
03/09/2022 07:53:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000124 on epoch=164
03/09/2022 07:53:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000100 on epoch=169
03/09/2022 07:53:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000079 on epoch=174
03/09/2022 07:53:28 - INFO - __main__ - Global step 350 Train loss 0.000109 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 07:53:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000184 on epoch=179
03/09/2022 07:53:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000111 on epoch=184
03/09/2022 07:53:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000069 on epoch=189
03/09/2022 07:53:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000037 on epoch=194
03/09/2022 07:53:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000068 on epoch=199
03/09/2022 07:53:53 - INFO - __main__ - Global step 400 Train loss 0.000094 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 07:53:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000031 on epoch=204
03/09/2022 07:54:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001879 on epoch=209
03/09/2022 07:54:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000056 on epoch=214
03/09/2022 07:54:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000232 on epoch=219
03/09/2022 07:54:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000097 on epoch=224
03/09/2022 07:54:17 - INFO - __main__ - Global step 450 Train loss 0.000459 Classification-F1 0.9372549019607843 on epoch=224
03/09/2022 07:54:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000155 on epoch=229
03/09/2022 07:54:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000046 on epoch=234
03/09/2022 07:54:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000041 on epoch=239
03/09/2022 07:54:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000054 on epoch=244
03/09/2022 07:54:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000190 on epoch=249
03/09/2022 07:54:42 - INFO - __main__ - Global step 500 Train loss 0.000097 Classification-F1 0.9375 on epoch=249
03/09/2022 07:54:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000088 on epoch=254
03/09/2022 07:54:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000211 on epoch=259
03/09/2022 07:54:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000017 on epoch=264
03/09/2022 07:55:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000019 on epoch=269
03/09/2022 07:55:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000018 on epoch=274
03/09/2022 07:55:06 - INFO - __main__ - Global step 550 Train loss 0.000071 Classification-F1 0.9372549019607843 on epoch=274
03/09/2022 07:55:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000038 on epoch=279
03/09/2022 07:55:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000068 on epoch=284
03/09/2022 07:55:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000172 on epoch=289
03/09/2022 07:55:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000010 on epoch=294
03/09/2022 07:55:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000009 on epoch=299
03/09/2022 07:55:31 - INFO - __main__ - Global step 600 Train loss 0.000059 Classification-F1 0.9372549019607843 on epoch=299
03/09/2022 07:55:31 - INFO - __main__ - save last model!
03/09/2022 07:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:55:31 - INFO - __main__ - Printing 3 examples
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 07:55:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:55:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:55:31 - INFO - __main__ - Printing 3 examples
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:55:31 - INFO - __main__ - ['negative']
03/09/2022 07:55:31 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:55:31 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:55:32 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:55:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:55:41 - INFO - __main__ - Starting training!
03/09/2022 07:56:13 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 07:56:14 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 07:56:14 - INFO - __main__ - Printing 3 examples
03/09/2022 07:56:14 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 07:56:14 - INFO - __main__ - ['negative']
03/09/2022 07:56:14 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 07:56:14 - INFO - __main__ - ['negative']
03/09/2022 07:56:14 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 07:56:14 - INFO - __main__ - ['negative']
03/09/2022 07:56:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:56:14 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:56:15 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 07:56:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0003_8_predictions.txt
03/09/2022 07:56:31 - INFO - __main__ - Classification-F1 on test data: 0.9098
03/09/2022 07:56:31 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9098409594524741
03/09/2022 07:56:31 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0002, bsz=8 ...
03/09/2022 07:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:56:32 - INFO - __main__ - Printing 3 examples
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 07:56:32 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:56:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 07:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 07:56:32 - INFO - __main__ - Printing 3 examples
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 07:56:32 - INFO - __main__ - ['negative']
03/09/2022 07:56:32 - INFO - __main__ - Tokenizing Input ...
03/09/2022 07:56:32 - INFO - __main__ - Tokenizing Output ...
03/09/2022 07:56:32 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 07:56:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 07:56:41 - INFO - __main__ - Starting training!
03/09/2022 07:56:45 - INFO - __main__ - Step 10 Global step 10 Train loss 23.860373 on epoch=4
03/09/2022 07:56:50 - INFO - __main__ - Step 20 Global step 20 Train loss 17.793711 on epoch=9
03/09/2022 07:56:55 - INFO - __main__ - Step 30 Global step 30 Train loss 16.316132 on epoch=14
03/09/2022 07:57:00 - INFO - __main__ - Step 40 Global step 40 Train loss 15.109976 on epoch=19
03/09/2022 07:57:04 - INFO - __main__ - Step 50 Global step 50 Train loss 14.917964 on epoch=24
03/09/2022 07:57:14 - INFO - __main__ - Global step 50 Train loss 17.599630 Classification-F1 0.0 on epoch=24
03/09/2022 07:57:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 07:57:55 - INFO - __main__ - Step 60 Global step 60 Train loss 13.591158 on epoch=29
03/09/2022 07:58:00 - INFO - __main__ - Step 70 Global step 70 Train loss 13.714678 on epoch=34
03/09/2022 07:58:04 - INFO - __main__ - Step 80 Global step 80 Train loss 13.165262 on epoch=39
03/09/2022 07:58:09 - INFO - __main__ - Step 90 Global step 90 Train loss 11.688146 on epoch=44
03/09/2022 07:58:14 - INFO - __main__ - Step 100 Global step 100 Train loss 10.676100 on epoch=49
03/09/2022 07:58:22 - INFO - __main__ - Global step 100 Train loss 12.567069 Classification-F1 0.02915601023017903 on epoch=49
03/09/2022 07:58:59 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.02915601023017903 on epoch=49, global_step=100
03/09/2022 07:59:04 - INFO - __main__ - Step 110 Global step 110 Train loss 8.411632 on epoch=54
03/09/2022 07:59:08 - INFO - __main__ - Step 120 Global step 120 Train loss 2.792435 on epoch=59
03/09/2022 07:59:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.403059 on epoch=64
03/09/2022 07:59:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.244948 on epoch=69
03/09/2022 07:59:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.126165 on epoch=74
03/09/2022 07:59:23 - INFO - __main__ - Global step 150 Train loss 2.395648 Classification-F1 0.906158357771261 on epoch=74
03/09/2022 08:00:00 - INFO - __main__ - Saving model with best Classification-F1: 0.02915601023017903 -> 0.906158357771261 on epoch=74, global_step=150
03/09/2022 08:00:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.129094 on epoch=79
03/09/2022 08:00:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.059671 on epoch=84
03/09/2022 08:00:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.098063 on epoch=89
03/09/2022 08:00:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.035027 on epoch=94
03/09/2022 08:00:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.014268 on epoch=99
03/09/2022 08:00:24 - INFO - __main__ - Global step 200 Train loss 0.067224 Classification-F1 0.9375 on epoch=99
03/09/2022 08:01:01 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9375 on epoch=99, global_step=200
03/09/2022 08:01:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.005691 on epoch=104
03/09/2022 08:01:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.008302 on epoch=109
03/09/2022 08:01:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.030871 on epoch=114
03/09/2022 08:01:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.005552 on epoch=119
03/09/2022 08:01:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.006916 on epoch=124
03/09/2022 08:01:25 - INFO - __main__ - Global step 250 Train loss 0.011466 Classification-F1 0.9375 on epoch=124
03/09/2022 08:01:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003948 on epoch=129
03/09/2022 08:01:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.004508 on epoch=134
03/09/2022 08:01:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.020001 on epoch=139
03/09/2022 08:01:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000661 on epoch=144
03/09/2022 08:01:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001497 on epoch=149
03/09/2022 08:01:49 - INFO - __main__ - Global step 300 Train loss 0.006123 Classification-F1 0.9375 on epoch=149
03/09/2022 08:01:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003789 on epoch=154
03/09/2022 08:01:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.019308 on epoch=159
03/09/2022 08:02:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001514 on epoch=164
03/09/2022 08:02:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001783 on epoch=169
03/09/2022 08:02:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000377 on epoch=174
03/09/2022 08:02:14 - INFO - __main__ - Global step 350 Train loss 0.005354 Classification-F1 0.9375 on epoch=174
03/09/2022 08:02:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000457 on epoch=179
03/09/2022 08:02:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000360 on epoch=184
03/09/2022 08:02:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000107 on epoch=189
03/09/2022 08:02:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000346 on epoch=194
03/09/2022 08:02:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000217 on epoch=199
03/09/2022 08:02:38 - INFO - __main__ - Global step 400 Train loss 0.000298 Classification-F1 0.9375 on epoch=199
03/09/2022 08:02:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.019902 on epoch=204
03/09/2022 08:02:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000300 on epoch=209
03/09/2022 08:02:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000169 on epoch=214
03/09/2022 08:02:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000044 on epoch=219
03/09/2022 08:03:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000055 on epoch=224
03/09/2022 08:03:02 - INFO - __main__ - Global step 450 Train loss 0.004094 Classification-F1 0.9372549019607843 on epoch=224
03/09/2022 08:03:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000101 on epoch=229
03/09/2022 08:03:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000299 on epoch=234
03/09/2022 08:03:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006415 on epoch=239
03/09/2022 08:03:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000193 on epoch=244
03/09/2022 08:03:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000154 on epoch=249
03/09/2022 08:03:26 - INFO - __main__ - Global step 500 Train loss 0.001432 Classification-F1 0.9375 on epoch=249
03/09/2022 08:03:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000623 on epoch=254
03/09/2022 08:03:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000158 on epoch=259
03/09/2022 08:03:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000029 on epoch=264
03/09/2022 08:03:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000032 on epoch=269
03/09/2022 08:03:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000040 on epoch=274
03/09/2022 08:03:50 - INFO - __main__ - Global step 550 Train loss 0.000177 Classification-F1 0.9687194525904204 on epoch=274
03/09/2022 08:04:27 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=274, global_step=550
03/09/2022 08:04:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000048 on epoch=279
03/09/2022 08:04:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000042 on epoch=284
03/09/2022 08:04:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000033 on epoch=289
03/09/2022 08:04:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000020 on epoch=294
03/09/2022 08:04:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000168 on epoch=299
03/09/2022 08:04:51 - INFO - __main__ - Global step 600 Train loss 0.000062 Classification-F1 0.9375 on epoch=299
03/09/2022 08:04:51 - INFO - __main__ - save last model!
03/09/2022 08:04:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:04:52 - INFO - __main__ - Printing 3 examples
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 08:04:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:04:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:04:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:04:52 - INFO - __main__ - Printing 3 examples
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 08:04:52 - INFO - __main__ - ['negative']
03/09/2022 08:04:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:04:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:04:52 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:05:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:05:01 - INFO - __main__ - Starting training!
03/09/2022 08:05:34 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:05:34 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:05:34 - INFO - __main__ - Printing 3 examples
03/09/2022 08:05:34 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:05:34 - INFO - __main__ - ['negative']
03/09/2022 08:05:34 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:05:34 - INFO - __main__ - ['negative']
03/09/2022 08:05:34 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:05:34 - INFO - __main__ - ['negative']
03/09/2022 08:05:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:05:35 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:05:36 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:05:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0002_8_predictions.txt
03/09/2022 08:05:50 - INFO - __main__ - Classification-F1 on test data: 0.9349
03/09/2022 08:05:51 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9348795923662854
03/09/2022 08:05:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0001, bsz=8 ...
03/09/2022 08:05:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:05:52 - INFO - __main__ - Printing 3 examples
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:05:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:05:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:05:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:05:52 - INFO - __main__ - Printing 3 examples
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/09/2022 08:05:52 - INFO - __main__ - ['negative']
03/09/2022 08:05:52 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:05:52 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:05:52 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:06:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:06:02 - INFO - __main__ - Starting training!
03/09/2022 08:06:06 - INFO - __main__ - Step 10 Global step 10 Train loss 23.636469 on epoch=4
03/09/2022 08:06:11 - INFO - __main__ - Step 20 Global step 20 Train loss 20.991814 on epoch=9
03/09/2022 08:06:16 - INFO - __main__ - Step 30 Global step 30 Train loss 17.794104 on epoch=14
03/09/2022 08:06:21 - INFO - __main__ - Step 40 Global step 40 Train loss 17.421976 on epoch=19
03/09/2022 08:06:26 - INFO - __main__ - Step 50 Global step 50 Train loss 16.222311 on epoch=24
03/09/2022 08:06:35 - INFO - __main__ - Global step 50 Train loss 19.213333 Classification-F1 0.0 on epoch=24
03/09/2022 08:07:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 08:07:17 - INFO - __main__ - Step 60 Global step 60 Train loss 16.271488 on epoch=29
03/09/2022 08:07:21 - INFO - __main__ - Step 70 Global step 70 Train loss 15.681972 on epoch=34
03/09/2022 08:07:26 - INFO - __main__ - Step 80 Global step 80 Train loss 14.611334 on epoch=39
03/09/2022 08:07:31 - INFO - __main__ - Step 90 Global step 90 Train loss 13.850306 on epoch=44
03/09/2022 08:07:36 - INFO - __main__ - Step 100 Global step 100 Train loss 14.034871 on epoch=49
03/09/2022 08:07:45 - INFO - __main__ - Global step 100 Train loss 14.889994 Classification-F1 0.0 on epoch=49
03/09/2022 08:07:50 - INFO - __main__ - Step 110 Global step 110 Train loss 13.538152 on epoch=54
03/09/2022 08:07:55 - INFO - __main__ - Step 120 Global step 120 Train loss 13.525801 on epoch=59
03/09/2022 08:07:59 - INFO - __main__ - Step 130 Global step 130 Train loss 12.821051 on epoch=64
03/09/2022 08:08:04 - INFO - __main__ - Step 140 Global step 140 Train loss 12.504519 on epoch=69
03/09/2022 08:08:09 - INFO - __main__ - Step 150 Global step 150 Train loss 12.402614 on epoch=74
03/09/2022 08:08:18 - INFO - __main__ - Global step 150 Train loss 12.958426 Classification-F1 0.0 on epoch=74
03/09/2022 08:08:23 - INFO - __main__ - Step 160 Global step 160 Train loss 11.517245 on epoch=79
03/09/2022 08:08:27 - INFO - __main__ - Step 170 Global step 170 Train loss 10.160054 on epoch=84
03/09/2022 08:08:32 - INFO - __main__ - Step 180 Global step 180 Train loss 9.957245 on epoch=89
03/09/2022 08:08:37 - INFO - __main__ - Step 190 Global step 190 Train loss 7.441612 on epoch=94
03/09/2022 08:08:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.113474 on epoch=99
03/09/2022 08:08:42 - INFO - __main__ - Global step 200 Train loss 8.437926 Classification-F1 0.539313399778516 on epoch=99
03/09/2022 08:09:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.539313399778516 on epoch=99, global_step=200
03/09/2022 08:09:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.922860 on epoch=104
03/09/2022 08:09:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.486589 on epoch=109
03/09/2022 08:09:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.302556 on epoch=114
03/09/2022 08:09:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.316880 on epoch=119
03/09/2022 08:09:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.113904 on epoch=124
03/09/2022 08:09:43 - INFO - __main__ - Global step 250 Train loss 0.428558 Classification-F1 0.9375 on epoch=124
03/09/2022 08:10:20 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.9375 on epoch=124, global_step=250
03/09/2022 08:10:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.104457 on epoch=129
03/09/2022 08:10:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.148595 on epoch=134
03/09/2022 08:10:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.072988 on epoch=139
03/09/2022 08:10:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.054976 on epoch=144
03/09/2022 08:10:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.023366 on epoch=149
03/09/2022 08:10:44 - INFO - __main__ - Global step 300 Train loss 0.080876 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 08:11:22 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=149, global_step=300
03/09/2022 08:11:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.031931 on epoch=154
03/09/2022 08:11:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.036194 on epoch=159
03/09/2022 08:11:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.036948 on epoch=164
03/09/2022 08:11:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.015061 on epoch=169
03/09/2022 08:11:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.018843 on epoch=174
03/09/2022 08:11:46 - INFO - __main__ - Global step 350 Train loss 0.027795 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 08:11:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.005130 on epoch=179
03/09/2022 08:11:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009297 on epoch=184
03/09/2022 08:12:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.084081 on epoch=189
03/09/2022 08:12:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028788 on epoch=194
03/09/2022 08:12:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007343 on epoch=199
03/09/2022 08:12:10 - INFO - __main__ - Global step 400 Train loss 0.026928 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 08:12:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001847 on epoch=204
03/09/2022 08:12:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001108 on epoch=209
03/09/2022 08:12:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.101802 on epoch=214
03/09/2022 08:12:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011905 on epoch=219
03/09/2022 08:12:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.022686 on epoch=224
03/09/2022 08:12:34 - INFO - __main__ - Global step 450 Train loss 0.027870 Classification-F1 0.9375 on epoch=224
03/09/2022 08:12:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003783 on epoch=229
03/09/2022 08:12:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.005573 on epoch=234
03/09/2022 08:12:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001114 on epoch=239
03/09/2022 08:12:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005899 on epoch=244
03/09/2022 08:12:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001337 on epoch=249
03/09/2022 08:12:58 - INFO - __main__ - Global step 500 Train loss 0.003541 Classification-F1 0.9687194525904204 on epoch=249
03/09/2022 08:13:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000330 on epoch=254
03/09/2022 08:13:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002521 on epoch=259
03/09/2022 08:13:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001333 on epoch=264
03/09/2022 08:13:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000776 on epoch=269
03/09/2022 08:13:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000176 on epoch=274
03/09/2022 08:13:22 - INFO - __main__ - Global step 550 Train loss 0.001027 Classification-F1 0.9687194525904204 on epoch=274
03/09/2022 08:13:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000427 on epoch=279
03/09/2022 08:13:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000164 on epoch=284
03/09/2022 08:13:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000126 on epoch=289
03/09/2022 08:13:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000125 on epoch=294
03/09/2022 08:13:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000098 on epoch=299
03/09/2022 08:13:46 - INFO - __main__ - Global step 600 Train loss 0.000188 Classification-F1 0.9375 on epoch=299
03/09/2022 08:13:46 - INFO - __main__ - save last model!
03/09/2022 08:13:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:13:47 - INFO - __main__ - Printing 3 examples
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 08:13:47 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:13:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:13:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:13:47 - INFO - __main__ - Printing 3 examples
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:13:47 - INFO - __main__ - ['negative']
03/09/2022 08:13:47 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:13:47 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:13:47 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:13:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:13:58 - INFO - __main__ - Starting training!
03/09/2022 08:14:28 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:14:29 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:14:29 - INFO - __main__ - Printing 3 examples
03/09/2022 08:14:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:14:29 - INFO - __main__ - ['negative']
03/09/2022 08:14:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:14:29 - INFO - __main__ - ['negative']
03/09/2022 08:14:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:14:29 - INFO - __main__ - ['negative']
03/09/2022 08:14:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:14:30 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:14:31 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:14:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_42_0.0001_8_predictions.txt
03/09/2022 08:14:45 - INFO - __main__ - Classification-F1 on test data: 0.6205
03/09/2022 08:14:45 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.6204981397493218
03/09/2022 08:14:45 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0005, bsz=8 ...
03/09/2022 08:14:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:14:46 - INFO - __main__ - Printing 3 examples
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:14:46 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:14:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:14:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:14:46 - INFO - __main__ - Printing 3 examples
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:14:46 - INFO - __main__ - ['negative']
03/09/2022 08:14:46 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:14:46 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:14:46 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:14:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:14:55 - INFO - __main__ - Starting training!
03/09/2022 08:15:00 - INFO - __main__ - Step 10 Global step 10 Train loss 22.251881 on epoch=4
03/09/2022 08:15:04 - INFO - __main__ - Step 20 Global step 20 Train loss 17.567472 on epoch=9
03/09/2022 08:15:09 - INFO - __main__ - Step 30 Global step 30 Train loss 15.551758 on epoch=14
03/09/2022 08:15:14 - INFO - __main__ - Step 40 Global step 40 Train loss 13.535991 on epoch=19
03/09/2022 08:15:18 - INFO - __main__ - Step 50 Global step 50 Train loss 11.469899 on epoch=24
03/09/2022 08:15:19 - INFO - __main__ - Global step 50 Train loss 16.075401 Classification-F1 0.037037037037037035 on epoch=24
03/09/2022 08:15:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.037037037037037035 on epoch=24, global_step=50
03/09/2022 08:16:01 - INFO - __main__ - Step 60 Global step 60 Train loss 8.815944 on epoch=29
03/09/2022 08:16:06 - INFO - __main__ - Step 70 Global step 70 Train loss 2.821101 on epoch=34
03/09/2022 08:16:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.464259 on epoch=39
03/09/2022 08:16:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.401780 on epoch=44
03/09/2022 08:16:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.352990 on epoch=49
03/09/2022 08:16:21 - INFO - __main__ - Global step 100 Train loss 2.571215 Classification-F1 0.4589371980676329 on epoch=49
03/09/2022 08:16:57 - INFO - __main__ - Saving model with best Classification-F1: 0.037037037037037035 -> 0.4589371980676329 on epoch=49, global_step=100
03/09/2022 08:17:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.321653 on epoch=54
03/09/2022 08:17:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.307997 on epoch=59
03/09/2022 08:17:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.299973 on epoch=64
03/09/2022 08:17:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.312559 on epoch=69
03/09/2022 08:17:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.229407 on epoch=74
03/09/2022 08:17:22 - INFO - __main__ - Global step 150 Train loss 0.294318 Classification-F1 0.5151515151515151 on epoch=74
03/09/2022 08:17:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5151515151515151 on epoch=74, global_step=150
03/09/2022 08:18:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.250126 on epoch=79
03/09/2022 08:18:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.214601 on epoch=84
03/09/2022 08:18:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.334930 on epoch=89
03/09/2022 08:18:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.231555 on epoch=94
03/09/2022 08:18:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.222284 on epoch=99
03/09/2022 08:18:24 - INFO - __main__ - Global step 200 Train loss 0.250699 Classification-F1 0.6190476190476191 on epoch=99
03/09/2022 08:19:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.6190476190476191 on epoch=99, global_step=200
03/09/2022 08:19:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.190467 on epoch=104
03/09/2022 08:19:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.110164 on epoch=109
03/09/2022 08:19:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.182844 on epoch=114
03/09/2022 08:19:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.075221 on epoch=119
03/09/2022 08:19:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.033980 on epoch=124
03/09/2022 08:19:25 - INFO - __main__ - Global step 250 Train loss 0.118535 Classification-F1 0.6532019704433498 on epoch=124
03/09/2022 08:20:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6532019704433498 on epoch=124, global_step=250
03/09/2022 08:20:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.090568 on epoch=129
03/09/2022 08:20:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.032693 on epoch=134
03/09/2022 08:20:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.060853 on epoch=139
03/09/2022 08:20:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.033612 on epoch=144
03/09/2022 08:20:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.018579 on epoch=149
03/09/2022 08:20:27 - INFO - __main__ - Global step 300 Train loss 0.047261 Classification-F1 0.6825396825396826 on epoch=149
03/09/2022 08:21:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6532019704433498 -> 0.6825396825396826 on epoch=149, global_step=300
03/09/2022 08:21:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.019702 on epoch=154
03/09/2022 08:21:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.032616 on epoch=159
03/09/2022 08:21:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.015770 on epoch=164
03/09/2022 08:21:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.078444 on epoch=169
03/09/2022 08:21:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.637926 on epoch=174
03/09/2022 08:21:30 - INFO - __main__ - Global step 350 Train loss 0.156892 Classification-F1 0.49090909090909085 on epoch=174
03/09/2022 08:21:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.428656 on epoch=179
03/09/2022 08:21:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.373516 on epoch=184
03/09/2022 08:21:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.304782 on epoch=189
03/09/2022 08:21:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.287166 on epoch=194
03/09/2022 08:21:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.231985 on epoch=199
03/09/2022 08:21:54 - INFO - __main__ - Global step 400 Train loss 0.325221 Classification-F1 0.7793103448275862 on epoch=199
03/09/2022 08:22:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6825396825396826 -> 0.7793103448275862 on epoch=199, global_step=400
03/09/2022 08:22:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.239375 on epoch=204
03/09/2022 08:22:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.189758 on epoch=209
03/09/2022 08:22:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.194997 on epoch=214
03/09/2022 08:22:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.209693 on epoch=219
03/09/2022 08:22:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.209705 on epoch=224
03/09/2022 08:22:54 - INFO - __main__ - Global step 450 Train loss 0.208706 Classification-F1 0.7117117117117117 on epoch=224
03/09/2022 08:22:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.163781 on epoch=229
03/09/2022 08:23:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.136325 on epoch=234
03/09/2022 08:23:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.220499 on epoch=239
03/09/2022 08:23:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.148210 on epoch=244
03/09/2022 08:23:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.106060 on epoch=249
03/09/2022 08:23:18 - INFO - __main__ - Global step 500 Train loss 0.154975 Classification-F1 0.7810361681329424 on epoch=249
03/09/2022 08:23:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7793103448275862 -> 0.7810361681329424 on epoch=249, global_step=500
03/09/2022 08:23:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.056073 on epoch=254
03/09/2022 08:24:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.065716 on epoch=259
03/09/2022 08:24:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.068170 on epoch=264
03/09/2022 08:24:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.162447 on epoch=269
03/09/2022 08:24:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.071408 on epoch=274
03/09/2022 08:24:18 - INFO - __main__ - Global step 550 Train loss 0.084763 Classification-F1 0.8095238095238095 on epoch=274
03/09/2022 08:24:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7810361681329424 -> 0.8095238095238095 on epoch=274, global_step=550
03/09/2022 08:25:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.021615 on epoch=279
03/09/2022 08:25:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.178152 on epoch=284
03/09/2022 08:25:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.039852 on epoch=289
03/09/2022 08:25:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.083398 on epoch=294
03/09/2022 08:25:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037946 on epoch=299
03/09/2022 08:25:19 - INFO - __main__ - Global step 600 Train loss 0.072193 Classification-F1 0.6389743589743591 on epoch=299
03/09/2022 08:25:19 - INFO - __main__ - save last model!
03/09/2022 08:25:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:25:21 - INFO - __main__ - Printing 3 examples
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 08:25:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:25:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:25:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:25:21 - INFO - __main__ - Printing 3 examples
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:25:21 - INFO - __main__ - ['negative']
03/09/2022 08:25:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:25:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:25:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:25:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:25:32 - INFO - __main__ - Starting training!
03/09/2022 08:26:02 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:26:03 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:26:03 - INFO - __main__ - Printing 3 examples
03/09/2022 08:26:03 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:26:03 - INFO - __main__ - ['negative']
03/09/2022 08:26:03 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:26:03 - INFO - __main__ - ['negative']
03/09/2022 08:26:03 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:26:03 - INFO - __main__ - ['negative']
03/09/2022 08:26:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:26:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:26:05 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:26:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0005_8_predictions.txt
03/09/2022 08:26:19 - INFO - __main__ - Classification-F1 on test data: 0.6672
03/09/2022 08:26:20 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.8095238095238095, test_performance=0.6672077922077921
03/09/2022 08:26:20 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0003, bsz=8 ...
03/09/2022 08:26:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:26:20 - INFO - __main__ - Printing 3 examples
03/09/2022 08:26:20 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:26:20 - INFO - __main__ - ['negative']
03/09/2022 08:26:20 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:26:20 - INFO - __main__ - ['negative']
03/09/2022 08:26:20 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:26:20 - INFO - __main__ - ['negative']
03/09/2022 08:26:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:26:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:26:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:26:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:26:21 - INFO - __main__ - Printing 3 examples
03/09/2022 08:26:21 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:26:21 - INFO - __main__ - ['negative']
03/09/2022 08:26:21 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:26:21 - INFO - __main__ - ['negative']
03/09/2022 08:26:21 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:26:21 - INFO - __main__ - ['negative']
03/09/2022 08:26:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:26:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:26:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:26:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:26:31 - INFO - __main__ - Starting training!
03/09/2022 08:26:35 - INFO - __main__ - Step 10 Global step 10 Train loss 21.368879 on epoch=4
03/09/2022 08:26:40 - INFO - __main__ - Step 20 Global step 20 Train loss 16.734297 on epoch=9
03/09/2022 08:26:45 - INFO - __main__ - Step 30 Global step 30 Train loss 15.040625 on epoch=14
03/09/2022 08:26:50 - INFO - __main__ - Step 40 Global step 40 Train loss 14.343817 on epoch=19
03/09/2022 08:26:54 - INFO - __main__ - Step 50 Global step 50 Train loss 13.159329 on epoch=24
03/09/2022 08:26:57 - INFO - __main__ - Global step 50 Train loss 16.129389 Classification-F1 0.0 on epoch=24
03/09/2022 08:27:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 08:27:40 - INFO - __main__ - Step 60 Global step 60 Train loss 11.777392 on epoch=29
03/09/2022 08:27:44 - INFO - __main__ - Step 70 Global step 70 Train loss 6.640307 on epoch=34
03/09/2022 08:27:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.903592 on epoch=39
03/09/2022 08:27:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.408728 on epoch=44
03/09/2022 08:27:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.427914 on epoch=49
03/09/2022 08:27:58 - INFO - __main__ - Global step 100 Train loss 4.231587 Classification-F1 0.3992490613266583 on epoch=49
03/09/2022 08:28:37 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3992490613266583 on epoch=49, global_step=100
03/09/2022 08:28:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.347317 on epoch=54
03/09/2022 08:28:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.375125 on epoch=59
03/09/2022 08:28:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.205902 on epoch=64
03/09/2022 08:28:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.175323 on epoch=69
03/09/2022 08:29:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.060962 on epoch=74
03/09/2022 08:29:02 - INFO - __main__ - Global step 150 Train loss 0.232926 Classification-F1 0.9687194525904204 on epoch=74
03/09/2022 08:29:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.9687194525904204 on epoch=74, global_step=150
03/09/2022 08:29:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.165163 on epoch=79
03/09/2022 08:29:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.026812 on epoch=84
03/09/2022 08:29:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.007364 on epoch=89
03/09/2022 08:29:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.003591 on epoch=94
03/09/2022 08:30:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000737 on epoch=99
03/09/2022 08:30:04 - INFO - __main__ - Global step 200 Train loss 0.040733 Classification-F1 0.9687194525904204 on epoch=99
03/09/2022 08:30:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.016254 on epoch=104
03/09/2022 08:30:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000882 on epoch=109
03/09/2022 08:30:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000475 on epoch=114
03/09/2022 08:30:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000983 on epoch=119
03/09/2022 08:30:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000171 on epoch=124
03/09/2022 08:30:28 - INFO - __main__ - Global step 250 Train loss 0.003753 Classification-F1 0.9687194525904204 on epoch=124
03/09/2022 08:30:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001272 on epoch=129
03/09/2022 08:30:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000229 on epoch=134
03/09/2022 08:30:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000189 on epoch=139
03/09/2022 08:30:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.091454 on epoch=144
03/09/2022 08:30:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.020303 on epoch=149
03/09/2022 08:30:53 - INFO - __main__ - Global step 300 Train loss 0.022689 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 08:30:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.166255 on epoch=154
03/09/2022 08:31:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003675 on epoch=159
03/09/2022 08:31:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.002011 on epoch=164
03/09/2022 08:31:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001172 on epoch=169
03/09/2022 08:31:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001479 on epoch=174
03/09/2022 08:31:17 - INFO - __main__ - Global step 350 Train loss 0.034918 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 08:31:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000633 on epoch=179
03/09/2022 08:31:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000361 on epoch=184
03/09/2022 08:31:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000231 on epoch=189
03/09/2022 08:31:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000111 on epoch=194
03/09/2022 08:31:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000161 on epoch=199
03/09/2022 08:31:42 - INFO - __main__ - Global step 400 Train loss 0.000300 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 08:31:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000114 on epoch=204
03/09/2022 08:31:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000081 on epoch=209
03/09/2022 08:31:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000074 on epoch=214
03/09/2022 08:32:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000084 on epoch=219
03/09/2022 08:32:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013712 on epoch=224
03/09/2022 08:32:06 - INFO - __main__ - Global step 450 Train loss 0.002813 Classification-F1 0.9372549019607843 on epoch=224
03/09/2022 08:32:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000869 on epoch=229
03/09/2022 08:32:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000177 on epoch=234
03/09/2022 08:32:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000102 on epoch=239
03/09/2022 08:32:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000325 on epoch=244
03/09/2022 08:32:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000743 on epoch=249
03/09/2022 08:32:30 - INFO - __main__ - Global step 500 Train loss 0.000443 Classification-F1 0.9372549019607843 on epoch=249
03/09/2022 08:32:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000361 on epoch=254
03/09/2022 08:32:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000151 on epoch=259
03/09/2022 08:32:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000459 on epoch=264
03/09/2022 08:32:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000279 on epoch=269
03/09/2022 08:32:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000050 on epoch=274
03/09/2022 08:32:54 - INFO - __main__ - Global step 550 Train loss 0.000260 Classification-F1 0.9372549019607843 on epoch=274
03/09/2022 08:32:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000065 on epoch=279
03/09/2022 08:33:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000132 on epoch=284
03/09/2022 08:33:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000401 on epoch=289
03/09/2022 08:33:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000101 on epoch=294
03/09/2022 08:33:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000013 on epoch=299
03/09/2022 08:33:19 - INFO - __main__ - Global step 600 Train loss 0.000143 Classification-F1 0.9372549019607843 on epoch=299
03/09/2022 08:33:19 - INFO - __main__ - save last model!
03/09/2022 08:33:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:33:20 - INFO - __main__ - Printing 3 examples
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 08:33:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:33:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:33:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:33:20 - INFO - __main__ - Printing 3 examples
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:33:20 - INFO - __main__ - ['negative']
03/09/2022 08:33:20 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:33:20 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:33:20 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:33:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:33:30 - INFO - __main__ - Starting training!
03/09/2022 08:34:02 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:34:03 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:34:03 - INFO - __main__ - Printing 3 examples
03/09/2022 08:34:03 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:34:03 - INFO - __main__ - ['negative']
03/09/2022 08:34:03 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:34:03 - INFO - __main__ - ['negative']
03/09/2022 08:34:03 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:34:03 - INFO - __main__ - ['negative']
03/09/2022 08:34:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:34:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:34:05 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:34:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0003_8_predictions.txt
03/09/2022 08:34:19 - INFO - __main__ - Classification-F1 on test data: 0.6270
03/09/2022 08:34:20 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.6269654371420562
03/09/2022 08:34:20 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0002, bsz=8 ...
03/09/2022 08:34:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:34:21 - INFO - __main__ - Printing 3 examples
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:34:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:34:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:34:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:34:21 - INFO - __main__ - Printing 3 examples
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:34:21 - INFO - __main__ - ['negative']
03/09/2022 08:34:21 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:34:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:34:21 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:34:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:34:31 - INFO - __main__ - Starting training!
03/09/2022 08:34:35 - INFO - __main__ - Step 10 Global step 10 Train loss 22.688114 on epoch=4
03/09/2022 08:34:40 - INFO - __main__ - Step 20 Global step 20 Train loss 18.796892 on epoch=9
03/09/2022 08:34:45 - INFO - __main__ - Step 30 Global step 30 Train loss 16.338926 on epoch=14
03/09/2022 08:34:49 - INFO - __main__ - Step 40 Global step 40 Train loss 15.414701 on epoch=19
03/09/2022 08:34:54 - INFO - __main__ - Step 50 Global step 50 Train loss 15.290901 on epoch=24
03/09/2022 08:35:03 - INFO - __main__ - Global step 50 Train loss 17.705908 Classification-F1 0.0 on epoch=24
03/09/2022 08:35:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 08:35:43 - INFO - __main__ - Step 60 Global step 60 Train loss 14.858658 on epoch=29
03/09/2022 08:35:47 - INFO - __main__ - Step 70 Global step 70 Train loss 14.334132 on epoch=34
03/09/2022 08:35:52 - INFO - __main__ - Step 80 Global step 80 Train loss 13.247999 on epoch=39
03/09/2022 08:35:57 - INFO - __main__ - Step 90 Global step 90 Train loss 12.469442 on epoch=44
03/09/2022 08:36:02 - INFO - __main__ - Step 100 Global step 100 Train loss 11.311871 on epoch=49
03/09/2022 08:36:08 - INFO - __main__ - Global step 100 Train loss 13.244420 Classification-F1 0.004901960784313725 on epoch=49
03/09/2022 08:36:43 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.004901960784313725 on epoch=49, global_step=100
03/09/2022 08:36:48 - INFO - __main__ - Step 110 Global step 110 Train loss 11.051880 on epoch=54
03/09/2022 08:36:53 - INFO - __main__ - Step 120 Global step 120 Train loss 9.712967 on epoch=59
03/09/2022 08:36:57 - INFO - __main__ - Step 130 Global step 130 Train loss 6.024409 on epoch=64
03/09/2022 08:37:02 - INFO - __main__ - Step 140 Global step 140 Train loss 4.397892 on epoch=69
03/09/2022 08:37:07 - INFO - __main__ - Step 150 Global step 150 Train loss 2.989900 on epoch=74
03/09/2022 08:37:08 - INFO - __main__ - Global step 150 Train loss 6.835410 Classification-F1 0.3333333333333333 on epoch=74
03/09/2022 08:37:44 - INFO - __main__ - Saving model with best Classification-F1: 0.004901960784313725 -> 0.3333333333333333 on epoch=74, global_step=150
03/09/2022 08:37:49 - INFO - __main__ - Step 160 Global step 160 Train loss 1.679665 on epoch=79
03/09/2022 08:37:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.840544 on epoch=84
03/09/2022 08:37:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.319929 on epoch=89
03/09/2022 08:38:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.222887 on epoch=94
03/09/2022 08:38:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.167890 on epoch=99
03/09/2022 08:38:09 - INFO - __main__ - Global step 200 Train loss 0.646183 Classification-F1 0.906158357771261 on epoch=99
03/09/2022 08:38:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.906158357771261 on epoch=99, global_step=200
03/09/2022 08:38:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.056543 on epoch=104
03/09/2022 08:38:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.068476 on epoch=109
03/09/2022 08:39:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.012718 on epoch=114
03/09/2022 08:39:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.022298 on epoch=119
03/09/2022 08:39:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.038339 on epoch=124
03/09/2022 08:39:11 - INFO - __main__ - Global step 250 Train loss 0.039675 Classification-F1 0.9687194525904204 on epoch=124
03/09/2022 08:39:48 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9687194525904204 on epoch=124, global_step=250
03/09/2022 08:39:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.008449 on epoch=129
03/09/2022 08:39:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.028437 on epoch=134
03/09/2022 08:40:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.038380 on epoch=139
03/09/2022 08:40:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.090657 on epoch=144
03/09/2022 08:40:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.014863 on epoch=149
03/09/2022 08:40:12 - INFO - __main__ - Global step 300 Train loss 0.036157 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 08:40:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.016899 on epoch=154
03/09/2022 08:40:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001072 on epoch=159
03/09/2022 08:40:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.037947 on epoch=164
03/09/2022 08:40:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.024001 on epoch=169
03/09/2022 08:40:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.088300 on epoch=174
03/09/2022 08:40:37 - INFO - __main__ - Global step 350 Train loss 0.033644 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 08:40:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006278 on epoch=179
03/09/2022 08:40:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.007735 on epoch=184
03/09/2022 08:40:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.005009 on epoch=189
03/09/2022 08:40:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025292 on epoch=194
03/09/2022 08:41:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.032200 on epoch=199
03/09/2022 08:41:01 - INFO - __main__ - Global step 400 Train loss 0.015303 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 08:41:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.031764 on epoch=204
03/09/2022 08:41:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.043734 on epoch=209
03/09/2022 08:41:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.035867 on epoch=214
03/09/2022 08:41:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008236 on epoch=219
03/09/2022 08:41:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.023809 on epoch=224
03/09/2022 08:41:26 - INFO - __main__ - Global step 450 Train loss 0.028682 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 08:41:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.010348 on epoch=229
03/09/2022 08:41:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.008600 on epoch=234
03/09/2022 08:41:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.043051 on epoch=239
03/09/2022 08:41:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.034173 on epoch=244
03/09/2022 08:41:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013083 on epoch=249
03/09/2022 08:41:51 - INFO - __main__ - Global step 500 Train loss 0.021851 Classification-F1 0.8745098039215686 on epoch=249
03/09/2022 08:41:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.030559 on epoch=254
03/09/2022 08:42:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.008252 on epoch=259
03/09/2022 08:42:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.022274 on epoch=264
03/09/2022 08:42:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003891 on epoch=269
03/09/2022 08:42:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.012610 on epoch=274
03/09/2022 08:42:15 - INFO - __main__ - Global step 550 Train loss 0.015517 Classification-F1 0.9372549019607843 on epoch=274
03/09/2022 08:42:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007819 on epoch=279
03/09/2022 08:42:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015584 on epoch=284
03/09/2022 08:42:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.025462 on epoch=289
03/09/2022 08:42:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006122 on epoch=294
03/09/2022 08:42:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.019564 on epoch=299
03/09/2022 08:42:40 - INFO - __main__ - Global step 600 Train loss 0.014910 Classification-F1 0.9687194525904204 on epoch=299
03/09/2022 08:42:40 - INFO - __main__ - save last model!
03/09/2022 08:42:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:42:40 - INFO - __main__ - Printing 3 examples
03/09/2022 08:42:40 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:42:40 - INFO - __main__ - ['negative']
03/09/2022 08:42:40 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:42:40 - INFO - __main__ - ['negative']
03/09/2022 08:42:40 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:42:40 - INFO - __main__ - ['negative']
03/09/2022 08:42:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/09/2022 08:42:40 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:42:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:42:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:42:41 - INFO - __main__ - Printing 3 examples
03/09/2022 08:42:41 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:42:41 - INFO - __main__ - ['negative']
03/09/2022 08:42:41 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:42:41 - INFO - __main__ - ['negative']
03/09/2022 08:42:41 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:42:41 - INFO - __main__ - ['negative']
03/09/2022 08:42:41 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:42:41 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:42:41 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:42:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:42:51 - INFO - __main__ - Starting training!
03/09/2022 08:43:20 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:43:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:43:20 - INFO - __main__ - Printing 3 examples
03/09/2022 08:43:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:43:20 - INFO - __main__ - ['negative']
03/09/2022 08:43:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:43:20 - INFO - __main__ - ['negative']
03/09/2022 08:43:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:43:20 - INFO - __main__ - ['negative']
03/09/2022 08:43:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:43:21 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:43:22 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:43:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0002_8_predictions.txt
03/09/2022 08:43:36 - INFO - __main__ - Classification-F1 on test data: 0.8934
03/09/2022 08:43:36 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.89344763252702
03/09/2022 08:43:36 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0001, bsz=8 ...
03/09/2022 08:43:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:43:37 - INFO - __main__ - Printing 3 examples
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:43:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:43:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:43:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:43:37 - INFO - __main__ - Printing 3 examples
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/09/2022 08:43:37 - INFO - __main__ - ['negative']
03/09/2022 08:43:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:43:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:43:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:43:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:43:46 - INFO - __main__ - Starting training!
03/09/2022 08:43:52 - INFO - __main__ - Step 10 Global step 10 Train loss 22.171103 on epoch=4
03/09/2022 08:43:56 - INFO - __main__ - Step 20 Global step 20 Train loss 18.792810 on epoch=9
03/09/2022 08:44:01 - INFO - __main__ - Step 30 Global step 30 Train loss 19.388229 on epoch=14
03/09/2022 08:44:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.818981 on epoch=19
03/09/2022 08:44:11 - INFO - __main__ - Step 50 Global step 50 Train loss 16.639771 on epoch=24
03/09/2022 08:44:18 - INFO - __main__ - Global step 50 Train loss 18.962179 Classification-F1 0.0 on epoch=24
03/09/2022 08:44:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 08:44:59 - INFO - __main__ - Step 60 Global step 60 Train loss 16.090057 on epoch=29
03/09/2022 08:45:03 - INFO - __main__ - Step 70 Global step 70 Train loss 15.288417 on epoch=34
03/09/2022 08:45:08 - INFO - __main__ - Step 80 Global step 80 Train loss 15.155014 on epoch=39
03/09/2022 08:45:13 - INFO - __main__ - Step 90 Global step 90 Train loss 15.282814 on epoch=44
03/09/2022 08:45:18 - INFO - __main__ - Step 100 Global step 100 Train loss 13.854181 on epoch=49
03/09/2022 08:45:23 - INFO - __main__ - Global step 100 Train loss 15.134097 Classification-F1 0.0 on epoch=49
03/09/2022 08:45:28 - INFO - __main__ - Step 110 Global step 110 Train loss 13.961119 on epoch=54
03/09/2022 08:45:33 - INFO - __main__ - Step 120 Global step 120 Train loss 13.306704 on epoch=59
03/09/2022 08:45:38 - INFO - __main__ - Step 130 Global step 130 Train loss 13.020518 on epoch=64
03/09/2022 08:45:42 - INFO - __main__ - Step 140 Global step 140 Train loss 12.639707 on epoch=69
03/09/2022 08:45:47 - INFO - __main__ - Step 150 Global step 150 Train loss 12.619484 on epoch=74
03/09/2022 08:45:51 - INFO - __main__ - Global step 150 Train loss 13.109506 Classification-F1 0.0 on epoch=74
03/09/2022 08:45:56 - INFO - __main__ - Step 160 Global step 160 Train loss 11.827681 on epoch=79
03/09/2022 08:46:01 - INFO - __main__ - Step 170 Global step 170 Train loss 11.969781 on epoch=84
03/09/2022 08:46:05 - INFO - __main__ - Step 180 Global step 180 Train loss 11.126127 on epoch=89
03/09/2022 08:46:10 - INFO - __main__ - Step 190 Global step 190 Train loss 9.877627 on epoch=94
03/09/2022 08:46:15 - INFO - __main__ - Step 200 Global step 200 Train loss 9.762724 on epoch=99
03/09/2022 08:46:19 - INFO - __main__ - Global step 200 Train loss 10.912788 Classification-F1 0.0056022408963585435 on epoch=99
03/09/2022 08:46:55 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0056022408963585435 on epoch=99, global_step=200
03/09/2022 08:46:59 - INFO - __main__ - Step 210 Global step 210 Train loss 8.789755 on epoch=104
03/09/2022 08:47:04 - INFO - __main__ - Step 220 Global step 220 Train loss 7.069996 on epoch=109
03/09/2022 08:47:09 - INFO - __main__ - Step 230 Global step 230 Train loss 2.689274 on epoch=114
03/09/2022 08:47:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.869983 on epoch=119
03/09/2022 08:47:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.502263 on epoch=124
03/09/2022 08:47:19 - INFO - __main__ - Global step 250 Train loss 3.984254 Classification-F1 0.9372549019607843 on epoch=124
03/09/2022 08:47:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0056022408963585435 -> 0.9372549019607843 on epoch=124, global_step=250
03/09/2022 08:47:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.248116 on epoch=129
03/09/2022 08:48:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.148988 on epoch=134
03/09/2022 08:48:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.144560 on epoch=139
03/09/2022 08:48:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.059206 on epoch=144
03/09/2022 08:48:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.176871 on epoch=149
03/09/2022 08:48:18 - INFO - __main__ - Global step 300 Train loss 0.155548 Classification-F1 0.9687194525904204 on epoch=149
03/09/2022 08:48:55 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=149, global_step=300
03/09/2022 08:48:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.042185 on epoch=154
03/09/2022 08:49:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040908 on epoch=159
03/09/2022 08:49:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.080848 on epoch=164
03/09/2022 08:49:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012471 on epoch=169
03/09/2022 08:49:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.011343 on epoch=174
03/09/2022 08:49:19 - INFO - __main__ - Global step 350 Train loss 0.037551 Classification-F1 0.9687194525904204 on epoch=174
03/09/2022 08:49:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.014025 on epoch=179
03/09/2022 08:49:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.009443 on epoch=184
03/09/2022 08:49:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.056932 on epoch=189
03/09/2022 08:49:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.022352 on epoch=194
03/09/2022 08:49:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009719 on epoch=199
03/09/2022 08:49:43 - INFO - __main__ - Global step 400 Train loss 0.022494 Classification-F1 0.9687194525904204 on epoch=199
03/09/2022 08:49:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.004688 on epoch=204
03/09/2022 08:49:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.077008 on epoch=209
03/09/2022 08:49:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011383 on epoch=214
03/09/2022 08:50:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.010205 on epoch=219
03/09/2022 08:50:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.219684 on epoch=224
03/09/2022 08:50:08 - INFO - __main__ - Global step 450 Train loss 0.064593 Classification-F1 0.9687194525904204 on epoch=224
03/09/2022 08:50:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.077457 on epoch=229
03/09/2022 08:50:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026050 on epoch=234
03/09/2022 08:50:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003592 on epoch=239
03/09/2022 08:50:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.082107 on epoch=244
03/09/2022 08:50:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005227 on epoch=249
03/09/2022 08:50:32 - INFO - __main__ - Global step 500 Train loss 0.038886 Classification-F1 0.9687194525904204 on epoch=249
03/09/2022 08:50:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001551 on epoch=254
03/09/2022 08:50:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.014485 on epoch=259
03/09/2022 08:50:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000494 on epoch=264
03/09/2022 08:50:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018314 on epoch=269
03/09/2022 08:50:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001013 on epoch=274
03/09/2022 08:50:56 - INFO - __main__ - Global step 550 Train loss 0.007172 Classification-F1 0.9687194525904204 on epoch=274
03/09/2022 08:51:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000756 on epoch=279
03/09/2022 08:51:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001092 on epoch=284
03/09/2022 08:51:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.059038 on epoch=289
03/09/2022 08:51:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001986 on epoch=294
03/09/2022 08:51:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005936 on epoch=299
03/09/2022 08:51:21 - INFO - __main__ - Global step 600 Train loss 0.013762 Classification-F1 0.9687194525904204 on epoch=299
03/09/2022 08:51:21 - INFO - __main__ - save last model!
03/09/2022 08:52:03 - INFO - __main__ - Loading checkpoint on the fly
03/09/2022 08:52:03 - INFO - __main__ - Start tokenizing ... 1000 instances
03/09/2022 08:52:03 - INFO - __main__ - Printing 3 examples
03/09/2022 08:52:03 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/09/2022 08:52:03 - INFO - __main__ - ['negative']
03/09/2022 08:52:03 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/09/2022 08:52:03 - INFO - __main__ - ['negative']
03/09/2022 08:52:03 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/09/2022 08:52:03 - INFO - __main__ - ['negative']
03/09/2022 08:52:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:52:04 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:52:05 - INFO - __main__ - Loaded 1000 examples from test data
03/09/2022 08:52:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-amazon_polarity/amazon_polarity_16_87_0.0001_8_predictions.txt
03/09/2022 08:52:19 - INFO - __main__ - Classification-F1 on test data: 0.9259
03/09/2022 08:52:20 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9258564581028872
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003762245178222656 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13638", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10716, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "13639", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10716, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10716, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (13726): No such process
Task: blimp-sentential_negation_npi_licensor_present, Checkpoint: None, Identifier: T5-large-ft-random
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_singletask_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24566
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_0o4b6pga/none_ejo459j1
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24566
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_0o4b6pga/none_ejo459j1/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_0o4b6pga/none_ejo459j1/attempt_0/1/error.json
03/09/2022 08:52:30 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 08:52:30 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present
Output directory () already exists and is not empty.
03/09/2022 08:52:30 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/09/2022 08:52:30 - INFO - __main__ - models/T5-large-ft-random/singletask-blimp-sentential_negation_npi_licensor_present
03/09/2022 08:52:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/09/2022 08:52:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/09/2022 08:52:31 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/09/2022 08:52:31 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/09/2022 08:52:31 - INFO - __main__ - args.device: cuda:0
03/09/2022 08:52:31 - INFO - __main__ - Using 2 gpus
03/09/2022 08:52:31 - INFO - __main__ - args.device: cuda:1
03/09/2022 08:52:31 - INFO - __main__ - Using 2 gpus
03/09/2022 08:52:31 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
03/09/2022 08:52:31 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/09/2022 08:52:36 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.0005, bsz=8 ...
03/09/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:52:37 - INFO - __main__ - Printing 3 examples
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:52:37 - INFO - __main__ - Printing 3 examples
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:52:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/09/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:52:37 - INFO - __main__ - Loaded 32 examples from train data
03/09/2022 08:52:37 - INFO - __main__ - Printing 3 examples
use DistributedSampler
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/09/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/09/2022 08:52:37 - INFO - __main__ - Printing 3 examples
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:52:37 - INFO - __main__ - ['sentence 1']
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
03/09/2022 08:52:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:52:37 - INFO - __main__ - Loaded 32 examples from dev data
03/09/2022 08:52:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:52:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/09/2022 08:52:50 - INFO - __main__ - Starting training!
03/09/2022 08:52:50 - INFO - __main__ - Starting training!
03/09/2022 08:52:55 - INFO - __main__ - Step 10 Global step 10 Train loss 22.138430 on epoch=4
03/09/2022 08:52:59 - INFO - __main__ - Step 20 Global step 20 Train loss 22.915096 on epoch=9
03/09/2022 08:53:04 - INFO - __main__ - Step 30 Global step 30 Train loss 15.590918 on epoch=14
03/09/2022 08:53:09 - INFO - __main__ - Step 40 Global step 40 Train loss 12.431821 on epoch=19
03/09/2022 08:53:13 - INFO - __main__ - Step 50 Global step 50 Train loss 11.689927 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/09/2022 08:53:14 - INFO - __main__ - Global step 50 Train loss 16.953238 ACC 0.0 on epoch=24
03/09/2022 08:53:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
03/09/2022 08:53:32 - INFO - __main__ - Step 60 Global step 60 Train loss 10.127098 on epoch=29
03/09/2022 08:53:37 - INFO - __main__ - Step 70 Global step 70 Train loss 8.544050 on epoch=34
03/09/2022 08:53:42 - INFO - __main__ - Step 80 Global step 80 Train loss 5.240985 on epoch=39
03/09/2022 08:53:46 - INFO - __main__ - Step 90 Global step 90 Train loss 2.728260 on epoch=44
03/09/2022 08:53:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.769987 on epoch=49
03/09/2022 08:53:51 - INFO - __main__ - Global step 100 Train loss 5.482077 ACC 0.65625 on epoch=49
